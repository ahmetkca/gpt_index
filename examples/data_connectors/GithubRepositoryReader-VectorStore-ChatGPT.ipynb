{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in /home/ahmetk/.pyenv/versions/gpt_index-github-reader/lib/python3.11/site-packages (1.5.6)\n"
     ]
    }
   ],
   "source": [
    "# This is due to the fact that we use asyncio.loop_until_complete in\n",
    "# the GithubRepositoryReader. Since the Jupyter kernel itself runs on\n",
    "# an event loop, we need to add some help with nesting\n",
    "!pip install nest_asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "from gpt_index import (\n",
    "    GPTSimpleVectorIndex,\n",
    "    GPTQdrantIndex,\n",
    "    GPTTreeIndex,\n",
    "    GPTFaissIndex,\n",
    "    GPTWeaviateIndex,\n",
    "    GPTListIndex,\n",
    "    GPTSimpleKeywordTableIndex,\n",
    "    GPTKeywordTableIndex,\n",
    "    GPTPineconeIndex,\n",
    "    GPTRAKEKeywordTableIndex,\n",
    "    GPTSQLStructStoreIndex,\n",
    "    GithubRepositoryReader,\n",
    ")\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env GITHUB_TOKEN=github_pat_xxxxxxxxxxxxxxxxxxxxxx_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "import os\n",
    "github_token = os.environ.get(\"GITHUB_TOKEN\")\n",
    "owner = \"jerryjliu\"\n",
    "repo = \"gpt_index\"\n",
    "branch = \"main\"\n",
    "reader = GithubRepositoryReader(\n",
    "    github_token=github_token,\n",
    "    owner=owner,\n",
    "    repo=repo,\n",
    "    use_parser=True,\n",
    "    verbose=True,\n",
    "    ignore_directories=[\"examples\", \"docs\", \".vscode\"],\n",
    "    ignore_file_extensions=[\n",
    "        \".png\",\n",
    "        \".jpg\",\n",
    "        \".jpeg\",\n",
    "        \".gif\",\n",
    "        \".svg\",\n",
    "        \".ico\",\n",
    "        \".json\",\n",
    "        \".csv\",\n",
    "    ],\n",
    "    concurrent_requests=5,\n",
    ")\n",
    "documents = reader.load_data(branch=branch)\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing .flake8... generating tree index... Processing .github/workflows/build_package.yml... generating tree index... Processing .github/workflows/lint.yml... generating tree index... Processing .github/workflows/unit_test.yml... generating tree index... Processing .gitignore... generating tree index... Processing .readthedocs.yaml... generating tree index... Processing CITATION.cff... generating tree index... Processing CONTRIBUTING.md... generating tree index... Processing LICENSE... generating tree index... Processing MANIFEST.in... generating tree index... Processing Makefile... generating tree index... Processing README.md... generating tree index... Processing data_requirements.txt... generating tree index... Processing experimental/README.md... generating tree index... Processing experimental/classifier/TitanicModel.ipynb... generating tree index... Processing experimental/classifier/utils.py... generating tree index... Processing gpt_index/VERSION... generating tree index... Processing gpt_index/__init__.py... generating tree index... Processing gpt_index/composability/__init__.py... generating tree index... Processing gpt_index/composability/graph.py... generating tree index... Processing gpt_index/constants.py... generating tree index... Processing gpt_index/data_structs/__init__.py... generating tree index... Processing gpt_index/data_structs/data_structs.py... generating tree index... Processing gpt_index/data_structs/struct_type.py... generating tree index... Processing gpt_index/data_structs/table.py... generating tree index... Processing gpt_index/docstore.py... generating tree index... Processing gpt_index/embeddings/__init__.py... generating tree index... Processing gpt_index/embeddings/base.py... generating tree index... Processing gpt_index/embeddings/langchain.py... generating tree index... Processing gpt_index/embeddings/openai.py... generating tree index... Processing gpt_index/embeddings/utils.py... generating tree index... Processing gpt_index/indices/__init__.py... generating tree index... Processing gpt_index/indices/base.py... generating tree index... Processing gpt_index/indices/common/__init__.py... generating tree index... Processing gpt_index/indices/common/struct_store/__init__.py... generating tree index... Processing gpt_index/indices/common/struct_store/base.py... generating tree index... Processing gpt_index/indices/common/tree/__init__.py... generating tree index... Processing gpt_index/indices/common/tree/base.py... generating tree index... Processing gpt_index/indices/keyword_table/README.md... generating tree index... Processing gpt_index/indices/keyword_table/__init__.py... generating tree index... Processing gpt_index/indices/keyword_table/base.py... generating tree index... Processing gpt_index/indices/keyword_table/rake_base.py... generating tree index... Processing gpt_index/indices/keyword_table/simple_base.py... generating tree index... Processing gpt_index/indices/keyword_table/utils.py... generating tree index... Processing gpt_index/indices/list/README.md... generating tree index... Processing gpt_index/indices/list/__init__.py... generating tree index... Processing gpt_index/indices/list/base.py... generating tree index... Processing gpt_index/indices/node_utils.py... generating tree index... Processing gpt_index/indices/prompt_helper.py... generating tree index... Processing gpt_index/indices/query/__init__.py... generating tree index... Processing gpt_index/indices/query/base.py... generating tree index... Processing gpt_index/indices/query/embedding_utils.py... generating tree index... Processing gpt_index/indices/query/keyword_table/__init__.py... generating tree index... Processing gpt_index/indices/query/keyword_table/query.py... generating tree index... Processing gpt_index/indices/query/list/__init__.py... generating tree index... Processing gpt_index/indices/query/list/embedding_query.py... generating tree index... Processing gpt_index/indices/query/list/query.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing gpt_index/indices/query/query_runner.py... generating tree index... Processing gpt_index/indices/query/schema.py... generating tree index... Processing gpt_index/indices/query/struct_store/__init__.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 150 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 165 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 162 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 167 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 201 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 201 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 217 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 202 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 244 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 275 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 208 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 275 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 232 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 217 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 308 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 222 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 455 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 239 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 388 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 262 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 422 tokens\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 406 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 412 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 209 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 214 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1112 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1400 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 666 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 518 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 420 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 818 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 520 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1787 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 730 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1865 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 497 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1897 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1612 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1721 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1135 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1027 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 505 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2078 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1217 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 932 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 2 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 406 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2459 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2468 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1066 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 541 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 3325 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/query/__init__.py\n",
      "Processing gpt_index/indices/query/struct_store/sql.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 551 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/VERSION\n",
      "Processing gpt_index/indices/query/tree/__init__.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 3146 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 632 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing MANIFEST.in\n",
      "Processing gpt_index/indices/query/tree/embedding_query.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 481 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/embeddings/__init__.py\n",
      "Processing gpt_index/indices/query/tree/leaf_query.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 3061 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 567 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 579 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 585 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/common/__init__.py\n",
      "Processing gpt_index/indices/query/tree/retrieve_query.py... generating tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/composability/__init__.py\n",
      "Processing gpt_index/indices/query/tree/summarize_query.py... generating tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing experimental/README.md\n",
      "Processing gpt_index/indices/query/vector_store/__init__.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 3703 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2663 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1094 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 609 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing Makefile\n",
      "Processing gpt_index/indices/query/vector_store/base.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 629 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing .flake8\n",
      "Processing gpt_index/indices/query/vector_store/faiss.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 302 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 843 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 280 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [query] Total LLM token usage: 615 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done generating tree index... defining a summary text for the tree index... done saving tree index... done processing gpt_index/indices/keyword_table/rake_base.py\n",
      "Processing gpt_index/indices/query/vector_store/pinecone.py... generating tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/constants.py\n",
      "Processing gpt_index/indices/query/vector_store/qdrant.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 697 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/common/tree/__init__.py\n",
      "Processing gpt_index/indices/query/vector_store/simple.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 629 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 734 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 630 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/list/__init__.py\n",
      "Processing gpt_index/indices/query/vector_store/weaviate.py... generating tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing CITATION.cff\n",
      "Processing gpt_index/indices/registry.py... generating tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/common/struct_store/__init__.py\n",
      "Processing gpt_index/indices/response/__init__.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 660 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/query/struct_store/__init__.py\n",
      "Processing gpt_index/indices/response/builder.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 972 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 724 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing .github/workflows/lint.yml\n",
      "Processing gpt_index/indices/struct_store/__init__.py... generating tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/query/list/__init__.py\n",
      "Processing gpt_index/indices/struct_store/base.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 848 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 706 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 916 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/keyword_table/simple_base.py\n",
      "Processing gpt_index/indices/struct_store/sql.py... generating tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing data_requirements.txt\n",
      "Processing gpt_index/indices/tree/README.md... generating tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/data_structs/table.py\n",
      "Processing gpt_index/indices/tree/__init__.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 849 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing LICENSE\n",
      "Processing gpt_index/indices/tree/base.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 703 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing .readthedocs.yaml\n",
      "Processing gpt_index/indices/tree/inserter.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 1117 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing .github/workflows/build_package.yml\n",
      "Processing gpt_index/indices/utils.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 815 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [query] Total LLM token usage: 1033 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/embeddings/langchain.py\n",
      "Processing gpt_index/indices/vector_store/__init__.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 191 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1108 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 487 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 237 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 199 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 550 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2168 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [query] Total LLM token usage: 994 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 1008 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing .github/workflows/unit_test.yml\n",
      "Processing gpt_index/indices/vector_store/base.py... generating tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/list/README.md\n",
      "Processing gpt_index/indices/vector_store/faiss.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 827 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/keyword_table/__init__.py\n",
      "Processing gpt_index/indices/vector_store/pinecone.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 1956 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/embeddings/base.py\n",
      "Processing gpt_index/indices/vector_store/qdrant.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 1192 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/data_structs/__init__.py\n",
      "Processing gpt_index/indices/vector_store/simple.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 370 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 1570 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/data_structs/struct_type.py\n",
      "Processing gpt_index/indices/vector_store/weaviate.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 1739 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 860 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/keyword_table/utils.py\n",
      "Processing gpt_index/langchain_helpers/__init__.py... generating tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/query/keyword_table/__init__.py\n",
      "Processing gpt_index/langchain_helpers/chain_wrapper.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 696 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 1749 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/query/embedding_utils.py\n",
      "Processing gpt_index/langchain_helpers/memory_wrapper.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 962 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 4937 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 1283 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 895 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 1347 tokens\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1382 tokens\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1518 tokens\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1919 tokens\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 5598 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 2383 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/__init__.py\n",
      "Processing gpt_index/langchain_helpers/sql_wrapper.py... generating tree index... done generating tree index... defining a summary text for the tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/embeddings/utils.py\n",
      "Processing gpt_index/langchain_helpers/text_splitter.py... generating tree index... done saving tree index... done processing gpt_index/indices/query/list/query.py\n",
      "Processing gpt_index/prompts/__init__.py... generating tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing CONTRIBUTING.md\n",
      "Processing gpt_index/prompts/base.py... generating tree index... done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/query/query_runner.py\n",
      "Processing gpt_index/prompts/default_prompts.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 211 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 1595 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/node_utils.py\n",
      "Processing gpt_index/prompts/prompt_type.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1881 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [query] Total LLM token usage: 2654 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1154 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/common/struct_store/base.py\n",
      "Processing gpt_index/prompts/prompts.py... generating tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 1839 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 2415 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2243 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing README.md\n",
      "Processing gpt_index/py.typed... generating tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/list/base.py\n",
      "Processing gpt_index/readers/__init__.py... generating tree index... done generating tree index... defining a summary text for the tree index... Processing gpt_index/readers/base.py... generating tree index... done generating tree index... defining a summary text for the tree index... Failed to generate tree index for gpt_index/py.typed: integer division or modulo by zero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 731 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/query/vector_store/__init__.py\n",
      "Processing gpt_index/readers/database.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 835 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 1792 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/query/tree/__init__.py\n",
      "Processing gpt_index/readers/discord_reader.py... generating tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing .gitignore\n",
      "Processing gpt_index/readers/download.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1498 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 194 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2531 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmetk/Projects/gpt_index/gpt_index/utils.py\", line 157, in retry_on_exceptions_with_backoff\n",
      "    return lambda_fn()\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/Projects/gpt_index/gpt_index/langchain_helpers/chain_wrapper.py\", line 100, in <lambda>\n",
      "    lambda: llm_chain.predict(**full_prompt_args),\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/llm.py\", line 104, in predict\n",
      "    return self(kwargs)[self.output_key]\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/base.py\", line 155, in __call__\n",
      "    raise e\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/base.py\", line 152, in __call__\n",
      "    outputs = self._call(inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/llm.py\", line 88, in _call\n",
      "    return self.apply([inputs])[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/llm.py\", line 79, in apply\n",
      "    response = self.generate(input_list)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/llm.py\", line 74, in generate\n",
      "    response = self.llm.generate(prompts, stop=stop)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/llms/base.py\", line 79, in generate\n",
      "    raise e\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/llms/base.py\", line 76, in generate\n",
      "    output = self._generate(prompts, stop=stop)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/llms/openai.py\", line 158, in _generate\n",
      "    response = self.client.create(prompt=_prompts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/openai/api_resources/completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "                           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/openai/api_requestor.py\", line 227, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/openai/api_requestor.py\", line 620, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/openai/api_requestor.py\", line 663, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 3900 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 8347 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/embeddings/openai.py\n",
      "Processing gpt_index/readers/faiss.py... generating tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 2379 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/query/list/embedding_query.py\n",
      "Processing gpt_index/readers/file/__init__.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 3095 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/common/tree/base.py\n",
      "Processing gpt_index/readers/file/base.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 708 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/tree/__init__.py\n",
      "Processing gpt_index/readers/file/base_parser.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3199 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 649 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing experimental/classifier/utils.py\n",
      "Processing gpt_index/readers/file/docs_parser.py... generating tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/response/__init__.py\n",
      "Processing gpt_index/readers/file/epub_parser.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3781 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 607 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/query/keyword_table/query.py\n",
      "Processing gpt_index/readers/file/image_parser.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1744 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/struct_store/__init__.py\n",
      "Processing gpt_index/readers/file/markdown_parser.py... generating tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 1000 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 2197 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/query/vector_store/base.py\n",
      "Processing gpt_index/readers/file/mbox_parser.py... generating tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/query/schema.py\n",
      "Processing gpt_index/readers/file/slides_parser.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 3085 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1027 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 477 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1338 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [query] Total LLM token usage: 808 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done saving tree index... done processing gpt_index/indices/vector_store/__init__.py\n",
      "Processing gpt_index/readers/file/tabular_parser.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 413 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [query] Total LLM token usage: 1017 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/registry.py\n",
      "Processing gpt_index/readers/file/video_audio.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 171 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 1351 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/query/tree/retrieve_query.py\n",
      "Processing gpt_index/readers/github_readers/__init__.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3132 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2192 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/docstore.py\n",
      "Processing gpt_index/readers/github_readers/github_api_client.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 1811 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/keyword_table/README.md\n",
      "Processing gpt_index/readers/github_readers/github_repository_reader.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2824 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3701 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/keyword_table/base.py\n",
      "Processing gpt_index/readers/github_readers/utils.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 3745 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 3831 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1250 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 713 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/langchain_helpers/__init__.py\n",
      "Processing gpt_index/readers/google_readers/__init__.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 183 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 1715 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/query/tree/summarize_query.py\n",
      "Processing gpt_index/readers/google_readers/gdocs.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2873 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 795 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [query] Total LLM token usage: 1379 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/utils.py\n",
      "Processing gpt_index/readers/make_com/__init__.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 669 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 2825 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/query/struct_store/sql.py\n",
      "Processing gpt_index/readers/make_com/wrapper.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 656 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 970 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/prompts/__init__.py\n",
      "Processing gpt_index/readers/mbox.py... generating tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 3751 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2304 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [query] Total LLM token usage: 2537 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/__init__.py\n",
      "Processing gpt_index/readers/mongo.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2089 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2887 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 1876 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/query/vector_store/weaviate.py\n",
      "Processing gpt_index/readers/notion.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 189 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 210 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1062 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 3512 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 4887 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1759 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1618 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [query] Total LLM token usage: 2072 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/query/vector_store/simple.py\n",
      "Processing gpt_index/readers/obsidian.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1440 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1885 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 9274 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [query] Total LLM token usage: 955 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done saving tree index... done processing gpt_index/prompts/prompt_type.py\n",
      "Processing gpt_index/readers/pinecone.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 2538 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/query/vector_store/faiss.py\n",
      "Processing gpt_index/readers/qdrant.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 1571 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/__init__.py\n",
      "Processing gpt_index/readers/schema/__init__.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2846 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2830 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [query] Total LLM token usage: 865 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 2038 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 740 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/base.py\n",
      "Processing gpt_index/readers/schema/base.py... generating tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/vector_store/base.py\n",
      "Processing gpt_index/readers/slack.py... generating tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 693 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/file/__init__.py\n",
      "Processing gpt_index/readers/string_iterable.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1974 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmetk/Projects/gpt_index/gpt_index/utils.py\", line 157, in retry_on_exceptions_with_backoff\n",
      "    return lambda_fn()\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/Projects/gpt_index/gpt_index/langchain_helpers/chain_wrapper.py\", line 100, in <lambda>\n",
      "    lambda: llm_chain.predict(**full_prompt_args),\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/llm.py\", line 104, in predict\n",
      "    return self(kwargs)[self.output_key]\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/base.py\", line 155, in __call__\n",
      "    raise e\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/base.py\", line 152, in __call__\n",
      "    outputs = self._call(inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/llm.py\", line 88, in _call\n",
      "    return self.apply([inputs])[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/llm.py\", line 79, in apply\n",
      "    response = self.generate(input_list)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/llm.py\", line 74, in generate\n",
      "    response = self.llm.generate(prompts, stop=stop)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/llms/base.py\", line 79, in generate\n",
      "    raise e\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/llms/base.py\", line 76, in generate\n",
      "    output = self._generate(prompts, stop=stop)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/llms/openai.py\", line 158, in _generate\n",
      "    response = self.client.create(prompt=_prompts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/openai/api_resources/completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "                           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/openai/api_requestor.py\", line 227, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/openai/api_requestor.py\", line 620, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/openai/api_requestor.py\", line 663, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 965 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 1840 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/langchain_helpers/sql_wrapper.py\n",
      "Processing gpt_index/readers/twitter.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 2575 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/query/vector_store/qdrant.py\n",
      "Processing gpt_index/readers/weaviate/__init__.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2065 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 838 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 591 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/github_readers/__init__.py\n",
      "Processing gpt_index/readers/weaviate/data_structs.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1062 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 2935 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 551 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 2331 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/query/tree/embedding_query.py\n",
      "Processing gpt_index/readers/weaviate/reader.py... generating tree index... done generating tree index... defining a summary text for the tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/langchain_helpers/memory_wrapper.py\n",
      "Processing gpt_index/readers/weaviate/utils.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 617 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 232 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/make_com/__init__.py\n",
      "Processing gpt_index/readers/web.py... generating tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2273 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 1924 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/tree/README.md\n",
      "Processing gpt_index/readers/wikipedia.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 667 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/google_readers/__init__.py\n",
      "Processing gpt_index/readers/youtube_transcript.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 4472 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 2 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 1317 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/file/epub_parser.py\n",
      "Processing gpt_index/response/__init__.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 4400 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2921 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 593 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 3360 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [query] Total LLM token usage: 1214 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/file/base_parser.py\n",
      "Processing gpt_index/response/schema.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 232 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 1854 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/download.py\n",
      "Processing gpt_index/schema.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 4488 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1556 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2997 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 3164 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 2864 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 600 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/tree/base.py\n",
      "Processing gpt_index/token_counter/__init__.py... generating tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/vector_store/simple.py\n",
      "Processing gpt_index/token_counter/mock_chain_wrapper.py... generating tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 183 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [query] Total LLM token usage: 1593 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/file/docs_parser.py\n",
      "Processing gpt_index/token_counter/mock_embed_model.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3721 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/struct_store/base.py\n",
      "Processing gpt_index/token_counter/token_counter.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1071 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 737 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3431 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/langchain_helpers/chain_wrapper.py\n",
      "Processing gpt_index/token_counter/utils.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 1155 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 1542 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/mbox.py\n",
      "Processing gpt_index/utils.py... generating tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/file/video_audio.py\n",
      "Processing pyproject.toml... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1017 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [query] Total LLM token usage: 4782 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/composability/graph.py\n",
      "Processing requirements.txt... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 210 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 3356 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1073 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1911 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [query] Total LLM token usage: 753 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/schema/__init__.py\n",
      "Processing setup.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 971 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 2351 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/schema/base.py\n",
      "Processing tests/__init__.py... generating tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/file/image_parser.py\n",
      "Processing tests/indices/__init__.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 879 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 226 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 2759 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/query/vector_store/pinecone.py\n",
      "Processing tests/indices/embedding/__init__.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> Building index from nodes: 2 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 484 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 1290 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 2122 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/obsidian.py\n",
      "Processing tests/indices/embedding/test_base.py... generating tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/database.py\n",
      "Processing tests/indices/keyword_table/__init__.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2984 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 5433 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 2896 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/prompts/base.py\n",
      "Processing tests/indices/keyword_table/test_base.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3699 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/vector_store/weaviate.py\n",
      "Processing tests/indices/keyword_table/test_utils.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 399 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 163 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 2072 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 164 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/faiss.py\n",
      "Processing tests/indices/list/__init__.py... generating tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 352 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 168 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 736 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 562 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/weaviate/__init__.py\n",
      "Processing tests/indices/list/test_base.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 2539 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/response/__init__.py\n",
      "Processing tests/indices/query/__init__.py... generating tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/file/mbox_parser.py\n",
      "Processing tests/indices/query/test_query_runner.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2315 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 164 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 1628 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 592 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/mongo.py\n",
      "Processing tests/indices/query/test_recursive.py... generating tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 1006 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1545 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/wikipedia.py\n",
      "Processing tests/indices/struct_store/__init__.py... generating tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 4246 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/prompts/default_prompts.py\n",
      "Processing tests/indices/struct_store/test_base.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 3595 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 3530 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/vector_store/pinecone.py\n",
      "Processing tests/indices/test_node_utils.py... generating tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/prompts/prompts.py\n",
      "Processing tests/indices/test_prompt_helper.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 217 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 167 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 1148 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/youtube_transcript.py\n",
      "Processing tests/indices/test_response.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 2751 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/file/tabular_parser.py\n",
      "Processing tests/indices/test_utils.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 1063 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/string_iterable.py\n",
      "Processing tests/indices/tree/__init__.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 804 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 2 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 221 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 3484 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/discord_reader.py\n",
      "Processing tests/indices/tree/test_base.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 670 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing pyproject.toml\n",
      "Processing tests/indices/vector_store/__init__.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 1662 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2326 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/make_com/wrapper.py\n",
      "Processing tests/indices/vector_store/test_base.py... generating tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 2790 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/file/markdown_parser.py\n",
      "Processing tests/langchain_helpers/__init__.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 483 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/__init__.py\n",
      "Processing tests/langchain_helpers/test_text_splitter.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 159 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 547 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1064 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/indices/__init__.py\n",
      "Processing tests/mock_utils/__init__.py... generating tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 529 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 511 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/indices/embedding/__init__.py\n",
      "Processing tests/mock_utils/mock_decorator.py... generating tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/indices/keyword_table/__init__.py\n",
      "Processing tests/mock_utils/mock_predict.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2072 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 774 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 1638 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 417 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/weaviate/utils.py\n",
      "Processing tests/mock_utils/mock_prompts.py... generating tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 1568 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 882 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/response/schema.py\n",
      "Processing tests/mock_utils/mock_text_splitter.py... generating tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/token_counter/utils.py\n",
      "Processing tests/mock_utils/mock_utils.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 1619 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/twitter.py\n",
      "Processing tests/prompts/__init__.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 3083 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [query] Total LLM token usage: 1009 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/token_counter/mock_embed_model.py\n",
      "Processing tests/prompts/test_base.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 695 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/token_counter/__init__.py\n",
      "Processing tests/readers/__init__.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 176 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 4729 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 976 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 157 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [query] Total LLM token usage: 849 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing requirements.txt\n",
      "Processing tests/readers/test_file.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 182 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 4793 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 669 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/vector_store/faiss.py\n",
      "Processing tests/readers/test_string_iterable.py... generating tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 6704 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [query] Total LLM token usage: 3108 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/file/slides_parser.py\n",
      "Processing tests/test_docstore.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 2510 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/qdrant.py\n",
      "Processing tests/test_utils.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 183 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 3700 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 231 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 7026 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 549 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/query/base.py\n",
      "Processing tests/token_predictor/__init__.py... generating tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/indices/struct_store/__init__.py\n",
      "Processing tests/token_predictor/test_base.py... generating tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 4906 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 467 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/struct_store/sql.py\n",
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/indices/tree/__init__.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 2 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 472 tokens\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1076 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 426 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2684 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 4554 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 5053 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/tree/inserter.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 272 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [query] Total LLM token usage: 2143 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/pinecone.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 686 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/indices/query/__init__.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1669 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 638 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/indices/list/__init__.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2979 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 6470 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 1547 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/data_structs/data_structs.py\n",
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/schema.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 540 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 3995 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/langchain_helpers/__init__.py\n",
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/notion.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 879 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 778 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 213 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 3724 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/github_readers/utils.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1341 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 519 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/prompts/__init__.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 6673 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 1533 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/indices/test_node_utils.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 4056 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/file/base.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 572 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 9944 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/readers/__init__.py\n",
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing experimental/classifier/TitanicModel.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 1429 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/langchain_helpers/test_text_splitter.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 1040 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/mock_utils/mock_decorator.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1437 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3053 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/weaviate/reader.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 1349 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 2905 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/indices/query/test_query_runner.py\n",
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/indices/embedding/test_base.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 624 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 3547 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/indices/vector_store/__init__.py\n",
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 650 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 1204 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/mock_utils/__init__.py\n",
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing setup.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 4882 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> [query] Total LLM token usage: 2221 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/token_counter/token_counter.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 1582 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/mock_utils/mock_prompts.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 864 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/mock_utils/mock_text_splitter.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3694 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 1039 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/indices/keyword_table/test_base.py\n",
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/indices/test_utils.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3893 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/google_readers/gdocs.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 678 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 7630 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/readers/test_string_iterable.py\n",
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 2 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 3697 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/slack.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 6012 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 2806 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/response/builder.py\n",
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/token_counter/mock_chain_wrapper.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 1029 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/mock_utils/mock_utils.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 9655 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 5421 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 5716 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/vector_store/qdrant.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 5802 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/prompt_helper.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 722 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/token_predictor/__init__.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 1317 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/token_predictor/test_base.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 2309 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/mock_utils/mock_predict.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 1642 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/indices/keyword_table/test_utils.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmetk/Projects/gpt_index/gpt_index/utils.py\", line 157, in retry_on_exceptions_with_backoff\n",
      "    return lambda_fn()\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/Projects/gpt_index/gpt_index/langchain_helpers/chain_wrapper.py\", line 100, in <lambda>\n",
      "    lambda: llm_chain.predict(**full_prompt_args),\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/llm.py\", line 104, in predict\n",
      "    return self(kwargs)[self.output_key]\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/base.py\", line 155, in __call__\n",
      "    raise e\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/base.py\", line 152, in __call__\n",
      "    outputs = self._call(inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/llm.py\", line 88, in _call\n",
      "    return self.apply([inputs])[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/llm.py\", line 79, in apply\n",
      "    response = self.generate(input_list)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/llm.py\", line 74, in generate\n",
      "    response = self.llm.generate(prompts, stop=stop)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/llms/base.py\", line 79, in generate\n",
      "    raise e\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/llms/base.py\", line 76, in generate\n",
      "    output = self._generate(prompts, stop=stop)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/llms/openai.py\", line 158, in _generate\n",
      "    response = self.client.create(prompt=_prompts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/openai/api_resources/completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "                           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/openai/api_requestor.py\", line 227, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/openai/api_requestor.py\", line 620, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/openai/api_requestor.py\", line 663, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n",
      "INFO:root:> [query] Total LLM token usage: 4629 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/indices/struct_store/test_base.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 5813 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/weaviate/data_structs.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 1458 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/test_docstore.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 5771 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/query/tree/leaf_query.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3710 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 2058 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/indices/test_response.py\n",
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/prompts/test_base.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 6791 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/langchain_helpers/text_splitter.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 6463 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: Write a concise and clear summary of the code file, emphasizing the key components, algorithms, data structures, and relationships between functions, classes, and variables. Explain the purpose and functionality of the code in a way that is easy to understand.\n",
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating tree index... defining a summary text for the tree index... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 7867 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/github_readers/github_api_client.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3635 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/utils.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 7914 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/indices/list/test_base.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 10993 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/indices/base.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 2191 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 5964 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/test_utils.py\n",
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/web.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 6476 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/readers/test_file.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 4804 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/indices/test_prompt_helper.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 9173 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/indices/vector_store/test_base.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 5836 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/indices/tree/test_base.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 11486 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing gpt_index/readers/github_readers/github_repository_reader.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 7497 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done defining a summary text for the tree index... done generating tree index, saving to disk... done saving tree index... done processing tests/indices/query/test_recursive.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43caa1e80>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=91, family=2, type=1, proto=6, laddr=('172.18.154.52', 55912), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c9e38c0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c1086e0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=145, family=2, type=1, proto=6, laddr=('172.18.154.52', 36652), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c9e1320>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=119, family=2, type=1, proto=6, laddr=('172.18.154.52', 36580), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=78, family=2, type=1, proto=6, laddr=('172.18.154.52', 36450), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc434734bb0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc434735080>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c9e33f0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=110, family=2, type=1, proto=6, laddr=('172.18.154.52', 36562), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c108ad0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=172, family=2, type=1, proto=6, laddr=('172.18.154.52', 41022), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c109a90>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c108c20>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=164, family=2, type=1, proto=6, laddr=('172.18.154.52', 55814), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=161, family=2, type=1, proto=6, laddr=('172.18.154.52', 55802), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=184, family=2, type=1, proto=6, laddr=('172.18.154.52', 38636), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc434734830>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c9e3000>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=167, family=2, type=1, proto=6, laddr=('172.18.154.52', 41014), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=112, family=2, type=1, proto=6, laddr=('172.18.154.52', 55846), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c1081a0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c9e3d20>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c1089f0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=177, family=2, type=1, proto=6, laddr=('172.18.154.52', 41046), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43caa1c50>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=124, family=2, type=1, proto=6, laddr=('172.18.154.52', 55872), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=158, family=2, type=1, proto=6, laddr=('172.18.154.52', 55796), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c10b3f0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=185, family=2, type=1, proto=6, laddr=('172.18.154.52', 38642), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=123, family=2, type=1, proto=6, laddr=('172.18.154.52', 36604), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c1084b0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c9e03d0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=141, family=2, type=1, proto=6, laddr=('172.18.154.52', 36650), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c99b700>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c9e1be0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=136, family=2, type=1, proto=6, laddr=('172.18.154.52', 36624), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=85, family=2, type=1, proto=6, laddr=('172.18.154.52', 36462), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c9e3690>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=113, family=2, type=1, proto=6, laddr=('172.18.154.52', 36578), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c9e2eb0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c10ba10>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c109e80>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=126, family=2, type=1, proto=6, laddr=('172.18.154.52', 55888), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c108600>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=105, family=2, type=1, proto=6, laddr=('172.18.154.52', 36550), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=102, family=2, type=1, proto=6, laddr=('172.18.154.52', 36542), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc434734c90>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c108520>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=84, family=2, type=1, proto=6, laddr=('172.18.154.52', 36484), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=147, family=2, type=1, proto=6, laddr=('172.18.154.52', 36660), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c108910>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c9e0360>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=109, family=2, type=1, proto=6, laddr=('172.18.154.52', 36554), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=178, family=2, type=1, proto=6, laddr=('172.18.154.52', 41056), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=160, family=2, type=1, proto=6, laddr=('172.18.154.52', 55798), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=165, family=2, type=1, proto=6, laddr=('172.18.154.52', 55940), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c108980>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=150, family=2, type=1, proto=6, laddr=('172.18.154.52', 36666), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c9e29e0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c9e3ee0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=98, family=2, type=1, proto=6, laddr=('172.18.154.52', 36518), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c109e10>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c108e50>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c10a2e0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=127, family=2, type=1, proto=6, laddr=('172.18.154.52', 36606), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=154, family=2, type=1, proto=6, laddr=('172.18.154.52', 55786), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c9e1e10>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=142, family=2, type=1, proto=6, laddr=('172.18.154.52', 55892), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=148, family=2, type=1, proto=6, laddr=('172.18.154.52', 55898), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=162, family=2, type=1, proto=6, laddr=('172.18.154.52', 55810), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=94, family=2, type=1, proto=6, laddr=('172.18.154.52', 36510), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c9e08a0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c9e3930>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=183, family=2, type=1, proto=6, laddr=('172.18.154.52', 38632), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c9e15c0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=121, family=2, type=1, proto=6, laddr=('172.18.154.52', 36590), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c9e1860>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c10b230>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=88, family=2, type=1, proto=6, laddr=('172.18.154.52', 55924), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=83, family=2, type=1, proto=6, laddr=('172.18.154.52', 36478), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c9e0de0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c108360>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=139, family=2, type=1, proto=6, laddr=('172.18.154.52', 36638), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c1082f0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=152, family=2, type=1, proto=6, laddr=('172.18.154.52', 36674), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc434734fa0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c1096a0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=75, family=2, type=1, proto=6, laddr=('172.18.154.52', 36442), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc4347351d0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=175, family=2, type=1, proto=6, laddr=('172.18.154.52', 41042), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=173, family=2, type=1, proto=6, laddr=('172.18.154.52', 41032), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=131, family=2, type=1, proto=6, laddr=('172.18.154.52', 55932), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c9e0d00>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=100, family=2, type=1, proto=6, laddr=('172.18.154.52', 36534), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c109160>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43caa07c0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=171, family=2, type=1, proto=6, laddr=('172.18.154.52', 55838), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c9e0a60>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=128, family=2, type=1, proto=6, laddr=('172.18.154.52', 36612), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc4347350f0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=179, family=2, type=1, proto=6, laddr=('172.18.154.52', 41066), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43cb3f3f0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=188, family=2, type=1, proto=6, laddr=('172.18.154.52', 54304), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=170, family=2, type=1, proto=6, laddr=('172.18.154.52', 55830), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c109940>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=134, family=2, type=1, proto=6, laddr=('172.18.154.52', 55938), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c109c50>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c10acf0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=96, family=2, type=1, proto=6, laddr=('172.18.154.52', 55844), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=149, family=2, type=1, proto=6, laddr=('172.18.154.52', 55906), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c99ab30>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=117, family=2, type=1, proto=6, laddr=('172.18.154.52', 55856), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c9e2890>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7fc43c9e2740>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=186, family=2, type=1, proto=6, laddr=('172.18.154.52', 38650), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=82, family=2, type=1, proto=6, laddr=('172.18.154.52', 36460), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=89, family=2, type=1, proto=6, laddr=('172.18.154.52', 36494), raddr=('52.152.96.252', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "from gpt_index import Document\n",
    "from gpt_index import QueryMode\n",
    "from gpt_index import SummaryPrompt\n",
    "from gpt_index import LLMPredictor\n",
    "from langchain import OpenAI\n",
    "from typing import List\n",
    "from gpt_index.indices.base import BaseGPTIndex\n",
    "from gpt_index import TreeInsertPrompt\n",
    "\n",
    "# CODE_FILE_SUMMARY_PROMPT_TMPL = (\n",
    "#     \"Write a summary of the following code file. Try to explain the purpose, \"\n",
    "#     \"functionality, and key elements of the code. \"\n",
    "#     \"Try to include as many details as possible, but also keep it concise.\\n\"\n",
    "#     \"\\n\"\n",
    "#     \"\\n\"\n",
    "#     \"{context_str}\\n\"\n",
    "#     \"\\n\"\n",
    "#     \"\\n\"\n",
    "#     'SUMMARY:\"\"\"\\n'\n",
    "# )\n",
    "# CODE_FILE_SUMMARY_PROMPT = SummaryPrompt(CODE_FILE_SUMMARY_PROMPT_TMPL)\n",
    "\n",
    "# CODE_FILE_INSERT_PROMPT_TMPL = (\n",
    "#     \"Context information is below. It is provided in a numbered list \"\n",
    "#     \"(1 to {num_chunks}),\"\n",
    "#     \"where each item in the list corresponds to a summary of a code file.\\n\"\n",
    "#     \"---------------------\\n\"\n",
    "#     \"{context_list}\"\n",
    "#     \"---------------------\\n\"\n",
    "#     \"Given the context information, here is a new piece of code file: {new_chunk_text}\\n\"\n",
    "#     \"Answer with the number corresponding to the summary that should be updated. \"\n",
    "#     \"The answer should be the number corresponding to the \"\n",
    "#     \"summary that is most relevant to the new piece of code.\\n\"\n",
    "# )\n",
    "# CODE_FILE_INSERT_PROMPT = TreeInsertPrompt(CODE_FILE_INSERT_PROMPT_TMPL)\n",
    "\n",
    "\n",
    "# ===-=-=-=-=-=-============-=-=-=-=-=-=-==============-=-=-=-=-=-=-============= #\n",
    "\n",
    "# CODE_FILE_SUMMARY_PROMPT_TMPL = (\n",
    "#     \"Write a concise and clear summary of the following code file. Emphasize the key components, \"\n",
    "#     \"algorithms, data structures, and relationships between functions, classes, and variables. \"\n",
    "#     \"Explain the purpose and functionality of the code in a way that is easy to understand.\\n\"\n",
    "#     \"\\n\"\n",
    "#     \"{context_str}\\n\"\n",
    "#     \"\\n\"\n",
    "#     \"\\n\"\n",
    "#     'SUMMARY:\"\"\"\\n'\n",
    "# )\n",
    "# CODE_FILE_SUMMARY_PROMPT = SummaryPrompt(CODE_FILE_SUMMARY_PROMPT_TMPL)\n",
    "\n",
    "# CODE_FILE_INSERT_PROMPT_TMPL = (\n",
    "#     \"Context information is below. It is provided in a numbered list \"\n",
    "#     \"(1 to {num_chunks}),\"\n",
    "#     \"where each item in the list corresponds to a summary of a code file.\\n\"\n",
    "#     \"---------------------\\n\"\n",
    "#     \"{context_list}\"\n",
    "#     \"---------------------\\n\"\n",
    "#     \"Given the context information, here is a new piece of code file: {new_chunk_text}\\n\"\n",
    "#     \"Answer with the number corresponding to the summary that should be updated. \"\n",
    "#     \"The answer should be the number corresponding to the \"\n",
    "#     \"summary that is most relevant to the new piece of code. Ensure the summaries are up-to-date and accurate.\\n\"\n",
    "# )\n",
    "# CODE_FILE_INSERT_PROMPT = TreeInsertPrompt(CODE_FILE_INSERT_PROMPT_TMPL)\n",
    "\n",
    "# ============-=-=-=-=-=-=-================-=-=-=-=-=-=-================-=-=-=-=-=-=-= #\n",
    "\n",
    "CODE_FILE_SUMMARY_PROMPT_TMPL = (\n",
    "    \"Write a concise and clear summary of the following code file, focusing on its key components, \"\n",
    "    \"algorithms, data structures, and relationships between functions, classes, and variables. \"\n",
    "    \"Explain the purpose and functionality of the code in an easily understandable manner.\\n\"\n",
    "    \"\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"\\n\"\n",
    "    \"SUMMARY:\\n\"\n",
    "    '\"\"\"\\n'\n",
    ")\n",
    "CODE_FILE_SUMMARY_PROMPT = SummaryPrompt(CODE_FILE_SUMMARY_PROMPT_TMPL)\n",
    "\n",
    "CODE_FILE_INSERT_PROMPT_TMPL = (\n",
    "    \"Please review the following context information, which is presented in a numbered list \"\n",
    "    \"of code file summaries (1 to {num_chunks}).\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_list}\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Here is a new piece of code file: {new_chunk_text}\\n\"\n",
    "    \"Please indicate which summary (by number) should be updated to accurately reflect the new code.\\n\"\n",
    ")\n",
    "CODE_FILE_INSERT_PROMPT = TreeInsertPrompt(CODE_FILE_INSERT_PROMPT_TMPL)\n",
    "\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "# model_name = \"text-chat-davinci-002-sh-alpha-aoruigiofdj83\"\n",
    "# model_name = \"text-davinci-002-render-sha\"\n",
    "# llm_predictor = LLMPredictor(llm=OpenAI(model_name=model_name))\n",
    "\n",
    "processed_files = set()\n",
    "tree_indexes: List[BaseGPTIndex] =[]\n",
    "for tree_index_filename in os.listdir(\"tree_indexes\"):\n",
    "    tree_index = GPTTreeIndex.load_from_disk(f\"tree_indexes/{tree_index_filename}\")\n",
    "    tree_indexes.append(tree_index)\n",
    "    for doc_id, doc in tree_index.docstore.docs.items():\n",
    "        processed_files.add(doc.extra_info[\"file_path\"])\n",
    "        break\n",
    "\n",
    "# for doc in documents:\n",
    "def generate_tree_index(doc):\n",
    "    print(f\"Processing {doc.extra_info['file_path']}\", end=\"... \")\n",
    "    if doc.extra_info[\"file_path\"] in processed_files:\n",
    "        print(f\"skipping because already processed.\")\n",
    "        # continue\n",
    "        return None\n",
    "    print(f\"generating tree index\", end=\"... \")\n",
    "    tree_index = GPTTreeIndex([doc], summary_template=CODE_FILE_SUMMARY_PROMPT, insert_prompt=CODE_FILE_INSERT_PROMPT)\n",
    "    print(f\"done generating tree index\", end=\"... \")\n",
    "    print(f\"defining a summary text for the tree index\", end=\"... \")\n",
    "    summary = tree_index.query(\n",
    "        \"Write a concise and clear summary of the code file, emphasizing the key components, \" +\n",
    "        \"algorithms, data structures, and relationships between functions, classes, and variables. \" +\n",
    "        \"Explain the purpose and functionality of the code in a way that is easy to understand.\", \n",
    "        mode=\"summarize\"\n",
    "    )\n",
    "    tree_index.set_text(str(summary))\n",
    "    print(f\"done defining a summary text for the tree index\", end=\"... \")\n",
    "    print(f\"done generating tree index, saving to disk\", end=\"... \")\n",
    "    tree_index.save_to_disk(f\"tree_indexes/tree_index_{doc.extra_info['file_path'].replace('/', '-')}.json\")\n",
    "    print(f\"done saving tree index\", end=\"... \")\n",
    "    # tree_indexes.append(tree_index)\n",
    "    print(f\"done processing {doc.extra_info['file_path']}\")\n",
    "    return tree_index\n",
    "    \n",
    "    \n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=60) as executor:\n",
    "    future_to_doc = {executor.submit(generate_tree_index, doc): doc for doc in documents}\n",
    "\n",
    "    for future in concurrent.futures.as_completed(future_to_doc):\n",
    "        doc = future_to_doc[future]\n",
    "        file_path = doc.extra_info[\"file_path\"]\n",
    "        try:\n",
    "            tree_index = future.result()\n",
    "            if tree_index is not None:\n",
    "                tree_indexes.append(tree_index)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate tree index for {file_path}: {e}\")\n",
    "\n",
    "# weaviate_index = GPTWeaviateIndex.load_from_disk(\"weaviate_index.json\")\n",
    "\n",
    "# weaviate_client = weaviate.Client(\"http://localhost:8080\")\n",
    "# weaviate_index = GPTWeaviateIndex(documents, llm_predictor=llm_predictor, weaviate_client=weaviate_client)\n",
    "\n",
    "# weaviate_index.save_to_disk(\"weaviate_index.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_index/indices/query: 5:\n",
      "\tgpt_index/indices/query/__init__.py\n",
      "\tgpt_index/indices/query/embedding_utils.py\n",
      "\tgpt_index/indices/query/query_runner.py\n",
      "\tgpt_index/indices/query/schema.py\n",
      "\tgpt_index/indices/query/base.py\n",
      "gpt_index: 6:\n",
      "\tgpt_index/VERSION\n",
      "\tgpt_index/constants.py\n",
      "\tgpt_index/docstore.py\n",
      "\tgpt_index/__init__.py\n",
      "\tgpt_index/schema.py\n",
      "\tgpt_index/utils.py\n",
      ": 13:\n",
      "\tMANIFEST.in\n",
      "\tMakefile\n",
      "\t.flake8\n",
      "\tCITATION.cff\n",
      "\tdata_requirements.txt\n",
      "\tLICENSE\n",
      "\t.readthedocs.yaml\n",
      "\tCONTRIBUTING.md\n",
      "\tREADME.md\n",
      "\t.gitignore\n",
      "\tpyproject.toml\n",
      "\trequirements.txt\n",
      "\tsetup.py\n",
      "gpt_index/embeddings: 5:\n",
      "\tgpt_index/embeddings/__init__.py\n",
      "\tgpt_index/embeddings/langchain.py\n",
      "\tgpt_index/embeddings/base.py\n",
      "\tgpt_index/embeddings/utils.py\n",
      "\tgpt_index/embeddings/openai.py\n",
      "gpt_index/indices/common: 1:\n",
      "\tgpt_index/indices/common/__init__.py\n",
      "gpt_index/composability: 2:\n",
      "\tgpt_index/composability/__init__.py\n",
      "\tgpt_index/composability/graph.py\n",
      "experimental: 1:\n",
      "\texperimental/README.md\n",
      "gpt_index/indices/keyword_table: 6:\n",
      "\tgpt_index/indices/keyword_table/rake_base.py\n",
      "\tgpt_index/indices/keyword_table/simple_base.py\n",
      "\tgpt_index/indices/keyword_table/__init__.py\n",
      "\tgpt_index/indices/keyword_table/utils.py\n",
      "\tgpt_index/indices/keyword_table/README.md\n",
      "\tgpt_index/indices/keyword_table/base.py\n",
      "gpt_index/indices/common/tree: 2:\n",
      "\tgpt_index/indices/common/tree/__init__.py\n",
      "\tgpt_index/indices/common/tree/base.py\n",
      "gpt_index/indices/list: 3:\n",
      "\tgpt_index/indices/list/__init__.py\n",
      "\tgpt_index/indices/list/README.md\n",
      "\tgpt_index/indices/list/base.py\n",
      "gpt_index/indices/common/struct_store: 2:\n",
      "\tgpt_index/indices/common/struct_store/__init__.py\n",
      "\tgpt_index/indices/common/struct_store/base.py\n",
      "gpt_index/indices/query/struct_store: 2:\n",
      "\tgpt_index/indices/query/struct_store/__init__.py\n",
      "\tgpt_index/indices/query/struct_store/sql.py\n",
      ".github/workflows: 3:\n",
      "\t.github/workflows/lint.yml\n",
      "\t.github/workflows/build_package.yml\n",
      "\t.github/workflows/unit_test.yml\n",
      "gpt_index/indices/query/list: 3:\n",
      "\tgpt_index/indices/query/list/__init__.py\n",
      "\tgpt_index/indices/query/list/query.py\n",
      "\tgpt_index/indices/query/list/embedding_query.py\n",
      "gpt_index/data_structs: 4:\n",
      "\tgpt_index/data_structs/table.py\n",
      "\tgpt_index/data_structs/__init__.py\n",
      "\tgpt_index/data_structs/struct_type.py\n",
      "\tgpt_index/data_structs/data_structs.py\n",
      "gpt_index/indices/query/keyword_table: 2:\n",
      "\tgpt_index/indices/query/keyword_table/__init__.py\n",
      "\tgpt_index/indices/query/keyword_table/query.py\n",
      "gpt_index/indices: 6:\n",
      "\tgpt_index/indices/__init__.py\n",
      "\tgpt_index/indices/node_utils.py\n",
      "\tgpt_index/indices/registry.py\n",
      "\tgpt_index/indices/utils.py\n",
      "\tgpt_index/indices/prompt_helper.py\n",
      "\tgpt_index/indices/base.py\n",
      "gpt_index/indices/query/vector_store: 7:\n",
      "\tgpt_index/indices/query/vector_store/__init__.py\n",
      "\tgpt_index/indices/query/vector_store/base.py\n",
      "\tgpt_index/indices/query/vector_store/weaviate.py\n",
      "\tgpt_index/indices/query/vector_store/simple.py\n",
      "\tgpt_index/indices/query/vector_store/faiss.py\n",
      "\tgpt_index/indices/query/vector_store/qdrant.py\n",
      "\tgpt_index/indices/query/vector_store/pinecone.py\n",
      "gpt_index/indices/query/tree: 5:\n",
      "\tgpt_index/indices/query/tree/__init__.py\n",
      "\tgpt_index/indices/query/tree/retrieve_query.py\n",
      "\tgpt_index/indices/query/tree/summarize_query.py\n",
      "\tgpt_index/indices/query/tree/embedding_query.py\n",
      "\tgpt_index/indices/query/tree/leaf_query.py\n",
      "gpt_index/indices/tree: 4:\n",
      "\tgpt_index/indices/tree/__init__.py\n",
      "\tgpt_index/indices/tree/README.md\n",
      "\tgpt_index/indices/tree/base.py\n",
      "\tgpt_index/indices/tree/inserter.py\n",
      "experimental/classifier: 2:\n",
      "\texperimental/classifier/utils.py\n",
      "\texperimental/classifier/TitanicModel.ipynb\n",
      "gpt_index/indices/response: 2:\n",
      "\tgpt_index/indices/response/__init__.py\n",
      "\tgpt_index/indices/response/builder.py\n",
      "gpt_index/indices/struct_store: 3:\n",
      "\tgpt_index/indices/struct_store/__init__.py\n",
      "\tgpt_index/indices/struct_store/base.py\n",
      "\tgpt_index/indices/struct_store/sql.py\n",
      "gpt_index/indices/vector_store: 7:\n",
      "\tgpt_index/indices/vector_store/__init__.py\n",
      "\tgpt_index/indices/vector_store/base.py\n",
      "\tgpt_index/indices/vector_store/simple.py\n",
      "\tgpt_index/indices/vector_store/weaviate.py\n",
      "\tgpt_index/indices/vector_store/pinecone.py\n",
      "\tgpt_index/indices/vector_store/faiss.py\n",
      "\tgpt_index/indices/vector_store/qdrant.py\n",
      "gpt_index/langchain_helpers: 5:\n",
      "\tgpt_index/langchain_helpers/__init__.py\n",
      "\tgpt_index/langchain_helpers/sql_wrapper.py\n",
      "\tgpt_index/langchain_helpers/memory_wrapper.py\n",
      "\tgpt_index/langchain_helpers/chain_wrapper.py\n",
      "\tgpt_index/langchain_helpers/text_splitter.py\n",
      "gpt_index/prompts: 5:\n",
      "\tgpt_index/prompts/__init__.py\n",
      "\tgpt_index/prompts/prompt_type.py\n",
      "\tgpt_index/prompts/base.py\n",
      "\tgpt_index/prompts/default_prompts.py\n",
      "\tgpt_index/prompts/prompts.py\n",
      "gpt_index/readers: 18:\n",
      "\tgpt_index/readers/__init__.py\n",
      "\tgpt_index/readers/base.py\n",
      "\tgpt_index/readers/download.py\n",
      "\tgpt_index/readers/mbox.py\n",
      "\tgpt_index/readers/obsidian.py\n",
      "\tgpt_index/readers/database.py\n",
      "\tgpt_index/readers/faiss.py\n",
      "\tgpt_index/readers/mongo.py\n",
      "\tgpt_index/readers/wikipedia.py\n",
      "\tgpt_index/readers/youtube_transcript.py\n",
      "\tgpt_index/readers/string_iterable.py\n",
      "\tgpt_index/readers/discord_reader.py\n",
      "\tgpt_index/readers/twitter.py\n",
      "\tgpt_index/readers/qdrant.py\n",
      "\tgpt_index/readers/pinecone.py\n",
      "\tgpt_index/readers/notion.py\n",
      "\tgpt_index/readers/slack.py\n",
      "\tgpt_index/readers/web.py\n",
      "gpt_index/readers/file: 11:\n",
      "\tgpt_index/readers/file/__init__.py\n",
      "\tgpt_index/readers/file/epub_parser.py\n",
      "\tgpt_index/readers/file/base_parser.py\n",
      "\tgpt_index/readers/file/docs_parser.py\n",
      "\tgpt_index/readers/file/video_audio.py\n",
      "\tgpt_index/readers/file/image_parser.py\n",
      "\tgpt_index/readers/file/mbox_parser.py\n",
      "\tgpt_index/readers/file/tabular_parser.py\n",
      "\tgpt_index/readers/file/markdown_parser.py\n",
      "\tgpt_index/readers/file/slides_parser.py\n",
      "\tgpt_index/readers/file/base.py\n",
      "gpt_index/readers/github_readers: 4:\n",
      "\tgpt_index/readers/github_readers/__init__.py\n",
      "\tgpt_index/readers/github_readers/utils.py\n",
      "\tgpt_index/readers/github_readers/github_api_client.py\n",
      "\tgpt_index/readers/github_readers/github_repository_reader.py\n",
      "gpt_index/readers/make_com: 2:\n",
      "\tgpt_index/readers/make_com/__init__.py\n",
      "\tgpt_index/readers/make_com/wrapper.py\n",
      "gpt_index/readers/google_readers: 2:\n",
      "\tgpt_index/readers/google_readers/__init__.py\n",
      "\tgpt_index/readers/google_readers/gdocs.py\n",
      "gpt_index/readers/schema: 2:\n",
      "\tgpt_index/readers/schema/__init__.py\n",
      "\tgpt_index/readers/schema/base.py\n",
      "gpt_index/readers/weaviate: 4:\n",
      "\tgpt_index/readers/weaviate/__init__.py\n",
      "\tgpt_index/readers/weaviate/utils.py\n",
      "\tgpt_index/readers/weaviate/reader.py\n",
      "\tgpt_index/readers/weaviate/data_structs.py\n",
      "gpt_index/response: 2:\n",
      "\tgpt_index/response/__init__.py\n",
      "\tgpt_index/response/schema.py\n",
      "tests: 3:\n",
      "\ttests/__init__.py\n",
      "\ttests/test_docstore.py\n",
      "\ttests/test_utils.py\n",
      "tests/indices: 5:\n",
      "\ttests/indices/__init__.py\n",
      "\ttests/indices/test_node_utils.py\n",
      "\ttests/indices/test_utils.py\n",
      "\ttests/indices/test_response.py\n",
      "\ttests/indices/test_prompt_helper.py\n",
      "tests/indices/embedding: 2:\n",
      "\ttests/indices/embedding/__init__.py\n",
      "\ttests/indices/embedding/test_base.py\n",
      "tests/indices/keyword_table: 3:\n",
      "\ttests/indices/keyword_table/__init__.py\n",
      "\ttests/indices/keyword_table/test_base.py\n",
      "\ttests/indices/keyword_table/test_utils.py\n",
      "gpt_index/token_counter: 5:\n",
      "\tgpt_index/token_counter/utils.py\n",
      "\tgpt_index/token_counter/mock_embed_model.py\n",
      "\tgpt_index/token_counter/__init__.py\n",
      "\tgpt_index/token_counter/token_counter.py\n",
      "\tgpt_index/token_counter/mock_chain_wrapper.py\n",
      "tests/indices/struct_store: 2:\n",
      "\ttests/indices/struct_store/__init__.py\n",
      "\ttests/indices/struct_store/test_base.py\n",
      "tests/indices/tree: 2:\n",
      "\ttests/indices/tree/__init__.py\n",
      "\ttests/indices/tree/test_base.py\n",
      "tests/indices/query: 3:\n",
      "\ttests/indices/query/__init__.py\n",
      "\ttests/indices/query/test_query_runner.py\n",
      "\ttests/indices/query/test_recursive.py\n",
      "tests/indices/list: 2:\n",
      "\ttests/indices/list/__init__.py\n",
      "\ttests/indices/list/test_base.py\n",
      "tests/langchain_helpers: 2:\n",
      "\ttests/langchain_helpers/__init__.py\n",
      "\ttests/langchain_helpers/test_text_splitter.py\n",
      "tests/prompts: 2:\n",
      "\ttests/prompts/__init__.py\n",
      "\ttests/prompts/test_base.py\n",
      "tests/readers: 3:\n",
      "\ttests/readers/__init__.py\n",
      "\ttests/readers/test_string_iterable.py\n",
      "\ttests/readers/test_file.py\n",
      "tests/mock_utils: 6:\n",
      "\ttests/mock_utils/mock_decorator.py\n",
      "\ttests/mock_utils/__init__.py\n",
      "\ttests/mock_utils/mock_prompts.py\n",
      "\ttests/mock_utils/mock_text_splitter.py\n",
      "\ttests/mock_utils/mock_utils.py\n",
      "\ttests/mock_utils/mock_predict.py\n",
      "tests/indices/vector_store: 2:\n",
      "\ttests/indices/vector_store/__init__.py\n",
      "\ttests/indices/vector_store/test_base.py\n",
      "tests/token_predictor: 2:\n",
      "\ttests/token_predictor/__init__.py\n",
      "\ttests/token_predictor/test_base.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/pinecone/core/client/rest.py:45: DeprecationWarning: HTTPResponse.getheader() is deprecated and will be removed in urllib3 v2.1.0. Instead use HTTPResponse.headers.get(name, default).\n",
      "  return self.urllib3_response.getheader(name, default)\n",
      "/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/pinecone/manage.py:65: ResourceWarning: unclosed <ssl.SSLSocket fd=74, family=2, type=1, proto=6, laddr=('172.18.154.52', 34640), raddr=('35.231.219.44', 443)>\n",
      "  return response['status']\n",
      "/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/pinecone/manage.py:65: ResourceWarning: unclosed <ssl.SSLSocket fd=74, family=2, type=1, proto=6, laddr=('172.18.154.52', 34654), raddr=('35.231.219.44', 443)>\n",
      "  return response['status']\n",
      "/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/pinecone/manage.py:65: ResourceWarning: unclosed <ssl.SSLSocket fd=74, family=2, type=1, proto=6, laddr=('172.18.154.52', 44358), raddr=('35.231.219.44', 443)>\n",
      "  return response['status']\n",
      "/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/pinecone/manage.py:65: ResourceWarning: unclosed <ssl.SSLSocket fd=74, family=2, type=1, proto=6, laddr=('172.18.154.52', 44360), raddr=('35.231.219.44', 443)>\n",
      "  return response['status']\n",
      "/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/pinecone/manage.py:65: ResourceWarning: unclosed <ssl.SSLSocket fd=74, family=2, type=1, proto=6, laddr=('172.18.154.52', 46804), raddr=('35.231.219.44', 443)>\n",
      "  return response['status']\n",
      "/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/pinecone/manage.py:65: ResourceWarning: unclosed <ssl.SSLSocket fd=74, family=2, type=1, proto=6, laddr=('172.18.154.52', 46814), raddr=('35.231.219.44', 443)>\n",
      "  return response['status']\n",
      "/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/pinecone/manage.py:65: ResourceWarning: unclosed <ssl.SSLSocket fd=74, family=2, type=1, proto=6, laddr=('172.18.154.52', 49662), raddr=('35.231.219.44', 443)>\n",
      "  return response['status']\n",
      "/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/pinecone/manage.py:146: ResourceWarning: unclosed <ssl.SSLSocket fd=72, family=2, type=1, proto=6, laddr=('172.18.154.52', 43770), raddr=('35.231.219.44', 443)>\n",
      "  if timeout and timeout < 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing gpt_index/indices/query... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 785 tokens\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "<QueryMode.SUMMARIZE: 'summarize'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 71\u001b[0m\n\u001b[1;32m     65\u001b[0m query_str_tpl \u001b[39m=\u001b[39m (\n\u001b[1;32m     66\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mSummarize the code files contained in the grouped list of tree indexes. \u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     67\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mEmphasize the key components, algorithms, data structures, and relationships\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     68\u001b[0m \u001b[39m\"\u001b[39m\u001b[39m between functions, classes, and variables in a concise and clear manner. \u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     69\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mExplain the purpose and functionality of the code files in an easily understandable way.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m query_str \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(query_str_tpl)\n\u001b[0;32m---> 71\u001b[0m summary \u001b[39m=\u001b[39m pincon_index\u001b[39m.\u001b[39;49mquery(query_str, mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msummarize\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     72\u001b[0m pincon_index\u001b[39m.\u001b[39mset_text(\u001b[39mstr\u001b[39m(summary))\n\u001b[1;32m     73\u001b[0m vec_stores\u001b[39m.\u001b[39mappend(pincon_index)\n",
      "File \u001b[0;32m~/Projects/gpt_index/gpt_index/indices/base.py:385\u001b[0m, in \u001b[0;36mBaseGPTIndex.query\u001b[0;34m(self, query_str, mode, **query_kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m query_config \u001b[39m=\u001b[39m QueryConfig(\n\u001b[1;32m    372\u001b[0m     index_struct_type\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_struct\u001b[39m.\u001b[39mget_type(),\n\u001b[1;32m    373\u001b[0m     query_mode\u001b[39m=\u001b[39mmode_enum,\n\u001b[1;32m    374\u001b[0m     query_kwargs\u001b[39m=\u001b[39mquery_kwargs,\n\u001b[1;32m    375\u001b[0m )\n\u001b[1;32m    376\u001b[0m query_runner \u001b[39m=\u001b[39m QueryRunner(\n\u001b[1;32m    377\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm_predictor,\n\u001b[1;32m    378\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prompt_helper,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    383\u001b[0m     recursive\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    384\u001b[0m )\n\u001b[0;32m--> 385\u001b[0m \u001b[39mreturn\u001b[39;00m query_runner\u001b[39m.\u001b[39;49mquery(query_str, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_index_struct)\n",
      "File \u001b[0;32m~/Projects/gpt_index/gpt_index/indices/query/query_runner.py:84\u001b[0m, in \u001b[0;36mQueryRunner.query\u001b[0;34m(self, query_str, index_struct)\u001b[0m\n\u001b[1;32m     81\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_config_dict[index_struct_type]\n\u001b[1;32m     82\u001b[0m mode \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mquery_mode\n\u001b[0;32m---> 84\u001b[0m query_cls \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_index_registry\u001b[39m.\u001b[39;49mtype_to_query[index_struct_type][mode]\n\u001b[1;32m     85\u001b[0m \u001b[39m# if recursive, pass self as query_runner to each individual query\u001b[39;00m\n\u001b[1;32m     86\u001b[0m query_runner \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recursive \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: <QueryMode.SUMMARIZE: 'summarize'>"
     ]
    }
   ],
   "source": [
    "# query_str = \"Explain how the code works and what it does in detail for the 'BaseGPTIndex' class.\"\n",
    "# query_str = 'Explain what is the \"create and refine\" in the context of generating a response.'\n",
    "# response = weaviate_index.query(\n",
    "#     query_str=query_str,\n",
    "#     mode=QueryMode.EMBEDDING,\n",
    "# )\n",
    "\n",
    "import tracemalloc\n",
    "\n",
    "tracemalloc.start()\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "from gpt_index import QuestionAnswerPrompt\n",
    "def group_tree_indexes_by_file_path(tree_indexes: List[GPTTreeIndex]) -> Dict[str, List[Tuple[GPTTreeIndex, str]]]:\n",
    "    grouped_tree_indexes = {}\n",
    "    for tree_index in tree_indexes:\n",
    "        for _, doc in tree_index.docstore.docs.items():\n",
    "            fp = \"/\".join(doc.extra_info[\"file_path\"].split(\"/\")[:-1])\n",
    "            if fp not in grouped_tree_indexes:\n",
    "                grouped_tree_indexes[fp] = []\n",
    "            grouped_tree_indexes[fp].append((tree_index, doc.extra_info[\"file_path\"]))\n",
    "            break\n",
    "    return grouped_tree_indexes\n",
    "\n",
    "grouped_tree_indexes = group_tree_indexes_by_file_path(tree_indexes)\n",
    "for file_path_, tree_indexes_ in grouped_tree_indexes.items():\n",
    "    print(f\"{file_path_}: {len(tree_indexes_)}:\")\n",
    "    for _, fp in tree_indexes_:\n",
    "        print(f\"\\t{fp}\")\n",
    "\n",
    "CODE_FILE_QA_PROMPT_TMPL = (\n",
    "    \"Context information related to the code is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Please answer the following question based on the context information of the code:\\n\"\n",
    "    \"{query_str}\\n\"\n",
    ")\n",
    "CODE_FILE_QA_PROMPT = QuestionAnswerPrompt(CODE_FILE_QA_PROMPT_TMPL)\n",
    "\n",
    "import pinecone\n",
    "api_key = \"1e89c53c-fde6-4133-8e42-903cddf88c1a\"\n",
    "pinecone.init(api_key=api_key, environment=\"us-east1-gcp\")\n",
    "pinecone.create_index(\"gpt-index\", dimension=1536, metric=\"euclidean\", pod_type=\"p1\")\n",
    "pinconindex = pinecone.Index(\"gpt-index\")\n",
    "\n",
    "\n",
    "vec_stores = []\n",
    "for fp, tree_indexes_ in grouped_tree_indexes.items():\n",
    "    print(f\"Processing {fp}... \", end=\"\")\n",
    "    try:\n",
    "        pincon_index = GPTPineconeIndex( # Don't use Pinecone\n",
    "            [tree_index for tree_index, _ in tree_indexes_],\n",
    "            pinecone_index=pinconindex, \n",
    "            chunk_size_limit=2048*2, \n",
    "            text_qa_template=CODE_FILE_QA_PROMPT\n",
    "        )\n",
    "    except Exception as e:\n",
    "        snapshot = tracemalloc.take_snapshot()\n",
    "        top_stats = snapshot.statistics('lineno')\n",
    "\n",
    "        print(\"[ Top 10 ]\")\n",
    "        for stat in top_stats[:10]:\n",
    "            print(stat)\n",
    "    query_str_tpl = (\n",
    "    \"Summarize the code files contained in the grouped list of tree indexes. \",\n",
    "    \"Emphasize the key components, algorithms, data structures, and relationships\",\n",
    "    \" between functions, classes, and variables in a concise and clear manner. \",\n",
    "    \"Explain the purpose and functionality of the code files in an easily understandable way.\")\n",
    "    query_str = \"\".join(query_str_tpl)\n",
    "    summary = pincon_index.query(query_str, mode=\"summarize\")\n",
    "    pincon_index.set_text(str(summary))\n",
    "    vec_stores.append(pincon_index)\n",
    "    print(f\"Done processing {fp}.\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# display(Markdown(f\"**Query:** {query_str}\"))\n",
    "# display(Markdown(f\"**Response:** {response.response}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_index-github-reader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bc2ab08ee48b6366504a28e3231c27a37c154a347ee8ac6184b716eff7bdbcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
