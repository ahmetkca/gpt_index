{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in /home/ahmetk/.pyenv/versions/gpt_index-github-reader/lib/python3.11/site-packages (1.5.6)\n"
     ]
    }
   ],
   "source": [
    "# This is due to the fact that we use asyncio.loop_until_complete in\n",
    "# the GithubRepositoryReader. Since the Jupyter kernel itself runs on\n",
    "# an event loop, we need to add some help with nesting\n",
    "!pip install nest_asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "from gpt_index import (\n",
    "    GPTSimpleVectorIndex,\n",
    "    GPTQdrantIndex,\n",
    "    GPTTreeIndex,\n",
    "    GPTFaissIndex,\n",
    "    GPTWeaviateIndex,\n",
    "    GPTListIndex,\n",
    "    GPTSimpleKeywordTableIndex,\n",
    "    GPTKeywordTableIndex,\n",
    "    GPTPineconeIndex,\n",
    "    GPTRAKEKeywordTableIndex,\n",
    "    GPTSQLStructStoreIndex,\n",
    "    GithubRepositoryReader,\n",
    ")\n",
    "from IPython.display import Markdown, display\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env GITHUB_TOKEN=github_pat_xxxxxxxxxxxxxxxxxx_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "github_token = os.environ.get(\"GITHUB_TOKEN\")\n",
    "owner = \"jerryjliu\"\n",
    "repo = \"gpt_index\"\n",
    "branch = \"main\"\n",
    "reader = GithubRepositoryReader(\n",
    "    github_token=github_token,\n",
    "    owner=owner,\n",
    "    repo=repo,\n",
    "    use_parser=True,\n",
    "    verbose=True,\n",
    "    # ignore_directories=[\"examples\", \"docs\", \".vscode\"],\n",
    "    directories_to_include=[\"gpt_index\"],\n",
    "    ignore_file_extensions=[\n",
    "        \".png\",\n",
    "        \".jpg\",\n",
    "        \".jpeg\",\n",
    "        \".gif\",\n",
    "        \".svg\",\n",
    "        \".ico\",\n",
    "        \".json\",\n",
    "        \".csv\",\n",
    "    ],\n",
    "    concurrent_requests=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current path: \n",
      "processing tree 76dce9173ad45f67844d7b36e68d2bbba2d6cb0e\n",
      "found blob .flake8\n",
      "recursing into .github\n",
      "ignoring tree .github due to directory\n",
      "found blob .gitignore\n",
      "found blob .readthedocs.yaml\n",
      "recursing into .vscode\n",
      "ignoring tree .vscode due to directory\n",
      "found blob CITATION.cff\n",
      "found blob CONTRIBUTING.md\n",
      "found blob LICENSE\n",
      "found blob MANIFEST.in\n",
      "found blob Makefile\n",
      "found blob README.md\n",
      "found blob data_requirements.txt\n",
      "recursing into docs\n",
      "ignoring tree docs due to directory\n",
      "recursing into examples\n",
      "ignoring tree examples due to directory\n",
      "recursing into experimental\n",
      "ignoring tree experimental due to directory\n",
      "recursing into gpt_index\n",
      "\tcurrent path: gpt_index\n",
      "\tprocessing tree 798979bcad0f6f89adc63e91060b9c406b0e85d9\n",
      "\tfound blob VERSION\n",
      "\tfound blob __init__.py\n",
      "\trecursing into composability\n",
      "\tignoring tree composability due to directory\n",
      "\tfound blob constants.py\n",
      "\trecursing into data_structs\n",
      "\tignoring tree data_structs due to directory\n",
      "\tfound blob docstore.py\n",
      "\trecursing into embeddings\n",
      "\tignoring tree embeddings due to directory\n",
      "\trecursing into indices\n",
      "\tignoring tree indices due to directory\n",
      "\trecursing into langchain_helpers\n",
      "\tignoring tree langchain_helpers due to directory\n",
      "\trecursing into prompts\n",
      "\tignoring tree prompts due to directory\n",
      "\tfound blob py.typed\n",
      "\trecursing into readers\n",
      "\tignoring tree readers due to directory\n",
      "\trecursing into response\n",
      "\tignoring tree response due to directory\n",
      "\tfound blob schema.py\n",
      "\trecursing into token_counter\n",
      "\tignoring tree token_counter due to directory\n",
      "\tfound blob utils.py\n",
      "found blob pyproject.toml\n",
      "found blob requirements.txt\n",
      "found blob setup.py\n",
      "recursing into tests\n",
      "ignoring tree tests due to directory\n",
      "got 20 blobs\n",
      "Time to get blobs ([('.flake8', 290), ('.gitignore', 1876), ('.readthedocs.yaml', 193), ('CITATION.cff', 298), ('CONTRIBUTING.md', 2434)]): 0.74 seconds\n",
      "generating document for .flake8\n",
      "could not parse .flake8 as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 290 characters- adding to documents - .flake8\n",
      "generating document for .gitignore\n",
      "could not parse .gitignore as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1876 characters- adding to documents - .gitignore\n",
      "generating document for .readthedocs.yaml\n",
      "could not parse .readthedocs.yaml as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 193 characters- adding to documents - .readthedocs.yaml\n",
      "generating document for CITATION.cff\n",
      "could not parse CITATION.cff as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 298 characters- adding to documents - CITATION.cff\n",
      "generating document for CONTRIBUTING.md\n",
      "parsing CONTRIBUTING.mdas .md with MarkdownParser\n",
      "created a temporary file/tmp/tmpe9e07fa7/tmpcrlkoag2..md for parsing CONTRIBUTING.md\n",
      "Time to get blobs ([('LICENSE', 1064), ('MANIFEST.in', 68), ('Makefile', 126), ('README.md', 3175), ('data_requirements.txt', 149)]): 0.56 seconds\n",
      "generating document for LICENSE\n",
      "could not parse LICENSE as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1064 characters- adding to documents - LICENSE\n",
      "generating document for MANIFEST.in\n",
      "could not parse MANIFEST.in as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 68 characters- adding to documents - MANIFEST.in\n",
      "generating document for Makefile\n",
      "could not parse Makefile as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 126 characters- adding to documents - Makefile\n",
      "generating document for README.md\n",
      "parsing README.mdas .md with MarkdownParser\n",
      "created a temporary file/tmp/tmpkm10mxu4/tmpa_6o4qo6..md for parsing README.md\n",
      "generating document for data_requirements.txt\n",
      "could not parse data_requirements.txt as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 149 characters- adding to documents - data_requirements.txt\n",
      "Time to get blobs ([('VERSION', 6), ('__init__.py', 3805), ('constants.py', 89), ('docstore.py', 4385), ('py.typed', 0)]): 0.82 seconds\n",
      "generating document for gpt_index/VERSION\n",
      "could not parse gpt_index/VERSION as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 6 characters- adding to documents - gpt_index/VERSION\n",
      "generating document for gpt_index/__init__.py\n",
      "could not parse gpt_index/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 3805 characters- adding to documents - gpt_index/__init__.py\n",
      "generating document for gpt_index/constants.py\n",
      "could not parse gpt_index/constants.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 89 characters- adding to documents - gpt_index/constants.py\n",
      "generating document for gpt_index/docstore.py\n",
      "could not parse gpt_index/docstore.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 4385 characters- adding to documents - gpt_index/docstore.py\n",
      "generating document for gpt_index/py.typed\n",
      "could not parse gpt_index/py.typed as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 0 characters- adding to documents - gpt_index/py.typed\n",
      "Time to get blobs ([('schema.py', 1657), ('utils.py', 4879), ('pyproject.toml', 139), ('requirements.txt', 291), ('setup.py', 974)]): 0.62 seconds\n",
      "generating document for gpt_index/schema.py\n",
      "could not parse gpt_index/schema.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1657 characters- adding to documents - gpt_index/schema.py\n",
      "generating document for gpt_index/utils.py\n",
      "could not parse gpt_index/utils.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 4879 characters- adding to documents - gpt_index/utils.py\n",
      "generating document for pyproject.toml\n",
      "could not parse pyproject.toml as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 139 characters- adding to documents - pyproject.toml\n",
      "generating document for requirements.txt\n",
      "could not parse requirements.txt as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 291 characters- adding to documents - requirements.txt\n",
      "generating document for setup.py\n",
      "could not parse setup.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 974 characters- adding to documents - setup.py\n",
      "Loaded 20 documents\n"
     ]
    }
   ],
   "source": [
    "documents = reader.load_data(branch=branch)\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing 196 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 778 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 847 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1194 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1025 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of the codes in the following files .flake8, build_package.yml, ?\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 1108 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of the codes in the following files lint.yml, unit_test.yml, ?\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/4 -- Index document id: 55775b74-3c8d-42f3-af09-febebdf2af38 -- Done!\n",
      ".flake8, build_package.yml, \n",
      "Index summary text: \n",
      "The .flake8 file contains settings for the flake8 linter, such as excluding certain directories and setting the max-line-length to 88. The build_package.yml file is a GitHub workflow that sets up Python 3.9, installs dependencies, builds the package, and then runs an import test.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 1127 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of the codes in the following files .gitignore, .readthedocs.yaml, ?\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/4 -- Index document id: 617a77a4-7f2b-44ed-b7cf-e00e42a86510 -- Done!\n",
      "lint.yml, unit_test.yml, \n",
      "Index summary text: \n",
      "lint.yml: Sets up Python 3.9, installs dependencies, and runs the linter to check for errors.\n",
      "\n",
      "unit_test.yml: Sets up Python 3.9 and 3.8, installs dependencies, and runs the unit tests to check for errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 2095 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of the codes in the following files CITATION.cff, CONTRIBUTING.md, ?\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 -- Index document id: 35b41143-b640-4c9a-a9de-12f96e319f90 -- Done!\n",
      ".gitignore, .readthedocs.yaml, \n",
      "Index summary text: \n",
      "The .gitignore file contains entries for files and directories that should be ignored by version control systems, such as .DS_Store, __pycache__, *.py[cod], *.so, .Python, bin/, build/, develop-eggs/, dist/, downloads/, eggs/, .eggs/, etc/, include/, lib/, lib64/, parts/, sdist/, share/, var/, wheels/, pip-wheel-metadata/, share/python-wheels/, *.egg-info/, .installed.cfg, *.egg, MANIFEST, *.manifest, *.spec, pip-log.txt, pip-delete-this-directory.txt, htmlcov/, .tox/, .nox/, .coverage, .coverage.*, .cache, nosetests.xml, coverage.xml, *.cover, *.py,cover, .hypothesis/, .pytest_cache/, *.mo, *.pot, *.log, local_settings.py, db.sqlite3, db.sqlite3-journal, instance/, .webassets-cache, .scrapy, docs/_build/, target/, .ipynb_checkpoints, notebooks/, profile_default/, ip.\n",
      "\n",
      "The .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 1467 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 -- Index document id: 4f406aa5-4603-4437-854e-af664c559efe -- Done!\n",
      "CITATION.cff, CONTRIBUTING.md, \n",
      "Index summary text: \n",
      "CITATION.cff: This file encourages users to cite GPT Index when using the software, and provides the authors, title, DOI, date released, and URL of the software.\n",
      "\n",
      "CONTRIBUTING.md: This file provides instructions on how to contribute to GPT Index, including setting up the environment, formatting/linting changes, testing changes, creating an example notebook, and opening a pull request against the main GPT Index repo.\n"
     ]
    }
   ],
   "source": [
    "from gpt_index import Document\n",
    "print(f\"Indexing {len(documents)} documents\")\n",
    "new_documents: list[list[Document]] = []\n",
    "batch_size = 3\n",
    "for x, doc in enumerate(documents):\n",
    "    if x % batch_size == 0:\n",
    "        new_documents.append([])\n",
    "    new_documents[-1].append(doc)\n",
    "    \n",
    "indexes = [ GPTTreeIndex(ndocs) for ndocs in new_documents[:4] ]\n",
    "\n",
    "i = 1\n",
    "indexes_len = len(indexes)\n",
    "for index in indexes:\n",
    "    files = ''\n",
    "    for _, doc in index.docstore.docs.items():\n",
    "        if doc and doc.extra_info and 'file_name' in doc.extra_info:\n",
    "            files += str(doc.extra_info['file_name']) + ', '\n",
    "\n",
    "    summary = index.query(\n",
    "        f\"What are the summaries of the codes in the following files {files}?\", mode=\"summarize\" \n",
    "    )\n",
    "    index.set_text(str(summary))\n",
    "    print(f\"{i}/{indexes_len} -- Index document id: {index.get_doc_id()} -- Done!\")\n",
    "    print(f\"{files}\")\n",
    "    print(f\"Index summary text: {str(summary)}\")\n",
    "    i += 1\n",
    "    # print(f\"Index document text: {str(summary)}\")\n",
    "    # input(\"Press enter to continue\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: How can I contribute to this open source GitHub repository?\n",
      "INFO:root:query keywords: ['repository', 'github', 'contribute', 'open', 'source']\n",
      "INFO:root:Extracted keywords: ['github', 'contribute']\n",
      "INFO:root:> Starting query: How can I contribute to this open source GitHub repository?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 4 indexes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 284 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: How can I contribute to this open source GitHub repository?\n",
      "INFO:root:> [query] Total LLM token usage: 443 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 1191 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Query:** What is the responsibility of ComposableGraph class?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Response:** \n",
       "\n",
       "ANSWER: The build_package.yml file is a GitHub workflow that builds a package on its own without additional pip install. It sets up Python 3.9, installs dependencies, builds the package, and then runs an import test. Additionally, environment setup requires forking the repo and creating a Python virtual environment. Formatting/Linting and Testing can be done with the commands `make format; make lint` and `pytest tests` respectively. For new features, an example Jupyter notebook can be added to the `examples` folder. Finally, instructions to open a pull request against the main GPT Index repo can be found."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Built {len(indexes)} indexes\")\n",
    "keyword_table = GPTSimpleKeywordTableIndex(indexes, max_keywords_per_chunk=50)\n",
    "\n",
    "from gpt_index.composability import ComposableGraph\n",
    "\n",
    "graph = ComposableGraph.build_from_index(keyword_table)\n",
    "\n",
    "query_configs = [\n",
    "    {\n",
    "        \"index_struct_type\": \"simple_dict\",\n",
    "        \"query_mode\": \"default\",\n",
    "        \"query_kwargs\": {\n",
    "            \"similarity_top_k\": 1\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"index_struct_type\": \"keyword_table\",\n",
    "        \"query_mode\": \"simple\",\n",
    "        \"query_kwargs\": {}\n",
    "    },\n",
    "]\n",
    "\n",
    "response = graph.query(\"How can I contribute to this open source GitHub repository?\", query_configs=query_configs)\n",
    "display(Markdown(f\"**Query:** What is the responsibility of ComposableGraph class?\"))\n",
    "display(Markdown(f\"**Response:** {response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from gpt_index.indices.base import BaseGPTIndex\n",
    "from gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\n",
    "from gpt_index.indices.prompt_helper import PromptHelper\n",
    "from langchain import OpenAI\n",
    "\n",
    "\n",
    "\n",
    "llm_predictor = LLMPredictor(\n",
    "    llm=OpenAI(temperature=0.25, model_name=\"text-davinci-003\", request_timeout=60)\n",
    ")\n",
    "\n",
    "# define prompt helper\n",
    "# set maximum input size\n",
    "max_input_size = int(4096)\n",
    "# set number of output tokens\n",
    "num_output = int(256)\n",
    "# set maximum chunk overlap\n",
    "max_chunk_overlap = int(20)\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "\n",
    "for doc in documents:\n",
    "    print(f\"{doc.get_doc_id()}: {doc.extra_info}\")\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "\n",
    "\n",
    "def construct_index(index_class, *args, **kwargs):\n",
    "    try:\n",
    "        print(f\"Constructing {index_class.__name__}...\")\n",
    "        index = index_class(documents, *args, **kwargs)\n",
    "        print(f\"Saving {index_class.__name__} to disk...\")\n",
    "        index.save_to_disk(f\"{index_class.__name__}.json\")\n",
    "        print(f\"Done constructing {index_class.__name__}\")\n",
    "        return index\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "indexClasses: List[BaseGPTIndex] = [\n",
    "    GPTSimpleVectorIndex,\n",
    "    # GPTQdrantIndex,\n",
    "    # GPTTreeIndex,\n",
    "    # GPTFaissIndex,\n",
    "    # GPTWeaviateIndex,\n",
    "    # GPTListIndex,\n",
    "    GPTSimpleKeywordTableIndex,\n",
    "    GPTKeywordTableIndex,\n",
    "    # GPTPineconeIndex,\n",
    "    GPTRAKEKeywordTableIndex,\n",
    "    # GPTSQLStructStoreIndex,\n",
    "]\n",
    "indexes = {\n",
    "    indexClass.__name__: construct_index(\n",
    "        indexClass,\n",
    "        # llm_predictor=llm_predictor,\n",
    "        # prompt_helper=prompt_helper,\n",
    "    )\n",
    "    for indexClass in indexClasses\n",
    "}\n",
    "\n",
    "indexes = {k: v for k, v in indexes.items() if v is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_index.indices.base import BaseGPTIndex\n",
    "\n",
    "indexName: str\n",
    "index: BaseGPTIndex\n",
    "for indexName, index in indexes.items():\n",
    "    print(f\"Index: {index}\")\n",
    "    response = index.query(\n",
    "        \"What is the difference between `GPTTreeIndex` and `GPTListIndex` classes?\",\n",
    "    )\n",
    "    display(Markdown(f\"<h2>{indexName}</h2> response:<br><p>{response}</p>\"))\n",
    "    input(\"Press enter to continue...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_index-github-reader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bc2ab08ee48b6366504a28e3231c27a37c154a347ee8ac6184b716eff7bdbcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
