{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in /home/ahmetk/.pyenv/versions/gpt_index-github-reader/lib/python3.11/site-packages (1.5.6)\n"
     ]
    }
   ],
   "source": [
    "# This is due to the fact that we use asyncio.loop_until_complete in\n",
    "# the GithubRepositoryReader. Since the Jupyter kernel itself runs on\n",
    "# an event loop, we need to add some help with nesting\n",
    "!pip install nest_asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "from gpt_index import (\n",
    "    GPTSimpleVectorIndex,\n",
    "    GPTQdrantIndex,\n",
    "    GPTTreeIndex,\n",
    "    GPTFaissIndex,\n",
    "    GPTWeaviateIndex,\n",
    "    GPTListIndex,\n",
    "    GPTSimpleKeywordTableIndex,\n",
    "    GPTKeywordTableIndex,\n",
    "    GPTPineconeIndex,\n",
    "    GPTRAKEKeywordTableIndex,\n",
    "    GPTSQLStructStoreIndex,\n",
    "    GithubRepositoryReader,\n",
    ")\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env GITHUB_TOKEN=github_pat_xxxxxxxxxxxxxxxxxxxxxx_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "import os\n",
    "github_token = os.environ.get(\"GITHUB_TOKEN\")\n",
    "owner = \"jerryjliu\"\n",
    "repo = \"gpt_index\"\n",
    "branch = \"main\"\n",
    "reader = GithubRepositoryReader(\n",
    "    github_token=github_token,\n",
    "    owner=owner,\n",
    "    repo=repo,\n",
    "    use_parser=True,\n",
    "    verbose=True,\n",
    "    ignore_directories=[\"examples\", \"docs\", \".vscode\"],\n",
    "    ignore_file_extensions=[\n",
    "        \".png\",\n",
    "        \".jpg\",\n",
    "        \".jpeg\",\n",
    "        \".gif\",\n",
    "        \".svg\",\n",
    "        \".ico\",\n",
    "        \".json\",\n",
    "        \".csv\",\n",
    "    ],\n",
    "    concurrent_requests=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current path: \n",
      "processing tree 6a4a7f614c36eb928f20f2e6373566f029116977\n",
      "found blob .flake8\n",
      "recursing into .github\n",
      "\tcurrent path: .github\n",
      "\tprocessing tree d5fb28aaff20564dc61cc003d66ba60d38cbfe7f\n",
      "\trecursing into workflows\n",
      "\t\tcurrent path: .github/workflows\n",
      "\t\tprocessing tree a7711de32a8df15bcef97cb1b66af9e85a8ffd4c\n",
      "\t\tfound blob build_package.yml\n",
      "\t\tfound blob lint.yml\n",
      "\t\tfound blob unit_test.yml\n",
      "found blob .gitignore\n",
      "found blob .readthedocs.yaml\n",
      "recursing into .vscode\n",
      "ignoring tree .vscode due to directory\n",
      "found blob CITATION.cff\n",
      "found blob CONTRIBUTING.md\n",
      "found blob LICENSE\n",
      "found blob MANIFEST.in\n",
      "found blob Makefile\n",
      "found blob README.md\n",
      "found blob data_requirements.txt\n",
      "recursing into docs\n",
      "ignoring tree docs due to directory\n",
      "recursing into examples\n",
      "ignoring tree examples due to directory\n",
      "recursing into experimental\n",
      "\tcurrent path: experimental\n",
      "\tprocessing tree 832a942385716cdbda057a030748f4d878ea7e38\n",
      "\tfound blob README.md\n",
      "\trecursing into classifier\n",
      "\t\tcurrent path: experimental/classifier\n",
      "\t\tprocessing tree f0ab0d7dec24600308023de6818d6ef386acec6c\n",
      "\t\tfound blob TitanicModel.ipynb\n",
      "\t\trecursing into data\n",
      "\t\t\tcurrent path: experimental/classifier/data\n",
      "\t\t\tprocessing tree 64485aa8e8737f4231ec1b9d2bb0f6428bf340cd\n",
      "\t\t\tfound blob train.csv\n",
      "\t\t\tignoring blob train.csv due to file extension\n",
      "\t\tfound blob utils.py\n",
      "recursing into gpt_index\n",
      "\tcurrent path: gpt_index\n",
      "\tprocessing tree a877265cbc3850a1f248067b6aed6d7cd9d117b4\n",
      "\tfound blob VERSION\n",
      "\tfound blob __init__.py\n",
      "\trecursing into composability\n",
      "\t\tcurrent path: gpt_index/composability\n",
      "\t\tprocessing tree c4d021576e381f0950a18d66c07ba2110e7370ea\n",
      "\t\tfound blob __init__.py\n",
      "\t\tfound blob graph.py\n",
      "\tfound blob constants.py\n",
      "\trecursing into data_structs\n",
      "\t\tcurrent path: gpt_index/data_structs\n",
      "\t\tprocessing tree e1ef76293d31f50676be3efd0c7dcd1b7d573a27\n",
      "\t\tfound blob __init__.py\n",
      "\t\tfound blob data_structs.py\n",
      "\t\tfound blob struct_type.py\n",
      "\t\tfound blob table.py\n",
      "\tfound blob docstore.py\n",
      "\trecursing into embeddings\n",
      "\t\tcurrent path: gpt_index/embeddings\n",
      "\t\tprocessing tree 57c72860e239f3554f6f6d63eb00fc6b3ee3112b\n",
      "\t\tfound blob __init__.py\n",
      "\t\tfound blob base.py\n",
      "\t\tfound blob langchain.py\n",
      "\t\tfound blob openai.py\n",
      "\t\tfound blob utils.py\n",
      "\trecursing into indices\n",
      "\t\tcurrent path: gpt_index/indices\n",
      "\t\tprocessing tree a3f5548e2cf9f41dd5be4acbd3e1c89d3ddade08\n",
      "\t\tfound blob __init__.py\n",
      "\t\tfound blob base.py\n",
      "\t\trecursing into common\n",
      "\t\t\tcurrent path: gpt_index/indices/common\n",
      "\t\t\tprocessing tree f9be44135e345d48f46e69ba59ae6445a022b668\n",
      "\t\t\tfound blob __init__.py\n",
      "\t\t\trecursing into struct_store\n",
      "\t\t\t\tcurrent path: gpt_index/indices/common/struct_store\n",
      "\t\t\t\tprocessing tree 606e6fdd8e46c5872d0fad00625a9a64c17aec02\n",
      "\t\t\t\tfound blob __init__.py\n",
      "\t\t\t\tfound blob base.py\n",
      "\t\t\trecursing into tree\n",
      "\t\t\t\tcurrent path: gpt_index/indices/common/tree\n",
      "\t\t\t\tprocessing tree 53699f495816e8434bef8b7af5c89c963ba5f33b\n",
      "\t\t\t\tfound blob __init__.py\n",
      "\t\t\t\tfound blob base.py\n",
      "\t\trecursing into keyword_table\n",
      "\t\t\tcurrent path: gpt_index/indices/keyword_table\n",
      "\t\t\tprocessing tree acdbc30d0b929f48e41710d4706165d60f45e7ca\n",
      "\t\t\tfound blob README.md\n",
      "\t\t\tfound blob __init__.py\n",
      "\t\t\tfound blob base.py\n",
      "\t\t\tfound blob rake_base.py\n",
      "\t\t\tfound blob simple_base.py\n",
      "\t\t\tfound blob utils.py\n",
      "\t\trecursing into list\n",
      "\t\t\tcurrent path: gpt_index/indices/list\n",
      "\t\t\tprocessing tree 8e9b006a56c435e1ea79f0abec1d4ee57095814c\n",
      "\t\t\tfound blob README.md\n",
      "\t\t\tfound blob __init__.py\n",
      "\t\t\tfound blob base.py\n",
      "\t\tfound blob node_utils.py\n",
      "\t\tfound blob prompt_helper.py\n",
      "\t\trecursing into query\n",
      "\t\t\tcurrent path: gpt_index/indices/query\n",
      "\t\t\tprocessing tree 9e1d7ceafa92002e5df462708b10589934516bee\n",
      "\t\t\tfound blob __init__.py\n",
      "\t\t\tfound blob base.py\n",
      "\t\t\tfound blob embedding_utils.py\n",
      "\t\t\trecursing into keyword_table\n",
      "\t\t\t\tcurrent path: gpt_index/indices/query/keyword_table\n",
      "\t\t\t\tprocessing tree 9c8644ad81c5f3cd181c0b257d6a08db6ef899d8\n",
      "\t\t\t\tfound blob __init__.py\n",
      "\t\t\t\tfound blob query.py\n",
      "\t\t\trecursing into list\n",
      "\t\t\t\tcurrent path: gpt_index/indices/query/list\n",
      "\t\t\t\tprocessing tree f9ab31314d5479739664942038ef0e6affe6afd6\n",
      "\t\t\t\tfound blob __init__.py\n",
      "\t\t\t\tfound blob embedding_query.py\n",
      "\t\t\t\tfound blob query.py\n",
      "\t\t\tfound blob query_runner.py\n",
      "\t\t\tfound blob schema.py\n",
      "\t\t\trecursing into struct_store\n",
      "\t\t\t\tcurrent path: gpt_index/indices/query/struct_store\n",
      "\t\t\t\tprocessing tree 516bd3a46aa2c97b75046e9851ed351e8ea0d3f0\n",
      "\t\t\t\tfound blob __init__.py\n",
      "\t\t\t\tfound blob sql.py\n",
      "\t\t\trecursing into tree\n",
      "\t\t\t\tcurrent path: gpt_index/indices/query/tree\n",
      "\t\t\t\tprocessing tree 406fb4a792f23b8dbd3780e50f4759012e4d85a4\n",
      "\t\t\t\tfound blob __init__.py\n",
      "\t\t\t\tfound blob embedding_query.py\n",
      "\t\t\t\tfound blob leaf_query.py\n",
      "\t\t\t\tfound blob retrieve_query.py\n",
      "\t\t\t\tfound blob summarize_query.py\n",
      "\t\t\trecursing into vector_store\n",
      "\t\t\t\tcurrent path: gpt_index/indices/query/vector_store\n",
      "\t\t\t\tprocessing tree 118f6392ed0cf50e1e998f80ce95b3c410fc938f\n",
      "\t\t\t\tfound blob __init__.py\n",
      "\t\t\t\tfound blob base.py\n",
      "\t\t\t\tfound blob faiss.py\n",
      "\t\t\t\tfound blob pinecone.py\n",
      "\t\t\t\tfound blob qdrant.py\n",
      "\t\t\t\tfound blob simple.py\n",
      "\t\t\t\tfound blob weaviate.py\n",
      "\t\tfound blob registry.py\n",
      "\t\trecursing into response\n",
      "\t\t\tcurrent path: gpt_index/indices/response\n",
      "\t\t\tprocessing tree efd649581a7a711a959fb43a260ffdf3f9b31b13\n",
      "\t\t\tfound blob __init__.py\n",
      "\t\t\tfound blob builder.py\n",
      "\t\trecursing into struct_store\n",
      "\t\t\tcurrent path: gpt_index/indices/struct_store\n",
      "\t\t\tprocessing tree 7ae51aee52c84e9d525da9434471a02745dc5e3f\n",
      "\t\t\tfound blob __init__.py\n",
      "\t\t\tfound blob base.py\n",
      "\t\t\tfound blob sql.py\n",
      "\t\trecursing into tree\n",
      "\t\t\tcurrent path: gpt_index/indices/tree\n",
      "\t\t\tprocessing tree 88dbb31520189edd36c296db36bff9c0006a857c\n",
      "\t\t\tfound blob README.md\n",
      "\t\t\tfound blob __init__.py\n",
      "\t\t\tfound blob base.py\n",
      "\t\t\tfound blob inserter.py\n",
      "\t\tfound blob utils.py\n",
      "\t\trecursing into vector_store\n",
      "\t\t\tcurrent path: gpt_index/indices/vector_store\n",
      "\t\t\tprocessing tree 32e859ab682876460c42afa72c553a63f73ffedf\n",
      "\t\t\tfound blob __init__.py\n",
      "\t\t\tfound blob base.py\n",
      "\t\t\tfound blob faiss.py\n",
      "\t\t\tfound blob pinecone.py\n",
      "\t\t\tfound blob qdrant.py\n",
      "\t\t\tfound blob simple.py\n",
      "\t\t\tfound blob weaviate.py\n",
      "\trecursing into langchain_helpers\n",
      "\t\tcurrent path: gpt_index/langchain_helpers\n",
      "\t\tprocessing tree 2327b9687265081f55c356c9843e6e077fd37327\n",
      "\t\tfound blob __init__.py\n",
      "\t\tfound blob chain_wrapper.py\n",
      "\t\tfound blob memory_wrapper.py\n",
      "\t\tfound blob sql_wrapper.py\n",
      "\t\tfound blob text_splitter.py\n",
      "\trecursing into prompts\n",
      "\t\tcurrent path: gpt_index/prompts\n",
      "\t\tprocessing tree 5991bdac6c900804e8eedb2f8eb8f7e758f1d4de\n",
      "\t\tfound blob __init__.py\n",
      "\t\tfound blob base.py\n",
      "\t\tfound blob default_prompts.py\n",
      "\t\tfound blob prompt_type.py\n",
      "\t\tfound blob prompts.py\n",
      "\tfound blob py.typed\n",
      "\trecursing into readers\n",
      "\t\tcurrent path: gpt_index/readers\n",
      "\t\tprocessing tree 006c3fe29cd5b98ba1c990c9d80dd4c1f6f0bb12\n",
      "\t\tfound blob __init__.py\n",
      "\t\tfound blob base.py\n",
      "\t\tfound blob database.py\n",
      "\t\tfound blob discord_reader.py\n",
      "\t\tfound blob download.py\n",
      "\t\tfound blob faiss.py\n",
      "\t\trecursing into file\n",
      "\t\t\tcurrent path: gpt_index/readers/file\n",
      "\t\t\tprocessing tree add9a6f4b33cc9bed15b25caf83258eaf7a511c5\n",
      "\t\t\tfound blob __init__.py\n",
      "\t\t\tfound blob base.py\n",
      "\t\t\tfound blob base_parser.py\n",
      "\t\t\tfound blob docs_parser.py\n",
      "\t\t\tfound blob epub_parser.py\n",
      "\t\t\tfound blob image_parser.py\n",
      "\t\t\tfound blob markdown_parser.py\n",
      "\t\t\tfound blob mbox_parser.py\n",
      "\t\t\tfound blob slides_parser.py\n",
      "\t\t\tfound blob tabular_parser.py\n",
      "\t\t\tfound blob video_audio.py\n",
      "\t\trecursing into github_readers\n",
      "\t\t\tcurrent path: gpt_index/readers/github_readers\n",
      "\t\t\tprocessing tree a56a6ec9f43c1b99e0ebfc8bf27043b3c1256cf8\n",
      "\t\t\tfound blob __init__.py\n",
      "\t\t\tfound blob github_api_client.py\n",
      "\t\t\tfound blob github_repository_reader.py\n",
      "\t\t\tfound blob utils.py\n",
      "\t\trecursing into google_readers\n",
      "\t\t\tcurrent path: gpt_index/readers/google_readers\n",
      "\t\t\tprocessing tree f5c4b5c85a90733503366765f0294f1f967fa245\n",
      "\t\t\tfound blob __init__.py\n",
      "\t\t\tfound blob gdocs.py\n",
      "\t\trecursing into make_com\n",
      "\t\t\tcurrent path: gpt_index/readers/make_com\n",
      "\t\t\tprocessing tree b5fb97505bbb062ea1370aeca5804e53a110de0e\n",
      "\t\t\tfound blob __init__.py\n",
      "\t\t\tfound blob wrapper.py\n",
      "\t\tfound blob mbox.py\n",
      "\t\tfound blob mongo.py\n",
      "\t\tfound blob notion.py\n",
      "\t\tfound blob obsidian.py\n",
      "\t\tfound blob pinecone.py\n",
      "\t\tfound blob qdrant.py\n",
      "\t\trecursing into schema\n",
      "\t\t\tcurrent path: gpt_index/readers/schema\n",
      "\t\t\tprocessing tree 1519b4b981468d49639d47d0cbe58103d57d9e86\n",
      "\t\t\tfound blob __init__.py\n",
      "\t\t\tfound blob base.py\n",
      "\t\tfound blob slack.py\n",
      "\t\tfound blob string_iterable.py\n",
      "\t\tfound blob twitter.py\n",
      "\t\trecursing into weaviate\n",
      "\t\t\tcurrent path: gpt_index/readers/weaviate\n",
      "\t\t\tprocessing tree 168da8e43c77ef09ed2a602fc733b787d3a9cb11\n",
      "\t\t\tfound blob __init__.py\n",
      "\t\t\tfound blob data_structs.py\n",
      "\t\t\tfound blob reader.py\n",
      "\t\t\tfound blob utils.py\n",
      "\t\tfound blob web.py\n",
      "\t\tfound blob wikipedia.py\n",
      "\t\tfound blob youtube_transcript.py\n",
      "\trecursing into response\n",
      "\t\tcurrent path: gpt_index/response\n",
      "\t\tprocessing tree 5c639a4a4d646446a36b4083600686615cd16ae0\n",
      "\t\tfound blob __init__.py\n",
      "\t\tfound blob schema.py\n",
      "\tfound blob schema.py\n",
      "\trecursing into token_counter\n",
      "\t\tcurrent path: gpt_index/token_counter\n",
      "\t\tprocessing tree 730723d1efb8d8109ff2c87b9bf246d764fdcc02\n",
      "\t\tfound blob __init__.py\n",
      "\t\tfound blob mock_chain_wrapper.py\n",
      "\t\tfound blob mock_embed_model.py\n",
      "\t\tfound blob token_counter.py\n",
      "\t\tfound blob utils.py\n",
      "\tfound blob utils.py\n",
      "found blob pyproject.toml\n",
      "found blob requirements.txt\n",
      "found blob setup.py\n",
      "recursing into tests\n",
      "\tcurrent path: tests\n",
      "\tprocessing tree f56ac80005a420776f71011ac3fec72569d7d2c2\n",
      "\tfound blob __init__.py\n",
      "\trecursing into indices\n",
      "\t\tcurrent path: tests/indices\n",
      "\t\tprocessing tree e348a31198da7c45dfabfb2164eeb21a41d41db2\n",
      "\t\tfound blob __init__.py\n",
      "\t\trecursing into embedding\n",
      "\t\t\tcurrent path: tests/indices/embedding\n",
      "\t\t\tprocessing tree feec89724d43884b91d8ee0b71a28b629fbbb2f1\n",
      "\t\t\tfound blob __init__.py\n",
      "\t\t\tfound blob test_base.py\n",
      "\t\trecursing into keyword_table\n",
      "\t\t\tcurrent path: tests/indices/keyword_table\n",
      "\t\t\tprocessing tree 2b468811deed3c3a493c40b7771460aa690ce1f8\n",
      "\t\t\tfound blob __init__.py\n",
      "\t\t\tfound blob test_base.py\n",
      "\t\t\tfound blob test_utils.py\n",
      "\t\trecursing into list\n",
      "\t\t\tcurrent path: tests/indices/list\n",
      "\t\t\tprocessing tree 167b09b62f93492ab1a10c1fc43f37741f51b6ac\n",
      "\t\t\tfound blob __init__.py\n",
      "\t\t\tfound blob test_base.py\n",
      "\t\trecursing into query\n",
      "\t\t\tcurrent path: tests/indices/query\n",
      "\t\t\tprocessing tree 983664779269b90031c3c6aaaaf2e80103feeb50\n",
      "\t\t\tfound blob __init__.py\n",
      "\t\t\tfound blob test_query_runner.py\n",
      "\t\t\tfound blob test_recursive.py\n",
      "\t\trecursing into struct_store\n",
      "\t\t\tcurrent path: tests/indices/struct_store\n",
      "\t\t\tprocessing tree e2845edb4acdf304bff04d9cb721f57abc9ca158\n",
      "\t\t\tfound blob __init__.py\n",
      "\t\t\tfound blob test_base.py\n",
      "\t\tfound blob test_node_utils.py\n",
      "\t\tfound blob test_prompt_helper.py\n",
      "\t\tfound blob test_response.py\n",
      "\t\tfound blob test_utils.py\n",
      "\t\trecursing into tree\n",
      "\t\t\tcurrent path: tests/indices/tree\n",
      "\t\t\tprocessing tree 0d7457744169bcaebf7992e0c6cc61731f6c240e\n",
      "\t\t\tfound blob __init__.py\n",
      "\t\t\tfound blob test_base.py\n",
      "\t\trecursing into vector_store\n",
      "\t\t\tcurrent path: tests/indices/vector_store\n",
      "\t\t\tprocessing tree efc59db1b74f12edaec7ce5b3724f8af8348d948\n",
      "\t\t\tfound blob __init__.py\n",
      "\t\t\tfound blob test_base.py\n",
      "\trecursing into langchain_helpers\n",
      "\t\tcurrent path: tests/langchain_helpers\n",
      "\t\tprocessing tree 84981102c9bb17e25e3665ab2bb929949b2e97b6\n",
      "\t\tfound blob __init__.py\n",
      "\t\tfound blob test_text_splitter.py\n",
      "\trecursing into mock_utils\n",
      "\t\tcurrent path: tests/mock_utils\n",
      "\t\tprocessing tree d7930a673527be5b6e05970fad9401bce6f2ef12\n",
      "\t\tfound blob __init__.py\n",
      "\t\tfound blob mock_decorator.py\n",
      "\t\tfound blob mock_predict.py\n",
      "\t\tfound blob mock_prompts.py\n",
      "\t\tfound blob mock_text_splitter.py\n",
      "\t\tfound blob mock_utils.py\n",
      "\trecursing into prompts\n",
      "\t\tcurrent path: tests/prompts\n",
      "\t\tprocessing tree 9fdbb59919c18d63fe64b2ed5d659a8a9e629142\n",
      "\t\tfound blob __init__.py\n",
      "\t\tfound blob test_base.py\n",
      "\trecursing into readers\n",
      "\t\tcurrent path: tests/readers\n",
      "\t\tprocessing tree bc14751635b4c559dc298412d9aad44c9a5a05be\n",
      "\t\tfound blob __init__.py\n",
      "\t\tfound blob test_file.py\n",
      "\t\tfound blob test_string_iterable.py\n",
      "\tfound blob test_docstore.py\n",
      "\tfound blob test_utils.py\n",
      "\trecursing into token_predictor\n",
      "\t\tcurrent path: tests/token_predictor\n",
      "\t\tprocessing tree e6c819445fca6b8a65e0620fa22682e5f3d94614\n",
      "\t\tfound blob __init__.py\n",
      "\t\tfound blob test_base.py\n",
      "got 196 blobs\n",
      "Time to get blobs ([('.flake8', 290), ('build_package.yml', 974), ('lint.yml', 711), ('unit_test.yml', 727), ('.gitignore', 1876)]): 0.72 seconds\n",
      "generating document for .flake8\n",
      "could not parse .flake8 as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 290 characters- adding to documents - .flake8\n",
      "generating document for .github/workflows/build_package.yml\n",
      "could not parse .github/workflows/build_package.yml as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 974 characters- adding to documents - .github/workflows/build_package.yml\n",
      "generating document for .github/workflows/lint.yml\n",
      "could not parse .github/workflows/lint.yml as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 711 characters- adding to documents - .github/workflows/lint.yml\n",
      "generating document for .github/workflows/unit_test.yml\n",
      "could not parse .github/workflows/unit_test.yml as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 727 characters- adding to documents - .github/workflows/unit_test.yml\n",
      "generating document for .gitignore\n",
      "could not parse .gitignore as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1876 characters- adding to documents - .gitignore\n",
      "Time to get blobs ([('.readthedocs.yaml', 193), ('CITATION.cff', 298), ('CONTRIBUTING.md', 2434), ('LICENSE', 1064), ('MANIFEST.in', 68)]): 0.61 seconds\n",
      "generating document for .readthedocs.yaml\n",
      "could not parse .readthedocs.yaml as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 193 characters- adding to documents - .readthedocs.yaml\n",
      "generating document for CITATION.cff\n",
      "could not parse CITATION.cff as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 298 characters- adding to documents - CITATION.cff\n",
      "generating document for CONTRIBUTING.md\n",
      "parsing CONTRIBUTING.mdas .md with MarkdownParser\n",
      "created a temporary file/tmp/tmpf2jg011g/tmpo6m89zgj..md for parsing CONTRIBUTING.md\n",
      "generating document for LICENSE\n",
      "could not parse LICENSE as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1064 characters- adding to documents - LICENSE\n",
      "generating document for MANIFEST.in\n",
      "could not parse MANIFEST.in as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 68 characters- adding to documents - MANIFEST.in\n",
      "Time to get blobs ([('Makefile', 126), ('README.md', 3175), ('data_requirements.txt', 149), ('README.md', 225), ('TitanicModel.ipynb', 14249)]): 0.65 seconds\n",
      "generating document for Makefile\n",
      "could not parse Makefile as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 126 characters- adding to documents - Makefile\n",
      "generating document for README.md\n",
      "parsing README.mdas .md with MarkdownParser\n",
      "created a temporary file/tmp/tmprafq01_f/tmpzghymfr6..md for parsing README.md\n",
      "generating document for data_requirements.txt\n",
      "could not parse data_requirements.txt as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 149 characters- adding to documents - data_requirements.txt\n",
      "generating document for experimental/README.md\n",
      "parsing experimental/README.mdas .md with MarkdownParser\n",
      "created a temporary file/tmp/tmp3iq76r_t/tmpslmlkj7d..md for parsing experimental/README.md\n",
      "generating document for experimental/classifier/TitanicModel.ipynb\n",
      "could not parse experimental/classifier/TitanicModel.ipynb as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 14249 characters- adding to documents - experimental/classifier/TitanicModel.ipynb\n",
      "Time to get blobs ([('utils.py', 5814), ('VERSION', 6), ('__init__.py', 3805), ('__init__.py', 170), ('graph.py', 7661)]): 0.59 seconds\n",
      "generating document for experimental/classifier/utils.py\n",
      "could not parse experimental/classifier/utils.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 5814 characters- adding to documents - experimental/classifier/utils.py\n",
      "generating document for gpt_index/VERSION\n",
      "could not parse gpt_index/VERSION as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 6 characters- adding to documents - gpt_index/VERSION\n",
      "generating document for gpt_index/__init__.py\n",
      "could not parse gpt_index/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 3805 characters- adding to documents - gpt_index/__init__.py\n",
      "generating document for gpt_index/composability/__init__.py\n",
      "could not parse gpt_index/composability/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 170 characters- adding to documents - gpt_index/composability/__init__.py\n",
      "generating document for gpt_index/composability/graph.py\n",
      "could not parse gpt_index/composability/graph.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 7661 characters- adding to documents - gpt_index/composability/graph.py\n",
      "Time to get blobs ([('constants.py', 89), ('__init__.py', 467), ('data_structs.py', 9531), ('struct_type.py', 1943), ('table.py', 722)]): 0.68 seconds\n",
      "generating document for gpt_index/constants.py\n",
      "could not parse gpt_index/constants.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 89 characters- adding to documents - gpt_index/constants.py\n",
      "generating document for gpt_index/data_structs/__init__.py\n",
      "could not parse gpt_index/data_structs/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 467 characters- adding to documents - gpt_index/data_structs/__init__.py\n",
      "generating document for gpt_index/data_structs/data_structs.py\n",
      "could not parse gpt_index/data_structs/data_structs.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 9531 characters- adding to documents - gpt_index/data_structs/data_structs.py\n",
      "generating document for gpt_index/data_structs/struct_type.py\n",
      "could not parse gpt_index/data_structs/struct_type.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1943 characters- adding to documents - gpt_index/data_structs/struct_type.py\n",
      "generating document for gpt_index/data_structs/table.py\n",
      "could not parse gpt_index/data_structs/table.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 722 characters- adding to documents - gpt_index/data_structs/table.py\n",
      "Time to get blobs ([('docstore.py', 4385), ('__init__.py', 17), ('base.py', 2631), ('langchain.py', 904), ('openai.py', 5951)]): 0.59 seconds\n",
      "generating document for gpt_index/docstore.py\n",
      "could not parse gpt_index/docstore.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 4385 characters- adding to documents - gpt_index/docstore.py\n",
      "generating document for gpt_index/embeddings/__init__.py\n",
      "could not parse gpt_index/embeddings/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 17 characters- adding to documents - gpt_index/embeddings/__init__.py\n",
      "generating document for gpt_index/embeddings/base.py\n",
      "could not parse gpt_index/embeddings/base.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 2631 characters- adding to documents - gpt_index/embeddings/base.py\n",
      "generating document for gpt_index/embeddings/langchain.py\n",
      "could not parse gpt_index/embeddings/langchain.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 904 characters- adding to documents - gpt_index/embeddings/langchain.py\n",
      "generating document for gpt_index/embeddings/openai.py\n",
      "could not parse gpt_index/embeddings/openai.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 5951 characters- adding to documents - gpt_index/embeddings/openai.py\n",
      "Time to get blobs ([('utils.py', 558), ('__init__.py', 615), ('base.py', 18445), ('__init__.py', 17), ('__init__.py', 19)]): 0.61 seconds\n",
      "generating document for gpt_index/embeddings/utils.py\n",
      "could not parse gpt_index/embeddings/utils.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 558 characters- adding to documents - gpt_index/embeddings/utils.py\n",
      "generating document for gpt_index/indices/__init__.py\n",
      "could not parse gpt_index/indices/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 615 characters- adding to documents - gpt_index/indices/__init__.py\n",
      "generating document for gpt_index/indices/base.py\n",
      "could not parse gpt_index/indices/base.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 18445 characters- adding to documents - gpt_index/indices/base.py\n",
      "generating document for gpt_index/indices/common/__init__.py\n",
      "could not parse gpt_index/indices/common/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 17 characters- adding to documents - gpt_index/indices/common/__init__.py\n",
      "generating document for gpt_index/indices/common/struct_store/__init__.py\n",
      "could not parse gpt_index/indices/common/struct_store/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 19 characters- adding to documents - gpt_index/indices/common/struct_store/__init__.py\n",
      "Time to get blobs ([('base.py', 3947), ('__init__.py', 17), ('base.py', 4176), ('README.md', 2462), ('__init__.py', 395)]): 0.53 seconds\n",
      "generating document for gpt_index/indices/common/struct_store/base.py\n",
      "could not parse gpt_index/indices/common/struct_store/base.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 3947 characters- adding to documents - gpt_index/indices/common/struct_store/base.py\n",
      "generating document for gpt_index/indices/common/tree/__init__.py\n",
      "could not parse gpt_index/indices/common/tree/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 17 characters- adding to documents - gpt_index/indices/common/tree/__init__.py\n",
      "generating document for gpt_index/indices/common/tree/base.py\n",
      "could not parse gpt_index/indices/common/tree/base.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 4176 characters- adding to documents - gpt_index/indices/common/tree/base.py\n",
      "generating document for gpt_index/indices/keyword_table/README.md\n",
      "parsing gpt_index/indices/keyword_table/README.mdas .md with MarkdownParser\n",
      "created a temporary file/tmp/tmpxcix2w3t/tmpu1sy1ya7..md for parsing gpt_index/indices/keyword_table/README.md\n",
      "generating document for gpt_index/indices/keyword_table/__init__.py\n",
      "could not parse gpt_index/indices/keyword_table/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 395 characters- adding to documents - gpt_index/indices/keyword_table/__init__.py\n",
      "Time to get blobs ([('base.py', 6107), ('rake_base.py', 646), ('simple_base.py', 838), ('utils.py', 2145), ('README.md', 1085)]): 0.70 seconds\n",
      "generating document for gpt_index/indices/keyword_table/base.py\n",
      "could not parse gpt_index/indices/keyword_table/base.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 6107 characters- adding to documents - gpt_index/indices/keyword_table/base.py\n",
      "generating document for gpt_index/indices/keyword_table/rake_base.py\n",
      "could not parse gpt_index/indices/keyword_table/rake_base.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 646 characters- adding to documents - gpt_index/indices/keyword_table/rake_base.py\n",
      "generating document for gpt_index/indices/keyword_table/simple_base.py\n",
      "could not parse gpt_index/indices/keyword_table/simple_base.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 838 characters- adding to documents - gpt_index/indices/keyword_table/simple_base.py\n",
      "generating document for gpt_index/indices/keyword_table/utils.py\n",
      "could not parse gpt_index/indices/keyword_table/utils.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 2145 characters- adding to documents - gpt_index/indices/keyword_table/utils.py\n",
      "generating document for gpt_index/indices/list/README.md\n",
      "parsing gpt_index/indices/list/README.mdas .md with MarkdownParser\n",
      "created a temporary file/tmp/tmp2s8437f6/tmpl9774dpv..md for parsing gpt_index/indices/list/README.md\n",
      "Time to get blobs ([('__init__.py', 123), ('base.py', 3843), ('node_utils.py', 1587), ('prompt_helper.py', 8270), ('__init__.py', 17)]): 0.63 seconds\n",
      "generating document for gpt_index/indices/list/__init__.py\n",
      "could not parse gpt_index/indices/list/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 123 characters- adding to documents - gpt_index/indices/list/__init__.py\n",
      "generating document for gpt_index/indices/list/base.py\n",
      "could not parse gpt_index/indices/list/base.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 3843 characters- adding to documents - gpt_index/indices/list/base.py\n",
      "generating document for gpt_index/indices/node_utils.py\n",
      "could not parse gpt_index/indices/node_utils.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1587 characters- adding to documents - gpt_index/indices/node_utils.py\n",
      "generating document for gpt_index/indices/prompt_helper.py\n",
      "could not parse gpt_index/indices/prompt_helper.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 8270 characters- adding to documents - gpt_index/indices/prompt_helper.py\n",
      "generating document for gpt_index/indices/query/__init__.py\n",
      "could not parse gpt_index/indices/query/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 17 characters- adding to documents - gpt_index/indices/query/__init__.py\n",
      "Time to get blobs ([('base.py', 11254), ('embedding_utils.py', 2193), ('__init__.py', 311), ('query.py', 6145), ('__init__.py', 251)]): 0.67 seconds\n",
      "generating document for gpt_index/indices/query/base.py\n",
      "could not parse gpt_index/indices/query/base.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 11254 characters- adding to documents - gpt_index/indices/query/base.py\n",
      "generating document for gpt_index/indices/query/embedding_utils.py\n",
      "could not parse gpt_index/indices/query/embedding_utils.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 2193 characters- adding to documents - gpt_index/indices/query/embedding_utils.py\n",
      "generating document for gpt_index/indices/query/keyword_table/__init__.py\n",
      "could not parse gpt_index/indices/query/keyword_table/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 311 characters- adding to documents - gpt_index/indices/query/keyword_table/__init__.py\n",
      "generating document for gpt_index/indices/query/keyword_table/query.py\n",
      "could not parse gpt_index/indices/query/keyword_table/query.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 6145 characters- adding to documents - gpt_index/indices/query/keyword_table/query.py\n",
      "generating document for gpt_index/indices/query/list/__init__.py\n",
      "could not parse gpt_index/indices/query/list/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 251 characters- adding to documents - gpt_index/indices/query/list/__init__.py\n",
      "Time to get blobs ([('embedding_query.py', 2963), ('query.py', 1356), ('query_runner.py', 3528), ('schema.py', 3141), ('__init__.py', 17)]): 0.59 seconds\n",
      "generating document for gpt_index/indices/query/list/embedding_query.py\n",
      "could not parse gpt_index/indices/query/list/embedding_query.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 2963 characters- adding to documents - gpt_index/indices/query/list/embedding_query.py\n",
      "generating document for gpt_index/indices/query/list/query.py\n",
      "could not parse gpt_index/indices/query/list/query.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1356 characters- adding to documents - gpt_index/indices/query/list/query.py\n",
      "generating document for gpt_index/indices/query/query_runner.py\n",
      "could not parse gpt_index/indices/query/query_runner.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 3528 characters- adding to documents - gpt_index/indices/query/query_runner.py\n",
      "generating document for gpt_index/indices/query/schema.py\n",
      "could not parse gpt_index/indices/query/schema.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 3141 characters- adding to documents - gpt_index/indices/query/schema.py\n",
      "generating document for gpt_index/indices/query/struct_store/__init__.py\n",
      "could not parse gpt_index/indices/query/struct_store/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 17 characters- adding to documents - gpt_index/indices/query/struct_store/__init__.py\n",
      "Time to get blobs ([('sql.py', 4463), ('__init__.py', 380), ('embedding_query.py', 4365), ('leaf_query.py', 7593), ('retrieve_query.py', 1526)]): 0.63 seconds\n",
      "generating document for gpt_index/indices/query/struct_store/sql.py\n",
      "could not parse gpt_index/indices/query/struct_store/sql.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 4463 characters- adding to documents - gpt_index/indices/query/struct_store/sql.py\n",
      "generating document for gpt_index/indices/query/tree/__init__.py\n",
      "could not parse gpt_index/indices/query/tree/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 380 characters- adding to documents - gpt_index/indices/query/tree/__init__.py\n",
      "generating document for gpt_index/indices/query/tree/embedding_query.py\n",
      "could not parse gpt_index/indices/query/tree/embedding_query.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 4365 characters- adding to documents - gpt_index/indices/query/tree/embedding_query.py\n",
      "generating document for gpt_index/indices/query/tree/leaf_query.py\n",
      "could not parse gpt_index/indices/query/tree/leaf_query.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 7593 characters- adding to documents - gpt_index/indices/query/tree/leaf_query.py\n",
      "generating document for gpt_index/indices/query/tree/retrieve_query.py\n",
      "could not parse gpt_index/indices/query/tree/retrieve_query.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1526 characters- adding to documents - gpt_index/indices/query/tree/retrieve_query.py\n",
      "Time to get blobs ([('summarize_query.py', 2008), ('__init__.py', 371), ('base.py', 779), ('faiss.py', 3443), ('pinecone.py', 3622)]): 0.52 seconds\n",
      "generating document for gpt_index/indices/query/tree/summarize_query.py\n",
      "could not parse gpt_index/indices/query/tree/summarize_query.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 2008 characters- adding to documents - gpt_index/indices/query/tree/summarize_query.py\n",
      "generating document for gpt_index/indices/query/vector_store/__init__.py\n",
      "could not parse gpt_index/indices/query/vector_store/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 371 characters- adding to documents - gpt_index/indices/query/vector_store/__init__.py\n",
      "generating document for gpt_index/indices/query/vector_store/base.py\n",
      "could not parse gpt_index/indices/query/vector_store/base.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 779 characters- adding to documents - gpt_index/indices/query/vector_store/base.py\n",
      "generating document for gpt_index/indices/query/vector_store/faiss.py\n",
      "could not parse gpt_index/indices/query/vector_store/faiss.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 3443 characters- adding to documents - gpt_index/indices/query/vector_store/faiss.py\n",
      "generating document for gpt_index/indices/query/vector_store/pinecone.py\n",
      "could not parse gpt_index/indices/query/vector_store/pinecone.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 3622 characters- adding to documents - gpt_index/indices/query/vector_store/pinecone.py\n",
      "Time to get blobs ([('qdrant.py', 3323), ('simple.py', 2762), ('weaviate.py', 2390), ('registry.py', 1001), ('__init__.py', 17)]): 0.63 seconds\n",
      "generating document for gpt_index/indices/query/vector_store/qdrant.py\n",
      "could not parse gpt_index/indices/query/vector_store/qdrant.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 3323 characters- adding to documents - gpt_index/indices/query/vector_store/qdrant.py\n",
      "generating document for gpt_index/indices/query/vector_store/simple.py\n",
      "could not parse gpt_index/indices/query/vector_store/simple.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 2762 characters- adding to documents - gpt_index/indices/query/vector_store/simple.py\n",
      "generating document for gpt_index/indices/query/vector_store/weaviate.py\n",
      "could not parse gpt_index/indices/query/vector_store/weaviate.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 2390 characters- adding to documents - gpt_index/indices/query/vector_store/weaviate.py\n",
      "generating document for gpt_index/indices/registry.py\n",
      "could not parse gpt_index/indices/registry.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1001 characters- adding to documents - gpt_index/indices/registry.py\n",
      "generating document for gpt_index/indices/response/__init__.py\n",
      "could not parse gpt_index/indices/response/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 17 characters- adding to documents - gpt_index/indices/response/__init__.py\n",
      "Time to get blobs ([('builder.py', 8737), ('__init__.py', 141), ('base.py', 5744), ('sql.py', 7681), ('README.md', 2741)]): 0.58 seconds\n",
      "generating document for gpt_index/indices/response/builder.py\n",
      "could not parse gpt_index/indices/response/builder.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 8737 characters- adding to documents - gpt_index/indices/response/builder.py\n",
      "generating document for gpt_index/indices/struct_store/__init__.py\n",
      "could not parse gpt_index/indices/struct_store/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 141 characters- adding to documents - gpt_index/indices/struct_store/__init__.py\n",
      "generating document for gpt_index/indices/struct_store/base.py\n",
      "could not parse gpt_index/indices/struct_store/base.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 5744 characters- adding to documents - gpt_index/indices/struct_store/base.py\n",
      "generating document for gpt_index/indices/struct_store/sql.py\n",
      "could not parse gpt_index/indices/struct_store/sql.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 7681 characters- adding to documents - gpt_index/indices/struct_store/sql.py\n",
      "generating document for gpt_index/indices/tree/README.md\n",
      "parsing gpt_index/indices/tree/README.mdas .md with MarkdownParser\n",
      "created a temporary file/tmp/tmpkg0ddpxr/tmp2hw0pu00..md for parsing gpt_index/indices/tree/README.md\n",
      "Time to get blobs ([('__init__.py', 144), ('base.py', 5195), ('inserter.py', 6867), ('utils.py', 1282), ('__init__.py', 514)]): 0.72 seconds\n",
      "generating document for gpt_index/indices/tree/__init__.py\n",
      "could not parse gpt_index/indices/tree/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 144 characters- adding to documents - gpt_index/indices/tree/__init__.py\n",
      "generating document for gpt_index/indices/tree/base.py\n",
      "could not parse gpt_index/indices/tree/base.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 5195 characters- adding to documents - gpt_index/indices/tree/base.py\n",
      "generating document for gpt_index/indices/tree/inserter.py\n",
      "could not parse gpt_index/indices/tree/inserter.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 6867 characters- adding to documents - gpt_index/indices/tree/inserter.py\n",
      "generating document for gpt_index/indices/utils.py\n",
      "could not parse gpt_index/indices/utils.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1282 characters- adding to documents - gpt_index/indices/utils.py\n",
      "generating document for gpt_index/indices/vector_store/__init__.py\n",
      "could not parse gpt_index/indices/vector_store/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 514 characters- adding to documents - gpt_index/indices/vector_store/__init__.py\n",
      "Time to get blobs ([('base.py', 2869), ('faiss.py', 7463), ('pinecone.py', 5622), ('qdrant.py', 7973), ('simple.py', 4147)]): 1.59 seconds\n",
      "generating document for gpt_index/indices/vector_store/base.py\n",
      "could not parse gpt_index/indices/vector_store/base.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 2869 characters- adding to documents - gpt_index/indices/vector_store/base.py\n",
      "generating document for gpt_index/indices/vector_store/faiss.py\n",
      "could not parse gpt_index/indices/vector_store/faiss.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 7463 characters- adding to documents - gpt_index/indices/vector_store/faiss.py\n",
      "generating document for gpt_index/indices/vector_store/pinecone.py\n",
      "could not parse gpt_index/indices/vector_store/pinecone.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 5622 characters- adding to documents - gpt_index/indices/vector_store/pinecone.py\n",
      "generating document for gpt_index/indices/vector_store/qdrant.py\n",
      "could not parse gpt_index/indices/vector_store/qdrant.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 7973 characters- adding to documents - gpt_index/indices/vector_store/qdrant.py\n",
      "generating document for gpt_index/indices/vector_store/simple.py\n",
      "could not parse gpt_index/indices/vector_store/simple.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 4147 characters- adding to documents - gpt_index/indices/vector_store/simple.py\n",
      "Time to get blobs ([('weaviate.py', 5611), ('__init__.py', 39), ('chain_wrapper.py', 5211), ('memory_wrapper.py', 3343), ('sql_wrapper.py', 2713)]): 0.69 seconds\n",
      "generating document for gpt_index/indices/vector_store/weaviate.py\n",
      "could not parse gpt_index/indices/vector_store/weaviate.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 5611 characters- adding to documents - gpt_index/indices/vector_store/weaviate.py\n",
      "generating document for gpt_index/langchain_helpers/__init__.py\n",
      "could not parse gpt_index/langchain_helpers/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 39 characters- adding to documents - gpt_index/langchain_helpers/__init__.py\n",
      "generating document for gpt_index/langchain_helpers/chain_wrapper.py\n",
      "could not parse gpt_index/langchain_helpers/chain_wrapper.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 5211 characters- adding to documents - gpt_index/langchain_helpers/chain_wrapper.py\n",
      "generating document for gpt_index/langchain_helpers/memory_wrapper.py\n",
      "could not parse gpt_index/langchain_helpers/memory_wrapper.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 3343 characters- adding to documents - gpt_index/langchain_helpers/memory_wrapper.py\n",
      "generating document for gpt_index/langchain_helpers/sql_wrapper.py\n",
      "could not parse gpt_index/langchain_helpers/sql_wrapper.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 2713 characters- adding to documents - gpt_index/langchain_helpers/sql_wrapper.py\n",
      "Time to get blobs ([('text_splitter.py', 9030), ('__init__.py', 85), ('base.py', 4237), ('default_prompts.py', 8254), ('prompt_type.py', 800)]): 0.54 seconds\n",
      "generating document for gpt_index/langchain_helpers/text_splitter.py\n",
      "could not parse gpt_index/langchain_helpers/text_splitter.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 9030 characters- adding to documents - gpt_index/langchain_helpers/text_splitter.py\n",
      "generating document for gpt_index/prompts/__init__.py\n",
      "could not parse gpt_index/prompts/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 85 characters- adding to documents - gpt_index/prompts/__init__.py\n",
      "generating document for gpt_index/prompts/base.py\n",
      "could not parse gpt_index/prompts/base.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 4237 characters- adding to documents - gpt_index/prompts/base.py\n",
      "generating document for gpt_index/prompts/default_prompts.py\n",
      "could not parse gpt_index/prompts/default_prompts.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 8254 characters- adding to documents - gpt_index/prompts/default_prompts.py\n",
      "generating document for gpt_index/prompts/prompt_type.py\n",
      "could not parse gpt_index/prompts/prompt_type.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 800 characters- adding to documents - gpt_index/prompts/prompt_type.py\n",
      "Time to get blobs ([('prompts.py', 6646), ('py.typed', 0), ('__init__.py', 2263), ('base.py', 657), ('database.py', 3219)]): 0.51 seconds\n",
      "generating document for gpt_index/prompts/prompts.py\n",
      "could not parse gpt_index/prompts/prompts.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 6646 characters- adding to documents - gpt_index/prompts/prompts.py\n",
      "generating document for gpt_index/py.typed\n",
      "could not parse gpt_index/py.typed as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 0 characters- adding to documents - gpt_index/py.typed\n",
      "generating document for gpt_index/readers/__init__.py\n",
      "could not parse gpt_index/readers/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 2263 characters- adding to documents - gpt_index/readers/__init__.py\n",
      "generating document for gpt_index/readers/base.py\n",
      "could not parse gpt_index/readers/base.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 657 characters- adding to documents - gpt_index/readers/base.py\n",
      "generating document for gpt_index/readers/database.py\n",
      "could not parse gpt_index/readers/database.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 3219 characters- adding to documents - gpt_index/readers/database.py\n",
      "Time to get blobs ([('discord_reader.py', 4942), ('download.py', 2411), ('faiss.py', 2472), ('__init__.py', 19), ('base.py', 6473)]): 0.50 seconds\n",
      "generating document for gpt_index/readers/discord_reader.py\n",
      "could not parse gpt_index/readers/discord_reader.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 4942 characters- adding to documents - gpt_index/readers/discord_reader.py\n",
      "generating document for gpt_index/readers/download.py\n",
      "could not parse gpt_index/readers/download.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 2411 characters- adding to documents - gpt_index/readers/download.py\n",
      "generating document for gpt_index/readers/faiss.py\n",
      "could not parse gpt_index/readers/faiss.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 2472 characters- adding to documents - gpt_index/readers/faiss.py\n",
      "generating document for gpt_index/readers/file/__init__.py\n",
      "could not parse gpt_index/readers/file/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 19 characters- adding to documents - gpt_index/readers/file/__init__.py\n",
      "generating document for gpt_index/readers/file/base.py\n",
      "could not parse gpt_index/readers/file/base.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 6473 characters- adding to documents - gpt_index/readers/file/base.py\n",
      "Time to get blobs ([('base_parser.py', 1115), ('docs_parser.py', 1502), ('epub_parser.py', 1207), ('image_parser.py', 2995), ('markdown_parser.py', 3596)]): 0.60 seconds\n",
      "generating document for gpt_index/readers/file/base_parser.py\n",
      "could not parse gpt_index/readers/file/base_parser.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1115 characters- adding to documents - gpt_index/readers/file/base_parser.py\n",
      "generating document for gpt_index/readers/file/docs_parser.py\n",
      "could not parse gpt_index/readers/file/docs_parser.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1502 characters- adding to documents - gpt_index/readers/file/docs_parser.py\n",
      "generating document for gpt_index/readers/file/epub_parser.py\n",
      "could not parse gpt_index/readers/file/epub_parser.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1207 characters- adding to documents - gpt_index/readers/file/epub_parser.py\n",
      "generating document for gpt_index/readers/file/image_parser.py\n",
      "could not parse gpt_index/readers/file/image_parser.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 2995 characters- adding to documents - gpt_index/readers/file/image_parser.py\n",
      "generating document for gpt_index/readers/file/markdown_parser.py\n",
      "could not parse gpt_index/readers/file/markdown_parser.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 3596 characters- adding to documents - gpt_index/readers/file/markdown_parser.py\n",
      "Time to get blobs ([('mbox_parser.py', 3188), ('slides_parser.py', 3789), ('tabular_parser.py', 3484), ('video_audio.py', 1766), ('__init__.py', 17)]): 0.61 seconds\n",
      "generating document for gpt_index/readers/file/mbox_parser.py\n",
      "could not parse gpt_index/readers/file/mbox_parser.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 3188 characters- adding to documents - gpt_index/readers/file/mbox_parser.py\n",
      "generating document for gpt_index/readers/file/slides_parser.py\n",
      "could not parse gpt_index/readers/file/slides_parser.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 3789 characters- adding to documents - gpt_index/readers/file/slides_parser.py\n",
      "generating document for gpt_index/readers/file/tabular_parser.py\n",
      "could not parse gpt_index/readers/file/tabular_parser.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 3484 characters- adding to documents - gpt_index/readers/file/tabular_parser.py\n",
      "generating document for gpt_index/readers/file/video_audio.py\n",
      "could not parse gpt_index/readers/file/video_audio.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1766 characters- adding to documents - gpt_index/readers/file/video_audio.py\n",
      "generating document for gpt_index/readers/github_readers/__init__.py\n",
      "could not parse gpt_index/readers/github_readers/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 17 characters- adding to documents - gpt_index/readers/github_readers/__init__.py\n",
      "Time to get blobs ([('github_api_client.py', 11686), ('github_repository_reader.py', 15614), ('utils.py', 5471), ('__init__.py', 17), ('gdocs.py', 5618)]): 0.58 seconds\n",
      "generating document for gpt_index/readers/github_readers/github_api_client.py\n",
      "could not parse gpt_index/readers/github_readers/github_api_client.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 11686 characters- adding to documents - gpt_index/readers/github_readers/github_api_client.py\n",
      "generating document for gpt_index/readers/github_readers/github_repository_reader.py\n",
      "could not parse gpt_index/readers/github_readers/github_repository_reader.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 15614 characters- adding to documents - gpt_index/readers/github_readers/github_repository_reader.py\n",
      "generating document for gpt_index/readers/github_readers/utils.py\n",
      "could not parse gpt_index/readers/github_readers/utils.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 5471 characters- adding to documents - gpt_index/readers/github_readers/utils.py\n",
      "generating document for gpt_index/readers/google_readers/__init__.py\n",
      "could not parse gpt_index/readers/google_readers/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 17 characters- adding to documents - gpt_index/readers/google_readers/__init__.py\n",
      "generating document for gpt_index/readers/google_readers/gdocs.py\n",
      "could not parse gpt_index/readers/google_readers/gdocs.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 5618 characters- adding to documents - gpt_index/readers/google_readers/gdocs.py\n",
      "Time to get blobs ([('__init__.py', 19), ('wrapper.py', 1630), ('mbox.py', 1257), ('mongo.py', 1820), ('notion.py', 5638)]): 0.61 seconds\n",
      "generating document for gpt_index/readers/make_com/__init__.py\n",
      "could not parse gpt_index/readers/make_com/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 19 characters- adding to documents - gpt_index/readers/make_com/__init__.py\n",
      "generating document for gpt_index/readers/make_com/wrapper.py\n",
      "could not parse gpt_index/readers/make_com/wrapper.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1630 characters- adding to documents - gpt_index/readers/make_com/wrapper.py\n",
      "generating document for gpt_index/readers/mbox.py\n",
      "could not parse gpt_index/readers/mbox.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1257 characters- adding to documents - gpt_index/readers/mbox.py\n",
      "generating document for gpt_index/readers/mongo.py\n",
      "could not parse gpt_index/readers/mongo.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1820 characters- adding to documents - gpt_index/readers/mongo.py\n",
      "generating document for gpt_index/readers/notion.py\n",
      "could not parse gpt_index/readers/notion.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 5638 characters- adding to documents - gpt_index/readers/notion.py\n",
      "Time to get blobs ([('obsidian.py', 1595), ('pinecone.py', 2761), ('qdrant.py', 3213), ('__init__.py', 17), ('base.py', 999)]): 0.54 seconds\n",
      "generating document for gpt_index/readers/obsidian.py\n",
      "could not parse gpt_index/readers/obsidian.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1595 characters- adding to documents - gpt_index/readers/obsidian.py\n",
      "generating document for gpt_index/readers/pinecone.py\n",
      "could not parse gpt_index/readers/pinecone.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 2761 characters- adding to documents - gpt_index/readers/pinecone.py\n",
      "generating document for gpt_index/readers/qdrant.py\n",
      "could not parse gpt_index/readers/qdrant.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 3213 characters- adding to documents - gpt_index/readers/qdrant.py\n",
      "generating document for gpt_index/readers/schema/__init__.py\n",
      "could not parse gpt_index/readers/schema/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 17 characters- adding to documents - gpt_index/readers/schema/__init__.py\n",
      "generating document for gpt_index/readers/schema/base.py\n",
      "could not parse gpt_index/readers/schema/base.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 999 characters- adding to documents - gpt_index/readers/schema/base.py\n",
      "Time to get blobs ([('slack.py', 5049), ('string_iterable.py', 963), ('twitter.py', 1913), ('__init__.py', 17), ('data_structs.py', 8047)]): 0.53 seconds\n",
      "generating document for gpt_index/readers/slack.py\n",
      "could not parse gpt_index/readers/slack.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 5049 characters- adding to documents - gpt_index/readers/slack.py\n",
      "generating document for gpt_index/readers/string_iterable.py\n",
      "could not parse gpt_index/readers/string_iterable.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 963 characters- adding to documents - gpt_index/readers/string_iterable.py\n",
      "generating document for gpt_index/readers/twitter.py\n",
      "could not parse gpt_index/readers/twitter.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1913 characters- adding to documents - gpt_index/readers/twitter.py\n",
      "generating document for gpt_index/readers/weaviate/__init__.py\n",
      "could not parse gpt_index/readers/weaviate/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 17 characters- adding to documents - gpt_index/readers/weaviate/__init__.py\n",
      "generating document for gpt_index/readers/weaviate/data_structs.py\n",
      "could not parse gpt_index/readers/weaviate/data_structs.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 8047 characters- adding to documents - gpt_index/readers/weaviate/data_structs.py\n",
      "Time to get blobs ([('reader.py', 3981), ('utils.py', 1864), ('web.py', 7986), ('wikipedia.py', 976), ('youtube_transcript.py', 1250)]): 0.52 seconds\n",
      "generating document for gpt_index/readers/weaviate/reader.py\n",
      "could not parse gpt_index/readers/weaviate/reader.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 3981 characters- adding to documents - gpt_index/readers/weaviate/reader.py\n",
      "generating document for gpt_index/readers/weaviate/utils.py\n",
      "could not parse gpt_index/readers/weaviate/utils.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1864 characters- adding to documents - gpt_index/readers/weaviate/utils.py\n",
      "generating document for gpt_index/readers/web.py\n",
      "could not parse gpt_index/readers/web.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 7986 characters- adding to documents - gpt_index/readers/web.py\n",
      "generating document for gpt_index/readers/wikipedia.py\n",
      "could not parse gpt_index/readers/wikipedia.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 976 characters- adding to documents - gpt_index/readers/wikipedia.py\n",
      "generating document for gpt_index/readers/youtube_transcript.py\n",
      "could not parse gpt_index/readers/youtube_transcript.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1250 characters- adding to documents - gpt_index/readers/youtube_transcript.py\n",
      "Time to get blobs ([('__init__.py', 19), ('schema.py', 2060), ('schema.py', 1657), ('__init__.py', 17), ('mock_chain_wrapper.py', 4014)]): 0.62 seconds\n",
      "generating document for gpt_index/response/__init__.py\n",
      "could not parse gpt_index/response/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 19 characters- adding to documents - gpt_index/response/__init__.py\n",
      "generating document for gpt_index/response/schema.py\n",
      "could not parse gpt_index/response/schema.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 2060 characters- adding to documents - gpt_index/response/schema.py\n",
      "generating document for gpt_index/schema.py\n",
      "could not parse gpt_index/schema.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1657 characters- adding to documents - gpt_index/schema.py\n",
      "generating document for gpt_index/token_counter/__init__.py\n",
      "could not parse gpt_index/token_counter/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 17 characters- adding to documents - gpt_index/token_counter/__init__.py\n",
      "generating document for gpt_index/token_counter/mock_chain_wrapper.py\n",
      "could not parse gpt_index/token_counter/mock_chain_wrapper.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 4014 characters- adding to documents - gpt_index/token_counter/mock_chain_wrapper.py\n",
      "Time to get blobs ([('mock_embed_model.py', 720), ('token_counter.py', 2623), ('utils.py', 529), ('utils.py', 4889), ('pyproject.toml', 139)]): 0.82 seconds\n",
      "generating document for gpt_index/token_counter/mock_embed_model.py\n",
      "could not parse gpt_index/token_counter/mock_embed_model.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 720 characters- adding to documents - gpt_index/token_counter/mock_embed_model.py\n",
      "generating document for gpt_index/token_counter/token_counter.py\n",
      "could not parse gpt_index/token_counter/token_counter.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 2623 characters- adding to documents - gpt_index/token_counter/token_counter.py\n",
      "generating document for gpt_index/token_counter/utils.py\n",
      "could not parse gpt_index/token_counter/utils.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 529 characters- adding to documents - gpt_index/token_counter/utils.py\n",
      "generating document for gpt_index/utils.py\n",
      "could not parse gpt_index/utils.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 4889 characters- adding to documents - gpt_index/utils.py\n",
      "generating document for pyproject.toml\n",
      "could not parse pyproject.toml as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 139 characters- adding to documents - pyproject.toml\n",
      "Time to get blobs ([('requirements.txt', 291), ('setup.py', 974), ('__init__.py', 17), ('__init__.py', 17), ('__init__.py', 17)]): 0.75 seconds\n",
      "generating document for requirements.txt\n",
      "could not parse requirements.txt as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 291 characters- adding to documents - requirements.txt\n",
      "generating document for setup.py\n",
      "could not parse setup.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 974 characters- adding to documents - setup.py\n",
      "generating document for tests/__init__.py\n",
      "could not parse tests/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 17 characters- adding to documents - tests/__init__.py\n",
      "generating document for tests/indices/__init__.py\n",
      "could not parse tests/indices/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 17 characters- adding to documents - tests/indices/__init__.py\n",
      "generating document for tests/indices/embedding/__init__.py\n",
      "could not parse tests/indices/embedding/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 17 characters- adding to documents - tests/indices/embedding/__init__.py\n",
      "Time to get blobs ([('test_base.py', 4637), ('__init__.py', 17), ('test_base.py', 6339), ('test_utils.py', 1008), ('__init__.py', 123)]): 0.52 seconds\n",
      "generating document for tests/indices/embedding/test_base.py\n",
      "could not parse tests/indices/embedding/test_base.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 4637 characters- adding to documents - tests/indices/embedding/test_base.py\n",
      "generating document for tests/indices/keyword_table/__init__.py\n",
      "could not parse tests/indices/keyword_table/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 17 characters- adding to documents - tests/indices/keyword_table/__init__.py\n",
      "generating document for tests/indices/keyword_table/test_base.py\n",
      "could not parse tests/indices/keyword_table/test_base.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 6339 characters- adding to documents - tests/indices/keyword_table/test_base.py\n",
      "generating document for tests/indices/keyword_table/test_utils.py\n",
      "could not parse tests/indices/keyword_table/test_utils.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1008 characters- adding to documents - tests/indices/keyword_table/test_utils.py\n",
      "generating document for tests/indices/list/__init__.py\n",
      "could not parse tests/indices/list/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 123 characters- adding to documents - tests/indices/list/__init__.py\n",
      "Time to get blobs ([('test_base.py', 13261), ('__init__.py', 19), ('test_query_runner.py', 1444), ('test_recursive.py', 12996), ('__init__.py', 19)]): 0.59 seconds\n",
      "generating document for tests/indices/list/test_base.py\n",
      "could not parse tests/indices/list/test_base.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 13261 characters- adding to documents - tests/indices/list/test_base.py\n",
      "generating document for tests/indices/query/__init__.py\n",
      "could not parse tests/indices/query/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 19 characters- adding to documents - tests/indices/query/__init__.py\n",
      "generating document for tests/indices/query/test_query_runner.py\n",
      "could not parse tests/indices/query/test_query_runner.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1444 characters- adding to documents - tests/indices/query/test_query_runner.py\n",
      "generating document for tests/indices/query/test_recursive.py\n",
      "could not parse tests/indices/query/test_recursive.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 12996 characters- adding to documents - tests/indices/query/test_recursive.py\n",
      "generating document for tests/indices/struct_store/__init__.py\n",
      "could not parse tests/indices/struct_store/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 19 characters- adding to documents - tests/indices/struct_store/__init__.py\n",
      "Time to get blobs ([('test_base.py', 7635), ('test_node_utils.py', 1972), ('test_prompt_helper.py', 6984), ('test_response.py', 5836), ('test_utils.py', 461)]): 0.50 seconds\n",
      "generating document for tests/indices/struct_store/test_base.py\n",
      "could not parse tests/indices/struct_store/test_base.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 7635 characters- adding to documents - tests/indices/struct_store/test_base.py\n",
      "generating document for tests/indices/test_node_utils.py\n",
      "could not parse tests/indices/test_node_utils.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1972 characters- adding to documents - tests/indices/test_node_utils.py\n",
      "generating document for tests/indices/test_prompt_helper.py\n",
      "could not parse tests/indices/test_prompt_helper.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 6984 characters- adding to documents - tests/indices/test_prompt_helper.py\n",
      "generating document for tests/indices/test_response.py\n",
      "could not parse tests/indices/test_response.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 5836 characters- adding to documents - tests/indices/test_response.py\n",
      "generating document for tests/indices/test_utils.py\n",
      "could not parse tests/indices/test_utils.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 461 characters- adding to documents - tests/indices/test_utils.py\n",
      "Time to get blobs ([('__init__.py', 17), ('test_base.py', 9621), ('__init__.py', 17), ('test_base.py', 14987), ('__init__.py', 19)]): 0.53 seconds\n",
      "generating document for tests/indices/tree/__init__.py\n",
      "could not parse tests/indices/tree/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 17 characters- adding to documents - tests/indices/tree/__init__.py\n",
      "generating document for tests/indices/tree/test_base.py\n",
      "could not parse tests/indices/tree/test_base.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 9621 characters- adding to documents - tests/indices/tree/test_base.py\n",
      "generating document for tests/indices/vector_store/__init__.py\n",
      "could not parse tests/indices/vector_store/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 17 characters- adding to documents - tests/indices/vector_store/__init__.py\n",
      "generating document for tests/indices/vector_store/test_base.py\n",
      "could not parse tests/indices/vector_store/test_base.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 14987 characters- adding to documents - tests/indices/vector_store/test_base.py\n",
      "generating document for tests/langchain_helpers/__init__.py\n",
      "could not parse tests/langchain_helpers/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 19 characters- adding to documents - tests/langchain_helpers/__init__.py\n",
      "Time to get blobs ([('test_text_splitter.py', 1860), ('__init__.py', 17), ('mock_decorator.py', 1144), ('mock_predict.py', 3242), ('mock_prompts.py', 1771)]): 0.54 seconds\n",
      "generating document for tests/langchain_helpers/test_text_splitter.py\n",
      "could not parse tests/langchain_helpers/test_text_splitter.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1860 characters- adding to documents - tests/langchain_helpers/test_text_splitter.py\n",
      "generating document for tests/mock_utils/__init__.py\n",
      "could not parse tests/mock_utils/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 17 characters- adding to documents - tests/mock_utils/__init__.py\n",
      "generating document for tests/mock_utils/mock_decorator.py\n",
      "could not parse tests/mock_utils/mock_decorator.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1144 characters- adding to documents - tests/mock_utils/mock_decorator.py\n",
      "generating document for tests/mock_utils/mock_predict.py\n",
      "could not parse tests/mock_utils/mock_predict.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 3242 characters- adding to documents - tests/mock_utils/mock_predict.py\n",
      "generating document for tests/mock_utils/mock_prompts.py\n",
      "could not parse tests/mock_utils/mock_prompts.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1771 characters- adding to documents - tests/mock_utils/mock_prompts.py\n",
      "Time to get blobs ([('mock_text_splitter.py', 618), ('mock_utils.py', 737), ('__init__.py', 17), ('test_base.py', 2563), ('__init__.py', 17)]): 1.55 seconds\n",
      "generating document for tests/mock_utils/mock_text_splitter.py\n",
      "could not parse tests/mock_utils/mock_text_splitter.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 618 characters- adding to documents - tests/mock_utils/mock_text_splitter.py\n",
      "generating document for tests/mock_utils/mock_utils.py\n",
      "could not parse tests/mock_utils/mock_utils.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 737 characters- adding to documents - tests/mock_utils/mock_utils.py\n",
      "generating document for tests/prompts/__init__.py\n",
      "could not parse tests/prompts/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 17 characters- adding to documents - tests/prompts/__init__.py\n",
      "generating document for tests/prompts/test_base.py\n",
      "could not parse tests/prompts/test_base.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 2563 characters- adding to documents - tests/prompts/test_base.py\n",
      "generating document for tests/readers/__init__.py\n",
      "could not parse tests/readers/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 17 characters- adding to documents - tests/readers/__init__.py\n",
      "Time to get blobs ([('test_file.py', 8400), ('test_string_iterable.py', 337), ('test_docstore.py', 1429), ('test_utils.py', 2654), ('__init__.py', 17)]): 0.58 seconds\n",
      "generating document for tests/readers/test_file.py\n",
      "could not parse tests/readers/test_file.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 8400 characters- adding to documents - tests/readers/test_file.py\n",
      "generating document for tests/readers/test_string_iterable.py\n",
      "could not parse tests/readers/test_string_iterable.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 337 characters- adding to documents - tests/readers/test_string_iterable.py\n",
      "generating document for tests/test_docstore.py\n",
      "could not parse tests/test_docstore.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1429 characters- adding to documents - tests/test_docstore.py\n",
      "generating document for tests/test_utils.py\n",
      "could not parse tests/test_utils.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 2654 characters- adding to documents - tests/test_utils.py\n",
      "generating document for tests/token_predictor/__init__.py\n",
      "could not parse tests/token_predictor/__init__.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 17 characters- adding to documents - tests/token_predictor/__init__.py\n",
      "Time to get blobs ([('test_base.py', 1542)]): 0.28 seconds\n",
      "generating document for tests/token_predictor/test_base.py\n",
      "could not parse tests/token_predictor/test_base.py as a supported file type - falling back to decoding as utf-8 raw text\n",
      "got 1542 characters- adding to documents - tests/token_predictor/test_base.py\n",
      "Loaded 196 documents\n"
     ]
    }
   ],
   "source": [
    "documents = reader.load_data(branch=branch)\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing  with 13 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 4482 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 5898 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed  with 13 documents\n",
      "Indexing .github/workflows with 3 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1507 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 1912 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed .github/workflows with 3 documents\n",
      "Indexing experimental with 1 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 192 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 423 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 3 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed experimental with 1 documents\n",
      "Indexing experimental/classifier with 2 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 11157 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 3 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 13265 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed experimental/classifier with 2 documents\n",
      "Indexing gpt_index with 7 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 7957 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 2 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 9666 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index with 7 documents\n",
      "Indexing gpt_index/composability with 2 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 3945 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 4728 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/composability with 2 documents\n",
      "Indexing gpt_index/data_structs with 4 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 6443 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 7524 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/data_structs with 4 documents\n",
      "Indexing gpt_index/embeddings with 5 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 5501 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 6691 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 4 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/embeddings with 5 documents\n",
      "Indexing gpt_index/indices with 6 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 15779 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 4 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 17830 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/indices with 6 documents\n",
      "Indexing gpt_index/indices/common with 1 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 163 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 396 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/indices/common with 1 documents\n",
      "Indexing gpt_index/indices/common/struct_store with 2 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2158 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 2374 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/indices/common/struct_store with 2 documents\n",
      "Indexing gpt_index/indices/common/tree with 2 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2513 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 2830 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/indices/common/tree with 2 documents\n",
      "Indexing gpt_index/indices/keyword_table with 6 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 5952 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 7318 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/indices/keyword_table with 6 documents\n",
      "Indexing gpt_index/indices/list with 3 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2398 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 2803 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/indices/list with 3 documents\n",
      "Indexing gpt_index/indices/query with 5 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 10153 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 2 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 11172 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/indices/query with 5 documents\n",
      "Indexing gpt_index/indices/query/keyword_table with 2 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 3308 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 3703 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/indices/query/keyword_table with 2 documents\n",
      "Indexing gpt_index/indices/query/list with 3 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2360 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 2786 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/indices/query/list with 3 documents\n",
      "Indexing gpt_index/indices/query/struct_store with 2 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2250 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 2719 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/indices/query/struct_store with 2 documents\n",
      "Indexing gpt_index/indices/query/tree with 5 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 8747 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 2 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 9728 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/indices/query/tree with 5 documents\n",
      "Indexing gpt_index/indices/query/vector_store with 7 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 9193 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 2 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 10719 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/indices/query/vector_store with 7 documents\n",
      "Indexing gpt_index/indices/response with 2 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 5001 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 5717 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/indices/response with 2 documents\n",
      "Indexing gpt_index/indices/struct_store with 3 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 7055 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 2 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 8010 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/indices/struct_store with 3 documents\n",
      "Indexing gpt_index/indices/tree with 4 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 7655 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 2 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 8467 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 4 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/indices/tree with 4 documents\n",
      "Indexing gpt_index/indices/vector_store with 7 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 17900 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 5 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 20168 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 3 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/indices/vector_store with 7 documents\n",
      "Indexing gpt_index/langchain_helpers with 5 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 11335 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 3 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 12896 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/langchain_helpers with 5 documents\n",
      "Indexing gpt_index/prompts with 5 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 9241 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 2 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 10892 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 7 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/prompts with 5 documents\n",
      "Indexing gpt_index/readers with 18 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 27274 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 7 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 30827 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 4 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/readers with 18 documents\n",
      "Indexing gpt_index/readers/file with 11 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 16175 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 4 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 18765 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/readers/file with 11 documents\n",
      "Indexing gpt_index/readers/github_readers with 4 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Building index from nodes: 5 chunks\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 19746 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 5 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 22117 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/readers/github_readers with 4 documents\n",
      "Indexing gpt_index/readers/google_readers with 2 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 3120 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 3790 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/readers/google_readers with 2 documents\n",
      "Indexing gpt_index/readers/make_com with 2 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 997 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 1394 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/readers/make_com with 2 documents\n",
      "Indexing gpt_index/readers/schema with 2 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 600 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 846 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/readers/schema with 2 documents\n",
      "Indexing gpt_index/readers/weaviate with 4 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 8091 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 2 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 9393 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/readers/weaviate with 4 documents\n",
      "Indexing gpt_index/response with 2 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1146 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 1477 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/response with 2 documents\n",
      "Indexing gpt_index/token_counter with 5 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 4413 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 5089 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed gpt_index/token_counter with 5 documents\n",
      "Indexing tests with 3 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2231 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 2514 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed tests with 3 documents\n",
      "Indexing tests/indices with 5 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 7946 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 2 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 8941 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed tests/indices with 5 documents\n",
      "Indexing tests/indices/embedding with 2 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 2425 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 2598 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed tests/indices/embedding with 2 documents\n",
      "Indexing tests/indices/keyword_table with 3 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 3773 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 5092 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed tests/indices/keyword_table with 3 documents\n",
      "Indexing tests/indices/list with 2 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 6765 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 7466 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed tests/indices/list with 2 documents\n",
      "Indexing tests/indices/query with 3 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 7454 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 2 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 8082 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed tests/indices/query with 3 documents\n",
      "Indexing tests/indices/struct_store with 2 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 3883 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 4632 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed tests/indices/struct_store with 2 documents\n",
      "Indexing tests/indices/tree with 2 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 4941 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 5452 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed tests/indices/tree with 2 documents\n",
      "Indexing tests/indices/vector_store with 2 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 7978 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 2 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 8622 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed tests/indices/vector_store with 2 documents\n",
      "Indexing tests/langchain_helpers with 2 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1013 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 1276 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed tests/langchain_helpers with 2 documents\n",
      "Indexing tests/mock_utils with 6 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 3926 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 4774 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed tests/mock_utils with 6 documents\n",
      "Indexing tests/prompts with 2 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 1268 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 1561 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed tests/prompts with 2 documents\n",
      "Indexing tests/readers with 3 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 5602 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 1 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 6762 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Building index from nodes: 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed tests/readers with 3 documents\n",
      "Indexing tests/token_predictor with 2 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 862 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "INFO:root:> Starting query: What are the summaries of these documents?\n",
      "INFO:root:> Building index from nodes: 0 chunks\n",
      "INFO:root:> [query] Total LLM token usage: 1134 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed tests/token_predictor with 2 documents\n"
     ]
    }
   ],
   "source": [
    "from gpt_index import Document\n",
    "from gpt_index import QueryMode\n",
    "from gpt_index import SummaryPrompt\n",
    "from typing import List\n",
    "verbose = False\n",
    "def seperate_documents_into_folders(documents: List[Document]):\n",
    "    folders = {}\n",
    "    for doc in documents:\n",
    "        file_path = doc.extra_info[\"file_path\"]\n",
    "        fp = \"/\".join(file_path.split(\"/\")[:-1])\n",
    "        folders.setdefault(fp, [])\n",
    "        folders[fp].append(doc)\n",
    "    return folders\n",
    "\n",
    "if verbose:\n",
    "    for folder, docs in seperate_documents_into_folders(documents).items():\n",
    "        print(f\"{folder}: {len(docs)}\")\n",
    "\n",
    "CODE_FILE_SUMMARY_PROMPT_TMPL = (\n",
    "    \"Write a summary of the following code file. Try to explain the purpose, \"\n",
    "    \"functionality, and key elements of the code. \"\n",
    "    \"Try to include as many details as possible, but also keep it concise.\\n\"\n",
    "    \"\\n\"\n",
    "    \"\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"\\n\"\n",
    "    \"\\n\"\n",
    "    'SUMMARY:\"\"\"\\n'\n",
    ")\n",
    "\n",
    "CODE_FILE_SUMMARY_PROMPT = SummaryPrompt(CODE_FILE_SUMMARY_PROMPT_TMPL)\n",
    "\n",
    "indexes_by_folder = {}\n",
    "for folder, docs in seperate_documents_into_folders(documents).items():\n",
    "    print(f\"Indexing {folder} with {len(docs)} documents\")\n",
    "    \n",
    "    \n",
    "    index = GPTTreeIndex(\n",
    "        documents=docs,\n",
    "        num_children=12,\n",
    "        summary_template=CODE_FILE_SUMMARY_PROMPT,\n",
    "    )\n",
    "    summary = index.query(\n",
    "        \"What are the summaries of these documents?\", mode=QueryMode.SUMMARIZE\n",
    "    )\n",
    "    index.set_text(str(summary))\n",
    "    indexes_by_folder[folder] = index\n",
    "    print(f\"Indexed {folder} with {len(docs)} documents\")\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the indexes to disk\n",
    "folder: str\n",
    "for folder, index in indexes_by_folder.items():\n",
    "    index.save_to_disk(f\"indexes/{folder.replace('/', '-')}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index gpt_index-indices-vector_store.json\n",
      "Loading index gpt_index-indices-query-tree.json\n",
      "Loading index gpt_index-readers-file.json\n",
      "Loading index .github-workflows.json\n",
      "Loading index gpt_index-readers.json\n",
      "Loading index gpt_index-indices-query-list.json\n",
      "Loading index gpt_index-readers-weaviate.json\n",
      "Loading index gpt_index-indices-common-struct_store.json\n",
      "Loading index gpt_index-readers-schema.json\n",
      "Loading index tests-indices.json\n",
      "Loading index tests-indices-keyword_table.json\n",
      "Loading index gpt_index-readers-make_com.json\n",
      "Loading index tests-indices-vector_store.json\n",
      "Loading index gpt_index-response.json\n",
      "Loading index gpt_index-indices-query-keyword_table.json\n",
      "Loading index tests-readers.json\n",
      "Loading index gpt_index-indices-common-tree.json\n",
      "Loading index tests-prompts.json\n",
      "Loading index experimental.json\n",
      "Loading index gpt_index-indices.json\n",
      "Loading index tests-indices-tree.json\n",
      "Loading index gpt_index.json\n",
      "Loading index gpt_index-langchain_helpers.json\n",
      "Loading index gpt_index-indices-query-struct_store.json\n",
      "Loading index .json\n",
      "Loading index gpt_index-token_counter.json\n",
      "Loading index gpt_index-indices-common.json\n",
      "Loading index experimental-classifier.json\n",
      "Loading index tests-indices-query.json\n",
      "Loading index tests-langchain_helpers.json\n",
      "Loading index gpt_index-embeddings.json\n",
      "Loading index gpt_index-indices-struct_store.json\n",
      "Loading index gpt_index-indices-list.json\n",
      "Loading index gpt_index-readers-github_readers.json\n",
      "Loading index tests-mock_utils.json\n",
      "Loading index tests-indices-list.json\n",
      "Loading index tests-indices-struct_store.json\n",
      "Loading index tests.json\n",
      "Loading index gpt_index-indices-response.json\n",
      "Loading index gpt_index-readers-google_readers.json\n",
      "Loading index gpt_index-indices-query-vector_store.json\n",
      "Loading index gpt_index-composability.json\n",
      "Loading index gpt_index-indices-tree.json\n",
      "Loading index gpt_index-prompts.json\n",
      "Loading index tests-indices-embedding.json\n",
      "Loading index gpt_index-indices-keyword_table.json\n",
      "Loading index gpt_index-data_structs.json\n",
      "Loading index gpt_index-indices-query.json\n",
      "Loading index tests-token_predictor.json\n"
     ]
    }
   ],
   "source": [
    "new_indexes_by_folder = {}\n",
    "# read the indexes from disk\n",
    "for index_filename in os.listdir(\"indexes\"):\n",
    "    print(f\"Loading index {index_filename}\")\n",
    "    index = GPTTreeIndex.load_from_disk(f\"indexes/{index_filename}\")\n",
    "    new_indexes_by_folder[index_filename.replace(\".json\", \"\").replace('-', '/')] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "list_index = GPTListIndex([*indexes_by_folder.values()])\n",
    "\n",
    "from gpt_index.composability import ComposableGraph\n",
    "\n",
    "graph = ComposableGraph.build_from_index(list_index)\n",
    "\n",
    "# [Optional] save to disk\n",
    "graph.save_to_disk(\"indexes/composable_graph.json\")\n",
    "\n",
    "# [Optional] load from disk\n",
    "graph = ComposableGraph.load_from_disk(\"indexes/composable_graph.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> Starting query: Find every class that extends the class `BaseGPTIndex`. What are the summaries of these classes?\n",
      "INFO:root:> Building index from nodes: 5 chunks\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahmetk/Projects/gpt_index/gpt_index/utils.py\", line 157, in retry_on_exceptions_with_backoff\n",
      "    return lambda_fn()\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/Projects/gpt_index/gpt_index/langchain_helpers/chain_wrapper.py\", line 100, in <lambda>\n",
      "    lambda: llm_chain.predict(**full_prompt_args),\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/llm.py\", line 104, in predict\n",
      "    return self(kwargs)[self.output_key]\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/base.py\", line 155, in __call__\n",
      "    raise e\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/base.py\", line 152, in __call__\n",
      "    outputs = self._call(inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/llm.py\", line 88, in _call\n",
      "    return self.apply([inputs])[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/llm.py\", line 79, in apply\n",
      "    response = self.generate(input_list)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/llm.py\", line 74, in generate\n",
      "    response = self.llm.generate(prompts, stop=stop)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/llms/base.py\", line 79, in generate\n",
      "    raise e\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/llms/base.py\", line 76, in generate\n",
      "    output = self._generate(prompts, stop=stop)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/llms/openai.py\", line 158, in _generate\n",
      "    response = self.client.create(prompt=_prompts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/openai/api_resources/completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "                           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/openai/api_requestor.py\", line 227, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/openai/api_requestor.py\", line 620, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/ahmetk/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/openai/api_requestor.py\", line 663, in _interpret_response_line\n",
      "    raise error.ServiceUnavailableError(\n",
      "openai.error.ServiceUnavailableError: The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 21\u001b[0m\n\u001b[1;32m      5\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mFind every class that extends the class `BaseGPTIndex`. What are the summaries of these classes?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m query_configs \u001b[39m=\u001b[39m [\n\u001b[1;32m      8\u001b[0m     QueryConfig(\n\u001b[1;32m      9\u001b[0m         index_struct_type\u001b[39m=\u001b[39mIndexStructType\u001b[39m.\u001b[39mTREE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     ),\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 21\u001b[0m response \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39;49mquery(\n\u001b[1;32m     22\u001b[0m     query, query_configs\u001b[39m=\u001b[39;49mquery_configs\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m display(Markdown(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m**Query:** \u001b[39m\u001b[39m{\u001b[39;00mquery\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     26\u001b[0m display(Markdown(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m**Response:** \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mresponse\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m~/Projects/gpt_index/gpt_index/composability/graph.py:121\u001b[0m, in \u001b[0;36mComposableGraph.query\u001b[0;34m(self, query_str, query_configs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39m# go over all the indices and create a registry\u001b[39;00m\n\u001b[1;32m    112\u001b[0m query_runner \u001b[39m=\u001b[39m QueryRunner(\n\u001b[1;32m    113\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm_predictor,\n\u001b[1;32m    114\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prompt_helper,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m     recursive\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    120\u001b[0m )\n\u001b[0;32m--> 121\u001b[0m \u001b[39mreturn\u001b[39;00m query_runner\u001b[39m.\u001b[39;49mquery(query_str, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_index_struct)\n",
      "File \u001b[0;32m~/Projects/gpt_index/gpt_index/indices/query/query_runner.py:95\u001b[0m, in \u001b[0;36mQueryRunner.query\u001b[0;34m(self, query_str, index_struct)\u001b[0m\n\u001b[1;32m     87\u001b[0m query_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_query_kwargs(config)\n\u001b[1;32m     88\u001b[0m query_obj \u001b[39m=\u001b[39m query_cls(\n\u001b[1;32m     89\u001b[0m     index_struct,\n\u001b[1;32m     90\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mquery_kwargs,\n\u001b[1;32m     91\u001b[0m     query_runner\u001b[39m=\u001b[39mquery_runner,\n\u001b[1;32m     92\u001b[0m     docstore\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_docstore,\n\u001b[1;32m     93\u001b[0m )\n\u001b[0;32m---> 95\u001b[0m \u001b[39mreturn\u001b[39;00m query_obj\u001b[39m.\u001b[39;49mquery(query_str)\n",
      "File \u001b[0;32m~/Projects/gpt_index/gpt_index/token_counter/token_counter.py:55\u001b[0m, in \u001b[0;36mllm_token_counter.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m start_token_ct \u001b[39m=\u001b[39m llm_predictor\u001b[39m.\u001b[39mtotal_tokens_used\n\u001b[1;32m     53\u001b[0m start_embed_token_ct \u001b[39m=\u001b[39m embed_model\u001b[39m.\u001b[39mtotal_tokens_used\n\u001b[0;32m---> 55\u001b[0m f_return_val \u001b[39m=\u001b[39m f(_self, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     57\u001b[0m net_tokens \u001b[39m=\u001b[39m llm_predictor\u001b[39m.\u001b[39mtotal_tokens_used \u001b[39m-\u001b[39m start_token_ct\n\u001b[1;32m     58\u001b[0m llm_predictor\u001b[39m.\u001b[39mlast_token_usage \u001b[39m=\u001b[39m net_tokens\n",
      "File \u001b[0;32m~/Projects/gpt_index/gpt_index/indices/query/base.py:267\u001b[0m, in \u001b[0;36mBaseGPTIndexQuery.query\u001b[0;34m(self, query_str)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[39m@llm_token_counter\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    265\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mquery\u001b[39m(\u001b[39mself\u001b[39m, query_str: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Response:\n\u001b[1;32m    266\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Answer a query.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_query(query_str)\n\u001b[1;32m    268\u001b[0m     \u001b[39m# if include_summary is True, then include summary text in answer\u001b[39;00m\n\u001b[1;32m    269\u001b[0m     \u001b[39m# summary text is set through `set_text` on the underlying index.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m     \u001b[39m# TODO: refactor response builder to be in the __init__\u001b[39;00m\n\u001b[1;32m    271\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_mode \u001b[39m!=\u001b[39m ResponseMode\u001b[39m.\u001b[39mNO_TEXT \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_include_summary:\n",
      "File \u001b[0;32m~/Projects/gpt_index/gpt_index/indices/query/base.py:249\u001b[0m, in \u001b[0;36mBaseGPTIndexQuery._query\u001b[0;34m(self, query_str)\u001b[0m\n\u001b[1;32m    247\u001b[0m node_texts \u001b[39m=\u001b[39m []\n\u001b[1;32m    248\u001b[0m \u001b[39mfor\u001b[39;00m node, similarity \u001b[39min\u001b[39;00m tuples:\n\u001b[0;32m--> 249\u001b[0m     text, response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_text_from_node(query_str, node)\n\u001b[1;32m    250\u001b[0m     source_builder\u001b[39m.\u001b[39madd_node(node, similarity\u001b[39m=\u001b[39msimilarity)\n\u001b[1;32m    251\u001b[0m     \u001b[39mif\u001b[39;00m response \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m         \u001b[39m# these are source nodes from within this node (when it's an index)\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/gpt_index/gpt_index/indices/query/base.py:184\u001b[0m, in \u001b[0;36mBaseGPTIndexQuery._get_text_from_node\u001b[0;34m(self, query_str, node, level)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m is_index_struct:\n\u001b[1;32m    183\u001b[0m     query_runner \u001b[39m=\u001b[39m cast(BaseQueryRunner, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_query_runner)\n\u001b[0;32m--> 184\u001b[0m     response \u001b[39m=\u001b[39m query_runner\u001b[39m.\u001b[39;49mquery(query_str, cast(IndexStruct, doc))\n\u001b[1;32m    185\u001b[0m     \u001b[39mreturn\u001b[39;00m TextChunk(\u001b[39mstr\u001b[39m(response), is_answer\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), response\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Projects/gpt_index/gpt_index/indices/query/query_runner.py:95\u001b[0m, in \u001b[0;36mQueryRunner.query\u001b[0;34m(self, query_str, index_struct)\u001b[0m\n\u001b[1;32m     87\u001b[0m query_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_query_kwargs(config)\n\u001b[1;32m     88\u001b[0m query_obj \u001b[39m=\u001b[39m query_cls(\n\u001b[1;32m     89\u001b[0m     index_struct,\n\u001b[1;32m     90\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mquery_kwargs,\n\u001b[1;32m     91\u001b[0m     query_runner\u001b[39m=\u001b[39mquery_runner,\n\u001b[1;32m     92\u001b[0m     docstore\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_docstore,\n\u001b[1;32m     93\u001b[0m )\n\u001b[0;32m---> 95\u001b[0m \u001b[39mreturn\u001b[39;00m query_obj\u001b[39m.\u001b[39;49mquery(query_str)\n",
      "File \u001b[0;32m~/Projects/gpt_index/gpt_index/token_counter/token_counter.py:55\u001b[0m, in \u001b[0;36mllm_token_counter.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m start_token_ct \u001b[39m=\u001b[39m llm_predictor\u001b[39m.\u001b[39mtotal_tokens_used\n\u001b[1;32m     53\u001b[0m start_embed_token_ct \u001b[39m=\u001b[39m embed_model\u001b[39m.\u001b[39mtotal_tokens_used\n\u001b[0;32m---> 55\u001b[0m f_return_val \u001b[39m=\u001b[39m f(_self, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     57\u001b[0m net_tokens \u001b[39m=\u001b[39m llm_predictor\u001b[39m.\u001b[39mtotal_tokens_used \u001b[39m-\u001b[39m start_token_ct\n\u001b[1;32m     58\u001b[0m llm_predictor\u001b[39m.\u001b[39mlast_token_usage \u001b[39m=\u001b[39m net_tokens\n",
      "File \u001b[0;32m~/Projects/gpt_index/gpt_index/indices/query/base.py:267\u001b[0m, in \u001b[0;36mBaseGPTIndexQuery.query\u001b[0;34m(self, query_str)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[39m@llm_token_counter\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    265\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mquery\u001b[39m(\u001b[39mself\u001b[39m, query_str: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Response:\n\u001b[1;32m    266\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Answer a query.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_query(query_str)\n\u001b[1;32m    268\u001b[0m     \u001b[39m# if include_summary is True, then include summary text in answer\u001b[39;00m\n\u001b[1;32m    269\u001b[0m     \u001b[39m# summary text is set through `set_text` on the underlying index.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m     \u001b[39m# TODO: refactor response builder to be in the __init__\u001b[39;00m\n\u001b[1;32m    271\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_mode \u001b[39m!=\u001b[39m ResponseMode\u001b[39m.\u001b[39mNO_TEXT \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_include_summary:\n",
      "File \u001b[0;32m~/Projects/gpt_index/gpt_index/indices/query/base.py:258\u001b[0m, in \u001b[0;36mBaseGPTIndexQuery._query\u001b[0;34m(self, query_str)\u001b[0m\n\u001b[1;32m    255\u001b[0m     node_texts\u001b[39m.\u001b[39mappend(text)\n\u001b[1;32m    257\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_mode \u001b[39m!=\u001b[39m ResponseMode\u001b[39m.\u001b[39mNO_TEXT:\n\u001b[0;32m--> 258\u001b[0m     response_str \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_give_response_for_nodes(query_str, node_texts)\n\u001b[1;32m    259\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    260\u001b[0m     response_str \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/gpt_index/gpt_index/indices/query/base.py:206\u001b[0m, in \u001b[0;36mBaseGPTIndexQuery._give_response_for_nodes\u001b[0;34m(self, query_str, text_chunks)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m text_chunks:\n\u001b[1;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponse_builder\u001b[39m.\u001b[39madd_text_chunks([text])\n\u001b[0;32m--> 206\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresponse_builder\u001b[39m.\u001b[39;49mget_response(\n\u001b[1;32m    207\u001b[0m     query_str,\n\u001b[1;32m    208\u001b[0m     mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_response_mode,\n\u001b[1;32m    209\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_response_kwargs,\n\u001b[1;32m    210\u001b[0m )\n\u001b[1;32m    212\u001b[0m \u001b[39mreturn\u001b[39;00m response \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/Projects/gpt_index/gpt_index/indices/response/builder.py:226\u001b[0m, in \u001b[0;36mResponseBuilder.get_response\u001b[0;34m(self, query_str, prev_response, mode, **response_kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_response_compact(query_str, prev_response)\n\u001b[1;32m    225\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m ResponseMode\u001b[39m.\u001b[39mTREE_SUMMARIZE:\n\u001b[0;32m--> 226\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_response_tree_summarize(\n\u001b[1;32m    227\u001b[0m         query_str, prev_response, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kwargs\n\u001b[1;32m    228\u001b[0m     )\n\u001b[1;32m    229\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid mode: \u001b[39m\u001b[39m{\u001b[39;00mmode\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/gpt_index/gpt_index/indices/response/builder.py:201\u001b[0m, in \u001b[0;36mResponseBuilder._get_response_tree_summarize\u001b[0;34m(self, query_str, prev_response, num_children)\u001b[0m\n\u001b[1;32m    191\u001b[0m all_nodes: Dict[\u001b[39mint\u001b[39m, Node] \u001b[39m=\u001b[39m {\n\u001b[1;32m    192\u001b[0m     i: Node(text\u001b[39m=\u001b[39mt) \u001b[39mfor\u001b[39;00m i, t \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(text_chunks)\n\u001b[1;32m    193\u001b[0m }\n\u001b[1;32m    195\u001b[0m index_builder \u001b[39m=\u001b[39m GPTTreeIndexBuilder(\n\u001b[1;32m    196\u001b[0m     num_children,\n\u001b[1;32m    197\u001b[0m     summary_template,\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_predictor,\n\u001b[1;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompt_helper,\n\u001b[1;32m    200\u001b[0m )\n\u001b[0;32m--> 201\u001b[0m root_nodes \u001b[39m=\u001b[39m index_builder\u001b[39m.\u001b[39;49mbuild_index_from_nodes(all_nodes, all_nodes)\n\u001b[1;32m    202\u001b[0m node_list \u001b[39m=\u001b[39m get_sorted_node_list(root_nodes)\n\u001b[1;32m    203\u001b[0m node_text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompt_helper\u001b[39m.\u001b[39mget_text_from_nodes(\n\u001b[1;32m    204\u001b[0m     node_list, prompt\u001b[39m=\u001b[39mtext_qa_template\n\u001b[1;32m    205\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/gpt_index/gpt_index/indices/common/tree/base.py:102\u001b[0m, in \u001b[0;36mGPTTreeIndexBuilder.build_index_from_nodes\u001b[0;34m(self, cur_nodes, all_nodes)\u001b[0m\n\u001b[1;32m     97\u001b[0m cur_nodes_chunk \u001b[39m=\u001b[39m cur_node_list[i : i \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_children]\n\u001b[1;32m     98\u001b[0m text_chunk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prompt_helper\u001b[39m.\u001b[39mget_text_from_nodes(\n\u001b[1;32m     99\u001b[0m     cur_nodes_chunk, prompt\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msummary_prompt\n\u001b[1;32m    100\u001b[0m )\n\u001b[0;32m--> 102\u001b[0m new_summary, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_llm_predictor\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m    103\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msummary_prompt, context_str\u001b[39m=\u001b[39;49mtext_chunk\n\u001b[1;32m    104\u001b[0m )\n\u001b[1;32m    106\u001b[0m logging\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m    107\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m> \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(cur_nodes)\u001b[39m}\u001b[39;00m\u001b[39m, summary: \u001b[39m\u001b[39m{\u001b[39;00mtruncate_text(new_summary,\u001b[39m \u001b[39m\u001b[39m50\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m )\n\u001b[1;32m    109\u001b[0m new_node \u001b[39m=\u001b[39m Node(\n\u001b[1;32m    110\u001b[0m     text\u001b[39m=\u001b[39mnew_summary,\n\u001b[1;32m    111\u001b[0m     index\u001b[39m=\u001b[39mcur_index,\n\u001b[1;32m    112\u001b[0m     child_indices\u001b[39m=\u001b[39m{n\u001b[39m.\u001b[39mindex \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m cur_nodes_chunk},\n\u001b[1;32m    113\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/gpt_index/gpt_index/langchain_helpers/chain_wrapper.py:125\u001b[0m, in \u001b[0;36mLLMPredictor.predict\u001b[0;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Predict the answer to a query.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \n\u001b[1;32m    123\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m formatted_prompt \u001b[39m=\u001b[39m prompt\u001b[39m.\u001b[39mformat(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprompt_args)\n\u001b[0;32m--> 125\u001b[0m llm_prediction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict(prompt, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprompt_args)\n\u001b[1;32m    126\u001b[0m logging\u001b[39m.\u001b[39mdebug(llm_prediction)\n\u001b[1;32m    128\u001b[0m \u001b[39m# We assume that the value of formatted_prompt is exactly the thing\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m# eventually sent to OpenAI, or whatever LLM downstream\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/gpt_index/gpt_index/langchain_helpers/chain_wrapper.py:99\u001b[0m, in \u001b[0;36mLLMPredictor._predict\u001b[0;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[1;32m     97\u001b[0m full_prompt_args \u001b[39m=\u001b[39m prompt\u001b[39m.\u001b[39mget_full_format_args(prompt_args)\n\u001b[1;32m     98\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry_on_throttling:\n\u001b[0;32m---> 99\u001b[0m     llm_prediction \u001b[39m=\u001b[39m retry_on_exceptions_with_backoff(\n\u001b[1;32m    100\u001b[0m         \u001b[39mlambda\u001b[39;49;00m: llm_chain\u001b[39m.\u001b[39;49mpredict(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfull_prompt_args),\n\u001b[1;32m    101\u001b[0m         [\n\u001b[1;32m    102\u001b[0m             ErrorToRetry(openai\u001b[39m.\u001b[39;49merror\u001b[39m.\u001b[39;49mRateLimitError),\n\u001b[1;32m    103\u001b[0m             ErrorToRetry(openai\u001b[39m.\u001b[39;49merror\u001b[39m.\u001b[39;49mServiceUnavailableError),\n\u001b[1;32m    104\u001b[0m             ErrorToRetry(openai\u001b[39m.\u001b[39;49merror\u001b[39m.\u001b[39;49mTryAgain),\n\u001b[1;32m    105\u001b[0m             ErrorToRetry(\n\u001b[1;32m    106\u001b[0m                 openai\u001b[39m.\u001b[39;49merror\u001b[39m.\u001b[39;49mAPIConnectionError, \u001b[39mlambda\u001b[39;49;00m e: e\u001b[39m.\u001b[39;49mshould_retry\n\u001b[1;32m    107\u001b[0m             ),\n\u001b[1;32m    108\u001b[0m         ],\n\u001b[1;32m    109\u001b[0m     )\n\u001b[1;32m    110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     llm_prediction \u001b[39m=\u001b[39m llm_chain\u001b[39m.\u001b[39mpredict(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfull_prompt_args)\n",
      "File \u001b[0;32m~/Projects/gpt_index/gpt_index/utils.py:157\u001b[0m, in \u001b[0;36mretry_on_exceptions_with_backoff\u001b[0;34m(lambda_fn, errors_to_retry, max_tries, min_backoff_secs, max_backoff_secs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m         \u001b[39mreturn\u001b[39;00m lambda_fn()\n\u001b[1;32m    158\u001b[0m     \u001b[39mexcept\u001b[39;00m exception_class_tuples \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    159\u001b[0m         traceback\u001b[39m.\u001b[39mprint_exc()\n",
      "File \u001b[0;32m~/Projects/gpt_index/gpt_index/langchain_helpers/chain_wrapper.py:100\u001b[0m, in \u001b[0;36mLLMPredictor._predict.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m full_prompt_args \u001b[39m=\u001b[39m prompt\u001b[39m.\u001b[39mget_full_format_args(prompt_args)\n\u001b[1;32m     98\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry_on_throttling:\n\u001b[1;32m     99\u001b[0m     llm_prediction \u001b[39m=\u001b[39m retry_on_exceptions_with_backoff(\n\u001b[0;32m--> 100\u001b[0m         \u001b[39mlambda\u001b[39;00m: llm_chain\u001b[39m.\u001b[39;49mpredict(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfull_prompt_args),\n\u001b[1;32m    101\u001b[0m         [\n\u001b[1;32m    102\u001b[0m             ErrorToRetry(openai\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mRateLimitError),\n\u001b[1;32m    103\u001b[0m             ErrorToRetry(openai\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mServiceUnavailableError),\n\u001b[1;32m    104\u001b[0m             ErrorToRetry(openai\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mTryAgain),\n\u001b[1;32m    105\u001b[0m             ErrorToRetry(\n\u001b[1;32m    106\u001b[0m                 openai\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mAPIConnectionError, \u001b[39mlambda\u001b[39;00m e: e\u001b[39m.\u001b[39mshould_retry\n\u001b[1;32m    107\u001b[0m             ),\n\u001b[1;32m    108\u001b[0m         ],\n\u001b[1;32m    109\u001b[0m     )\n\u001b[1;32m    110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     llm_prediction \u001b[39m=\u001b[39m llm_chain\u001b[39m.\u001b[39mpredict(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfull_prompt_args)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/llm.py:104\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m     91\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \n\u001b[1;32m     93\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/base.py:155\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m--> 155\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    156\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_end(outputs, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_outputs(outputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/base.py:152\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    147\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    148\u001b[0m     inputs,\n\u001b[1;32m    149\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    150\u001b[0m )\n\u001b[1;32m    151\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 152\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs)\n\u001b[1;32m    153\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/llm.py:88\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39mself\u001b[39m, inputs: Dict[\u001b[39mstr\u001b[39m, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply([inputs])[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/llm.py:79\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[0;34m(self, input_list)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, input_list: List[Dict[\u001b[39mstr\u001b[39m, Any]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]]:\n\u001b[1;32m     78\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Utilize the LLM generate method for speed gains.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(input_list)\n\u001b[1;32m     80\u001b[0m     outputs \u001b[39m=\u001b[39m []\n\u001b[1;32m     81\u001b[0m     \u001b[39mfor\u001b[39;00m generation \u001b[39min\u001b[39;00m response\u001b[39m.\u001b[39mgenerations:\n\u001b[1;32m     82\u001b[0m         \u001b[39m# Get the text of the top generated string.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/chains/llm.py:74\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     71\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mIf `stop` is present in any inputs, should be present in all.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m         )\n\u001b[1;32m     73\u001b[0m     prompts\u001b[39m.\u001b[39mappend(prompt)\n\u001b[0;32m---> 74\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate(prompts, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[1;32m     75\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/llms/base.py:79\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m---> 79\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     80\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_end(output, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m     81\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/llms/base.py:76\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m     73\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m}, prompts, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose\n\u001b[1;32m     74\u001b[0m )\n\u001b[1;32m     75\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(prompts, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[1;32m     77\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/langchain/llms/openai.py:158\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    156\u001b[0m _keys \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mcompletion_tokens\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mprompt_tokens\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtotal_tokens\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m    157\u001b[0m \u001b[39mfor\u001b[39;00m _prompts \u001b[39min\u001b[39;00m sub_prompts:\n\u001b[0;32m--> 158\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(prompt\u001b[39m=\u001b[39;49m_prompts, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    159\u001b[0m     choices\u001b[39m.\u001b[39mextend(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    160\u001b[0m     _keys_to_use \u001b[39m=\u001b[39m _keys\u001b[39m.\u001b[39mintersection(response[\u001b[39m\"\u001b[39m\u001b[39musage\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/openai/api_requestor.py:217\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    207\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    208\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    216\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 217\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[1;32m    218\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[1;32m    219\u001b[0m         url,\n\u001b[1;32m    220\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    221\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    222\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    223\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    224\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    225\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    227\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    228\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/openai/api_requestor.py:517\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    515\u001b[0m     _thread_context\u001b[39m.\u001b[39msession \u001b[39m=\u001b[39m _make_session()\n\u001b[1;32m    516\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 517\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    518\u001b[0m         method,\n\u001b[1;32m    519\u001b[0m         abs_url,\n\u001b[1;32m    520\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    521\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    522\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    523\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    524\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    527\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/envs/gpt_index-github-reader/lib/python3.11/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1373\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1375\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline(_MAXLINE \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gpt_index import QueryConfig\n",
    "from gpt_index import IndexStructType\n",
    "\n",
    "# query = \"Explain the purpose of the class `GPTTreeIndex` in the file gpt_index/indices/tree/base.py.\"\n",
    "query = \"Find every class that extends the class `BaseGPTIndex`. What are the summaries of these classes?\"\n",
    "\n",
    "query_configs = [\n",
    "    QueryConfig(\n",
    "        index_struct_type=IndexStructType.TREE,\n",
    "        query_mode=QueryMode.SUMMARIZE,\n",
    "        # query_kwargs={\n",
    "        #     \"child_branch_factor\": 2\n",
    "        # }\n",
    "    ),\n",
    "    QueryConfig(\n",
    "        index_struct_type=IndexStructType.LIST,\n",
    "        query_mode=QueryMode.EMBEDDING,\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = graph.query(\n",
    "    query, query_configs=query_configs\n",
    ")\n",
    "\n",
    "display(Markdown(f\"**Query:** {query}\"))\n",
    "display(Markdown(f\"**Response:** {response.response}\\n\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_index-github-reader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bc2ab08ee48b6366504a28e3231c27a37c154a347ee8ac6184b716eff7bdbcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
