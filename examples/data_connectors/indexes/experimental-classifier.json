{"index_struct": {"text": "\nThe documents summarize the process of building a machine learning model to predict survival on the Titanic. The first document outlines the evaluation process with a training prompt string, while the second document outlines the evaluation process without a training prompt string. The third document outlines the process of building an index with GPT List Index and querying the index. The fourth document outlines the process of getting predictions and evaluating them. The code files use the sklearn and numpy libraries to calculate accuracy scores and manipulate arrays, and the utils.py file contains helper functions for Titanic GPT-3 experiments. These functions include get_train_and_eval_data, get_sorted_dict_str, get_label_str, get_train_str, extract_float_given_response, get_eval_preds, and several prompts for summarizing and refining the data.", "doc_id": "affc8890-15e9-401f-a833-a7ffcacf1cd3", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 1,\n   \"id\": \"f445c1d1-acb9-431e-a7ff-50c41f064359\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stderr\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\\n\",\n      \"[nltk_data] Downloading package stopwords to\\n\",\n      \"[nltk_data]     /Users/jerryliu/nltk_data...\\n\",\n      \"[nltk_data]   Package stopwords is already up-to-date!\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 0, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "1": {"text": "   }\n   ],\n   \"source\": [\n    \"from utils import (\\n\",\n    \"    get_train_str,\\n\",\n    \"    get_train_and_eval_data,\\n\",\n    \"    get_eval_preds,\\n\",\n    \"    train_prompt\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"import warnings\\n\",\n    \"warnings.filterwarnings('ignore')\\n\",\n    \"warnings.simplefilter('ignore')\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 2,\n   \"id\": \"cf3cbd90-d5e1-4c30-a3bc-8b39fbd85d70\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# load up the titanic data\\n\",\n    \"train_df, train_labels, eval_df, eval_labels", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 1, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "2": {"text": "train_labels, eval_df, eval_labels = get_train_and_eval_data('data/train.csv')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"fa2634f9-cb33-4f1e-81f9-3a3b285e2580\",\n   \"metadata\": {\n    \"tags\": []\n   },\n   \"source\": [\n    \"## Few-shot Prompting with GPT-3 for Titanic Dataset\\n\",\n    \"In this section, we can show how we can prompt GPT-3 on its own (without using GPT Index) to attain ~80% accuracy on Titanic! \\n\",\n    \"\\n\",\n    \"We can do this by simply providing a few example inputs. Or we can simply provide no example inputs at all (zero-shot). Both achieve the same results.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 3,\n   \"id\":", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 2, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "3": {"text": "  \"execution_count\": 3,\n   \"id\": \"d0698fd2-1361-49ae-8c17-8124e9b932a4\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"The following structured data is provided in \\\"Feature Name\\\":\\\"Feature Value\\\" format.\\n\",\n      \"Each datapoint describes a passenger on the Titanic.\\n\",\n      \"The task is to decide whether the passenger survived.\\n\",\n      \"Some example datapoints are given below: \\n\",\n      \"-------------------\\n\",\n      \"{train_str}\\n\",\n      \"-------------------\\n\",\n      \"Given this, predict whether the following passenger survived. Return answer as a number between 0 or 1. \\n\",\n      \"{eval_str}\\n\",\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 3, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "4": {"text": "     \"{eval_str}\\n\",\n      \"Survived: \\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# first demonstrate the prompt template\\n\",\n    \"print(train_prompt.template)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 4,\n   \"id\": \"4b39e2e7-be07-42f8-a27a-3419e84cfb2c\",\n   \"metadata\": {\n    \"scrolled\": true,\n    \"tags\": []\n   },\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"Example datapoints in `train_str`: \\n\",\n      \"This is the Data:\\n\",\n      \"Age:28.0\\n\",\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 4, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "5": {"text": " \"Age:28.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:7.8958\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:male\\n\",\n      \"SibSp:0\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 0\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:17.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:7.925\\n\",\n      \"Parch:2\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:female\\n\",\n      \"SibSp:4\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 1\\n\",\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 5, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "6": {"text": "     \"Survived: 1\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:30.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:16.1\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:male\\n\",\n      \"SibSp:1\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 0\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:22.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:7.25\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:male\\n\",\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 6, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "7": {"text": "    \"Sex:male\\n\",\n      \"SibSp:0\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 0\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:45.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:13.5\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:2\\n\",\n      \"Sex:female\\n\",\n      \"SibSp:0\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 1\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:25.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:0.0\\n\",\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 7, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "8": {"text": "  \"Fare:0.0\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:male\\n\",\n      \"SibSp:0\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 1\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:18.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:20.2125\\n\",\n      \"Parch:1\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:male\\n\",\n      \"SibSp:1\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 0\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 8, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "9": {"text": "  \"This is the Data:\\n\",\n      \"Age:33.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:9.5\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:male\\n\",\n      \"SibSp:0\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 0\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:24.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:65.0\\n\",\n      \"Parch:2\\n\",\n      \"Pclass:2\\n\",\n      \"Sex:female\\n\",\n      \"SibSp:1\\n\",\n      \"This is the correct answer:\\n\",\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 9, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "10": {"text": "     \"This is the correct answer:\\n\",\n      \"Survived: 1\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:26.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:7.925\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:female\\n\",\n      \"SibSp:0\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 1\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# Get \\\"training\\\" prompt string \\n\",\n    \"train_n = 10\\n\",\n    \"eval_n = 40\\n\",\n    \"train_str = get_train_str(train_df, train_labels, train_n=train_n)\\n\",\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 10, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "11": {"text": "train_n=train_n)\\n\",\n    \"print(f\\\"Example datapoints in `train_str`: \\\\n{train_str}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"819a06f7-3171-4edb-b90c-0a3eae308a04\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Do evaluation with the training prompt string\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"4a7f2202-518c-41a3-80ab-1e98bbcca903\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from sklearn.metrics import accuracy_score\\n\",\n    \"import numpy as np\\n\",\n    \"\\n\",\n    \"eval_preds = get_eval_preds(train_prompt, train_str,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 11, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "12": {"text": "get_eval_preds(train_prompt, train_str, eval_df, n=eval_n)\\n\",\n    \"eval_label_chunk = eval_labels[:eval_n]\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 7,\n   \"id\": \"64323a4d-6eea-4e40-9eac-b2deed60192b\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"ACCURACY: 0.8\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"acc = accuracy_score(eval_label_chunk, np.array(eval_preds).round())\\n\",\n    \"print(f'ACCURACY: {acc}')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 12, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "13": {"text": " {\n   \"cell_type\": \"markdown\",\n   \"id\": \"11790d28-8f34-42dd-b11f-6aad21fd5f46\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Do evaluation with no training prompt string! \"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"aaf993e5-c363-4f18-a28f-09761e49cb6d\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from sklearn.metrics import accuracy_score\\n\",\n    \"import numpy as np\\n\",\n    \"\\n\",\n    \"eval_preds_null = get_eval_preds(train_prompt, \\\"\\\", eval_df, n=eval_n)\\n\",\n    \"eval_label_chunk = eval_labels[:eval_n]\"\n   ]\n  },\n  {\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 13, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "14": {"text": "  ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 11,\n   \"id\": \"c3b8bcd5-5972-4ce5-9aa1-57460cdde199\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"ACCURACY: 0.8\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"acc_null = accuracy_score(eval_label_chunk, np.array(eval_preds_null).round())\\n\",\n    \"print(f'ACCURACY: {acc_null}')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"8f0a5e4b-e627-4b47-a807-939813596594\",\n   \"metadata\": {\n    \"tags\":", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 14, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "15": {"text": "  \"metadata\": {\n    \"tags\": []\n   },\n   \"source\": [\n    \"## Extending with GPT List Index\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"42a1ca28-96e9-4cd2-bd48-0673917ad057\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Build Index\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 12,\n   \"id\": \"6c59b030-855d-4e27-89c3-74c972d1bf19\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from gpt_index import GPTListIndex\\n\",\n    \"from gpt_index.readers.schema.base import Document\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 13,\n   \"id\":", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 15, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "16": {"text": "  \"execution_count\": 13,\n   \"id\": \"8f9556de-e323-4318-bb71-cff75bf8c3c1\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"index = GPTListIndex([])\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"e27720fc-af36-40fd-8c55-41485248aa9f\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# insertion into index \\n\",\n    \"batch_size = 40\\n\",\n    \"num_train_chunks = 5\\n\",\n    \"\\n\",\n    \"for i in range(num_train_chunks):\\n\",\n    \"    print(f\\\"Inserting chunk: {i}/{num_train_chunks}\\\")\\n\",\n    \"    start_idx =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 16, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "17": {"text": "   \"    start_idx = i*batch_size\\n\",\n    \"    end_idx = (i+1)*batch_size\\n\",\n    \"    train_batch = train_df.iloc[start_idx:end_idx+batch_size]\\n\",\n    \"    labels_batch = train_labels.iloc[start_idx:end_idx+batch_size]\\n\",\n    \"    all_train_str = get_train_str(train_batch, labels_batch, train_n=batch_size)\\n\",\n    \"    index.insert(Document(all_train_str))\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"e78db088-6649-44db-b52a-766316713b96\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Query Index\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\":", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 17, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "18": {"text": "\"code\",\n   \"execution_count\": 15,\n   \"id\": \"9cb90564-1de2-412f-8318-d5280855004e\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from utils import query_str, qa_data_prompt, refine_prompt\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 16,\n   \"id\": \"77c1ae36-e0af-47bc-a656-4971af699755\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"'Which is the relationship between these features and predicting survival?'\"\n      ]\n     },\n     \"execution_count\": 16,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\":", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 18, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "19": {"text": "   }\n   ],\n   \"source\": [\n    \"query_str\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 17,\n   \"id\": \"c403710f-d4b3-4287-94f5-e275ea19b476\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"> Starting query: Which is the relationship between these features and predicting survival?\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"response = index.query(\\n\",\n    \"    query_str, \\n\",\n    \"    text_qa_template=qa_data_prompt, \\n\",\n    \"    refine_template=refine_prompt, \\n\",\n    \")\"\n   ]\n  },\n  {\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 19, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "20": {"text": "   \")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 19,\n   \"id\": \"d2545ab1-980a-4fbd-8add-7ef957801644\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"\\n\",\n      \"\\n\",\n      \"There is no definitive answer to this question, as the relationship between the features and predicting survival will vary depending on the data. However, some possible relationships include: age (younger passengers are more likely to survive), sex (females are more likely to survive), fare (passengers who paid more for their ticket are more likely to survive), and pclass (passengers in first or second class are more likely to survive).\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 20, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "21": {"text": "}\n   ],\n   \"source\": [\n    \"print(response)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"d0d7d260-2283-49f6-ac40-35c7071cc54d\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Get Predictions and Evaluate\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 26,\n   \"id\": \"e7b98057-957c-48ef-be85-59ff9813d201\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"The following structured data is provided in \\\"Feature Name\\\":\\\"Feature Value\\\" format.\\n\",\n      \"Each datapoint describes a passenger on the Titanic.\\n\",\n      \"The task", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 21, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "22": {"text": "Titanic.\\n\",\n      \"The task is to decide whether the passenger survived.\\n\",\n      \"We discovered the following relationship between features and survival:\\n\",\n      \"-------------------\\n\",\n      \"{train_str}\\n\",\n      \"-------------------\\n\",\n      \"Given this, predict whether the following passenger survived. \\n\",\n      \"Return answer as a number between 0 or 1. \\n\",\n      \"{eval_str}\\n\",\n      \"Survived: \\n\",\n      \"\\n\",\n      \"\\n\",\n      \"`train_str`: \\n\",\n      \"\\n\",\n      \"There is no definitive answer to this question, as the relationship between the features and predicting survival will vary depending on the data. However, some possible relationships include: age (younger passengers are more likely to survive), sex (females are more likely to survive), fare", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 22, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "23": {"text": "survive), sex (females are more likely to survive), fare (passengers who paid more for their ticket are more likely to survive), and pclass (passengers in first or second class are more likely to survive).\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# get eval preds\\n\",\n    \"from utils import train_prompt_with_context\\n\",\n    \"\\n\",\n    \"train_str = response\\n\",\n    \"print(train_prompt_with_context.template)\\n\",\n    \"print(f'\\\\n\\\\n`train_str`: {train_str}')\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"659c6a3f-1c5d-4314-87dc-908e76d50e4a\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# do evaluation\\n\",\n    \"from", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 23, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "24": {"text": "  \"# do evaluation\\n\",\n    \"from sklearn.metrics import accuracy_score\\n\",\n    \"import numpy as np\\n\",\n    \"eval_n = 40\\n\",\n    \"eval_preds = get_eval_preds(train_prompt_with_context, train_str, eval_df, n=eval_n)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 28,\n   \"id\": \"7424e7d3-2576-42bc-b626-cf8088265004\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"ACCURACY: 0.85\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"eval_label_chunk = eval_labels[:eval_n]\\n\",\n    \"acc =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 24, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "25": {"text": "   \"acc = accuracy_score(eval_label_chunk, np.array(eval_preds).round())\\n\",\n    \"print(f'ACCURACY: {acc}')\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"e010b497-eeed-4142-a8ac-f5545e85fcc2\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": []\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"gpt_retrieve_venv\",\n   \"language\": \"python\",\n   \"name\": \"gpt_retrieve_venv\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 25, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "26": {"text": "  \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.4\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 5\n}\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 26, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "27": {"text": "\"\"\"Helper functions for Titanic GPT-3 experiments.\"\"\"\n\n# form prompt, run GPT\nimport re\nfrom typing import List, Optional, Tuple\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfrom gpt_index.indices.utils import extract_numbers_given_response\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.base import Prompt\n\n\ndef get_train_and_eval_data(\n    csv_path: str,\n) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n    \"\"\"Get train and eval data.\"\"\"\n    df = pd.read_csv(csv_path)\n    label_col = \"Survived\"\n    cols_to_drop = [\"PassengerId\", \"Ticket\", \"Name\", \"Cabin\"]\n    df = df.drop(cols_to_drop, axis=1)\n    labels = df.pop(label_col)\n    train_df, eval_df, train_labels, eval_labels", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/utils.py", "file_name": "utils.py"}, "index": 27, "child_indices": [], "ref_doc_id": "ee8ec9935ad93ab3a4e6b42017174b65c2d21de5", "node_info": null}, "28": {"text": "eval_df, train_labels, eval_labels = train_test_split(\n        df, labels, test_size=0.25, random_state=0\n    )\n    return train_df, train_labels, eval_df, eval_labels\n\n\ndef get_sorted_dict_str(d: dict) -> str:\n    \"\"\"Get sorted dict string.\"\"\"\n    keys = sorted(list(d.keys()))\n    return \"\\n\".join([f\"{k}:{d[k]}\" for k in keys])\n\n\ndef get_label_str(labels: pd.Series, i: int) -> str:\n    \"\"\"Get label string.\"\"\"\n    return f\"{labels.name}: {labels.iloc[i]}\"\n\n\ndef get_train_str(\n    train_df: pd.DataFrame, train_labels: pd.Series, train_n: int = 10\n) -> str:\n    \"\"\"Get train str.\"\"\"\n    dict_list = train_df.to_dict(\"records\")[:train_n]\n    item_list = []\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/utils.py", "file_name": "utils.py"}, "index": 28, "child_indices": [], "ref_doc_id": "ee8ec9935ad93ab3a4e6b42017174b65c2d21de5", "node_info": null}, "29": {"text": "   item_list = []\n    for i, d in enumerate(dict_list):\n        dict_str = get_sorted_dict_str(d)\n        label_str = get_label_str(train_labels, i)\n        item_str = (\n            f\"This is the Data:\\n{dict_str}\\nThis is the correct answer:\\n{label_str}\"\n        )\n        item_list.append(item_str)\n\n    return \"\\n\\n\".join(item_list)\n\n\ndef extract_float_given_response(response: str, n: int = 1) -> Optional[float]:\n    \"\"\"Extract number given the GPT-generated response.\n\n    Used by tree-structured indices.\n\n    \"\"\"\n    numbers = re.findall(r\"\\d+\\.\\d+\", response)\n    if len(numbers) == 0:\n        # if no floats, try extracting ints, and convert to", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/utils.py", "file_name": "utils.py"}, "index": 29, "child_indices": [], "ref_doc_id": "ee8ec9935ad93ab3a4e6b42017174b65c2d21de5", "node_info": null}, "30": {"text": "# if no floats, try extracting ints, and convert to float\n        new_numbers = extract_numbers_given_response(response, n=n)\n        if new_numbers is None:\n            return None\n        else:\n            return float(numbers[0])\n    else:\n        return float(numbers[0])\n\n\ndef get_eval_preds(\n    train_prompt: Prompt, train_str: str, eval_df: pd.DataFrame, n: int = 20\n) -> List:\n    \"\"\"Get eval preds.\"\"\"\n    llm_predictor = LLMPredictor()\n    eval_preds = []\n    for i in range(n):\n        eval_str = get_sorted_dict_str(eval_df.iloc[i].to_dict())\n        response, _ = llm_predictor.predict(\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/utils.py", "file_name": "utils.py"}, "index": 30, "child_indices": [], "ref_doc_id": "ee8ec9935ad93ab3a4e6b42017174b65c2d21de5", "node_info": null}, "31": {"text": "           train_prompt, train_str=train_str, eval_str=eval_str\n        )\n        pred = extract_float_given_response(response)\n        print(f\"Getting preds: {i}/{n}: {pred}\")\n        if pred is None:\n            # something went wrong, impute a 0.5\n            eval_preds.append(0.5)\n        else:\n            eval_preds.append(pred)\n    return eval_preds\n\n\n# default train prompt\n\ntrain_prompt_str = (\n    \"The following structured data is provided in \"\n    '\"Feature Name\":\"Feature Value\" format.\\n'\n    \"Each datapoint describes a passenger on the Titanic.\\n\"\n    \"The task is to decide whether the passenger survived.\\n\"\n    \"Some example datapoints are", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/utils.py", "file_name": "utils.py"}, "index": 31, "child_indices": [], "ref_doc_id": "ee8ec9935ad93ab3a4e6b42017174b65c2d21de5", "node_info": null}, "32": {"text": "   \"Some example datapoints are given below: \\n\"\n    \"-------------------\\n\"\n    \"{train_str}\\n\"\n    \"-------------------\\n\"\n    \"Given this, predict whether the following passenger survived. \"\n    \"Return answer as a number between 0 or 1. \\n\"\n    \"{eval_str}\\n\"\n    \"Survived: \"\n)\n\ntrain_prompt = Prompt(\n    input_variables=[\"train_str\", \"eval_str\"], template=train_prompt_str\n)\n\n\n# prompt to summarize the data\nquery_str = \"Which is the relationship between these features and predicting survival?\"\nqa_data_str = (\n    \"The following structured data is provided in \"\n    '\"Feature Name\":\"Feature Value\" format.\\n'\n    \"Each datapoint describes a passenger on the Titanic.\\n\"\n    \"The task is to decide whether the passenger survived.\\n\"\n    \"Some example datapoints are given below: \\n\"\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/utils.py", "file_name": "utils.py"}, "index": 32, "child_indices": [], "ref_doc_id": "ee8ec9935ad93ab3a4e6b42017174b65c2d21de5", "node_info": null}, "33": {"text": "example datapoints are given below: \\n\"\n    \"-------------------\\n\"\n    \"{context_str}\\n\"\n    \"-------------------\\n\"\n    \"Given this, answer the question: {query_str}\"\n)\n\nqa_data_prompt = Prompt(\n    input_variables=[\"context_str\", \"query_str\"], template=qa_data_str\n)\n\n# prompt to refine the answer\nrefine_str = (\n    \"The original question is as follows: {query_str}\\n\"\n    \"We have provided an existing answer: {existing_answer}\\n\"\n    \"The following structured data is provided in \"\n    '\"Feature Name\":\"Feature Value\" format.\\n'\n    \"Each datapoint describes a passenger on the Titanic.\\n\"\n    \"The task is to decide whether the passenger survived.\\n\"\n    \"We have the opportunity to refine the existing answer\"\n    \"(only if needed) with some more datapoints below.\\n\"\n    \"------------\\n\"\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/utils.py", "file_name": "utils.py"}, "index": 33, "child_indices": [], "ref_doc_id": "ee8ec9935ad93ab3a4e6b42017174b65c2d21de5", "node_info": null}, "34": {"text": "   \"------------\\n\"\n    \"{context_msg}\\n\"\n    \"------------\\n\"\n    \"Given the new context, refine the original answer to better \"\n    \"answer the question. \"\n    \"If the context isn't useful, return the original answer.\"\n)\nrefine_prompt = Prompt(\n    input_variables=[\"query_str\", \"existing_answer\", \"context_msg\"],\n    template=refine_str,\n)\n\n\n# train prompt with refined context\n\ntrain_prompt_with_context_str = (\n    \"The following structured data is provided in \"\n    '\"Feature Name\":\"Feature Value\" format.\\n'\n    \"Each datapoint describes a passenger on the Titanic.\\n\"\n    \"The task is to decide whether the passenger survived.\\n\"\n    \"We discovered the following relationship between features and survival:\\n\"\n    \"-------------------\\n\"\n    \"{train_str}\\n\"\n    \"-------------------\\n\"\n    \"Given this, predict whether the following", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/utils.py", "file_name": "utils.py"}, "index": 34, "child_indices": [], "ref_doc_id": "ee8ec9935ad93ab3a4e6b42017174b65c2d21de5", "node_info": null}, "35": {"text": "   \"Given this, predict whether the following passenger survived. \\n\"\n    \"Return answer as a number between 0 or 1. \\n\"\n    \"{eval_str}\\n\"\n    \"Survived: \"\n)\n\ntrain_prompt_with_context = Prompt(\n    input_variables=[\"train_str\", \"eval_str\"], template=train_prompt_with_context_str\n)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/utils.py", "file_name": "utils.py"}, "index": 35, "child_indices": [], "ref_doc_id": "ee8ec9935ad93ab3a4e6b42017174b65c2d21de5", "node_info": null}, "36": {"text": "This code file, TitanicModel.ipynb, is used to create a few-shot prompting model with GPT-3 for the Titanic dataset. It loads up the Titanic data, provides a prompt template, and uses the get_train_and_eval_data, get_eval_preds, and train_prompt functions from the utils module to generate example inputs and predict whether a passenger survived. It also uses the sklearn.metrics module to calculate the accuracy score. The code is designed to achieve ~80% accuracy on the Titanic dataset.", "doc_id": null, "embedding": null, "extra_info": null, "index": 36, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "37": {"text": "This code file is a Jupyter Notebook that builds a classifier model to predict survival on the Titanic. It uses the sklearn library to calculate accuracy scores and the GPTListIndex library to build an index and query it. It also uses the utils library to get evaluation predictions, train prompt strings, and refine prompts. The code evaluates the model with and without a training prompt string, and then extends the model with GPT List Index. It builds an index, queries it, and then gets predictions and evaluates them. Finally, it prints out the structured data provided in \"Feature Name\":\"Feature Value\" format.", "doc_id": null, "embedding": null, "extra_info": null, "index": 37, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "38": {"text": "This code file, TitanicModel.ipynb, is used to evaluate a classifier model for the Titanic dataset. It uses the sklearn library to calculate accuracy scores and the numpy library to manipulate arrays. It also imports the utils.py file, which contains helper functions for Titanic GPT-3 experiments. These functions include get_train_and_eval_data, which splits the data into train and eval sets, get_sorted_dict_str, which returns a string of the sorted dictionary, get_label_str, which returns a string of the label, get_train_str, which returns a string of the train data, extract_float_given_response, which extracts a float from a given response, get_eval_preds, which returns a list of predictions, and several prompts for summarizing and refining the data.", "doc_id": null, "embedding": null, "extra_info": null, "index": 38, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"36": {"text": "This code file, TitanicModel.ipynb, is used to create a few-shot prompting model with GPT-3 for the Titanic dataset. It loads up the Titanic data, provides a prompt template, and uses the get_train_and_eval_data, get_eval_preds, and train_prompt functions from the utils module to generate example inputs and predict whether a passenger survived. It also uses the sklearn.metrics module to calculate the accuracy score. The code is designed to achieve ~80% accuracy on the Titanic dataset.", "doc_id": null, "embedding": null, "extra_info": null, "index": 36, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "37": {"text": "This code file is a Jupyter Notebook that builds a classifier model to predict survival on the Titanic. It uses the sklearn library to calculate accuracy scores and the GPTListIndex library to build an index and query it. It also uses the utils library to get evaluation predictions, train prompt strings, and refine prompts. The code evaluates the model with and without a training prompt string, and then extends the model with GPT List Index. It builds an index, queries it, and then gets predictions and evaluates them. Finally, it prints out the structured data provided in \"Feature Name\":\"Feature Value\" format.", "doc_id": null, "embedding": null, "extra_info": null, "index": 37, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "38": {"text": "This code file, TitanicModel.ipynb, is used to evaluate a classifier model for the Titanic dataset. It uses the sklearn library to calculate accuracy scores and the numpy library to manipulate arrays. It also imports the utils.py file, which contains helper functions for Titanic GPT-3 experiments. These functions include get_train_and_eval_data, which splits the data into train and eval sets, get_sorted_dict_str, which returns a string of the sorted dictionary, get_label_str, which returns a string of the label, get_train_str, which returns a string of the train data, extract_float_given_response, which extracts a float from a given response, get_eval_preds, which returns a list of predictions, and several prompts for summarizing and refining the data.", "doc_id": null, "embedding": null, "extra_info": null, "index": 38, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"09e8676ed9c2cd06b8836e56e3473f5b54e05398": {"text": "{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 1,\n   \"id\": \"f445c1d1-acb9-431e-a7ff-50c41f064359\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stderr\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\\n\",\n      \"[nltk_data] Downloading package stopwords to\\n\",\n      \"[nltk_data]     /Users/jerryliu/nltk_data...\\n\",\n      \"[nltk_data]   Package stopwords is already up-to-date!\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"from utils import (\\n\",\n    \"    get_train_str,\\n\",\n    \"    get_train_and_eval_data,\\n\",\n    \"    get_eval_preds,\\n\",\n    \"    train_prompt\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"import warnings\\n\",\n    \"warnings.filterwarnings('ignore')\\n\",\n    \"warnings.simplefilter('ignore')\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 2,\n   \"id\": \"cf3cbd90-d5e1-4c30-a3bc-8b39fbd85d70\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# load up the titanic data\\n\",\n    \"train_df, train_labels, eval_df, eval_labels = get_train_and_eval_data('data/train.csv')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"fa2634f9-cb33-4f1e-81f9-3a3b285e2580\",\n   \"metadata\": {\n    \"tags\": []\n   },\n   \"source\": [\n    \"## Few-shot Prompting with GPT-3 for Titanic Dataset\\n\",\n    \"In this section, we can show how we can prompt GPT-3 on its own (without using GPT Index) to attain ~80% accuracy on Titanic! \\n\",\n    \"\\n\",\n    \"We can do this by simply providing a few example inputs. Or we can simply provide no example inputs at all (zero-shot). Both achieve the same results.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 3,\n   \"id\": \"d0698fd2-1361-49ae-8c17-8124e9b932a4\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"The following structured data is provided in \\\"Feature Name\\\":\\\"Feature Value\\\" format.\\n\",\n      \"Each datapoint describes a passenger on the Titanic.\\n\",\n      \"The task is to decide whether the passenger survived.\\n\",\n      \"Some example datapoints are given below: \\n\",\n      \"-------------------\\n\",\n      \"{train_str}\\n\",\n      \"-------------------\\n\",\n      \"Given this, predict whether the following passenger survived. Return answer as a number between 0 or 1. \\n\",\n      \"{eval_str}\\n\",\n      \"Survived: \\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# first demonstrate the prompt template\\n\",\n    \"print(train_prompt.template)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 4,\n   \"id\": \"4b39e2e7-be07-42f8-a27a-3419e84cfb2c\",\n   \"metadata\": {\n    \"scrolled\": true,\n    \"tags\": []\n   },\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"Example datapoints in `train_str`: \\n\",\n      \"This is the Data:\\n\",\n      \"Age:28.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:7.8958\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:male\\n\",\n      \"SibSp:0\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 0\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:17.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:7.925\\n\",\n      \"Parch:2\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:female\\n\",\n      \"SibSp:4\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 1\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:30.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:16.1\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:male\\n\",\n      \"SibSp:1\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 0\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:22.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:7.25\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:male\\n\",\n      \"SibSp:0\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 0\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:45.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:13.5\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:2\\n\",\n      \"Sex:female\\n\",\n      \"SibSp:0\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 1\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:25.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:0.0\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:male\\n\",\n      \"SibSp:0\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 1\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:18.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:20.2125\\n\",\n      \"Parch:1\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:male\\n\",\n      \"SibSp:1\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 0\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:33.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:9.5\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:male\\n\",\n      \"SibSp:0\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 0\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:24.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:65.0\\n\",\n      \"Parch:2\\n\",\n      \"Pclass:2\\n\",\n      \"Sex:female\\n\",\n      \"SibSp:1\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 1\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:26.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:7.925\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:female\\n\",\n      \"SibSp:0\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 1\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# Get \\\"training\\\" prompt string \\n\",\n    \"train_n = 10\\n\",\n    \"eval_n = 40\\n\",\n    \"train_str = get_train_str(train_df, train_labels, train_n=train_n)\\n\",\n    \"print(f\\\"Example datapoints in `train_str`: \\\\n{train_str}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"819a06f7-3171-4edb-b90c-0a3eae308a04\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Do evaluation with the training prompt string\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"4a7f2202-518c-41a3-80ab-1e98bbcca903\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from sklearn.metrics import accuracy_score\\n\",\n    \"import numpy as np\\n\",\n    \"\\n\",\n    \"eval_preds = get_eval_preds(train_prompt, train_str, eval_df, n=eval_n)\\n\",\n    \"eval_label_chunk = eval_labels[:eval_n]\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 7,\n   \"id\": \"64323a4d-6eea-4e40-9eac-b2deed60192b\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"ACCURACY: 0.8\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"acc = accuracy_score(eval_label_chunk, np.array(eval_preds).round())\\n\",\n    \"print(f'ACCURACY: {acc}')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"11790d28-8f34-42dd-b11f-6aad21fd5f46\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Do evaluation with no training prompt string! \"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"aaf993e5-c363-4f18-a28f-09761e49cb6d\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from sklearn.metrics import accuracy_score\\n\",\n    \"import numpy as np\\n\",\n    \"\\n\",\n    \"eval_preds_null = get_eval_preds(train_prompt, \\\"\\\", eval_df, n=eval_n)\\n\",\n    \"eval_label_chunk = eval_labels[:eval_n]\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 11,\n   \"id\": \"c3b8bcd5-5972-4ce5-9aa1-57460cdde199\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"ACCURACY: 0.8\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"acc_null = accuracy_score(eval_label_chunk, np.array(eval_preds_null).round())\\n\",\n    \"print(f'ACCURACY: {acc_null}')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"8f0a5e4b-e627-4b47-a807-939813596594\",\n   \"metadata\": {\n    \"tags\": []\n   },\n   \"source\": [\n    \"## Extending with GPT List Index\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"42a1ca28-96e9-4cd2-bd48-0673917ad057\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Build Index\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 12,\n   \"id\": \"6c59b030-855d-4e27-89c3-74c972d1bf19\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from gpt_index import GPTListIndex\\n\",\n    \"from gpt_index.readers.schema.base import Document\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 13,\n   \"id\": \"8f9556de-e323-4318-bb71-cff75bf8c3c1\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"index = GPTListIndex([])\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"e27720fc-af36-40fd-8c55-41485248aa9f\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# insertion into index \\n\",\n    \"batch_size = 40\\n\",\n    \"num_train_chunks = 5\\n\",\n    \"\\n\",\n    \"for i in range(num_train_chunks):\\n\",\n    \"    print(f\\\"Inserting chunk: {i}/{num_train_chunks}\\\")\\n\",\n    \"    start_idx = i*batch_size\\n\",\n    \"    end_idx = (i+1)*batch_size\\n\",\n    \"    train_batch = train_df.iloc[start_idx:end_idx+batch_size]\\n\",\n    \"    labels_batch = train_labels.iloc[start_idx:end_idx+batch_size]\\n\",\n    \"    all_train_str = get_train_str(train_batch, labels_batch, train_n=batch_size)\\n\",\n    \"    index.insert(Document(all_train_str))\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"e78db088-6649-44db-b52a-766316713b96\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Query Index\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 15,\n   \"id\": \"9cb90564-1de2-412f-8318-d5280855004e\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from utils import query_str, qa_data_prompt, refine_prompt\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 16,\n   \"id\": \"77c1ae36-e0af-47bc-a656-4971af699755\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"'Which is the relationship between these features and predicting survival?'\"\n      ]\n     },\n     \"execution_count\": 16,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"query_str\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 17,\n   \"id\": \"c403710f-d4b3-4287-94f5-e275ea19b476\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"> Starting query: Which is the relationship between these features and predicting survival?\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"response = index.query(\\n\",\n    \"    query_str, \\n\",\n    \"    text_qa_template=qa_data_prompt, \\n\",\n    \"    refine_template=refine_prompt, \\n\",\n    \")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 19,\n   \"id\": \"d2545ab1-980a-4fbd-8add-7ef957801644\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"\\n\",\n      \"\\n\",\n      \"There is no definitive answer to this question, as the relationship between the features and predicting survival will vary depending on the data. However, some possible relationships include: age (younger passengers are more likely to survive), sex (females are more likely to survive), fare (passengers who paid more for their ticket are more likely to survive), and pclass (passengers in first or second class are more likely to survive).\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"print(response)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"d0d7d260-2283-49f6-ac40-35c7071cc54d\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Get Predictions and Evaluate\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 26,\n   \"id\": \"e7b98057-957c-48ef-be85-59ff9813d201\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"The following structured data is provided in \\\"Feature Name\\\":\\\"Feature Value\\\" format.\\n\",\n      \"Each datapoint describes a passenger on the Titanic.\\n\",\n      \"The task is to decide whether the passenger survived.\\n\",\n      \"We discovered the following relationship between features and survival:\\n\",\n      \"-------------------\\n\",\n      \"{train_str}\\n\",\n      \"-------------------\\n\",\n      \"Given this, predict whether the following passenger survived. \\n\",\n      \"Return answer as a number between 0 or 1. \\n\",\n      \"{eval_str}\\n\",\n      \"Survived: \\n\",\n      \"\\n\",\n      \"\\n\",\n      \"`train_str`: \\n\",\n      \"\\n\",\n      \"There is no definitive answer to this question, as the relationship between the features and predicting survival will vary depending on the data. However, some possible relationships include: age (younger passengers are more likely to survive), sex (females are more likely to survive), fare (passengers who paid more for their ticket are more likely to survive), and pclass (passengers in first or second class are more likely to survive).\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# get eval preds\\n\",\n    \"from utils import train_prompt_with_context\\n\",\n    \"\\n\",\n    \"train_str = response\\n\",\n    \"print(train_prompt_with_context.template)\\n\",\n    \"print(f'\\\\n\\\\n`train_str`: {train_str}')\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"659c6a3f-1c5d-4314-87dc-908e76d50e4a\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# do evaluation\\n\",\n    \"from sklearn.metrics import accuracy_score\\n\",\n    \"import numpy as np\\n\",\n    \"eval_n = 40\\n\",\n    \"eval_preds = get_eval_preds(train_prompt_with_context, train_str, eval_df, n=eval_n)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 28,\n   \"id\": \"7424e7d3-2576-42bc-b626-cf8088265004\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"ACCURACY: 0.85\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"eval_label_chunk = eval_labels[:eval_n]\\n\",\n    \"acc = accuracy_score(eval_label_chunk, np.array(eval_preds).round())\\n\",\n    \"print(f'ACCURACY: {acc}')\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"e010b497-eeed-4142-a8ac-f5545e85fcc2\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": []\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"gpt_retrieve_venv\",\n   \"language\": \"python\",\n   \"name\": \"gpt_retrieve_venv\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.4\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 5\n}\n", "doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "__type__": "Document"}, "ee8ec9935ad93ab3a4e6b42017174b65c2d21de5": {"text": "\"\"\"Helper functions for Titanic GPT-3 experiments.\"\"\"\n\n# form prompt, run GPT\nimport re\nfrom typing import List, Optional, Tuple\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfrom gpt_index.indices.utils import extract_numbers_given_response\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.base import Prompt\n\n\ndef get_train_and_eval_data(\n    csv_path: str,\n) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n    \"\"\"Get train and eval data.\"\"\"\n    df = pd.read_csv(csv_path)\n    label_col = \"Survived\"\n    cols_to_drop = [\"PassengerId\", \"Ticket\", \"Name\", \"Cabin\"]\n    df = df.drop(cols_to_drop, axis=1)\n    labels = df.pop(label_col)\n    train_df, eval_df, train_labels, eval_labels = train_test_split(\n        df, labels, test_size=0.25, random_state=0\n    )\n    return train_df, train_labels, eval_df, eval_labels\n\n\ndef get_sorted_dict_str(d: dict) -> str:\n    \"\"\"Get sorted dict string.\"\"\"\n    keys = sorted(list(d.keys()))\n    return \"\\n\".join([f\"{k}:{d[k]}\" for k in keys])\n\n\ndef get_label_str(labels: pd.Series, i: int) -> str:\n    \"\"\"Get label string.\"\"\"\n    return f\"{labels.name}: {labels.iloc[i]}\"\n\n\ndef get_train_str(\n    train_df: pd.DataFrame, train_labels: pd.Series, train_n: int = 10\n) -> str:\n    \"\"\"Get train str.\"\"\"\n    dict_list = train_df.to_dict(\"records\")[:train_n]\n    item_list = []\n    for i, d in enumerate(dict_list):\n        dict_str = get_sorted_dict_str(d)\n        label_str = get_label_str(train_labels, i)\n        item_str = (\n            f\"This is the Data:\\n{dict_str}\\nThis is the correct answer:\\n{label_str}\"\n        )\n        item_list.append(item_str)\n\n    return \"\\n\\n\".join(item_list)\n\n\ndef extract_float_given_response(response: str, n: int = 1) -> Optional[float]:\n    \"\"\"Extract number given the GPT-generated response.\n\n    Used by tree-structured indices.\n\n    \"\"\"\n    numbers = re.findall(r\"\\d+\\.\\d+\", response)\n    if len(numbers) == 0:\n        # if no floats, try extracting ints, and convert to float\n        new_numbers = extract_numbers_given_response(response, n=n)\n        if new_numbers is None:\n            return None\n        else:\n            return float(numbers[0])\n    else:\n        return float(numbers[0])\n\n\ndef get_eval_preds(\n    train_prompt: Prompt, train_str: str, eval_df: pd.DataFrame, n: int = 20\n) -> List:\n    \"\"\"Get eval preds.\"\"\"\n    llm_predictor = LLMPredictor()\n    eval_preds = []\n    for i in range(n):\n        eval_str = get_sorted_dict_str(eval_df.iloc[i].to_dict())\n        response, _ = llm_predictor.predict(\n            train_prompt, train_str=train_str, eval_str=eval_str\n        )\n        pred = extract_float_given_response(response)\n        print(f\"Getting preds: {i}/{n}: {pred}\")\n        if pred is None:\n            # something went wrong, impute a 0.5\n            eval_preds.append(0.5)\n        else:\n            eval_preds.append(pred)\n    return eval_preds\n\n\n# default train prompt\n\ntrain_prompt_str = (\n    \"The following structured data is provided in \"\n    '\"Feature Name\":\"Feature Value\" format.\\n'\n    \"Each datapoint describes a passenger on the Titanic.\\n\"\n    \"The task is to decide whether the passenger survived.\\n\"\n    \"Some example datapoints are given below: \\n\"\n    \"-------------------\\n\"\n    \"{train_str}\\n\"\n    \"-------------------\\n\"\n    \"Given this, predict whether the following passenger survived. \"\n    \"Return answer as a number between 0 or 1. \\n\"\n    \"{eval_str}\\n\"\n    \"Survived: \"\n)\n\ntrain_prompt = Prompt(\n    input_variables=[\"train_str\", \"eval_str\"], template=train_prompt_str\n)\n\n\n# prompt to summarize the data\nquery_str = \"Which is the relationship between these features and predicting survival?\"\nqa_data_str = (\n    \"The following structured data is provided in \"\n    '\"Feature Name\":\"Feature Value\" format.\\n'\n    \"Each datapoint describes a passenger on the Titanic.\\n\"\n    \"The task is to decide whether the passenger survived.\\n\"\n    \"Some example datapoints are given below: \\n\"\n    \"-------------------\\n\"\n    \"{context_str}\\n\"\n    \"-------------------\\n\"\n    \"Given this, answer the question: {query_str}\"\n)\n\nqa_data_prompt = Prompt(\n    input_variables=[\"context_str\", \"query_str\"], template=qa_data_str\n)\n\n# prompt to refine the answer\nrefine_str = (\n    \"The original question is as follows: {query_str}\\n\"\n    \"We have provided an existing answer: {existing_answer}\\n\"\n    \"The following structured data is provided in \"\n    '\"Feature Name\":\"Feature Value\" format.\\n'\n    \"Each datapoint describes a passenger on the Titanic.\\n\"\n    \"The task is to decide whether the passenger survived.\\n\"\n    \"We have the opportunity to refine the existing answer\"\n    \"(only if needed) with some more datapoints below.\\n\"\n    \"------------\\n\"\n    \"{context_msg}\\n\"\n    \"------------\\n\"\n    \"Given the new context, refine the original answer to better \"\n    \"answer the question. \"\n    \"If the context isn't useful, return the original answer.\"\n)\nrefine_prompt = Prompt(\n    input_variables=[\"query_str\", \"existing_answer\", \"context_msg\"],\n    template=refine_str,\n)\n\n\n# train prompt with refined context\n\ntrain_prompt_with_context_str = (\n    \"The following structured data is provided in \"\n    '\"Feature Name\":\"Feature Value\" format.\\n'\n    \"Each datapoint describes a passenger on the Titanic.\\n\"\n    \"The task is to decide whether the passenger survived.\\n\"\n    \"We discovered the following relationship between features and survival:\\n\"\n    \"-------------------\\n\"\n    \"{train_str}\\n\"\n    \"-------------------\\n\"\n    \"Given this, predict whether the following passenger survived. \\n\"\n    \"Return answer as a number between 0 or 1. \\n\"\n    \"{eval_str}\\n\"\n    \"Survived: \"\n)\n\ntrain_prompt_with_context = Prompt(\n    input_variables=[\"train_str\", \"eval_str\"], template=train_prompt_with_context_str\n)\n", "doc_id": "ee8ec9935ad93ab3a4e6b42017174b65c2d21de5", "embedding": null, "extra_info": {"file_path": "experimental/classifier/utils.py", "file_name": "utils.py"}, "__type__": "Document"}, "affc8890-15e9-401f-a833-a7ffcacf1cd3": {"text": "\nThe documents summarize the process of building a machine learning model to predict survival on the Titanic. The first document outlines the evaluation process with a training prompt string, while the second document outlines the evaluation process without a training prompt string. The third document outlines the process of building an index with GPT List Index and querying the index. The fourth document outlines the process of getting predictions and evaluating them. The code files use the sklearn and numpy libraries to calculate accuracy scores and manipulate arrays, and the utils.py file contains helper functions for Titanic GPT-3 experiments. These functions include get_train_and_eval_data, get_sorted_dict_str, get_label_str, get_train_str, extract_float_given_response, get_eval_preds, and several prompts for summarizing and refining the data.", "doc_id": "affc8890-15e9-401f-a833-a7ffcacf1cd3", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 1,\n   \"id\": \"f445c1d1-acb9-431e-a7ff-50c41f064359\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stderr\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\\n\",\n      \"[nltk_data] Downloading package stopwords to\\n\",\n      \"[nltk_data]     /Users/jerryliu/nltk_data...\\n\",\n      \"[nltk_data]   Package stopwords is already up-to-date!\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 0, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "1": {"text": "   }\n   ],\n   \"source\": [\n    \"from utils import (\\n\",\n    \"    get_train_str,\\n\",\n    \"    get_train_and_eval_data,\\n\",\n    \"    get_eval_preds,\\n\",\n    \"    train_prompt\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"import warnings\\n\",\n    \"warnings.filterwarnings('ignore')\\n\",\n    \"warnings.simplefilter('ignore')\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 2,\n   \"id\": \"cf3cbd90-d5e1-4c30-a3bc-8b39fbd85d70\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# load up the titanic data\\n\",\n    \"train_df, train_labels, eval_df, eval_labels", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 1, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "2": {"text": "train_labels, eval_df, eval_labels = get_train_and_eval_data('data/train.csv')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"fa2634f9-cb33-4f1e-81f9-3a3b285e2580\",\n   \"metadata\": {\n    \"tags\": []\n   },\n   \"source\": [\n    \"## Few-shot Prompting with GPT-3 for Titanic Dataset\\n\",\n    \"In this section, we can show how we can prompt GPT-3 on its own (without using GPT Index) to attain ~80% accuracy on Titanic! \\n\",\n    \"\\n\",\n    \"We can do this by simply providing a few example inputs. Or we can simply provide no example inputs at all (zero-shot). Both achieve the same results.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 3,\n   \"id\":", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 2, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "3": {"text": "  \"execution_count\": 3,\n   \"id\": \"d0698fd2-1361-49ae-8c17-8124e9b932a4\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"The following structured data is provided in \\\"Feature Name\\\":\\\"Feature Value\\\" format.\\n\",\n      \"Each datapoint describes a passenger on the Titanic.\\n\",\n      \"The task is to decide whether the passenger survived.\\n\",\n      \"Some example datapoints are given below: \\n\",\n      \"-------------------\\n\",\n      \"{train_str}\\n\",\n      \"-------------------\\n\",\n      \"Given this, predict whether the following passenger survived. Return answer as a number between 0 or 1. \\n\",\n      \"{eval_str}\\n\",\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 3, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "4": {"text": "     \"{eval_str}\\n\",\n      \"Survived: \\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# first demonstrate the prompt template\\n\",\n    \"print(train_prompt.template)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 4,\n   \"id\": \"4b39e2e7-be07-42f8-a27a-3419e84cfb2c\",\n   \"metadata\": {\n    \"scrolled\": true,\n    \"tags\": []\n   },\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"Example datapoints in `train_str`: \\n\",\n      \"This is the Data:\\n\",\n      \"Age:28.0\\n\",\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 4, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "5": {"text": " \"Age:28.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:7.8958\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:male\\n\",\n      \"SibSp:0\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 0\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:17.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:7.925\\n\",\n      \"Parch:2\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:female\\n\",\n      \"SibSp:4\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 1\\n\",\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 5, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "6": {"text": "     \"Survived: 1\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:30.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:16.1\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:male\\n\",\n      \"SibSp:1\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 0\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:22.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:7.25\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:male\\n\",\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 6, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "7": {"text": "    \"Sex:male\\n\",\n      \"SibSp:0\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 0\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:45.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:13.5\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:2\\n\",\n      \"Sex:female\\n\",\n      \"SibSp:0\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 1\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:25.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:0.0\\n\",\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 7, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "8": {"text": "  \"Fare:0.0\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:male\\n\",\n      \"SibSp:0\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 1\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:18.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:20.2125\\n\",\n      \"Parch:1\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:male\\n\",\n      \"SibSp:1\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 0\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 8, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "9": {"text": "  \"This is the Data:\\n\",\n      \"Age:33.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:9.5\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:male\\n\",\n      \"SibSp:0\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 0\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:24.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:65.0\\n\",\n      \"Parch:2\\n\",\n      \"Pclass:2\\n\",\n      \"Sex:female\\n\",\n      \"SibSp:1\\n\",\n      \"This is the correct answer:\\n\",\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 9, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "10": {"text": "     \"This is the correct answer:\\n\",\n      \"Survived: 1\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:26.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:7.925\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:female\\n\",\n      \"SibSp:0\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 1\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# Get \\\"training\\\" prompt string \\n\",\n    \"train_n = 10\\n\",\n    \"eval_n = 40\\n\",\n    \"train_str = get_train_str(train_df, train_labels, train_n=train_n)\\n\",\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 10, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "11": {"text": "train_n=train_n)\\n\",\n    \"print(f\\\"Example datapoints in `train_str`: \\\\n{train_str}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"819a06f7-3171-4edb-b90c-0a3eae308a04\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Do evaluation with the training prompt string\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"4a7f2202-518c-41a3-80ab-1e98bbcca903\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from sklearn.metrics import accuracy_score\\n\",\n    \"import numpy as np\\n\",\n    \"\\n\",\n    \"eval_preds = get_eval_preds(train_prompt, train_str,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 11, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "12": {"text": "get_eval_preds(train_prompt, train_str, eval_df, n=eval_n)\\n\",\n    \"eval_label_chunk = eval_labels[:eval_n]\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 7,\n   \"id\": \"64323a4d-6eea-4e40-9eac-b2deed60192b\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"ACCURACY: 0.8\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"acc = accuracy_score(eval_label_chunk, np.array(eval_preds).round())\\n\",\n    \"print(f'ACCURACY: {acc}')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 12, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "13": {"text": " {\n   \"cell_type\": \"markdown\",\n   \"id\": \"11790d28-8f34-42dd-b11f-6aad21fd5f46\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Do evaluation with no training prompt string! \"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"aaf993e5-c363-4f18-a28f-09761e49cb6d\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from sklearn.metrics import accuracy_score\\n\",\n    \"import numpy as np\\n\",\n    \"\\n\",\n    \"eval_preds_null = get_eval_preds(train_prompt, \\\"\\\", eval_df, n=eval_n)\\n\",\n    \"eval_label_chunk = eval_labels[:eval_n]\"\n   ]\n  },\n  {\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 13, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "14": {"text": "  ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 11,\n   \"id\": \"c3b8bcd5-5972-4ce5-9aa1-57460cdde199\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"ACCURACY: 0.8\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"acc_null = accuracy_score(eval_label_chunk, np.array(eval_preds_null).round())\\n\",\n    \"print(f'ACCURACY: {acc_null}')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"8f0a5e4b-e627-4b47-a807-939813596594\",\n   \"metadata\": {\n    \"tags\":", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 14, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "15": {"text": "  \"metadata\": {\n    \"tags\": []\n   },\n   \"source\": [\n    \"## Extending with GPT List Index\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"42a1ca28-96e9-4cd2-bd48-0673917ad057\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Build Index\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 12,\n   \"id\": \"6c59b030-855d-4e27-89c3-74c972d1bf19\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from gpt_index import GPTListIndex\\n\",\n    \"from gpt_index.readers.schema.base import Document\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 13,\n   \"id\":", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 15, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "16": {"text": "  \"execution_count\": 13,\n   \"id\": \"8f9556de-e323-4318-bb71-cff75bf8c3c1\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"index = GPTListIndex([])\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"e27720fc-af36-40fd-8c55-41485248aa9f\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# insertion into index \\n\",\n    \"batch_size = 40\\n\",\n    \"num_train_chunks = 5\\n\",\n    \"\\n\",\n    \"for i in range(num_train_chunks):\\n\",\n    \"    print(f\\\"Inserting chunk: {i}/{num_train_chunks}\\\")\\n\",\n    \"    start_idx =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 16, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "17": {"text": "   \"    start_idx = i*batch_size\\n\",\n    \"    end_idx = (i+1)*batch_size\\n\",\n    \"    train_batch = train_df.iloc[start_idx:end_idx+batch_size]\\n\",\n    \"    labels_batch = train_labels.iloc[start_idx:end_idx+batch_size]\\n\",\n    \"    all_train_str = get_train_str(train_batch, labels_batch, train_n=batch_size)\\n\",\n    \"    index.insert(Document(all_train_str))\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"e78db088-6649-44db-b52a-766316713b96\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Query Index\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\":", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 17, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "18": {"text": "\"code\",\n   \"execution_count\": 15,\n   \"id\": \"9cb90564-1de2-412f-8318-d5280855004e\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from utils import query_str, qa_data_prompt, refine_prompt\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 16,\n   \"id\": \"77c1ae36-e0af-47bc-a656-4971af699755\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"'Which is the relationship between these features and predicting survival?'\"\n      ]\n     },\n     \"execution_count\": 16,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\":", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 18, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "19": {"text": "   }\n   ],\n   \"source\": [\n    \"query_str\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 17,\n   \"id\": \"c403710f-d4b3-4287-94f5-e275ea19b476\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"> Starting query: Which is the relationship between these features and predicting survival?\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"response = index.query(\\n\",\n    \"    query_str, \\n\",\n    \"    text_qa_template=qa_data_prompt, \\n\",\n    \"    refine_template=refine_prompt, \\n\",\n    \")\"\n   ]\n  },\n  {\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 19, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "20": {"text": "   \")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 19,\n   \"id\": \"d2545ab1-980a-4fbd-8add-7ef957801644\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"\\n\",\n      \"\\n\",\n      \"There is no definitive answer to this question, as the relationship between the features and predicting survival will vary depending on the data. However, some possible relationships include: age (younger passengers are more likely to survive), sex (females are more likely to survive), fare (passengers who paid more for their ticket are more likely to survive), and pclass (passengers in first or second class are more likely to survive).\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 20, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "21": {"text": "}\n   ],\n   \"source\": [\n    \"print(response)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"d0d7d260-2283-49f6-ac40-35c7071cc54d\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Get Predictions and Evaluate\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 26,\n   \"id\": \"e7b98057-957c-48ef-be85-59ff9813d201\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"The following structured data is provided in \\\"Feature Name\\\":\\\"Feature Value\\\" format.\\n\",\n      \"Each datapoint describes a passenger on the Titanic.\\n\",\n      \"The task", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 21, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "22": {"text": "Titanic.\\n\",\n      \"The task is to decide whether the passenger survived.\\n\",\n      \"We discovered the following relationship between features and survival:\\n\",\n      \"-------------------\\n\",\n      \"{train_str}\\n\",\n      \"-------------------\\n\",\n      \"Given this, predict whether the following passenger survived. \\n\",\n      \"Return answer as a number between 0 or 1. \\n\",\n      \"{eval_str}\\n\",\n      \"Survived: \\n\",\n      \"\\n\",\n      \"\\n\",\n      \"`train_str`: \\n\",\n      \"\\n\",\n      \"There is no definitive answer to this question, as the relationship between the features and predicting survival will vary depending on the data. However, some possible relationships include: age (younger passengers are more likely to survive), sex (females are more likely to survive), fare", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 22, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "23": {"text": "survive), sex (females are more likely to survive), fare (passengers who paid more for their ticket are more likely to survive), and pclass (passengers in first or second class are more likely to survive).\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# get eval preds\\n\",\n    \"from utils import train_prompt_with_context\\n\",\n    \"\\n\",\n    \"train_str = response\\n\",\n    \"print(train_prompt_with_context.template)\\n\",\n    \"print(f'\\\\n\\\\n`train_str`: {train_str}')\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"659c6a3f-1c5d-4314-87dc-908e76d50e4a\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# do evaluation\\n\",\n    \"from", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 23, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "24": {"text": "  \"# do evaluation\\n\",\n    \"from sklearn.metrics import accuracy_score\\n\",\n    \"import numpy as np\\n\",\n    \"eval_n = 40\\n\",\n    \"eval_preds = get_eval_preds(train_prompt_with_context, train_str, eval_df, n=eval_n)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 28,\n   \"id\": \"7424e7d3-2576-42bc-b626-cf8088265004\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"ACCURACY: 0.85\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"eval_label_chunk = eval_labels[:eval_n]\\n\",\n    \"acc =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 24, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "25": {"text": "   \"acc = accuracy_score(eval_label_chunk, np.array(eval_preds).round())\\n\",\n    \"print(f'ACCURACY: {acc}')\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"e010b497-eeed-4142-a8ac-f5545e85fcc2\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": []\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"gpt_retrieve_venv\",\n   \"language\": \"python\",\n   \"name\": \"gpt_retrieve_venv\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 25, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "26": {"text": "  \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.4\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 5\n}\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "index": 26, "child_indices": [], "ref_doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "node_info": null}, "27": {"text": "\"\"\"Helper functions for Titanic GPT-3 experiments.\"\"\"\n\n# form prompt, run GPT\nimport re\nfrom typing import List, Optional, Tuple\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfrom gpt_index.indices.utils import extract_numbers_given_response\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.base import Prompt\n\n\ndef get_train_and_eval_data(\n    csv_path: str,\n) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n    \"\"\"Get train and eval data.\"\"\"\n    df = pd.read_csv(csv_path)\n    label_col = \"Survived\"\n    cols_to_drop = [\"PassengerId\", \"Ticket\", \"Name\", \"Cabin\"]\n    df = df.drop(cols_to_drop, axis=1)\n    labels = df.pop(label_col)\n    train_df, eval_df, train_labels, eval_labels", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/utils.py", "file_name": "utils.py"}, "index": 27, "child_indices": [], "ref_doc_id": "ee8ec9935ad93ab3a4e6b42017174b65c2d21de5", "node_info": null}, "28": {"text": "eval_df, train_labels, eval_labels = train_test_split(\n        df, labels, test_size=0.25, random_state=0\n    )\n    return train_df, train_labels, eval_df, eval_labels\n\n\ndef get_sorted_dict_str(d: dict) -> str:\n    \"\"\"Get sorted dict string.\"\"\"\n    keys = sorted(list(d.keys()))\n    return \"\\n\".join([f\"{k}:{d[k]}\" for k in keys])\n\n\ndef get_label_str(labels: pd.Series, i: int) -> str:\n    \"\"\"Get label string.\"\"\"\n    return f\"{labels.name}: {labels.iloc[i]}\"\n\n\ndef get_train_str(\n    train_df: pd.DataFrame, train_labels: pd.Series, train_n: int = 10\n) -> str:\n    \"\"\"Get train str.\"\"\"\n    dict_list = train_df.to_dict(\"records\")[:train_n]\n    item_list = []\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/utils.py", "file_name": "utils.py"}, "index": 28, "child_indices": [], "ref_doc_id": "ee8ec9935ad93ab3a4e6b42017174b65c2d21de5", "node_info": null}, "29": {"text": "   item_list = []\n    for i, d in enumerate(dict_list):\n        dict_str = get_sorted_dict_str(d)\n        label_str = get_label_str(train_labels, i)\n        item_str = (\n            f\"This is the Data:\\n{dict_str}\\nThis is the correct answer:\\n{label_str}\"\n        )\n        item_list.append(item_str)\n\n    return \"\\n\\n\".join(item_list)\n\n\ndef extract_float_given_response(response: str, n: int = 1) -> Optional[float]:\n    \"\"\"Extract number given the GPT-generated response.\n\n    Used by tree-structured indices.\n\n    \"\"\"\n    numbers = re.findall(r\"\\d+\\.\\d+\", response)\n    if len(numbers) == 0:\n        # if no floats, try extracting ints, and convert to", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/utils.py", "file_name": "utils.py"}, "index": 29, "child_indices": [], "ref_doc_id": "ee8ec9935ad93ab3a4e6b42017174b65c2d21de5", "node_info": null}, "30": {"text": "# if no floats, try extracting ints, and convert to float\n        new_numbers = extract_numbers_given_response(response, n=n)\n        if new_numbers is None:\n            return None\n        else:\n            return float(numbers[0])\n    else:\n        return float(numbers[0])\n\n\ndef get_eval_preds(\n    train_prompt: Prompt, train_str: str, eval_df: pd.DataFrame, n: int = 20\n) -> List:\n    \"\"\"Get eval preds.\"\"\"\n    llm_predictor = LLMPredictor()\n    eval_preds = []\n    for i in range(n):\n        eval_str = get_sorted_dict_str(eval_df.iloc[i].to_dict())\n        response, _ = llm_predictor.predict(\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/utils.py", "file_name": "utils.py"}, "index": 30, "child_indices": [], "ref_doc_id": "ee8ec9935ad93ab3a4e6b42017174b65c2d21de5", "node_info": null}, "31": {"text": "           train_prompt, train_str=train_str, eval_str=eval_str\n        )\n        pred = extract_float_given_response(response)\n        print(f\"Getting preds: {i}/{n}: {pred}\")\n        if pred is None:\n            # something went wrong, impute a 0.5\n            eval_preds.append(0.5)\n        else:\n            eval_preds.append(pred)\n    return eval_preds\n\n\n# default train prompt\n\ntrain_prompt_str = (\n    \"The following structured data is provided in \"\n    '\"Feature Name\":\"Feature Value\" format.\\n'\n    \"Each datapoint describes a passenger on the Titanic.\\n\"\n    \"The task is to decide whether the passenger survived.\\n\"\n    \"Some example datapoints are", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/utils.py", "file_name": "utils.py"}, "index": 31, "child_indices": [], "ref_doc_id": "ee8ec9935ad93ab3a4e6b42017174b65c2d21de5", "node_info": null}, "32": {"text": "   \"Some example datapoints are given below: \\n\"\n    \"-------------------\\n\"\n    \"{train_str}\\n\"\n    \"-------------------\\n\"\n    \"Given this, predict whether the following passenger survived. \"\n    \"Return answer as a number between 0 or 1. \\n\"\n    \"{eval_str}\\n\"\n    \"Survived: \"\n)\n\ntrain_prompt = Prompt(\n    input_variables=[\"train_str\", \"eval_str\"], template=train_prompt_str\n)\n\n\n# prompt to summarize the data\nquery_str = \"Which is the relationship between these features and predicting survival?\"\nqa_data_str = (\n    \"The following structured data is provided in \"\n    '\"Feature Name\":\"Feature Value\" format.\\n'\n    \"Each datapoint describes a passenger on the Titanic.\\n\"\n    \"The task is to decide whether the passenger survived.\\n\"\n    \"Some example datapoints are given below: \\n\"\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/utils.py", "file_name": "utils.py"}, "index": 32, "child_indices": [], "ref_doc_id": "ee8ec9935ad93ab3a4e6b42017174b65c2d21de5", "node_info": null}, "33": {"text": "example datapoints are given below: \\n\"\n    \"-------------------\\n\"\n    \"{context_str}\\n\"\n    \"-------------------\\n\"\n    \"Given this, answer the question: {query_str}\"\n)\n\nqa_data_prompt = Prompt(\n    input_variables=[\"context_str\", \"query_str\"], template=qa_data_str\n)\n\n# prompt to refine the answer\nrefine_str = (\n    \"The original question is as follows: {query_str}\\n\"\n    \"We have provided an existing answer: {existing_answer}\\n\"\n    \"The following structured data is provided in \"\n    '\"Feature Name\":\"Feature Value\" format.\\n'\n    \"Each datapoint describes a passenger on the Titanic.\\n\"\n    \"The task is to decide whether the passenger survived.\\n\"\n    \"We have the opportunity to refine the existing answer\"\n    \"(only if needed) with some more datapoints below.\\n\"\n    \"------------\\n\"\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/utils.py", "file_name": "utils.py"}, "index": 33, "child_indices": [], "ref_doc_id": "ee8ec9935ad93ab3a4e6b42017174b65c2d21de5", "node_info": null}, "34": {"text": "   \"------------\\n\"\n    \"{context_msg}\\n\"\n    \"------------\\n\"\n    \"Given the new context, refine the original answer to better \"\n    \"answer the question. \"\n    \"If the context isn't useful, return the original answer.\"\n)\nrefine_prompt = Prompt(\n    input_variables=[\"query_str\", \"existing_answer\", \"context_msg\"],\n    template=refine_str,\n)\n\n\n# train prompt with refined context\n\ntrain_prompt_with_context_str = (\n    \"The following structured data is provided in \"\n    '\"Feature Name\":\"Feature Value\" format.\\n'\n    \"Each datapoint describes a passenger on the Titanic.\\n\"\n    \"The task is to decide whether the passenger survived.\\n\"\n    \"We discovered the following relationship between features and survival:\\n\"\n    \"-------------------\\n\"\n    \"{train_str}\\n\"\n    \"-------------------\\n\"\n    \"Given this, predict whether the following", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/utils.py", "file_name": "utils.py"}, "index": 34, "child_indices": [], "ref_doc_id": "ee8ec9935ad93ab3a4e6b42017174b65c2d21de5", "node_info": null}, "35": {"text": "   \"Given this, predict whether the following passenger survived. \\n\"\n    \"Return answer as a number between 0 or 1. \\n\"\n    \"{eval_str}\\n\"\n    \"Survived: \"\n)\n\ntrain_prompt_with_context = Prompt(\n    input_variables=[\"train_str\", \"eval_str\"], template=train_prompt_with_context_str\n)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "experimental/classifier/utils.py", "file_name": "utils.py"}, "index": 35, "child_indices": [], "ref_doc_id": "ee8ec9935ad93ab3a4e6b42017174b65c2d21de5", "node_info": null}, "36": {"text": "This code file, TitanicModel.ipynb, is used to create a few-shot prompting model with GPT-3 for the Titanic dataset. It loads up the Titanic data, provides a prompt template, and uses the get_train_and_eval_data, get_eval_preds, and train_prompt functions from the utils module to generate example inputs and predict whether a passenger survived. It also uses the sklearn.metrics module to calculate the accuracy score. The code is designed to achieve ~80% accuracy on the Titanic dataset.", "doc_id": null, "embedding": null, "extra_info": null, "index": 36, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "37": {"text": "This code file is a Jupyter Notebook that builds a classifier model to predict survival on the Titanic. It uses the sklearn library to calculate accuracy scores and the GPTListIndex library to build an index and query it. It also uses the utils library to get evaluation predictions, train prompt strings, and refine prompts. The code evaluates the model with and without a training prompt string, and then extends the model with GPT List Index. It builds an index, queries it, and then gets predictions and evaluates them. Finally, it prints out the structured data provided in \"Feature Name\":\"Feature Value\" format.", "doc_id": null, "embedding": null, "extra_info": null, "index": 37, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "38": {"text": "This code file, TitanicModel.ipynb, is used to evaluate a classifier model for the Titanic dataset. It uses the sklearn library to calculate accuracy scores and the numpy library to manipulate arrays. It also imports the utils.py file, which contains helper functions for Titanic GPT-3 experiments. These functions include get_train_and_eval_data, which splits the data into train and eval sets, get_sorted_dict_str, which returns a string of the sorted dictionary, get_label_str, which returns a string of the label, get_train_str, which returns a string of the train data, extract_float_given_response, which extracts a float from a given response, get_eval_preds, which returns a list of predictions, and several prompts for summarizing and refining the data.", "doc_id": null, "embedding": null, "extra_info": null, "index": 38, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"36": {"text": "This code file, TitanicModel.ipynb, is used to create a few-shot prompting model with GPT-3 for the Titanic dataset. It loads up the Titanic data, provides a prompt template, and uses the get_train_and_eval_data, get_eval_preds, and train_prompt functions from the utils module to generate example inputs and predict whether a passenger survived. It also uses the sklearn.metrics module to calculate the accuracy score. The code is designed to achieve ~80% accuracy on the Titanic dataset.", "doc_id": null, "embedding": null, "extra_info": null, "index": 36, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "37": {"text": "This code file is a Jupyter Notebook that builds a classifier model to predict survival on the Titanic. It uses the sklearn library to calculate accuracy scores and the GPTListIndex library to build an index and query it. It also uses the utils library to get evaluation predictions, train prompt strings, and refine prompts. The code evaluates the model with and without a training prompt string, and then extends the model with GPT List Index. It builds an index, queries it, and then gets predictions and evaluates them. Finally, it prints out the structured data provided in \"Feature Name\":\"Feature Value\" format.", "doc_id": null, "embedding": null, "extra_info": null, "index": 37, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "38": {"text": "This code file, TitanicModel.ipynb, is used to evaluate a classifier model for the Titanic dataset. It uses the sklearn library to calculate accuracy scores and the numpy library to manipulate arrays. It also imports the utils.py file, which contains helper functions for Titanic GPT-3 experiments. These functions include get_train_and_eval_data, which splits the data into train and eval sets, get_sorted_dict_str, which returns a string of the sorted dictionary, get_label_str, which returns a string of the label, get_train_str, which returns a string of the train data, extract_float_given_response, which extracts a float from a given response, get_eval_preds, which returns a list of predictions, and several prompts for summarizing and refining the data.", "doc_id": null, "embedding": null, "extra_info": null, "index": 38, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}