{"index_struct": {"text": "\nThe __init__.py file is an init file.\n\nThe base.py file contains the BaseEmbedding class which provides methods for getting query and text embeddings, and calculating similarity between embeddings.\n\nThe langchain.py file contains the LangchainEmbedding class which is a wrapper for the Langchain embeddings class.\n\nThe openai.py file contains the OpenAIEmbeddingMode, OpenAIEmbeddingModelType, OpenAIEmbeddingModeModel, and get_embedding functions which provide methods for getting embeddings from OpenAI.\n\nThe gpt_index/embeddings/openai.py file contains code for the BaseEmbedding class, which is a base class for embeddings. It contains methods for getting query and text embeddings, calculating similarity between embeddings, and tracking the total tokens used and the last token usage. It also contains code for the LangchainEmbedding and OpenAIEmbedding classes, which are wrappers for external embeddings from Langchain and OpenAI, respectively. The OpenAIEmbedding class contains methods for setting the embedding mode and model type,", "doc_id": "f1e6d009-43a7-483a-a632-a9e716d40264", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "node_info": null}, "1": {"text": "\"\"\"Base embeddings file.\"\"\"\n\nfrom abc import abstractmethod\nfrom enum import Enum\nfrom typing import Callable, List, Optional\n\nimport numpy as np\n\nfrom gpt_index.utils import globals_helper\n\n# TODO: change to numpy array\nEMB_TYPE = List\n\n\nclass SimilarityMode(str, Enum):\n    \"\"\"Modes for similarity/distance.\"\"\"\n\n    DEFAULT = \"cosine\"\n    DOT_PRODUCT = \"dot_product\"\n    EUCLIDEAN = \"euclidean\"\n\n\nclass BaseEmbedding:\n    \"\"\"Base class for embeddings.\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Init params.\"\"\"\n        self._total_tokens_used = 0\n        self._last_token_usage: Optional[int] = None\n        self._tokenizer: Callable = globals_helper.tokenizer\n\n    @abstractmethod\n    def _get_query_embedding(self, query: str) ->", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/base.py", "file_name": "base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "6b44f11bc6046627318f35f08e5701d0cdce4208", "node_info": null}, "2": {"text": "def _get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding.\"\"\"\n\n    def get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding.\"\"\"\n        query_embedding = self._get_query_embedding(query)\n        query_tokens_count = len(self._tokenizer(query))\n        self._total_tokens_used += query_tokens_count\n        return query_embedding\n\n    @abstractmethod\n    def _get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get text embedding.\"\"\"\n\n    def get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get text embedding.\"\"\"\n        text_embedding = self._get_text_embedding(text)\n        text_tokens_count =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/base.py", "file_name": "base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "6b44f11bc6046627318f35f08e5701d0cdce4208", "node_info": null}, "3": {"text": "       text_tokens_count = len(self._tokenizer(text))\n        self._total_tokens_used += text_tokens_count\n        return text_embedding\n\n    def similarity(\n        self,\n        embedding1: EMB_TYPE,\n        embedding2: EMB_TYPE,\n        mode: SimilarityMode = SimilarityMode.DEFAULT,\n    ) -> float:\n        \"\"\"Get embedding similarity.\"\"\"\n        if mode == SimilarityMode.EUCLIDEAN:\n            return float(np.linalg.norm(np.array(embedding1) - np.array(embedding2)))\n        elif mode == SimilarityMode.DOT_PRODUCT:\n            product = np.dot(embedding1, embedding2)\n            return product\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/base.py", "file_name": "base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "6b44f11bc6046627318f35f08e5701d0cdce4208", "node_info": null}, "4": {"text": "       return product\n        else:\n            product = np.dot(embedding1, embedding2)\n            norm = np.linalg.norm(embedding1) * np.linalg.norm(embedding2)\n            return product / norm\n\n    @property\n    def total_tokens_used(self) -> int:\n        \"\"\"Get the total tokens used so far.\"\"\"\n        return self._total_tokens_used\n\n    @property\n    def last_token_usage(self) -> int:\n        \"\"\"Get the last token usage.\"\"\"\n        if self._last_token_usage is None:\n            return 0\n        return self._last_token_usage\n\n    @last_token_usage.setter\n    def last_token_usage(self, value: int) -> None:\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/base.py", "file_name": "base.py"}, "index": 4, "child_indices": [], "ref_doc_id": "6b44f11bc6046627318f35f08e5701d0cdce4208", "node_info": null}, "5": {"text": "value: int) -> None:\n        \"\"\"Set the last token usage.\"\"\"\n        self._last_token_usage = value\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/base.py", "file_name": "base.py"}, "index": 5, "child_indices": [], "ref_doc_id": "6b44f11bc6046627318f35f08e5701d0cdce4208", "node_info": null}, "6": {"text": "\"\"\"Langchain Embedding Wrapper Module.\"\"\"\n\n\nfrom typing import List\n\nfrom langchain.embeddings.base import Embeddings as LCEmbeddings\n\nfrom gpt_index.embeddings.base import BaseEmbedding\n\n\nclass LangchainEmbedding(BaseEmbedding):\n    \"\"\"External embeddings (taken from Langchain).\n\n    Args:\n        langchain_embedding (langchain.embeddings.Embeddings): Langchain\n            embeddings class.\n    \"\"\"\n\n    def __init__(self, langchain_embedding: LCEmbeddings) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__()\n        self._langchain_embedding = langchain_embedding\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding.\"\"\"\n        return self._langchain_embedding.embed_query(query)\n\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/langchain.py", "file_name": "langchain.py"}, "index": 6, "child_indices": [], "ref_doc_id": "8b01c1a38fd076f3f050d480e1a614de889880a0", "node_info": null}, "7": {"text": "   def _get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get text embedding.\"\"\"\n        return self._langchain_embedding.embed_documents([text])[0]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/langchain.py", "file_name": "langchain.py"}, "index": 7, "child_indices": [], "ref_doc_id": "8b01c1a38fd076f3f050d480e1a614de889880a0", "node_info": null}, "8": {"text": "\"\"\"OpenAI embeddings file.\"\"\"\n\nfrom enum import Enum\nfrom typing import List, Optional\n\nimport openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\n\nfrom gpt_index.embeddings.base import BaseEmbedding\n\n\nclass OpenAIEmbeddingMode(str, Enum):\n    \"\"\"OpenAI embedding mode.\"\"\"\n\n    SIMILARITY_MODE = \"similarity\"\n    TEXT_SEARCH_MODE = \"text_search\"\n\n\nclass OpenAIEmbeddingModelType(str, Enum):\n    \"\"\"OpenAI embedding model type.\"\"\"\n\n    DAVINCI = \"davinci\"\n    CURIE = \"curie\"\n    BABBAGE = \"babbage\"\n    ADA = \"ada\"\n    TEXT_EMBED_ADA_002 = \"text-embedding-ada-002\"\n\n\nclass OpenAIEmbeddingModeModel(str, Enum):\n    \"\"\"OpenAI embedding mode model.\"\"\"\n\n    # davinci\n    TEXT_SIMILARITY_DAVINCI =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/openai.py", "file_name": "openai.py"}, "index": 8, "child_indices": [], "ref_doc_id": "19f1d4072db017613451127614ed9fec18193f70", "node_info": null}, "9": {"text": "   TEXT_SIMILARITY_DAVINCI = \"text-similarity-davinci-001\"\n    TEXT_SEARCH_DAVINCI_QUERY = \"text-search-davinci-query-001\"\n    TEXT_SEARCH_DAVINCI_DOC = \"text-search-davinci-doc-001\"\n\n    # curie\n    TEXT_SIMILARITY_CURIE = \"text-similarity-curie-001\"\n    TEXT_SEARCH_CURIE_QUERY = \"text-search-curie-query-001\"\n    TEXT_SEARCH_CURIE_DOC = \"text-search-curie-doc-001\"\n\n    # babbage\n    TEXT_SIMILARITY_BABBAGE = \"text-similarity-babbage-001\"\n    TEXT_SEARCH_BABBAGE_QUERY = \"text-search-babbage-query-001\"\n    TEXT_SEARCH_BABBAGE_DOC = \"text-search-babbage-doc-001\"\n\n    # ada\n    TEXT_SIMILARITY_ADA =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/openai.py", "file_name": "openai.py"}, "index": 9, "child_indices": [], "ref_doc_id": "19f1d4072db017613451127614ed9fec18193f70", "node_info": null}, "10": {"text": " # ada\n    TEXT_SIMILARITY_ADA = \"text-similarity-ada-001\"\n    TEXT_SEARCH_ADA_QUERY = \"text-search-ada-query-001\"\n    TEXT_SEARCH_ADA_DOC = \"text-search-ada-doc-001\"\n\n    # text-embedding-ada-002\n    TEXT_EMBED_ADA_002 = \"text-embedding-ada-002\"\n\n\n# convenient shorthand\nOAEM = OpenAIEmbeddingMode\nOAEMT = OpenAIEmbeddingModelType\nOAEMM = OpenAIEmbeddingModeModel\n\nEMBED_MAX_TOKEN_LIMIT = 2048\n\n\n_QUERY_MODE_MODEL_DICT = {\n    (OAEM.SIMILARITY_MODE, \"davinci\"): OAEMM.TEXT_SIMILARITY_DAVINCI,\n    (OAEM.SIMILARITY_MODE, \"curie\"): OAEMM.TEXT_SIMILARITY_CURIE,\n    (OAEM.SIMILARITY_MODE, \"babbage\"):", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/openai.py", "file_name": "openai.py"}, "index": 10, "child_indices": [], "ref_doc_id": "19f1d4072db017613451127614ed9fec18193f70", "node_info": null}, "11": {"text": "(OAEM.SIMILARITY_MODE, \"babbage\"): OAEMM.TEXT_SIMILARITY_BABBAGE,\n    (OAEM.SIMILARITY_MODE, \"ada\"): OAEMM.TEXT_SIMILARITY_ADA,\n    (OAEM.SIMILARITY_MODE, \"text-embedding-ada-002\"): OAEMM.TEXT_EMBED_ADA_002,\n    (OAEM.TEXT_SEARCH_MODE, \"davinci\"): OAEMM.TEXT_SEARCH_DAVINCI_QUERY,\n    (OAEM.TEXT_SEARCH_MODE, \"curie\"): OAEMM.TEXT_SEARCH_CURIE_QUERY,\n    (OAEM.TEXT_SEARCH_MODE, \"babbage\"): OAEMM.TEXT_SEARCH_BABBAGE_QUERY,\n    (OAEM.TEXT_SEARCH_MODE, \"ada\"): OAEMM.TEXT_SEARCH_ADA_QUERY,\n    (OAEM.TEXT_SEARCH_MODE, \"text-embedding-ada-002\"):", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/openai.py", "file_name": "openai.py"}, "index": 11, "child_indices": [], "ref_doc_id": "19f1d4072db017613451127614ed9fec18193f70", "node_info": null}, "12": {"text": "\"text-embedding-ada-002\"): OAEMM.TEXT_EMBED_ADA_002,\n}\n\n_TEXT_MODE_MODEL_DICT = {\n    (OAEM.SIMILARITY_MODE, \"davinci\"): OAEMM.TEXT_SIMILARITY_DAVINCI,\n    (OAEM.SIMILARITY_MODE, \"curie\"): OAEMM.TEXT_SIMILARITY_CURIE,\n    (OAEM.SIMILARITY_MODE, \"babbage\"): OAEMM.TEXT_SIMILARITY_BABBAGE,\n    (OAEM.SIMILARITY_MODE, \"ada\"): OAEMM.TEXT_SIMILARITY_ADA,\n    (OAEM.SIMILARITY_MODE, \"text-embedding-ada-002\"): OAEMM.TEXT_EMBED_ADA_002,\n    (OAEM.TEXT_SEARCH_MODE, \"davinci\"): OAEMM.TEXT_SEARCH_DAVINCI_DOC,\n    (OAEM.TEXT_SEARCH_MODE, \"curie\"):", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/openai.py", "file_name": "openai.py"}, "index": 12, "child_indices": [], "ref_doc_id": "19f1d4072db017613451127614ed9fec18193f70", "node_info": null}, "13": {"text": "(OAEM.TEXT_SEARCH_MODE, \"curie\"): OAEMM.TEXT_SEARCH_CURIE_DOC,\n    (OAEM.TEXT_SEARCH_MODE, \"babbage\"): OAEMM.TEXT_SEARCH_BABBAGE_DOC,\n    (OAEM.TEXT_SEARCH_MODE, \"ada\"): OAEMM.TEXT_SEARCH_ADA_DOC,\n    (OAEM.TEXT_SEARCH_MODE, \"text-embedding-ada-002\"): OAEMM.TEXT_EMBED_ADA_002,\n}\n\n\n@retry(wait=wait_random_exponential(min=20, max=60), stop=stop_after_attempt(100))\ndef get_embedding(\n    text: str,\n    engine: Optional[str] = None,\n) -> List[float]:\n    \"\"\"Get embedding.\n\n    NOTE: Copied from OpenAI's embedding utils:\n    https://github.com/openai/openai-python/blob/main/openai/embeddings_utils.py\n\n    Copied here to avoid importing unnecessary dependencies\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/openai.py", "file_name": "openai.py"}, "index": 13, "child_indices": [], "ref_doc_id": "19f1d4072db017613451127614ed9fec18193f70", "node_info": null}, "14": {"text": "  Copied here to avoid importing unnecessary dependencies\n    like matplotlib, plotly, scipy, sklearn.\n\n    \"\"\"\n    text = text.replace(\"\\n\", \" \")\n    return openai.Embedding.create(input=[text], engine=engine)[\"data\"][0][\"embedding\"]\n\n\nclass OpenAIEmbedding(BaseEmbedding):\n    \"\"\"OpenAI class for embeddings.\n\n    Args:\n        mode (str): Mode for embedding.\n            Defaults to OpenAIEmbeddingMode.TEXT_SEARCH_MODE.\n            Options are:\n\n            - OpenAIEmbeddingMode.SIMILARITY_MODE\n            - OpenAIEmbeddingMode.TEXT_SEARCH_MODE\n\n        model (str): Model for embedding.\n            Defaults to OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002.\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/openai.py", "file_name": "openai.py"}, "index": 14, "child_indices": [], "ref_doc_id": "19f1d4072db017613451127614ed9fec18193f70", "node_info": null}, "15": {"text": "           Options are:\n\n            - OpenAIEmbeddingModelType.DAVINCI\n            - OpenAIEmbeddingModelType.CURIE\n            - OpenAIEmbeddingModelType.BABBAGE\n            - OpenAIEmbeddingModelType.ADA\n            - OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002\n\n        deployment_name (Optional[str]): Optional deployment of model. Defaults to None.\n            If this value is not None, mode and model will be ignored.\n            Only available for using AzureOpenAI.\n    \"\"\"\n\n    def __init__(\n        self,\n        mode: str = OpenAIEmbeddingMode.TEXT_SEARCH_MODE,\n        model: str", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/openai.py", "file_name": "openai.py"}, "index": 15, "child_indices": [], "ref_doc_id": "19f1d4072db017613451127614ed9fec18193f70", "node_info": null}, "16": {"text": "       model: str = OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002,\n        deployment_name: Optional[str] = None,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__()\n        self.mode = OpenAIEmbeddingMode(mode)\n        self.model = OpenAIEmbeddingModelType(model)\n        self.deployment_name = deployment_name\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding.\"\"\"\n        if self.deployment_name is not None:\n            engine = self.deployment_name\n        else:\n            key = (self.mode, self.model)\n            if key not in", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/openai.py", "file_name": "openai.py"}, "index": 16, "child_indices": [], "ref_doc_id": "19f1d4072db017613451127614ed9fec18193f70", "node_info": null}, "17": {"text": "           if key not in _QUERY_MODE_MODEL_DICT:\n                raise ValueError(f\"Invalid mode, model combination: {key}\")\n            engine = _QUERY_MODE_MODEL_DICT[key]\n        return get_embedding(query, engine=engine)\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get text embedding.\"\"\"\n        if self.deployment_name is not None:\n            engine = self.deployment_name\n        else:\n            key = (self.mode, self.model)\n            if key not in _TEXT_MODE_MODEL_DICT:\n                raise ValueError(f\"Invalid mode, model combination: {key}\")\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/openai.py", "file_name": "openai.py"}, "index": 17, "child_indices": [], "ref_doc_id": "19f1d4072db017613451127614ed9fec18193f70", "node_info": null}, "18": {"text": "mode, model combination: {key}\")\n            engine = _TEXT_MODE_MODEL_DICT[key]\n        return get_embedding(text, engine=engine)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/openai.py", "file_name": "openai.py"}, "index": 18, "child_indices": [], "ref_doc_id": "19f1d4072db017613451127614ed9fec18193f70", "node_info": null}, "19": {"text": "\"\"\"Embedding utils for gpt index.\"\"\"\n\nfrom typing import List\n\n\ndef save_embedding(embedding: List[float], file_path: str) -> None:\n    \"\"\"Save embedding to file.\"\"\"\n    with open(file_path, \"w\") as f:\n        f.write(\",\".join([str(x) for x in embedding]))\n\n\ndef load_embedding(file_path: str) -> List[float]:\n    \"\"\"Load embedding from file. Will only return first embedding in file.\"\"\"\n    with open(file_path, \"r\") as f:\n        for line in f:\n            embedding = [float(x) for x in line.strip().split(\",\")]\n            break\n        return embedding\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/utils.py", "file_name": "utils.py"}, "index": 19, "child_indices": [], "ref_doc_id": "b854a6fa5a966671ea193573c1ecc21b6ad63014", "node_info": null}, "20": {"text": "This code file contains the code for the BaseEmbedding class, which is a base class for embeddings. It contains methods for getting query and text embeddings, calculating similarity between embeddings, and tracking the total tokens used and the last token usage. It also contains code for the LangchainEmbedding and OpenAIEmbedding classes, which are wrappers for external embeddings from Langchain and OpenAI, respectively. The OpenAIEmbedding class contains methods for setting the embedding mode and model type, and for retrieving the query and text embeddings.", "doc_id": null, "embedding": null, "extra_info": null, "index": 20, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "21": {"text": "Openai.py is a code file that contains functions and classes related to OpenAI embeddings. It includes functions for getting query and text embeddings, as well as a class for OpenAI embeddings. It also includes a function for saving and loading embeddings from a file. The code is used to create embeddings from text and query inputs, and to save and load embeddings from a file.", "doc_id": null, "embedding": null, "extra_info": null, "index": 21, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"20": {"text": "This code file contains the code for the BaseEmbedding class, which is a base class for embeddings. It contains methods for getting query and text embeddings, calculating similarity between embeddings, and tracking the total tokens used and the last token usage. It also contains code for the LangchainEmbedding and OpenAIEmbedding classes, which are wrappers for external embeddings from Langchain and OpenAI, respectively. The OpenAIEmbedding class contains methods for setting the embedding mode and model type, and for retrieving the query and text embeddings.", "doc_id": null, "embedding": null, "extra_info": null, "index": 20, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "21": {"text": "Openai.py is a code file that contains functions and classes related to OpenAI embeddings. It includes functions for getting query and text embeddings, as well as a class for OpenAI embeddings. It also includes a function for saving and loading embeddings from a file. The code is used to create embeddings from text and query inputs, and to save and load embeddings from a file.", "doc_id": null, "embedding": null, "extra_info": null, "index": 21, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"1d4640565ae2765d9ca96a509dc9809217f62f2f": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "6b44f11bc6046627318f35f08e5701d0cdce4208": {"text": "\"\"\"Base embeddings file.\"\"\"\n\nfrom abc import abstractmethod\nfrom enum import Enum\nfrom typing import Callable, List, Optional\n\nimport numpy as np\n\nfrom gpt_index.utils import globals_helper\n\n# TODO: change to numpy array\nEMB_TYPE = List\n\n\nclass SimilarityMode(str, Enum):\n    \"\"\"Modes for similarity/distance.\"\"\"\n\n    DEFAULT = \"cosine\"\n    DOT_PRODUCT = \"dot_product\"\n    EUCLIDEAN = \"euclidean\"\n\n\nclass BaseEmbedding:\n    \"\"\"Base class for embeddings.\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Init params.\"\"\"\n        self._total_tokens_used = 0\n        self._last_token_usage: Optional[int] = None\n        self._tokenizer: Callable = globals_helper.tokenizer\n\n    @abstractmethod\n    def _get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding.\"\"\"\n\n    def get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding.\"\"\"\n        query_embedding = self._get_query_embedding(query)\n        query_tokens_count = len(self._tokenizer(query))\n        self._total_tokens_used += query_tokens_count\n        return query_embedding\n\n    @abstractmethod\n    def _get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get text embedding.\"\"\"\n\n    def get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get text embedding.\"\"\"\n        text_embedding = self._get_text_embedding(text)\n        text_tokens_count = len(self._tokenizer(text))\n        self._total_tokens_used += text_tokens_count\n        return text_embedding\n\n    def similarity(\n        self,\n        embedding1: EMB_TYPE,\n        embedding2: EMB_TYPE,\n        mode: SimilarityMode = SimilarityMode.DEFAULT,\n    ) -> float:\n        \"\"\"Get embedding similarity.\"\"\"\n        if mode == SimilarityMode.EUCLIDEAN:\n            return float(np.linalg.norm(np.array(embedding1) - np.array(embedding2)))\n        elif mode == SimilarityMode.DOT_PRODUCT:\n            product = np.dot(embedding1, embedding2)\n            return product\n        else:\n            product = np.dot(embedding1, embedding2)\n            norm = np.linalg.norm(embedding1) * np.linalg.norm(embedding2)\n            return product / norm\n\n    @property\n    def total_tokens_used(self) -> int:\n        \"\"\"Get the total tokens used so far.\"\"\"\n        return self._total_tokens_used\n\n    @property\n    def last_token_usage(self) -> int:\n        \"\"\"Get the last token usage.\"\"\"\n        if self._last_token_usage is None:\n            return 0\n        return self._last_token_usage\n\n    @last_token_usage.setter\n    def last_token_usage(self, value: int) -> None:\n        \"\"\"Set the last token usage.\"\"\"\n        self._last_token_usage = value\n", "doc_id": "6b44f11bc6046627318f35f08e5701d0cdce4208", "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/base.py", "file_name": "base.py"}, "__type__": "Document"}, "8b01c1a38fd076f3f050d480e1a614de889880a0": {"text": "\"\"\"Langchain Embedding Wrapper Module.\"\"\"\n\n\nfrom typing import List\n\nfrom langchain.embeddings.base import Embeddings as LCEmbeddings\n\nfrom gpt_index.embeddings.base import BaseEmbedding\n\n\nclass LangchainEmbedding(BaseEmbedding):\n    \"\"\"External embeddings (taken from Langchain).\n\n    Args:\n        langchain_embedding (langchain.embeddings.Embeddings): Langchain\n            embeddings class.\n    \"\"\"\n\n    def __init__(self, langchain_embedding: LCEmbeddings) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__()\n        self._langchain_embedding = langchain_embedding\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding.\"\"\"\n        return self._langchain_embedding.embed_query(query)\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get text embedding.\"\"\"\n        return self._langchain_embedding.embed_documents([text])[0]\n", "doc_id": "8b01c1a38fd076f3f050d480e1a614de889880a0", "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/langchain.py", "file_name": "langchain.py"}, "__type__": "Document"}, "19f1d4072db017613451127614ed9fec18193f70": {"text": "\"\"\"OpenAI embeddings file.\"\"\"\n\nfrom enum import Enum\nfrom typing import List, Optional\n\nimport openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\n\nfrom gpt_index.embeddings.base import BaseEmbedding\n\n\nclass OpenAIEmbeddingMode(str, Enum):\n    \"\"\"OpenAI embedding mode.\"\"\"\n\n    SIMILARITY_MODE = \"similarity\"\n    TEXT_SEARCH_MODE = \"text_search\"\n\n\nclass OpenAIEmbeddingModelType(str, Enum):\n    \"\"\"OpenAI embedding model type.\"\"\"\n\n    DAVINCI = \"davinci\"\n    CURIE = \"curie\"\n    BABBAGE = \"babbage\"\n    ADA = \"ada\"\n    TEXT_EMBED_ADA_002 = \"text-embedding-ada-002\"\n\n\nclass OpenAIEmbeddingModeModel(str, Enum):\n    \"\"\"OpenAI embedding mode model.\"\"\"\n\n    # davinci\n    TEXT_SIMILARITY_DAVINCI = \"text-similarity-davinci-001\"\n    TEXT_SEARCH_DAVINCI_QUERY = \"text-search-davinci-query-001\"\n    TEXT_SEARCH_DAVINCI_DOC = \"text-search-davinci-doc-001\"\n\n    # curie\n    TEXT_SIMILARITY_CURIE = \"text-similarity-curie-001\"\n    TEXT_SEARCH_CURIE_QUERY = \"text-search-curie-query-001\"\n    TEXT_SEARCH_CURIE_DOC = \"text-search-curie-doc-001\"\n\n    # babbage\n    TEXT_SIMILARITY_BABBAGE = \"text-similarity-babbage-001\"\n    TEXT_SEARCH_BABBAGE_QUERY = \"text-search-babbage-query-001\"\n    TEXT_SEARCH_BABBAGE_DOC = \"text-search-babbage-doc-001\"\n\n    # ada\n    TEXT_SIMILARITY_ADA = \"text-similarity-ada-001\"\n    TEXT_SEARCH_ADA_QUERY = \"text-search-ada-query-001\"\n    TEXT_SEARCH_ADA_DOC = \"text-search-ada-doc-001\"\n\n    # text-embedding-ada-002\n    TEXT_EMBED_ADA_002 = \"text-embedding-ada-002\"\n\n\n# convenient shorthand\nOAEM = OpenAIEmbeddingMode\nOAEMT = OpenAIEmbeddingModelType\nOAEMM = OpenAIEmbeddingModeModel\n\nEMBED_MAX_TOKEN_LIMIT = 2048\n\n\n_QUERY_MODE_MODEL_DICT = {\n    (OAEM.SIMILARITY_MODE, \"davinci\"): OAEMM.TEXT_SIMILARITY_DAVINCI,\n    (OAEM.SIMILARITY_MODE, \"curie\"): OAEMM.TEXT_SIMILARITY_CURIE,\n    (OAEM.SIMILARITY_MODE, \"babbage\"): OAEMM.TEXT_SIMILARITY_BABBAGE,\n    (OAEM.SIMILARITY_MODE, \"ada\"): OAEMM.TEXT_SIMILARITY_ADA,\n    (OAEM.SIMILARITY_MODE, \"text-embedding-ada-002\"): OAEMM.TEXT_EMBED_ADA_002,\n    (OAEM.TEXT_SEARCH_MODE, \"davinci\"): OAEMM.TEXT_SEARCH_DAVINCI_QUERY,\n    (OAEM.TEXT_SEARCH_MODE, \"curie\"): OAEMM.TEXT_SEARCH_CURIE_QUERY,\n    (OAEM.TEXT_SEARCH_MODE, \"babbage\"): OAEMM.TEXT_SEARCH_BABBAGE_QUERY,\n    (OAEM.TEXT_SEARCH_MODE, \"ada\"): OAEMM.TEXT_SEARCH_ADA_QUERY,\n    (OAEM.TEXT_SEARCH_MODE, \"text-embedding-ada-002\"): OAEMM.TEXT_EMBED_ADA_002,\n}\n\n_TEXT_MODE_MODEL_DICT = {\n    (OAEM.SIMILARITY_MODE, \"davinci\"): OAEMM.TEXT_SIMILARITY_DAVINCI,\n    (OAEM.SIMILARITY_MODE, \"curie\"): OAEMM.TEXT_SIMILARITY_CURIE,\n    (OAEM.SIMILARITY_MODE, \"babbage\"): OAEMM.TEXT_SIMILARITY_BABBAGE,\n    (OAEM.SIMILARITY_MODE, \"ada\"): OAEMM.TEXT_SIMILARITY_ADA,\n    (OAEM.SIMILARITY_MODE, \"text-embedding-ada-002\"): OAEMM.TEXT_EMBED_ADA_002,\n    (OAEM.TEXT_SEARCH_MODE, \"davinci\"): OAEMM.TEXT_SEARCH_DAVINCI_DOC,\n    (OAEM.TEXT_SEARCH_MODE, \"curie\"): OAEMM.TEXT_SEARCH_CURIE_DOC,\n    (OAEM.TEXT_SEARCH_MODE, \"babbage\"): OAEMM.TEXT_SEARCH_BABBAGE_DOC,\n    (OAEM.TEXT_SEARCH_MODE, \"ada\"): OAEMM.TEXT_SEARCH_ADA_DOC,\n    (OAEM.TEXT_SEARCH_MODE, \"text-embedding-ada-002\"): OAEMM.TEXT_EMBED_ADA_002,\n}\n\n\n@retry(wait=wait_random_exponential(min=20, max=60), stop=stop_after_attempt(100))\ndef get_embedding(\n    text: str,\n    engine: Optional[str] = None,\n) -> List[float]:\n    \"\"\"Get embedding.\n\n    NOTE: Copied from OpenAI's embedding utils:\n    https://github.com/openai/openai-python/blob/main/openai/embeddings_utils.py\n\n    Copied here to avoid importing unnecessary dependencies\n    like matplotlib, plotly, scipy, sklearn.\n\n    \"\"\"\n    text = text.replace(\"\\n\", \" \")\n    return openai.Embedding.create(input=[text], engine=engine)[\"data\"][0][\"embedding\"]\n\n\nclass OpenAIEmbedding(BaseEmbedding):\n    \"\"\"OpenAI class for embeddings.\n\n    Args:\n        mode (str): Mode for embedding.\n            Defaults to OpenAIEmbeddingMode.TEXT_SEARCH_MODE.\n            Options are:\n\n            - OpenAIEmbeddingMode.SIMILARITY_MODE\n            - OpenAIEmbeddingMode.TEXT_SEARCH_MODE\n\n        model (str): Model for embedding.\n            Defaults to OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002.\n            Options are:\n\n            - OpenAIEmbeddingModelType.DAVINCI\n            - OpenAIEmbeddingModelType.CURIE\n            - OpenAIEmbeddingModelType.BABBAGE\n            - OpenAIEmbeddingModelType.ADA\n            - OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002\n\n        deployment_name (Optional[str]): Optional deployment of model. Defaults to None.\n            If this value is not None, mode and model will be ignored.\n            Only available for using AzureOpenAI.\n    \"\"\"\n\n    def __init__(\n        self,\n        mode: str = OpenAIEmbeddingMode.TEXT_SEARCH_MODE,\n        model: str = OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002,\n        deployment_name: Optional[str] = None,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__()\n        self.mode = OpenAIEmbeddingMode(mode)\n        self.model = OpenAIEmbeddingModelType(model)\n        self.deployment_name = deployment_name\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding.\"\"\"\n        if self.deployment_name is not None:\n            engine = self.deployment_name\n        else:\n            key = (self.mode, self.model)\n            if key not in _QUERY_MODE_MODEL_DICT:\n                raise ValueError(f\"Invalid mode, model combination: {key}\")\n            engine = _QUERY_MODE_MODEL_DICT[key]\n        return get_embedding(query, engine=engine)\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get text embedding.\"\"\"\n        if self.deployment_name is not None:\n            engine = self.deployment_name\n        else:\n            key = (self.mode, self.model)\n            if key not in _TEXT_MODE_MODEL_DICT:\n                raise ValueError(f\"Invalid mode, model combination: {key}\")\n            engine = _TEXT_MODE_MODEL_DICT[key]\n        return get_embedding(text, engine=engine)\n", "doc_id": "19f1d4072db017613451127614ed9fec18193f70", "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/openai.py", "file_name": "openai.py"}, "__type__": "Document"}, "b854a6fa5a966671ea193573c1ecc21b6ad63014": {"text": "\"\"\"Embedding utils for gpt index.\"\"\"\n\nfrom typing import List\n\n\ndef save_embedding(embedding: List[float], file_path: str) -> None:\n    \"\"\"Save embedding to file.\"\"\"\n    with open(file_path, \"w\") as f:\n        f.write(\",\".join([str(x) for x in embedding]))\n\n\ndef load_embedding(file_path: str) -> List[float]:\n    \"\"\"Load embedding from file. Will only return first embedding in file.\"\"\"\n    with open(file_path, \"r\") as f:\n        for line in f:\n            embedding = [float(x) for x in line.strip().split(\",\")]\n            break\n        return embedding\n", "doc_id": "b854a6fa5a966671ea193573c1ecc21b6ad63014", "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/utils.py", "file_name": "utils.py"}, "__type__": "Document"}, "f1e6d009-43a7-483a-a632-a9e716d40264": {"text": "\nThe __init__.py file is an init file.\n\nThe base.py file contains the BaseEmbedding class which provides methods for getting query and text embeddings, and calculating similarity between embeddings.\n\nThe langchain.py file contains the LangchainEmbedding class which is a wrapper for the Langchain embeddings class.\n\nThe openai.py file contains the OpenAIEmbeddingMode, OpenAIEmbeddingModelType, OpenAIEmbeddingModeModel, and get_embedding functions which provide methods for getting embeddings from OpenAI.\n\nThe gpt_index/embeddings/openai.py file contains code for the BaseEmbedding class, which is a base class for embeddings. It contains methods for getting query and text embeddings, calculating similarity between embeddings, and tracking the total tokens used and the last token usage. It also contains code for the LangchainEmbedding and OpenAIEmbedding classes, which are wrappers for external embeddings from Langchain and OpenAI, respectively. The OpenAIEmbedding class contains methods for setting the embedding mode and model type,", "doc_id": "f1e6d009-43a7-483a-a632-a9e716d40264", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "node_info": null}, "1": {"text": "\"\"\"Base embeddings file.\"\"\"\n\nfrom abc import abstractmethod\nfrom enum import Enum\nfrom typing import Callable, List, Optional\n\nimport numpy as np\n\nfrom gpt_index.utils import globals_helper\n\n# TODO: change to numpy array\nEMB_TYPE = List\n\n\nclass SimilarityMode(str, Enum):\n    \"\"\"Modes for similarity/distance.\"\"\"\n\n    DEFAULT = \"cosine\"\n    DOT_PRODUCT = \"dot_product\"\n    EUCLIDEAN = \"euclidean\"\n\n\nclass BaseEmbedding:\n    \"\"\"Base class for embeddings.\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Init params.\"\"\"\n        self._total_tokens_used = 0\n        self._last_token_usage: Optional[int] = None\n        self._tokenizer: Callable = globals_helper.tokenizer\n\n    @abstractmethod\n    def _get_query_embedding(self, query: str) ->", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/base.py", "file_name": "base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "6b44f11bc6046627318f35f08e5701d0cdce4208", "node_info": null}, "2": {"text": "def _get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding.\"\"\"\n\n    def get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding.\"\"\"\n        query_embedding = self._get_query_embedding(query)\n        query_tokens_count = len(self._tokenizer(query))\n        self._total_tokens_used += query_tokens_count\n        return query_embedding\n\n    @abstractmethod\n    def _get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get text embedding.\"\"\"\n\n    def get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get text embedding.\"\"\"\n        text_embedding = self._get_text_embedding(text)\n        text_tokens_count =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/base.py", "file_name": "base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "6b44f11bc6046627318f35f08e5701d0cdce4208", "node_info": null}, "3": {"text": "       text_tokens_count = len(self._tokenizer(text))\n        self._total_tokens_used += text_tokens_count\n        return text_embedding\n\n    def similarity(\n        self,\n        embedding1: EMB_TYPE,\n        embedding2: EMB_TYPE,\n        mode: SimilarityMode = SimilarityMode.DEFAULT,\n    ) -> float:\n        \"\"\"Get embedding similarity.\"\"\"\n        if mode == SimilarityMode.EUCLIDEAN:\n            return float(np.linalg.norm(np.array(embedding1) - np.array(embedding2)))\n        elif mode == SimilarityMode.DOT_PRODUCT:\n            product = np.dot(embedding1, embedding2)\n            return product\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/base.py", "file_name": "base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "6b44f11bc6046627318f35f08e5701d0cdce4208", "node_info": null}, "4": {"text": "       return product\n        else:\n            product = np.dot(embedding1, embedding2)\n            norm = np.linalg.norm(embedding1) * np.linalg.norm(embedding2)\n            return product / norm\n\n    @property\n    def total_tokens_used(self) -> int:\n        \"\"\"Get the total tokens used so far.\"\"\"\n        return self._total_tokens_used\n\n    @property\n    def last_token_usage(self) -> int:\n        \"\"\"Get the last token usage.\"\"\"\n        if self._last_token_usage is None:\n            return 0\n        return self._last_token_usage\n\n    @last_token_usage.setter\n    def last_token_usage(self, value: int) -> None:\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/base.py", "file_name": "base.py"}, "index": 4, "child_indices": [], "ref_doc_id": "6b44f11bc6046627318f35f08e5701d0cdce4208", "node_info": null}, "5": {"text": "value: int) -> None:\n        \"\"\"Set the last token usage.\"\"\"\n        self._last_token_usage = value\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/base.py", "file_name": "base.py"}, "index": 5, "child_indices": [], "ref_doc_id": "6b44f11bc6046627318f35f08e5701d0cdce4208", "node_info": null}, "6": {"text": "\"\"\"Langchain Embedding Wrapper Module.\"\"\"\n\n\nfrom typing import List\n\nfrom langchain.embeddings.base import Embeddings as LCEmbeddings\n\nfrom gpt_index.embeddings.base import BaseEmbedding\n\n\nclass LangchainEmbedding(BaseEmbedding):\n    \"\"\"External embeddings (taken from Langchain).\n\n    Args:\n        langchain_embedding (langchain.embeddings.Embeddings): Langchain\n            embeddings class.\n    \"\"\"\n\n    def __init__(self, langchain_embedding: LCEmbeddings) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__()\n        self._langchain_embedding = langchain_embedding\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding.\"\"\"\n        return self._langchain_embedding.embed_query(query)\n\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/langchain.py", "file_name": "langchain.py"}, "index": 6, "child_indices": [], "ref_doc_id": "8b01c1a38fd076f3f050d480e1a614de889880a0", "node_info": null}, "7": {"text": "   def _get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get text embedding.\"\"\"\n        return self._langchain_embedding.embed_documents([text])[0]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/langchain.py", "file_name": "langchain.py"}, "index": 7, "child_indices": [], "ref_doc_id": "8b01c1a38fd076f3f050d480e1a614de889880a0", "node_info": null}, "8": {"text": "\"\"\"OpenAI embeddings file.\"\"\"\n\nfrom enum import Enum\nfrom typing import List, Optional\n\nimport openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\n\nfrom gpt_index.embeddings.base import BaseEmbedding\n\n\nclass OpenAIEmbeddingMode(str, Enum):\n    \"\"\"OpenAI embedding mode.\"\"\"\n\n    SIMILARITY_MODE = \"similarity\"\n    TEXT_SEARCH_MODE = \"text_search\"\n\n\nclass OpenAIEmbeddingModelType(str, Enum):\n    \"\"\"OpenAI embedding model type.\"\"\"\n\n    DAVINCI = \"davinci\"\n    CURIE = \"curie\"\n    BABBAGE = \"babbage\"\n    ADA = \"ada\"\n    TEXT_EMBED_ADA_002 = \"text-embedding-ada-002\"\n\n\nclass OpenAIEmbeddingModeModel(str, Enum):\n    \"\"\"OpenAI embedding mode model.\"\"\"\n\n    # davinci\n    TEXT_SIMILARITY_DAVINCI =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/openai.py", "file_name": "openai.py"}, "index": 8, "child_indices": [], "ref_doc_id": "19f1d4072db017613451127614ed9fec18193f70", "node_info": null}, "9": {"text": "   TEXT_SIMILARITY_DAVINCI = \"text-similarity-davinci-001\"\n    TEXT_SEARCH_DAVINCI_QUERY = \"text-search-davinci-query-001\"\n    TEXT_SEARCH_DAVINCI_DOC = \"text-search-davinci-doc-001\"\n\n    # curie\n    TEXT_SIMILARITY_CURIE = \"text-similarity-curie-001\"\n    TEXT_SEARCH_CURIE_QUERY = \"text-search-curie-query-001\"\n    TEXT_SEARCH_CURIE_DOC = \"text-search-curie-doc-001\"\n\n    # babbage\n    TEXT_SIMILARITY_BABBAGE = \"text-similarity-babbage-001\"\n    TEXT_SEARCH_BABBAGE_QUERY = \"text-search-babbage-query-001\"\n    TEXT_SEARCH_BABBAGE_DOC = \"text-search-babbage-doc-001\"\n\n    # ada\n    TEXT_SIMILARITY_ADA =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/openai.py", "file_name": "openai.py"}, "index": 9, "child_indices": [], "ref_doc_id": "19f1d4072db017613451127614ed9fec18193f70", "node_info": null}, "10": {"text": " # ada\n    TEXT_SIMILARITY_ADA = \"text-similarity-ada-001\"\n    TEXT_SEARCH_ADA_QUERY = \"text-search-ada-query-001\"\n    TEXT_SEARCH_ADA_DOC = \"text-search-ada-doc-001\"\n\n    # text-embedding-ada-002\n    TEXT_EMBED_ADA_002 = \"text-embedding-ada-002\"\n\n\n# convenient shorthand\nOAEM = OpenAIEmbeddingMode\nOAEMT = OpenAIEmbeddingModelType\nOAEMM = OpenAIEmbeddingModeModel\n\nEMBED_MAX_TOKEN_LIMIT = 2048\n\n\n_QUERY_MODE_MODEL_DICT = {\n    (OAEM.SIMILARITY_MODE, \"davinci\"): OAEMM.TEXT_SIMILARITY_DAVINCI,\n    (OAEM.SIMILARITY_MODE, \"curie\"): OAEMM.TEXT_SIMILARITY_CURIE,\n    (OAEM.SIMILARITY_MODE, \"babbage\"):", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/openai.py", "file_name": "openai.py"}, "index": 10, "child_indices": [], "ref_doc_id": "19f1d4072db017613451127614ed9fec18193f70", "node_info": null}, "11": {"text": "(OAEM.SIMILARITY_MODE, \"babbage\"): OAEMM.TEXT_SIMILARITY_BABBAGE,\n    (OAEM.SIMILARITY_MODE, \"ada\"): OAEMM.TEXT_SIMILARITY_ADA,\n    (OAEM.SIMILARITY_MODE, \"text-embedding-ada-002\"): OAEMM.TEXT_EMBED_ADA_002,\n    (OAEM.TEXT_SEARCH_MODE, \"davinci\"): OAEMM.TEXT_SEARCH_DAVINCI_QUERY,\n    (OAEM.TEXT_SEARCH_MODE, \"curie\"): OAEMM.TEXT_SEARCH_CURIE_QUERY,\n    (OAEM.TEXT_SEARCH_MODE, \"babbage\"): OAEMM.TEXT_SEARCH_BABBAGE_QUERY,\n    (OAEM.TEXT_SEARCH_MODE, \"ada\"): OAEMM.TEXT_SEARCH_ADA_QUERY,\n    (OAEM.TEXT_SEARCH_MODE, \"text-embedding-ada-002\"):", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/openai.py", "file_name": "openai.py"}, "index": 11, "child_indices": [], "ref_doc_id": "19f1d4072db017613451127614ed9fec18193f70", "node_info": null}, "12": {"text": "\"text-embedding-ada-002\"): OAEMM.TEXT_EMBED_ADA_002,\n}\n\n_TEXT_MODE_MODEL_DICT = {\n    (OAEM.SIMILARITY_MODE, \"davinci\"): OAEMM.TEXT_SIMILARITY_DAVINCI,\n    (OAEM.SIMILARITY_MODE, \"curie\"): OAEMM.TEXT_SIMILARITY_CURIE,\n    (OAEM.SIMILARITY_MODE, \"babbage\"): OAEMM.TEXT_SIMILARITY_BABBAGE,\n    (OAEM.SIMILARITY_MODE, \"ada\"): OAEMM.TEXT_SIMILARITY_ADA,\n    (OAEM.SIMILARITY_MODE, \"text-embedding-ada-002\"): OAEMM.TEXT_EMBED_ADA_002,\n    (OAEM.TEXT_SEARCH_MODE, \"davinci\"): OAEMM.TEXT_SEARCH_DAVINCI_DOC,\n    (OAEM.TEXT_SEARCH_MODE, \"curie\"):", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/openai.py", "file_name": "openai.py"}, "index": 12, "child_indices": [], "ref_doc_id": "19f1d4072db017613451127614ed9fec18193f70", "node_info": null}, "13": {"text": "(OAEM.TEXT_SEARCH_MODE, \"curie\"): OAEMM.TEXT_SEARCH_CURIE_DOC,\n    (OAEM.TEXT_SEARCH_MODE, \"babbage\"): OAEMM.TEXT_SEARCH_BABBAGE_DOC,\n    (OAEM.TEXT_SEARCH_MODE, \"ada\"): OAEMM.TEXT_SEARCH_ADA_DOC,\n    (OAEM.TEXT_SEARCH_MODE, \"text-embedding-ada-002\"): OAEMM.TEXT_EMBED_ADA_002,\n}\n\n\n@retry(wait=wait_random_exponential(min=20, max=60), stop=stop_after_attempt(100))\ndef get_embedding(\n    text: str,\n    engine: Optional[str] = None,\n) -> List[float]:\n    \"\"\"Get embedding.\n\n    NOTE: Copied from OpenAI's embedding utils:\n    https://github.com/openai/openai-python/blob/main/openai/embeddings_utils.py\n\n    Copied here to avoid importing unnecessary dependencies\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/openai.py", "file_name": "openai.py"}, "index": 13, "child_indices": [], "ref_doc_id": "19f1d4072db017613451127614ed9fec18193f70", "node_info": null}, "14": {"text": "  Copied here to avoid importing unnecessary dependencies\n    like matplotlib, plotly, scipy, sklearn.\n\n    \"\"\"\n    text = text.replace(\"\\n\", \" \")\n    return openai.Embedding.create(input=[text], engine=engine)[\"data\"][0][\"embedding\"]\n\n\nclass OpenAIEmbedding(BaseEmbedding):\n    \"\"\"OpenAI class for embeddings.\n\n    Args:\n        mode (str): Mode for embedding.\n            Defaults to OpenAIEmbeddingMode.TEXT_SEARCH_MODE.\n            Options are:\n\n            - OpenAIEmbeddingMode.SIMILARITY_MODE\n            - OpenAIEmbeddingMode.TEXT_SEARCH_MODE\n\n        model (str): Model for embedding.\n            Defaults to OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002.\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/openai.py", "file_name": "openai.py"}, "index": 14, "child_indices": [], "ref_doc_id": "19f1d4072db017613451127614ed9fec18193f70", "node_info": null}, "15": {"text": "           Options are:\n\n            - OpenAIEmbeddingModelType.DAVINCI\n            - OpenAIEmbeddingModelType.CURIE\n            - OpenAIEmbeddingModelType.BABBAGE\n            - OpenAIEmbeddingModelType.ADA\n            - OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002\n\n        deployment_name (Optional[str]): Optional deployment of model. Defaults to None.\n            If this value is not None, mode and model will be ignored.\n            Only available for using AzureOpenAI.\n    \"\"\"\n\n    def __init__(\n        self,\n        mode: str = OpenAIEmbeddingMode.TEXT_SEARCH_MODE,\n        model: str", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/openai.py", "file_name": "openai.py"}, "index": 15, "child_indices": [], "ref_doc_id": "19f1d4072db017613451127614ed9fec18193f70", "node_info": null}, "16": {"text": "       model: str = OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002,\n        deployment_name: Optional[str] = None,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__()\n        self.mode = OpenAIEmbeddingMode(mode)\n        self.model = OpenAIEmbeddingModelType(model)\n        self.deployment_name = deployment_name\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding.\"\"\"\n        if self.deployment_name is not None:\n            engine = self.deployment_name\n        else:\n            key = (self.mode, self.model)\n            if key not in", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/openai.py", "file_name": "openai.py"}, "index": 16, "child_indices": [], "ref_doc_id": "19f1d4072db017613451127614ed9fec18193f70", "node_info": null}, "17": {"text": "           if key not in _QUERY_MODE_MODEL_DICT:\n                raise ValueError(f\"Invalid mode, model combination: {key}\")\n            engine = _QUERY_MODE_MODEL_DICT[key]\n        return get_embedding(query, engine=engine)\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get text embedding.\"\"\"\n        if self.deployment_name is not None:\n            engine = self.deployment_name\n        else:\n            key = (self.mode, self.model)\n            if key not in _TEXT_MODE_MODEL_DICT:\n                raise ValueError(f\"Invalid mode, model combination: {key}\")\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/openai.py", "file_name": "openai.py"}, "index": 17, "child_indices": [], "ref_doc_id": "19f1d4072db017613451127614ed9fec18193f70", "node_info": null}, "18": {"text": "mode, model combination: {key}\")\n            engine = _TEXT_MODE_MODEL_DICT[key]\n        return get_embedding(text, engine=engine)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/openai.py", "file_name": "openai.py"}, "index": 18, "child_indices": [], "ref_doc_id": "19f1d4072db017613451127614ed9fec18193f70", "node_info": null}, "19": {"text": "\"\"\"Embedding utils for gpt index.\"\"\"\n\nfrom typing import List\n\n\ndef save_embedding(embedding: List[float], file_path: str) -> None:\n    \"\"\"Save embedding to file.\"\"\"\n    with open(file_path, \"w\") as f:\n        f.write(\",\".join([str(x) for x in embedding]))\n\n\ndef load_embedding(file_path: str) -> List[float]:\n    \"\"\"Load embedding from file. Will only return first embedding in file.\"\"\"\n    with open(file_path, \"r\") as f:\n        for line in f:\n            embedding = [float(x) for x in line.strip().split(\",\")]\n            break\n        return embedding\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/utils.py", "file_name": "utils.py"}, "index": 19, "child_indices": [], "ref_doc_id": "b854a6fa5a966671ea193573c1ecc21b6ad63014", "node_info": null}, "20": {"text": "This code file contains the code for the BaseEmbedding class, which is a base class for embeddings. It contains methods for getting query and text embeddings, calculating similarity between embeddings, and tracking the total tokens used and the last token usage. It also contains code for the LangchainEmbedding and OpenAIEmbedding classes, which are wrappers for external embeddings from Langchain and OpenAI, respectively. The OpenAIEmbedding class contains methods for setting the embedding mode and model type, and for retrieving the query and text embeddings.", "doc_id": null, "embedding": null, "extra_info": null, "index": 20, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "21": {"text": "Openai.py is a code file that contains functions and classes related to OpenAI embeddings. It includes functions for getting query and text embeddings, as well as a class for OpenAI embeddings. It also includes a function for saving and loading embeddings from a file. The code is used to create embeddings from text and query inputs, and to save and load embeddings from a file.", "doc_id": null, "embedding": null, "extra_info": null, "index": 21, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"20": {"text": "This code file contains the code for the BaseEmbedding class, which is a base class for embeddings. It contains methods for getting query and text embeddings, calculating similarity between embeddings, and tracking the total tokens used and the last token usage. It also contains code for the LangchainEmbedding and OpenAIEmbedding classes, which are wrappers for external embeddings from Langchain and OpenAI, respectively. The OpenAIEmbedding class contains methods for setting the embedding mode and model type, and for retrieving the query and text embeddings.", "doc_id": null, "embedding": null, "extra_info": null, "index": 20, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "21": {"text": "Openai.py is a code file that contains functions and classes related to OpenAI embeddings. It includes functions for getting query and text embeddings, as well as a class for OpenAI embeddings. It also includes a function for saving and loading embeddings from a file. The code is used to create embeddings from text and query inputs, and to save and load embeddings from a file.", "doc_id": null, "embedding": null, "extra_info": null, "index": 21, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}