{"index_struct": {"text": "\nThe summaries of the documents are generated by taking the text chunks from the documents and using them to create a summary based on the summary prompt provided to the GPTTreeIndexBuilder class. The summaries are generated using the LLM predictor, which uses natural language processing to generate a summary of the text.", "doc_id": "4858949c-403b-4d26-8e2c-d22b7198643a", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/common/tree/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "node_info": null}, "1": {"text": "\"\"\"Common classes/functions for tree index operations.\"\"\"\n\n\nimport logging\nfrom typing import Dict, Sequence\n\nfrom gpt_index.data_structs.data_structs import IndexGraph, Node\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.utils import get_sorted_node_list, truncate_text\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.prompts import SummaryPrompt\nfrom gpt_index.schema import BaseDocument\n\n\nclass GPTTreeIndexBuilder:\n    \"\"\"GPT tree index builder.\n\n    Helper class to build the tree-structured index,\n    or to synthesize an answer.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        num_children: int,\n        summary_prompt: SummaryPrompt,\n        llm_predictor: LLMPredictor,\n        prompt_helper:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/common/tree/base.py", "file_name": "base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "cb36bdd5d0c7925d04e4e3fcb9363e168384f0ef", "node_info": null}, "2": {"text": "       prompt_helper: PromptHelper,\n    ) -> None:\n        \"\"\"Initialize with params.\"\"\"\n        if num_children < 2:\n            raise ValueError(\"Invalid number of children.\")\n        self.num_children = num_children\n        self.summary_prompt = summary_prompt\n        self._llm_predictor = llm_predictor\n        self._prompt_helper = prompt_helper\n\n    def _get_nodes_from_document(\n        self, start_idx: int, document: BaseDocument\n    ) -> Dict[int, Node]:\n        \"\"\"Add document to index.\"\"\"\n        # NOTE: summary prompt does not need to be partially formatted\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n        ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/common/tree/base.py", "file_name": "base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "cb36bdd5d0c7925d04e4e3fcb9363e168384f0ef", "node_info": null}, "3": {"text": "           self.summary_prompt, self.num_children\n        )\n        text_chunks = text_splitter.split_text(\n            document.get_text(), extra_info_str=document.extra_info_str\n        )\n        doc_nodes = {\n            (start_idx + i): Node(\n                text=t,\n                index=(start_idx + i),\n                ref_doc_id=document.get_doc_id(),\n                embedding=document.embedding,\n                extra_info=document.extra_info,\n            )\n            for i, t in", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/common/tree/base.py", "file_name": "base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "cb36bdd5d0c7925d04e4e3fcb9363e168384f0ef", "node_info": null}, "4": {"text": "           for i, t in enumerate(text_chunks)\n        }\n        return doc_nodes\n\n    def build_from_text(\n        self,\n        documents: Sequence[BaseDocument],\n        build_tree: bool = True,\n    ) -> IndexGraph:\n        \"\"\"Build from text.\n\n        Returns:\n            IndexGraph: graph object consisting of all_nodes, root_nodes\n\n        \"\"\"\n        all_nodes: Dict[int, Node] = {}\n        for d in documents:\n            all_nodes.update(self._get_nodes_from_document(len(all_nodes), d))\n\n        if build_tree:\n            # instantiate all_nodes from initial text", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/common/tree/base.py", "file_name": "base.py"}, "index": 4, "child_indices": [], "ref_doc_id": "cb36bdd5d0c7925d04e4e3fcb9363e168384f0ef", "node_info": null}, "5": {"text": "     # instantiate all_nodes from initial text chunks\n            root_nodes = self.build_index_from_nodes(all_nodes, all_nodes)\n        else:\n            # if build_tree is False, then don't surface any root nodes\n            root_nodes = {}\n        return IndexGraph(all_nodes=all_nodes, root_nodes=root_nodes)\n\n    def build_index_from_nodes(\n        self,\n        cur_nodes: Dict[int, Node],\n        all_nodes: Dict[int, Node],\n    ) -> Dict[int, Node]:\n        \"\"\"Consolidates chunks recursively, in a bottoms-up fashion.\"\"\"\n        cur_node_list = get_sorted_node_list(cur_nodes)\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/common/tree/base.py", "file_name": "base.py"}, "index": 5, "child_indices": [], "ref_doc_id": "cb36bdd5d0c7925d04e4e3fcb9363e168384f0ef", "node_info": null}, "6": {"text": "       cur_index = len(all_nodes)\n        new_node_dict = {}\n        logging.info(\n            f\"> Building index from nodes: {len(cur_nodes) // self.num_children} chunks\"\n        )\n        for i in range(0, len(cur_node_list), self.num_children):\n            cur_nodes_chunk = cur_node_list[i : i + self.num_children]\n            text_chunk = self._prompt_helper.get_text_from_nodes(\n                cur_nodes_chunk, prompt=self.summary_prompt\n            )\n\n            new_summary, _ = self._llm_predictor.predict(\n               ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/common/tree/base.py", "file_name": "base.py"}, "index": 6, "child_indices": [], "ref_doc_id": "cb36bdd5d0c7925d04e4e3fcb9363e168384f0ef", "node_info": null}, "7": {"text": "               self.summary_prompt, context_str=text_chunk\n            )\n\n            logging.debug(\n                f\"> {i}/{len(cur_nodes)}, summary: {truncate_text(new_summary, 50)}\"\n            )\n            new_node = Node(\n                text=new_summary,\n                index=cur_index,\n                child_indices={n.index for n in cur_nodes_chunk},\n            )\n            new_node_dict[cur_index] = new_node\n            cur_index += 1\n\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/common/tree/base.py", "file_name": "base.py"}, "index": 7, "child_indices": [], "ref_doc_id": "cb36bdd5d0c7925d04e4e3fcb9363e168384f0ef", "node_info": null}, "8": {"text": "   cur_index += 1\n\n        all_nodes.update(new_node_dict)\n\n        if len(new_node_dict) <= self.num_children:\n            return new_node_dict\n        else:\n            return self.build_index_from_nodes(new_node_dict, all_nodes)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/common/tree/base.py", "file_name": "base.py"}, "index": 8, "child_indices": [], "ref_doc_id": "cb36bdd5d0c7925d04e4e3fcb9363e168384f0ef", "node_info": null}, "9": {"text": "This code file contains the GPTTreeIndexBuilder class, which is a helper class used to build a tree-structured index or to synthesize an answer. It has an init method which takes in the number of children, a summary prompt, an LLM predictor, and a prompt helper. It also has a _get_nodes_from_document method which adds a document to the index, and a build_from_text method which builds the index from text. Finally, it has a build_index_from_nodes method which consolidates chunks recursively in a bottoms-up fashion.", "doc_id": null, "embedding": null, "extra_info": null, "index": 9, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"9": {"text": "This code file contains the GPTTreeIndexBuilder class, which is a helper class used to build a tree-structured index or to synthesize an answer. It has an init method which takes in the number of children, a summary prompt, an LLM predictor, and a prompt helper. It also has a _get_nodes_from_document method which adds a document to the index, and a build_from_text method which builds the index from text. Finally, it has a build_index_from_nodes method which consolidates chunks recursively in a bottoms-up fashion.", "doc_id": null, "embedding": null, "extra_info": null, "index": 9, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"1d4640565ae2765d9ca96a509dc9809217f62f2f": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/common/tree/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "cb36bdd5d0c7925d04e4e3fcb9363e168384f0ef": {"text": "\"\"\"Common classes/functions for tree index operations.\"\"\"\n\n\nimport logging\nfrom typing import Dict, Sequence\n\nfrom gpt_index.data_structs.data_structs import IndexGraph, Node\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.utils import get_sorted_node_list, truncate_text\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.prompts import SummaryPrompt\nfrom gpt_index.schema import BaseDocument\n\n\nclass GPTTreeIndexBuilder:\n    \"\"\"GPT tree index builder.\n\n    Helper class to build the tree-structured index,\n    or to synthesize an answer.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        num_children: int,\n        summary_prompt: SummaryPrompt,\n        llm_predictor: LLMPredictor,\n        prompt_helper: PromptHelper,\n    ) -> None:\n        \"\"\"Initialize with params.\"\"\"\n        if num_children < 2:\n            raise ValueError(\"Invalid number of children.\")\n        self.num_children = num_children\n        self.summary_prompt = summary_prompt\n        self._llm_predictor = llm_predictor\n        self._prompt_helper = prompt_helper\n\n    def _get_nodes_from_document(\n        self, start_idx: int, document: BaseDocument\n    ) -> Dict[int, Node]:\n        \"\"\"Add document to index.\"\"\"\n        # NOTE: summary prompt does not need to be partially formatted\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.summary_prompt, self.num_children\n        )\n        text_chunks = text_splitter.split_text(\n            document.get_text(), extra_info_str=document.extra_info_str\n        )\n        doc_nodes = {\n            (start_idx + i): Node(\n                text=t,\n                index=(start_idx + i),\n                ref_doc_id=document.get_doc_id(),\n                embedding=document.embedding,\n                extra_info=document.extra_info,\n            )\n            for i, t in enumerate(text_chunks)\n        }\n        return doc_nodes\n\n    def build_from_text(\n        self,\n        documents: Sequence[BaseDocument],\n        build_tree: bool = True,\n    ) -> IndexGraph:\n        \"\"\"Build from text.\n\n        Returns:\n            IndexGraph: graph object consisting of all_nodes, root_nodes\n\n        \"\"\"\n        all_nodes: Dict[int, Node] = {}\n        for d in documents:\n            all_nodes.update(self._get_nodes_from_document(len(all_nodes), d))\n\n        if build_tree:\n            # instantiate all_nodes from initial text chunks\n            root_nodes = self.build_index_from_nodes(all_nodes, all_nodes)\n        else:\n            # if build_tree is False, then don't surface any root nodes\n            root_nodes = {}\n        return IndexGraph(all_nodes=all_nodes, root_nodes=root_nodes)\n\n    def build_index_from_nodes(\n        self,\n        cur_nodes: Dict[int, Node],\n        all_nodes: Dict[int, Node],\n    ) -> Dict[int, Node]:\n        \"\"\"Consolidates chunks recursively, in a bottoms-up fashion.\"\"\"\n        cur_node_list = get_sorted_node_list(cur_nodes)\n        cur_index = len(all_nodes)\n        new_node_dict = {}\n        logging.info(\n            f\"> Building index from nodes: {len(cur_nodes) // self.num_children} chunks\"\n        )\n        for i in range(0, len(cur_node_list), self.num_children):\n            cur_nodes_chunk = cur_node_list[i : i + self.num_children]\n            text_chunk = self._prompt_helper.get_text_from_nodes(\n                cur_nodes_chunk, prompt=self.summary_prompt\n            )\n\n            new_summary, _ = self._llm_predictor.predict(\n                self.summary_prompt, context_str=text_chunk\n            )\n\n            logging.debug(\n                f\"> {i}/{len(cur_nodes)}, summary: {truncate_text(new_summary, 50)}\"\n            )\n            new_node = Node(\n                text=new_summary,\n                index=cur_index,\n                child_indices={n.index for n in cur_nodes_chunk},\n            )\n            new_node_dict[cur_index] = new_node\n            cur_index += 1\n\n        all_nodes.update(new_node_dict)\n\n        if len(new_node_dict) <= self.num_children:\n            return new_node_dict\n        else:\n            return self.build_index_from_nodes(new_node_dict, all_nodes)\n", "doc_id": "cb36bdd5d0c7925d04e4e3fcb9363e168384f0ef", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/common/tree/base.py", "file_name": "base.py"}, "__type__": "Document"}, "4858949c-403b-4d26-8e2c-d22b7198643a": {"text": "\nThe summaries of the documents are generated by taking the text chunks from the documents and using them to create a summary based on the summary prompt provided to the GPTTreeIndexBuilder class. The summaries are generated using the LLM predictor, which uses natural language processing to generate a summary of the text.", "doc_id": "4858949c-403b-4d26-8e2c-d22b7198643a", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/common/tree/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "node_info": null}, "1": {"text": "\"\"\"Common classes/functions for tree index operations.\"\"\"\n\n\nimport logging\nfrom typing import Dict, Sequence\n\nfrom gpt_index.data_structs.data_structs import IndexGraph, Node\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.utils import get_sorted_node_list, truncate_text\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.prompts import SummaryPrompt\nfrom gpt_index.schema import BaseDocument\n\n\nclass GPTTreeIndexBuilder:\n    \"\"\"GPT tree index builder.\n\n    Helper class to build the tree-structured index,\n    or to synthesize an answer.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        num_children: int,\n        summary_prompt: SummaryPrompt,\n        llm_predictor: LLMPredictor,\n        prompt_helper:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/common/tree/base.py", "file_name": "base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "cb36bdd5d0c7925d04e4e3fcb9363e168384f0ef", "node_info": null}, "2": {"text": "       prompt_helper: PromptHelper,\n    ) -> None:\n        \"\"\"Initialize with params.\"\"\"\n        if num_children < 2:\n            raise ValueError(\"Invalid number of children.\")\n        self.num_children = num_children\n        self.summary_prompt = summary_prompt\n        self._llm_predictor = llm_predictor\n        self._prompt_helper = prompt_helper\n\n    def _get_nodes_from_document(\n        self, start_idx: int, document: BaseDocument\n    ) -> Dict[int, Node]:\n        \"\"\"Add document to index.\"\"\"\n        # NOTE: summary prompt does not need to be partially formatted\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n        ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/common/tree/base.py", "file_name": "base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "cb36bdd5d0c7925d04e4e3fcb9363e168384f0ef", "node_info": null}, "3": {"text": "           self.summary_prompt, self.num_children\n        )\n        text_chunks = text_splitter.split_text(\n            document.get_text(), extra_info_str=document.extra_info_str\n        )\n        doc_nodes = {\n            (start_idx + i): Node(\n                text=t,\n                index=(start_idx + i),\n                ref_doc_id=document.get_doc_id(),\n                embedding=document.embedding,\n                extra_info=document.extra_info,\n            )\n            for i, t in", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/common/tree/base.py", "file_name": "base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "cb36bdd5d0c7925d04e4e3fcb9363e168384f0ef", "node_info": null}, "4": {"text": "           for i, t in enumerate(text_chunks)\n        }\n        return doc_nodes\n\n    def build_from_text(\n        self,\n        documents: Sequence[BaseDocument],\n        build_tree: bool = True,\n    ) -> IndexGraph:\n        \"\"\"Build from text.\n\n        Returns:\n            IndexGraph: graph object consisting of all_nodes, root_nodes\n\n        \"\"\"\n        all_nodes: Dict[int, Node] = {}\n        for d in documents:\n            all_nodes.update(self._get_nodes_from_document(len(all_nodes), d))\n\n        if build_tree:\n            # instantiate all_nodes from initial text", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/common/tree/base.py", "file_name": "base.py"}, "index": 4, "child_indices": [], "ref_doc_id": "cb36bdd5d0c7925d04e4e3fcb9363e168384f0ef", "node_info": null}, "5": {"text": "     # instantiate all_nodes from initial text chunks\n            root_nodes = self.build_index_from_nodes(all_nodes, all_nodes)\n        else:\n            # if build_tree is False, then don't surface any root nodes\n            root_nodes = {}\n        return IndexGraph(all_nodes=all_nodes, root_nodes=root_nodes)\n\n    def build_index_from_nodes(\n        self,\n        cur_nodes: Dict[int, Node],\n        all_nodes: Dict[int, Node],\n    ) -> Dict[int, Node]:\n        \"\"\"Consolidates chunks recursively, in a bottoms-up fashion.\"\"\"\n        cur_node_list = get_sorted_node_list(cur_nodes)\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/common/tree/base.py", "file_name": "base.py"}, "index": 5, "child_indices": [], "ref_doc_id": "cb36bdd5d0c7925d04e4e3fcb9363e168384f0ef", "node_info": null}, "6": {"text": "       cur_index = len(all_nodes)\n        new_node_dict = {}\n        logging.info(\n            f\"> Building index from nodes: {len(cur_nodes) // self.num_children} chunks\"\n        )\n        for i in range(0, len(cur_node_list), self.num_children):\n            cur_nodes_chunk = cur_node_list[i : i + self.num_children]\n            text_chunk = self._prompt_helper.get_text_from_nodes(\n                cur_nodes_chunk, prompt=self.summary_prompt\n            )\n\n            new_summary, _ = self._llm_predictor.predict(\n               ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/common/tree/base.py", "file_name": "base.py"}, "index": 6, "child_indices": [], "ref_doc_id": "cb36bdd5d0c7925d04e4e3fcb9363e168384f0ef", "node_info": null}, "7": {"text": "               self.summary_prompt, context_str=text_chunk\n            )\n\n            logging.debug(\n                f\"> {i}/{len(cur_nodes)}, summary: {truncate_text(new_summary, 50)}\"\n            )\n            new_node = Node(\n                text=new_summary,\n                index=cur_index,\n                child_indices={n.index for n in cur_nodes_chunk},\n            )\n            new_node_dict[cur_index] = new_node\n            cur_index += 1\n\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/common/tree/base.py", "file_name": "base.py"}, "index": 7, "child_indices": [], "ref_doc_id": "cb36bdd5d0c7925d04e4e3fcb9363e168384f0ef", "node_info": null}, "8": {"text": "   cur_index += 1\n\n        all_nodes.update(new_node_dict)\n\n        if len(new_node_dict) <= self.num_children:\n            return new_node_dict\n        else:\n            return self.build_index_from_nodes(new_node_dict, all_nodes)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/common/tree/base.py", "file_name": "base.py"}, "index": 8, "child_indices": [], "ref_doc_id": "cb36bdd5d0c7925d04e4e3fcb9363e168384f0ef", "node_info": null}, "9": {"text": "This code file contains the GPTTreeIndexBuilder class, which is a helper class used to build a tree-structured index or to synthesize an answer. It has an init method which takes in the number of children, a summary prompt, an LLM predictor, and a prompt helper. It also has a _get_nodes_from_document method which adds a document to the index, and a build_from_text method which builds the index from text. Finally, it has a build_index_from_nodes method which consolidates chunks recursively in a bottoms-up fashion.", "doc_id": null, "embedding": null, "extra_info": null, "index": 9, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"9": {"text": "This code file contains the GPTTreeIndexBuilder class, which is a helper class used to build a tree-structured index or to synthesize an answer. It has an init method which takes in the number of children, a summary prompt, an LLM predictor, and a prompt helper. It also has a _get_nodes_from_document method which adds a document to the index, and a build_from_text method which builds the index from text. Finally, it has a build_index_from_nodes method which consolidates chunks recursively in a bottoms-up fashion.", "doc_id": null, "embedding": null, "extra_info": null, "index": 9, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}