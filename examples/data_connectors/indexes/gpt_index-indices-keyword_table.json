{"index_struct": {"text": "\nThe documents contain code for keyword table indices, which are used to extract keywords from text documents. The code includes three classes which use different keyword extraction techniques (GPT, RAKE, and regex). During index construction, GPTKeywordIndex takes in a dataset of text documents as input, and chunks them up into smaller document chunks. During query-time, the GPTKeywordIndex extracts a set of relevant keywords from the query using a customized variant of the same keyword extraction prompt. These keywords are then used to fetch the set of candidate text chunk ID's. The text chunk ID's are ordered by number of matching keywords (from highest to lowest), and truncated after a cutoff. An initial answer to the query is constructed using the first text chunk. The answer is then refined through feeding in subsequent text chunks as context. The worst-case runtime to execute a query should be $O(k*c)$, where $k$ is the number of extracted keywords, and $c$ is the number of candidate text chunks.", "doc_id": "a3c43b6b-458f-43f8-b3fa-a3db3d29c666", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\n\n\ud83d\udd11 GPTKeywordIndex\n\nGPTKeywordIndex is a keyword-based table data structure (inspired by \"hash tables\").\n\n\n\n\n\nIndex Construction\n\nDuring index construction, GPTKeywordIndex first takes in a dataset of text documents as input, and chunks them up into smaller document chunks. For each text chunk, GPTKeywordIndex uses GPT to extract a set of relevant keywords with a **keyword extraction prompt**. (keywords can include short phrases, like \"new york city\"). These keywords are then stored in a table, referencing the same text chunk.\n\n\n\n\n\nQuery\n\nThere are three query modes: `default`, `simple`, and `rake`.\n\n**Default**\n\nDuring query-time, the GPTKeywordIndex extracts a set of relevant keywords from the query using a customized variant of the same **keyword extraction prompt**. These keywords are then used to fetch the set of candidate text chunk ID's. The text chunk ID's are ordered by number of matching keywords (from highest to lowest), and truncated", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/README.md", "file_name": "gpt_index/indices/keyword_table/README.md"}, "index": 0, "child_indices": [], "ref_doc_id": "4c51f91a5611ecfd8531f01a74baf77f9f2484cf", "node_info": null}, "1": {"text": "of matching keywords (from highest to lowest), and truncated after a cutoff $d$, which represents the maximum number of text chunks to consider.\n\nWe construct an answer using the _create and refine_ paradigm. An initial answer to the query is constructed using the first text chunk. The answer is then _refined_ through feeding in subsequent text chunks as context. Refinement could mean keeping the original answer, making small edits to the original answer, or rewriting the original answer completely.\n\n**Simple (Regex)**\nInstead of using GPT for keyword extraction, this mode uses a simple regex query to find words, filtering out stopwords.\n\n**RAKE**\nUse the popular RAKE keyword extractor.\n\n\n\n\n\nUsage\n\n```python\nfrom gpt_index import GPTKeywordTableIndex, SimpleDirectoryReader\n\n\n\n\n\nbuild index\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = GPTKeywordTableIndex(documents)\n\n\n\n\nsave index\nindex.save_to_disk('index_table.json')\n\n\n\n\nload index from disk\nindex =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/README.md", "file_name": "gpt_index/indices/keyword_table/README.md"}, "index": 1, "child_indices": [], "ref_doc_id": "4c51f91a5611ecfd8531f01a74baf77f9f2484cf", "node_info": null}, "2": {"text": "index from disk\nindex = GPTKeywordTableIndex.load_from_disk('index_table.json')\n\n\n\n\nquery\nresponse = index.query(\"\", mode=\"default\")\n```\n\n\n\n\n\nFAQ/Additional\n\n**Runtime**\n\nWorst-case runtime to execute a query should be $O(k*c)$, where $k$ is the number of extracted keywords, and $c$ is the number of text chunks per query.\n\nHowever the number of queries to GPT is limited by $O(d)$, where $d$ is a\nuser-specified parameter indicating the maximum number of text chunks to query.\n\n**How much does this cost to run?**\n\nAssuming `num_chunks_per_query=10`, then this equates to \\$~0.40 per query.\n\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/README.md", "file_name": "gpt_index/indices/keyword_table/README.md"}, "index": 2, "child_indices": [], "ref_doc_id": "4c51f91a5611ecfd8531f01a74baf77f9f2484cf", "node_info": null}, "3": {"text": "\"\"\"Keyword Table Index Data Structures.\"\"\"\n\n# indices\nfrom gpt_index.indices.keyword_table.base import GPTKeywordTableIndex\nfrom gpt_index.indices.keyword_table.rake_base import GPTRAKEKeywordTableIndex\nfrom gpt_index.indices.keyword_table.simple_base import GPTSimpleKeywordTableIndex\n\n__all__ = [\n    \"GPTKeywordTableIndex\",\n    \"GPTSimpleKeywordTableIndex\",\n    \"GPTRAKEKeywordTableIndex\",\n]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/__init__.py", "file_name": "__init__.py"}, "index": 3, "child_indices": [], "ref_doc_id": "43a973b9b0920e2d6a499bee82a9aee62fe2321f", "node_info": null}, "4": {"text": "\"\"\"Keyword-table based index.\n\nSimilar to a \"hash table\" in concept. GPT Index first tries\nto extract keywords from the source text, and stores the\nkeywords as keys per item. It similarly extracts keywords\nfrom the query text. Then, it tries to match those keywords to\nexisting keywords in the table.\n\n\"\"\"\n\nfrom abc import abstractmethod\nfrom typing import Any, Dict, Optional, Sequence, Set, Type\n\nfrom gpt_index.data_structs.data_structs import KeywordTable\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.keyword_table.utils import extract_keywords_given_response\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.keyword_table.query import (\n    GPTKeywordTableGPTQuery,\n    GPTKeywordTableRAKEQuery,\n    GPTKeywordTableSimpleQuery,\n)\nfrom gpt_index.indices.query.schema import QueryMode\nfrom", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/base.py", "file_name": "base.py"}, "index": 4, "child_indices": [], "ref_doc_id": "71bad2125cd5052fdc9fe562d68fc8ecf47ddb51", "node_info": null}, "5": {"text": "import QueryMode\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.default_prompts import (\n    DEFAULT_KEYWORD_EXTRACT_TEMPLATE,\n    DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE,\n)\nfrom gpt_index.prompts.prompts import KeywordExtractPrompt\nfrom gpt_index.schema import BaseDocument\n\nDQKET = DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE\n\n\nclass BaseGPTKeywordTableIndex(BaseGPTIndex[KeywordTable]):\n    \"\"\"GPT Keyword Table Index.\n\n    This index extracts keywords from the text, and maps each\n    keyword to the node(s) that it corresponds to. In this sense it mimicks a\n    \"hash table\". During index construction, the keyword table is constructed\n    by extracting keywords from each node and creating an internal mapping.\n\n    During query time, the keywords are extracted from the query text, and", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/base.py", "file_name": "base.py"}, "index": 5, "child_indices": [], "ref_doc_id": "71bad2125cd5052fdc9fe562d68fc8ecf47ddb51", "node_info": null}, "6": {"text": "During query time, the keywords are extracted from the query text, and these\n    keywords are used to index into the keyword table. The retrieved nodes\n    are then used to answer the query.\n\n    Args:\n        keyword_extract_template (Optional[KeywordExtractPrompt]): A Keyword\n            Extraction Prompt\n            (see :ref:`Prompt-Templates`).\n\n    \"\"\"\n\n    index_struct_cls = KeywordTable\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[KeywordTable] = None,\n        keyword_extract_template: Optional[KeywordExtractPrompt] = None,\n        max_keywords_per_chunk: int = 10,\n        llm_predictor: Optional[LLMPredictor] =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/base.py", "file_name": "base.py"}, "index": 6, "child_indices": [], "ref_doc_id": "71bad2125cd5052fdc9fe562d68fc8ecf47ddb51", "node_info": null}, "7": {"text": " llm_predictor: Optional[LLMPredictor] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        # need to set parameters before building index in base class.\n        self.max_keywords_per_chunk = max_keywords_per_chunk\n        self.keyword_extract_template = (\n            keyword_extract_template or DEFAULT_KEYWORD_EXTRACT_TEMPLATE\n        )\n        # NOTE: Partially format keyword extract template here.\n        self.keyword_extract_template = self.keyword_extract_template.partial_format(\n            max_keywords=self.max_keywords_per_chunk\n        )\n        super().__init__(\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/base.py", "file_name": "base.py"}, "index": 7, "child_indices": [], "ref_doc_id": "71bad2125cd5052fdc9fe562d68fc8ecf47ddb51", "node_info": null}, "8": {"text": "           documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            **kwargs,\n        )\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.keyword_extract_template, 1\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTKeywordTableGPTQuery,\n            QueryMode.SIMPLE: GPTKeywordTableSimpleQuery,\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/base.py", "file_name": "base.py"}, "index": 8, "child_indices": [], "ref_doc_id": "71bad2125cd5052fdc9fe562d68fc8ecf47ddb51", "node_info": null}, "9": {"text": "           QueryMode.RAKE: GPTKeywordTableRAKEQuery,\n        }\n\n    @abstractmethod\n    def _extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n\n    def _build_index_from_documents(\n        self, documents: Sequence[BaseDocument]\n    ) -> KeywordTable:\n        \"\"\"Build the index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.keyword_extract_template, 1\n        )\n        # do simple concatenation\n        index_struct = KeywordTable(table={})\n        for d in documents:\n            nodes =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/base.py", "file_name": "base.py"}, "index": 9, "child_indices": [], "ref_doc_id": "71bad2125cd5052fdc9fe562d68fc8ecf47ddb51", "node_info": null}, "10": {"text": "           nodes = self._get_nodes_from_document(d, text_splitter)\n            for n in nodes:\n                keywords = self._extract_keywords(n.get_text())\n                index_struct.add_node(list(keywords), n)\n\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        nodes = self._get_nodes_from_document(document, self._text_splitter)\n        for n in nodes:\n            keywords = self._extract_keywords(n.get_text())\n            self._index_struct.add_node(list(keywords), n)\n\n    def _delete(self, doc_id:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/base.py", "file_name": "base.py"}, "index": 10, "child_indices": [], "ref_doc_id": "71bad2125cd5052fdc9fe562d68fc8ecf47ddb51", "node_info": null}, "11": {"text": "n)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        # get set of ids that correspond to node\n        node_idxs_to_delete = set()\n        for node_idx, node in self._index_struct.text_chunks.items():\n            if node.ref_doc_id != doc_id:\n                continue\n            node_idxs_to_delete.add(node_idx)\n        for node_idx in node_idxs_to_delete:\n            del self._index_struct.text_chunks[node_idx]\n\n        # delete node_idxs from keyword to node idxs mapping\n        keywords_to_delete = set()\n        for keyword,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/base.py", "file_name": "base.py"}, "index": 11, "child_indices": [], "ref_doc_id": "71bad2125cd5052fdc9fe562d68fc8ecf47ddb51", "node_info": null}, "12": {"text": "= set()\n        for keyword, node_idxs in self._index_struct.table.items():\n            if node_idxs_to_delete.intersection(node_idxs):\n                self._index_struct.table[keyword] = node_idxs.difference(\n                    node_idxs_to_delete\n                )\n                if not self._index_struct.table[keyword]:\n                    keywords_to_delete.add(keyword)\n\n        for keyword in keywords_to_delete:\n            del self._index_struct.table[keyword]\n\n\nclass GPTKeywordTableIndex(BaseGPTKeywordTableIndex):\n    \"\"\"GPT Keyword Table Index.\n\n    This index uses a", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/base.py", "file_name": "base.py"}, "index": 12, "child_indices": [], "ref_doc_id": "71bad2125cd5052fdc9fe562d68fc8ecf47ddb51", "node_info": null}, "13": {"text": "\"\"\"GPT Keyword Table Index.\n\n    This index uses a GPT model to extract keywords from the text.\n\n    \"\"\"\n\n    def _extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n        response, _ = self._llm_predictor.predict(\n            self.keyword_extract_template,\n            text=text,\n        )\n        keywords = extract_keywords_given_response(response, start_token=\"KEYWORDS:\")\n        return keywords\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/base.py", "file_name": "base.py"}, "index": 13, "child_indices": [], "ref_doc_id": "71bad2125cd5052fdc9fe562d68fc8ecf47ddb51", "node_info": null}, "14": {"text": "\"\"\"RAKE keyword-table based index.\n\nSimilar to GPTKeywordTableIndex, but uses RAKE instead of GPT.\n\n\"\"\"\n\nfrom typing import Set\n\nfrom gpt_index.indices.keyword_table.base import BaseGPTKeywordTableIndex\nfrom gpt_index.indices.keyword_table.utils import rake_extract_keywords\n\n\nclass GPTRAKEKeywordTableIndex(BaseGPTKeywordTableIndex):\n    \"\"\"GPT RAKE Keyword Table Index.\n\n    This index uses a RAKE keyword extractor to extract keywords from the text.\n\n    \"\"\"\n\n    def _extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n        return rake_extract_keywords(text, max_keywords=self.max_keywords_per_chunk)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/rake_base.py", "file_name": "rake_base.py"}, "index": 14, "child_indices": [], "ref_doc_id": "61b26ce6dc58e56ef751b4db489efa42582a4dc6", "node_info": null}, "15": {"text": "\"\"\"Simple keyword-table based index.\n\nSimilar to GPTKeywordTableIndex, but uses a simpler keyword extraction\ntechnique that doesn't involve GPT - just uses regex.\n\n\"\"\"\n\nfrom typing import Set\n\nfrom gpt_index.indices.keyword_table.base import BaseGPTKeywordTableIndex\nfrom gpt_index.indices.keyword_table.utils import simple_extract_keywords\nfrom gpt_index.prompts.default_prompts import DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE\n\nDQKET = DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE\n\n\nclass GPTSimpleKeywordTableIndex(BaseGPTKeywordTableIndex):\n    \"\"\"GPT Simple Keyword Table Index.\n\n    This index uses a simple regex extractor to extract keywords from the text.\n\n    \"\"\"\n\n    def _extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n        return", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/simple_base.py", "file_name": "simple_base.py"}, "index": 15, "child_indices": [], "ref_doc_id": "4d542c38da0d37d7504500a56ee9473f24f51eff", "node_info": null}, "16": {"text": "keywords from text.\"\"\"\n        return simple_extract_keywords(text, self.max_keywords_per_chunk)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/simple_base.py", "file_name": "simple_base.py"}, "index": 16, "child_indices": [], "ref_doc_id": "4d542c38da0d37d7504500a56ee9473f24f51eff", "node_info": null}, "17": {"text": "\"\"\"Utils for keyword table.\"\"\"\n\nimport re\nfrom typing import Optional, Set\n\nimport nltk\nimport pandas as pd\n\nfrom gpt_index.indices.utils import expand_tokens_with_subtokens\nfrom gpt_index.utils import globals_helper\n\n\ndef simple_extract_keywords(\n    text_chunk: str, max_keywords: Optional[int] = None, filter_stopwords: bool = True\n) -> Set[str]:\n    \"\"\"Extract keywords with simple algorithm.\"\"\"\n    tokens = [t.strip().lower() for t in re.findall(r\"\\w+\", text_chunk)]\n    if filter_stopwords:\n        tokens = [t for t in tokens if t not in globals_helper.stopwords]\n    value_counts = pd.Series(tokens).value_counts()\n    keywords = value_counts.index.tolist()[:max_keywords]\n    return set(keywords)\n\n\ndef", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/utils.py", "file_name": "utils.py"}, "index": 17, "child_indices": [], "ref_doc_id": "4afd893ac94d8ef62581ff3fdcf0ca7800e773ac", "node_info": null}, "18": {"text": "   return set(keywords)\n\n\ndef rake_extract_keywords(\n    text_chunk: str,\n    max_keywords: Optional[int] = None,\n    expand_with_subtokens: bool = True,\n) -> Set[str]:\n    \"\"\"Extract keywords with RAKE.\"\"\"\n    nltk.download(\"punkt\")\n    try:\n        from rake_nltk import Rake\n    except ImportError:\n        raise ImportError(\"Please install rake_nltk: `pip install rake_nltk`\")\n\n    r = Rake()\n    r.extract_keywords_from_text(text_chunk)\n    keywords = r.get_ranked_phrases()[:max_keywords]\n    if expand_with_subtokens:\n        return set(expand_tokens_with_subtokens(keywords))\n    else:\n        return set(keywords)\n\n\ndef", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/utils.py", "file_name": "utils.py"}, "index": 18, "child_indices": [], "ref_doc_id": "4afd893ac94d8ef62581ff3fdcf0ca7800e773ac", "node_info": null}, "19": {"text": "       return set(keywords)\n\n\ndef extract_keywords_given_response(\n    response: str, lowercase: bool = True, start_token: str = \"\"\n) -> Set[str]:\n    \"\"\"Extract keywords given the GPT-generated response.\n\n    Used by keyword table indices.\n    Parses <start_token>: <word1>, <word2>, ... into [word1, word2, ...]\n    Raises exception if response doesn't start with <start_token>\n    \"\"\"\n    results = []\n    response = response.strip()  # Strip newlines from responses.\n\n    if response.startswith(start_token):\n        response = response[len(start_token) :]\n\n    keywords = response.split(\",\")\n    for k in keywords:\n        rk = k\n        if lowercase:\n            rk = rk.lower()\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/utils.py", "file_name": "utils.py"}, "index": 19, "child_indices": [], "ref_doc_id": "4afd893ac94d8ef62581ff3fdcf0ca7800e773ac", "node_info": null}, "20": {"text": "rk = rk.lower()\n        results.append(rk.strip())\n\n    # if keyword consists of multiple words, split into subwords\n    # (removing stopwords)\n    return expand_tokens_with_subtokens(set(results))\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/utils.py", "file_name": "utils.py"}, "index": 20, "child_indices": [], "ref_doc_id": "4afd893ac94d8ef62581ff3fdcf0ca7800e773ac", "node_info": null}, "21": {"text": "GPTKeywordIndex is a keyword-based table data structure (inspired by \"hash tables\") used to store text documents. During index construction, GPT is used to extract a set of relevant keywords from the text chunks, which are then stored in a table. During query-time, the GPTKeywordIndex extracts a set of relevant keywords from the query using a customized variant of the same keyword extraction prompt. These keywords are then used to fetch the set of candidate text chunk ID's. The text chunk ID's are ordered by number of matching keywords (from highest to lowest), and truncated after a cutoff. There are three query modes: default, simple, and rake. Runtime for a query should be $O(k*c)$, where $k$ is the number of extracted keywords, and $c$ is the number of text chunks per query. Assuming `num_chunks_per_query=10`, then this equates to \\$~0.40 per query.", "doc_id": null, "embedding": null, "extra_info": null, "index": 21, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "22": {"text": "This code file contains code for keyword table indices, which are used to extract keywords from text. It contains three classes: GPTKeywordTableIndex, GPTRAKEKeywordTableIndex, and GPTSimpleKeywordTableIndex. Each class has a _extract_keywords() method which uses a different keyword extraction technique (GPT, RAKE, or regex). Additionally, there are utility functions for extracting keywords from a GPT response, and for extracting keywords with RAKE or regex.", "doc_id": null, "embedding": null, "extra_info": null, "index": 22, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"21": {"text": "GPTKeywordIndex is a keyword-based table data structure (inspired by \"hash tables\") used to store text documents. During index construction, GPT is used to extract a set of relevant keywords from the text chunks, which are then stored in a table. During query-time, the GPTKeywordIndex extracts a set of relevant keywords from the query using a customized variant of the same keyword extraction prompt. These keywords are then used to fetch the set of candidate text chunk ID's. The text chunk ID's are ordered by number of matching keywords (from highest to lowest), and truncated after a cutoff. There are three query modes: default, simple, and rake. Runtime for a query should be $O(k*c)$, where $k$ is the number of extracted keywords, and $c$ is the number of text chunks per query. Assuming `num_chunks_per_query=10`, then this equates to \\$~0.40 per query.", "doc_id": null, "embedding": null, "extra_info": null, "index": 21, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "22": {"text": "This code file contains code for keyword table indices, which are used to extract keywords from text. It contains three classes: GPTKeywordTableIndex, GPTRAKEKeywordTableIndex, and GPTSimpleKeywordTableIndex. Each class has a _extract_keywords() method which uses a different keyword extraction technique (GPT, RAKE, or regex). Additionally, there are utility functions for extracting keywords from a GPT response, and for extracting keywords with RAKE or regex.", "doc_id": null, "embedding": null, "extra_info": null, "index": 22, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"4c51f91a5611ecfd8531f01a74baf77f9f2484cf": {"text": "\n\n\ud83d\udd11 GPTKeywordIndex\n\nGPTKeywordIndex is a keyword-based table data structure (inspired by \"hash tables\").\n\n\n\n\n\nIndex Construction\n\nDuring index construction, GPTKeywordIndex first takes in a dataset of text documents as input, and chunks them up into smaller document chunks. For each text chunk, GPTKeywordIndex uses GPT to extract a set of relevant keywords with a **keyword extraction prompt**. (keywords can include short phrases, like \"new york city\"). These keywords are then stored in a table, referencing the same text chunk.\n\n\n\n\n\nQuery\n\nThere are three query modes: `default`, `simple`, and `rake`.\n\n**Default**\n\nDuring query-time, the GPTKeywordIndex extracts a set of relevant keywords from the query using a customized variant of the same **keyword extraction prompt**. These keywords are then used to fetch the set of candidate text chunk ID's. The text chunk ID's are ordered by number of matching keywords (from highest to lowest), and truncated after a cutoff $d$, which represents the maximum number of text chunks to consider.\n\nWe construct an answer using the _create and refine_ paradigm. An initial answer to the query is constructed using the first text chunk. The answer is then _refined_ through feeding in subsequent text chunks as context. Refinement could mean keeping the original answer, making small edits to the original answer, or rewriting the original answer completely.\n\n**Simple (Regex)**\nInstead of using GPT for keyword extraction, this mode uses a simple regex query to find words, filtering out stopwords.\n\n**RAKE**\nUse the popular RAKE keyword extractor.\n\n\n\n\n\nUsage\n\n```python\nfrom gpt_index import GPTKeywordTableIndex, SimpleDirectoryReader\n\n\n\n\n\nbuild index\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = GPTKeywordTableIndex(documents)\n\n\n\n\nsave index\nindex.save_to_disk('index_table.json')\n\n\n\n\nload index from disk\nindex = GPTKeywordTableIndex.load_from_disk('index_table.json')\n\n\n\n\nquery\nresponse = index.query(\"\", mode=\"default\")\n```\n\n\n\n\n\nFAQ/Additional\n\n**Runtime**\n\nWorst-case runtime to execute a query should be $O(k*c)$, where $k$ is the number of extracted keywords, and $c$ is the number of text chunks per query.\n\nHowever the number of queries to GPT is limited by $O(d)$, where $d$ is a\nuser-specified parameter indicating the maximum number of text chunks to query.\n\n**How much does this cost to run?**\n\nAssuming `num_chunks_per_query=10`, then this equates to \\$~0.40 per query.\n\n", "doc_id": "4c51f91a5611ecfd8531f01a74baf77f9f2484cf", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/README.md", "file_name": "gpt_index/indices/keyword_table/README.md"}, "__type__": "Document"}, "43a973b9b0920e2d6a499bee82a9aee62fe2321f": {"text": "\"\"\"Keyword Table Index Data Structures.\"\"\"\n\n# indices\nfrom gpt_index.indices.keyword_table.base import GPTKeywordTableIndex\nfrom gpt_index.indices.keyword_table.rake_base import GPTRAKEKeywordTableIndex\nfrom gpt_index.indices.keyword_table.simple_base import GPTSimpleKeywordTableIndex\n\n__all__ = [\n    \"GPTKeywordTableIndex\",\n    \"GPTSimpleKeywordTableIndex\",\n    \"GPTRAKEKeywordTableIndex\",\n]\n", "doc_id": "43a973b9b0920e2d6a499bee82a9aee62fe2321f", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "71bad2125cd5052fdc9fe562d68fc8ecf47ddb51": {"text": "\"\"\"Keyword-table based index.\n\nSimilar to a \"hash table\" in concept. GPT Index first tries\nto extract keywords from the source text, and stores the\nkeywords as keys per item. It similarly extracts keywords\nfrom the query text. Then, it tries to match those keywords to\nexisting keywords in the table.\n\n\"\"\"\n\nfrom abc import abstractmethod\nfrom typing import Any, Dict, Optional, Sequence, Set, Type\n\nfrom gpt_index.data_structs.data_structs import KeywordTable\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.keyword_table.utils import extract_keywords_given_response\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.keyword_table.query import (\n    GPTKeywordTableGPTQuery,\n    GPTKeywordTableRAKEQuery,\n    GPTKeywordTableSimpleQuery,\n)\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.default_prompts import (\n    DEFAULT_KEYWORD_EXTRACT_TEMPLATE,\n    DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE,\n)\nfrom gpt_index.prompts.prompts import KeywordExtractPrompt\nfrom gpt_index.schema import BaseDocument\n\nDQKET = DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE\n\n\nclass BaseGPTKeywordTableIndex(BaseGPTIndex[KeywordTable]):\n    \"\"\"GPT Keyword Table Index.\n\n    This index extracts keywords from the text, and maps each\n    keyword to the node(s) that it corresponds to. In this sense it mimicks a\n    \"hash table\". During index construction, the keyword table is constructed\n    by extracting keywords from each node and creating an internal mapping.\n\n    During query time, the keywords are extracted from the query text, and these\n    keywords are used to index into the keyword table. The retrieved nodes\n    are then used to answer the query.\n\n    Args:\n        keyword_extract_template (Optional[KeywordExtractPrompt]): A Keyword\n            Extraction Prompt\n            (see :ref:`Prompt-Templates`).\n\n    \"\"\"\n\n    index_struct_cls = KeywordTable\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[KeywordTable] = None,\n        keyword_extract_template: Optional[KeywordExtractPrompt] = None,\n        max_keywords_per_chunk: int = 10,\n        llm_predictor: Optional[LLMPredictor] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        # need to set parameters before building index in base class.\n        self.max_keywords_per_chunk = max_keywords_per_chunk\n        self.keyword_extract_template = (\n            keyword_extract_template or DEFAULT_KEYWORD_EXTRACT_TEMPLATE\n        )\n        # NOTE: Partially format keyword extract template here.\n        self.keyword_extract_template = self.keyword_extract_template.partial_format(\n            max_keywords=self.max_keywords_per_chunk\n        )\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            **kwargs,\n        )\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.keyword_extract_template, 1\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTKeywordTableGPTQuery,\n            QueryMode.SIMPLE: GPTKeywordTableSimpleQuery,\n            QueryMode.RAKE: GPTKeywordTableRAKEQuery,\n        }\n\n    @abstractmethod\n    def _extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n\n    def _build_index_from_documents(\n        self, documents: Sequence[BaseDocument]\n    ) -> KeywordTable:\n        \"\"\"Build the index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.keyword_extract_template, 1\n        )\n        # do simple concatenation\n        index_struct = KeywordTable(table={})\n        for d in documents:\n            nodes = self._get_nodes_from_document(d, text_splitter)\n            for n in nodes:\n                keywords = self._extract_keywords(n.get_text())\n                index_struct.add_node(list(keywords), n)\n\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        nodes = self._get_nodes_from_document(document, self._text_splitter)\n        for n in nodes:\n            keywords = self._extract_keywords(n.get_text())\n            self._index_struct.add_node(list(keywords), n)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        # get set of ids that correspond to node\n        node_idxs_to_delete = set()\n        for node_idx, node in self._index_struct.text_chunks.items():\n            if node.ref_doc_id != doc_id:\n                continue\n            node_idxs_to_delete.add(node_idx)\n        for node_idx in node_idxs_to_delete:\n            del self._index_struct.text_chunks[node_idx]\n\n        # delete node_idxs from keyword to node idxs mapping\n        keywords_to_delete = set()\n        for keyword, node_idxs in self._index_struct.table.items():\n            if node_idxs_to_delete.intersection(node_idxs):\n                self._index_struct.table[keyword] = node_idxs.difference(\n                    node_idxs_to_delete\n                )\n                if not self._index_struct.table[keyword]:\n                    keywords_to_delete.add(keyword)\n\n        for keyword in keywords_to_delete:\n            del self._index_struct.table[keyword]\n\n\nclass GPTKeywordTableIndex(BaseGPTKeywordTableIndex):\n    \"\"\"GPT Keyword Table Index.\n\n    This index uses a GPT model to extract keywords from the text.\n\n    \"\"\"\n\n    def _extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n        response, _ = self._llm_predictor.predict(\n            self.keyword_extract_template,\n            text=text,\n        )\n        keywords = extract_keywords_given_response(response, start_token=\"KEYWORDS:\")\n        return keywords\n", "doc_id": "71bad2125cd5052fdc9fe562d68fc8ecf47ddb51", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/base.py", "file_name": "base.py"}, "__type__": "Document"}, "61b26ce6dc58e56ef751b4db489efa42582a4dc6": {"text": "\"\"\"RAKE keyword-table based index.\n\nSimilar to GPTKeywordTableIndex, but uses RAKE instead of GPT.\n\n\"\"\"\n\nfrom typing import Set\n\nfrom gpt_index.indices.keyword_table.base import BaseGPTKeywordTableIndex\nfrom gpt_index.indices.keyword_table.utils import rake_extract_keywords\n\n\nclass GPTRAKEKeywordTableIndex(BaseGPTKeywordTableIndex):\n    \"\"\"GPT RAKE Keyword Table Index.\n\n    This index uses a RAKE keyword extractor to extract keywords from the text.\n\n    \"\"\"\n\n    def _extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n        return rake_extract_keywords(text, max_keywords=self.max_keywords_per_chunk)\n", "doc_id": "61b26ce6dc58e56ef751b4db489efa42582a4dc6", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/rake_base.py", "file_name": "rake_base.py"}, "__type__": "Document"}, "4d542c38da0d37d7504500a56ee9473f24f51eff": {"text": "\"\"\"Simple keyword-table based index.\n\nSimilar to GPTKeywordTableIndex, but uses a simpler keyword extraction\ntechnique that doesn't involve GPT - just uses regex.\n\n\"\"\"\n\nfrom typing import Set\n\nfrom gpt_index.indices.keyword_table.base import BaseGPTKeywordTableIndex\nfrom gpt_index.indices.keyword_table.utils import simple_extract_keywords\nfrom gpt_index.prompts.default_prompts import DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE\n\nDQKET = DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE\n\n\nclass GPTSimpleKeywordTableIndex(BaseGPTKeywordTableIndex):\n    \"\"\"GPT Simple Keyword Table Index.\n\n    This index uses a simple regex extractor to extract keywords from the text.\n\n    \"\"\"\n\n    def _extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n        return simple_extract_keywords(text, self.max_keywords_per_chunk)\n", "doc_id": "4d542c38da0d37d7504500a56ee9473f24f51eff", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/simple_base.py", "file_name": "simple_base.py"}, "__type__": "Document"}, "4afd893ac94d8ef62581ff3fdcf0ca7800e773ac": {"text": "\"\"\"Utils for keyword table.\"\"\"\n\nimport re\nfrom typing import Optional, Set\n\nimport nltk\nimport pandas as pd\n\nfrom gpt_index.indices.utils import expand_tokens_with_subtokens\nfrom gpt_index.utils import globals_helper\n\n\ndef simple_extract_keywords(\n    text_chunk: str, max_keywords: Optional[int] = None, filter_stopwords: bool = True\n) -> Set[str]:\n    \"\"\"Extract keywords with simple algorithm.\"\"\"\n    tokens = [t.strip().lower() for t in re.findall(r\"\\w+\", text_chunk)]\n    if filter_stopwords:\n        tokens = [t for t in tokens if t not in globals_helper.stopwords]\n    value_counts = pd.Series(tokens).value_counts()\n    keywords = value_counts.index.tolist()[:max_keywords]\n    return set(keywords)\n\n\ndef rake_extract_keywords(\n    text_chunk: str,\n    max_keywords: Optional[int] = None,\n    expand_with_subtokens: bool = True,\n) -> Set[str]:\n    \"\"\"Extract keywords with RAKE.\"\"\"\n    nltk.download(\"punkt\")\n    try:\n        from rake_nltk import Rake\n    except ImportError:\n        raise ImportError(\"Please install rake_nltk: `pip install rake_nltk`\")\n\n    r = Rake()\n    r.extract_keywords_from_text(text_chunk)\n    keywords = r.get_ranked_phrases()[:max_keywords]\n    if expand_with_subtokens:\n        return set(expand_tokens_with_subtokens(keywords))\n    else:\n        return set(keywords)\n\n\ndef extract_keywords_given_response(\n    response: str, lowercase: bool = True, start_token: str = \"\"\n) -> Set[str]:\n    \"\"\"Extract keywords given the GPT-generated response.\n\n    Used by keyword table indices.\n    Parses <start_token>: <word1>, <word2>, ... into [word1, word2, ...]\n    Raises exception if response doesn't start with <start_token>\n    \"\"\"\n    results = []\n    response = response.strip()  # Strip newlines from responses.\n\n    if response.startswith(start_token):\n        response = response[len(start_token) :]\n\n    keywords = response.split(\",\")\n    for k in keywords:\n        rk = k\n        if lowercase:\n            rk = rk.lower()\n        results.append(rk.strip())\n\n    # if keyword consists of multiple words, split into subwords\n    # (removing stopwords)\n    return expand_tokens_with_subtokens(set(results))\n", "doc_id": "4afd893ac94d8ef62581ff3fdcf0ca7800e773ac", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/utils.py", "file_name": "utils.py"}, "__type__": "Document"}, "a3c43b6b-458f-43f8-b3fa-a3db3d29c666": {"text": "\nThe documents contain code for keyword table indices, which are used to extract keywords from text documents. The code includes three classes which use different keyword extraction techniques (GPT, RAKE, and regex). During index construction, GPTKeywordIndex takes in a dataset of text documents as input, and chunks them up into smaller document chunks. During query-time, the GPTKeywordIndex extracts a set of relevant keywords from the query using a customized variant of the same keyword extraction prompt. These keywords are then used to fetch the set of candidate text chunk ID's. The text chunk ID's are ordered by number of matching keywords (from highest to lowest), and truncated after a cutoff. An initial answer to the query is constructed using the first text chunk. The answer is then refined through feeding in subsequent text chunks as context. The worst-case runtime to execute a query should be $O(k*c)$, where $k$ is the number of extracted keywords, and $c$ is the number of candidate text chunks.", "doc_id": "a3c43b6b-458f-43f8-b3fa-a3db3d29c666", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\n\n\ud83d\udd11 GPTKeywordIndex\n\nGPTKeywordIndex is a keyword-based table data structure (inspired by \"hash tables\").\n\n\n\n\n\nIndex Construction\n\nDuring index construction, GPTKeywordIndex first takes in a dataset of text documents as input, and chunks them up into smaller document chunks. For each text chunk, GPTKeywordIndex uses GPT to extract a set of relevant keywords with a **keyword extraction prompt**. (keywords can include short phrases, like \"new york city\"). These keywords are then stored in a table, referencing the same text chunk.\n\n\n\n\n\nQuery\n\nThere are three query modes: `default`, `simple`, and `rake`.\n\n**Default**\n\nDuring query-time, the GPTKeywordIndex extracts a set of relevant keywords from the query using a customized variant of the same **keyword extraction prompt**. These keywords are then used to fetch the set of candidate text chunk ID's. The text chunk ID's are ordered by number of matching keywords (from highest to lowest), and truncated", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/README.md", "file_name": "gpt_index/indices/keyword_table/README.md"}, "index": 0, "child_indices": [], "ref_doc_id": "4c51f91a5611ecfd8531f01a74baf77f9f2484cf", "node_info": null}, "1": {"text": "of matching keywords (from highest to lowest), and truncated after a cutoff $d$, which represents the maximum number of text chunks to consider.\n\nWe construct an answer using the _create and refine_ paradigm. An initial answer to the query is constructed using the first text chunk. The answer is then _refined_ through feeding in subsequent text chunks as context. Refinement could mean keeping the original answer, making small edits to the original answer, or rewriting the original answer completely.\n\n**Simple (Regex)**\nInstead of using GPT for keyword extraction, this mode uses a simple regex query to find words, filtering out stopwords.\n\n**RAKE**\nUse the popular RAKE keyword extractor.\n\n\n\n\n\nUsage\n\n```python\nfrom gpt_index import GPTKeywordTableIndex, SimpleDirectoryReader\n\n\n\n\n\nbuild index\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = GPTKeywordTableIndex(documents)\n\n\n\n\nsave index\nindex.save_to_disk('index_table.json')\n\n\n\n\nload index from disk\nindex =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/README.md", "file_name": "gpt_index/indices/keyword_table/README.md"}, "index": 1, "child_indices": [], "ref_doc_id": "4c51f91a5611ecfd8531f01a74baf77f9f2484cf", "node_info": null}, "2": {"text": "index from disk\nindex = GPTKeywordTableIndex.load_from_disk('index_table.json')\n\n\n\n\nquery\nresponse = index.query(\"\", mode=\"default\")\n```\n\n\n\n\n\nFAQ/Additional\n\n**Runtime**\n\nWorst-case runtime to execute a query should be $O(k*c)$, where $k$ is the number of extracted keywords, and $c$ is the number of text chunks per query.\n\nHowever the number of queries to GPT is limited by $O(d)$, where $d$ is a\nuser-specified parameter indicating the maximum number of text chunks to query.\n\n**How much does this cost to run?**\n\nAssuming `num_chunks_per_query=10`, then this equates to \\$~0.40 per query.\n\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/README.md", "file_name": "gpt_index/indices/keyword_table/README.md"}, "index": 2, "child_indices": [], "ref_doc_id": "4c51f91a5611ecfd8531f01a74baf77f9f2484cf", "node_info": null}, "3": {"text": "\"\"\"Keyword Table Index Data Structures.\"\"\"\n\n# indices\nfrom gpt_index.indices.keyword_table.base import GPTKeywordTableIndex\nfrom gpt_index.indices.keyword_table.rake_base import GPTRAKEKeywordTableIndex\nfrom gpt_index.indices.keyword_table.simple_base import GPTSimpleKeywordTableIndex\n\n__all__ = [\n    \"GPTKeywordTableIndex\",\n    \"GPTSimpleKeywordTableIndex\",\n    \"GPTRAKEKeywordTableIndex\",\n]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/__init__.py", "file_name": "__init__.py"}, "index": 3, "child_indices": [], "ref_doc_id": "43a973b9b0920e2d6a499bee82a9aee62fe2321f", "node_info": null}, "4": {"text": "\"\"\"Keyword-table based index.\n\nSimilar to a \"hash table\" in concept. GPT Index first tries\nto extract keywords from the source text, and stores the\nkeywords as keys per item. It similarly extracts keywords\nfrom the query text. Then, it tries to match those keywords to\nexisting keywords in the table.\n\n\"\"\"\n\nfrom abc import abstractmethod\nfrom typing import Any, Dict, Optional, Sequence, Set, Type\n\nfrom gpt_index.data_structs.data_structs import KeywordTable\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.keyword_table.utils import extract_keywords_given_response\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.keyword_table.query import (\n    GPTKeywordTableGPTQuery,\n    GPTKeywordTableRAKEQuery,\n    GPTKeywordTableSimpleQuery,\n)\nfrom gpt_index.indices.query.schema import QueryMode\nfrom", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/base.py", "file_name": "base.py"}, "index": 4, "child_indices": [], "ref_doc_id": "71bad2125cd5052fdc9fe562d68fc8ecf47ddb51", "node_info": null}, "5": {"text": "import QueryMode\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.default_prompts import (\n    DEFAULT_KEYWORD_EXTRACT_TEMPLATE,\n    DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE,\n)\nfrom gpt_index.prompts.prompts import KeywordExtractPrompt\nfrom gpt_index.schema import BaseDocument\n\nDQKET = DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE\n\n\nclass BaseGPTKeywordTableIndex(BaseGPTIndex[KeywordTable]):\n    \"\"\"GPT Keyword Table Index.\n\n    This index extracts keywords from the text, and maps each\n    keyword to the node(s) that it corresponds to. In this sense it mimicks a\n    \"hash table\". During index construction, the keyword table is constructed\n    by extracting keywords from each node and creating an internal mapping.\n\n    During query time, the keywords are extracted from the query text, and", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/base.py", "file_name": "base.py"}, "index": 5, "child_indices": [], "ref_doc_id": "71bad2125cd5052fdc9fe562d68fc8ecf47ddb51", "node_info": null}, "6": {"text": "During query time, the keywords are extracted from the query text, and these\n    keywords are used to index into the keyword table. The retrieved nodes\n    are then used to answer the query.\n\n    Args:\n        keyword_extract_template (Optional[KeywordExtractPrompt]): A Keyword\n            Extraction Prompt\n            (see :ref:`Prompt-Templates`).\n\n    \"\"\"\n\n    index_struct_cls = KeywordTable\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[KeywordTable] = None,\n        keyword_extract_template: Optional[KeywordExtractPrompt] = None,\n        max_keywords_per_chunk: int = 10,\n        llm_predictor: Optional[LLMPredictor] =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/base.py", "file_name": "base.py"}, "index": 6, "child_indices": [], "ref_doc_id": "71bad2125cd5052fdc9fe562d68fc8ecf47ddb51", "node_info": null}, "7": {"text": " llm_predictor: Optional[LLMPredictor] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        # need to set parameters before building index in base class.\n        self.max_keywords_per_chunk = max_keywords_per_chunk\n        self.keyword_extract_template = (\n            keyword_extract_template or DEFAULT_KEYWORD_EXTRACT_TEMPLATE\n        )\n        # NOTE: Partially format keyword extract template here.\n        self.keyword_extract_template = self.keyword_extract_template.partial_format(\n            max_keywords=self.max_keywords_per_chunk\n        )\n        super().__init__(\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/base.py", "file_name": "base.py"}, "index": 7, "child_indices": [], "ref_doc_id": "71bad2125cd5052fdc9fe562d68fc8ecf47ddb51", "node_info": null}, "8": {"text": "           documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            **kwargs,\n        )\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.keyword_extract_template, 1\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTKeywordTableGPTQuery,\n            QueryMode.SIMPLE: GPTKeywordTableSimpleQuery,\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/base.py", "file_name": "base.py"}, "index": 8, "child_indices": [], "ref_doc_id": "71bad2125cd5052fdc9fe562d68fc8ecf47ddb51", "node_info": null}, "9": {"text": "           QueryMode.RAKE: GPTKeywordTableRAKEQuery,\n        }\n\n    @abstractmethod\n    def _extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n\n    def _build_index_from_documents(\n        self, documents: Sequence[BaseDocument]\n    ) -> KeywordTable:\n        \"\"\"Build the index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.keyword_extract_template, 1\n        )\n        # do simple concatenation\n        index_struct = KeywordTable(table={})\n        for d in documents:\n            nodes =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/base.py", "file_name": "base.py"}, "index": 9, "child_indices": [], "ref_doc_id": "71bad2125cd5052fdc9fe562d68fc8ecf47ddb51", "node_info": null}, "10": {"text": "           nodes = self._get_nodes_from_document(d, text_splitter)\n            for n in nodes:\n                keywords = self._extract_keywords(n.get_text())\n                index_struct.add_node(list(keywords), n)\n\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        nodes = self._get_nodes_from_document(document, self._text_splitter)\n        for n in nodes:\n            keywords = self._extract_keywords(n.get_text())\n            self._index_struct.add_node(list(keywords), n)\n\n    def _delete(self, doc_id:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/base.py", "file_name": "base.py"}, "index": 10, "child_indices": [], "ref_doc_id": "71bad2125cd5052fdc9fe562d68fc8ecf47ddb51", "node_info": null}, "11": {"text": "n)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        # get set of ids that correspond to node\n        node_idxs_to_delete = set()\n        for node_idx, node in self._index_struct.text_chunks.items():\n            if node.ref_doc_id != doc_id:\n                continue\n            node_idxs_to_delete.add(node_idx)\n        for node_idx in node_idxs_to_delete:\n            del self._index_struct.text_chunks[node_idx]\n\n        # delete node_idxs from keyword to node idxs mapping\n        keywords_to_delete = set()\n        for keyword,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/base.py", "file_name": "base.py"}, "index": 11, "child_indices": [], "ref_doc_id": "71bad2125cd5052fdc9fe562d68fc8ecf47ddb51", "node_info": null}, "12": {"text": "= set()\n        for keyword, node_idxs in self._index_struct.table.items():\n            if node_idxs_to_delete.intersection(node_idxs):\n                self._index_struct.table[keyword] = node_idxs.difference(\n                    node_idxs_to_delete\n                )\n                if not self._index_struct.table[keyword]:\n                    keywords_to_delete.add(keyword)\n\n        for keyword in keywords_to_delete:\n            del self._index_struct.table[keyword]\n\n\nclass GPTKeywordTableIndex(BaseGPTKeywordTableIndex):\n    \"\"\"GPT Keyword Table Index.\n\n    This index uses a", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/base.py", "file_name": "base.py"}, "index": 12, "child_indices": [], "ref_doc_id": "71bad2125cd5052fdc9fe562d68fc8ecf47ddb51", "node_info": null}, "13": {"text": "\"\"\"GPT Keyword Table Index.\n\n    This index uses a GPT model to extract keywords from the text.\n\n    \"\"\"\n\n    def _extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n        response, _ = self._llm_predictor.predict(\n            self.keyword_extract_template,\n            text=text,\n        )\n        keywords = extract_keywords_given_response(response, start_token=\"KEYWORDS:\")\n        return keywords\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/base.py", "file_name": "base.py"}, "index": 13, "child_indices": [], "ref_doc_id": "71bad2125cd5052fdc9fe562d68fc8ecf47ddb51", "node_info": null}, "14": {"text": "\"\"\"RAKE keyword-table based index.\n\nSimilar to GPTKeywordTableIndex, but uses RAKE instead of GPT.\n\n\"\"\"\n\nfrom typing import Set\n\nfrom gpt_index.indices.keyword_table.base import BaseGPTKeywordTableIndex\nfrom gpt_index.indices.keyword_table.utils import rake_extract_keywords\n\n\nclass GPTRAKEKeywordTableIndex(BaseGPTKeywordTableIndex):\n    \"\"\"GPT RAKE Keyword Table Index.\n\n    This index uses a RAKE keyword extractor to extract keywords from the text.\n\n    \"\"\"\n\n    def _extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n        return rake_extract_keywords(text, max_keywords=self.max_keywords_per_chunk)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/rake_base.py", "file_name": "rake_base.py"}, "index": 14, "child_indices": [], "ref_doc_id": "61b26ce6dc58e56ef751b4db489efa42582a4dc6", "node_info": null}, "15": {"text": "\"\"\"Simple keyword-table based index.\n\nSimilar to GPTKeywordTableIndex, but uses a simpler keyword extraction\ntechnique that doesn't involve GPT - just uses regex.\n\n\"\"\"\n\nfrom typing import Set\n\nfrom gpt_index.indices.keyword_table.base import BaseGPTKeywordTableIndex\nfrom gpt_index.indices.keyword_table.utils import simple_extract_keywords\nfrom gpt_index.prompts.default_prompts import DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE\n\nDQKET = DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE\n\n\nclass GPTSimpleKeywordTableIndex(BaseGPTKeywordTableIndex):\n    \"\"\"GPT Simple Keyword Table Index.\n\n    This index uses a simple regex extractor to extract keywords from the text.\n\n    \"\"\"\n\n    def _extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n        return", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/simple_base.py", "file_name": "simple_base.py"}, "index": 15, "child_indices": [], "ref_doc_id": "4d542c38da0d37d7504500a56ee9473f24f51eff", "node_info": null}, "16": {"text": "keywords from text.\"\"\"\n        return simple_extract_keywords(text, self.max_keywords_per_chunk)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/simple_base.py", "file_name": "simple_base.py"}, "index": 16, "child_indices": [], "ref_doc_id": "4d542c38da0d37d7504500a56ee9473f24f51eff", "node_info": null}, "17": {"text": "\"\"\"Utils for keyword table.\"\"\"\n\nimport re\nfrom typing import Optional, Set\n\nimport nltk\nimport pandas as pd\n\nfrom gpt_index.indices.utils import expand_tokens_with_subtokens\nfrom gpt_index.utils import globals_helper\n\n\ndef simple_extract_keywords(\n    text_chunk: str, max_keywords: Optional[int] = None, filter_stopwords: bool = True\n) -> Set[str]:\n    \"\"\"Extract keywords with simple algorithm.\"\"\"\n    tokens = [t.strip().lower() for t in re.findall(r\"\\w+\", text_chunk)]\n    if filter_stopwords:\n        tokens = [t for t in tokens if t not in globals_helper.stopwords]\n    value_counts = pd.Series(tokens).value_counts()\n    keywords = value_counts.index.tolist()[:max_keywords]\n    return set(keywords)\n\n\ndef", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/utils.py", "file_name": "utils.py"}, "index": 17, "child_indices": [], "ref_doc_id": "4afd893ac94d8ef62581ff3fdcf0ca7800e773ac", "node_info": null}, "18": {"text": "   return set(keywords)\n\n\ndef rake_extract_keywords(\n    text_chunk: str,\n    max_keywords: Optional[int] = None,\n    expand_with_subtokens: bool = True,\n) -> Set[str]:\n    \"\"\"Extract keywords with RAKE.\"\"\"\n    nltk.download(\"punkt\")\n    try:\n        from rake_nltk import Rake\n    except ImportError:\n        raise ImportError(\"Please install rake_nltk: `pip install rake_nltk`\")\n\n    r = Rake()\n    r.extract_keywords_from_text(text_chunk)\n    keywords = r.get_ranked_phrases()[:max_keywords]\n    if expand_with_subtokens:\n        return set(expand_tokens_with_subtokens(keywords))\n    else:\n        return set(keywords)\n\n\ndef", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/utils.py", "file_name": "utils.py"}, "index": 18, "child_indices": [], "ref_doc_id": "4afd893ac94d8ef62581ff3fdcf0ca7800e773ac", "node_info": null}, "19": {"text": "       return set(keywords)\n\n\ndef extract_keywords_given_response(\n    response: str, lowercase: bool = True, start_token: str = \"\"\n) -> Set[str]:\n    \"\"\"Extract keywords given the GPT-generated response.\n\n    Used by keyword table indices.\n    Parses <start_token>: <word1>, <word2>, ... into [word1, word2, ...]\n    Raises exception if response doesn't start with <start_token>\n    \"\"\"\n    results = []\n    response = response.strip()  # Strip newlines from responses.\n\n    if response.startswith(start_token):\n        response = response[len(start_token) :]\n\n    keywords = response.split(\",\")\n    for k in keywords:\n        rk = k\n        if lowercase:\n            rk = rk.lower()\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/utils.py", "file_name": "utils.py"}, "index": 19, "child_indices": [], "ref_doc_id": "4afd893ac94d8ef62581ff3fdcf0ca7800e773ac", "node_info": null}, "20": {"text": "rk = rk.lower()\n        results.append(rk.strip())\n\n    # if keyword consists of multiple words, split into subwords\n    # (removing stopwords)\n    return expand_tokens_with_subtokens(set(results))\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/utils.py", "file_name": "utils.py"}, "index": 20, "child_indices": [], "ref_doc_id": "4afd893ac94d8ef62581ff3fdcf0ca7800e773ac", "node_info": null}, "21": {"text": "GPTKeywordIndex is a keyword-based table data structure (inspired by \"hash tables\") used to store text documents. During index construction, GPT is used to extract a set of relevant keywords from the text chunks, which are then stored in a table. During query-time, the GPTKeywordIndex extracts a set of relevant keywords from the query using a customized variant of the same keyword extraction prompt. These keywords are then used to fetch the set of candidate text chunk ID's. The text chunk ID's are ordered by number of matching keywords (from highest to lowest), and truncated after a cutoff. There are three query modes: default, simple, and rake. Runtime for a query should be $O(k*c)$, where $k$ is the number of extracted keywords, and $c$ is the number of text chunks per query. Assuming `num_chunks_per_query=10`, then this equates to \\$~0.40 per query.", "doc_id": null, "embedding": null, "extra_info": null, "index": 21, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "22": {"text": "This code file contains code for keyword table indices, which are used to extract keywords from text. It contains three classes: GPTKeywordTableIndex, GPTRAKEKeywordTableIndex, and GPTSimpleKeywordTableIndex. Each class has a _extract_keywords() method which uses a different keyword extraction technique (GPT, RAKE, or regex). Additionally, there are utility functions for extracting keywords from a GPT response, and for extracting keywords with RAKE or regex.", "doc_id": null, "embedding": null, "extra_info": null, "index": 22, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"21": {"text": "GPTKeywordIndex is a keyword-based table data structure (inspired by \"hash tables\") used to store text documents. During index construction, GPT is used to extract a set of relevant keywords from the text chunks, which are then stored in a table. During query-time, the GPTKeywordIndex extracts a set of relevant keywords from the query using a customized variant of the same keyword extraction prompt. These keywords are then used to fetch the set of candidate text chunk ID's. The text chunk ID's are ordered by number of matching keywords (from highest to lowest), and truncated after a cutoff. There are three query modes: default, simple, and rake. Runtime for a query should be $O(k*c)$, where $k$ is the number of extracted keywords, and $c$ is the number of text chunks per query. Assuming `num_chunks_per_query=10`, then this equates to \\$~0.40 per query.", "doc_id": null, "embedding": null, "extra_info": null, "index": 21, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "22": {"text": "This code file contains code for keyword table indices, which are used to extract keywords from text. It contains three classes: GPTKeywordTableIndex, GPTRAKEKeywordTableIndex, and GPTSimpleKeywordTableIndex. Each class has a _extract_keywords() method which uses a different keyword extraction technique (GPT, RAKE, or regex). Additionally, there are utility functions for extracting keywords from a GPT response, and for extracting keywords with RAKE or regex.", "doc_id": null, "embedding": null, "extra_info": null, "index": 22, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}