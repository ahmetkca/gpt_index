{"index_struct": {"text": "\n__init__.py: Query classes for tree indices.\nembedding_query.py: Query Tree using embedding similarity between query and node text.\nleaf_query.py: Leaf query mechanism.", "doc_id": "2e1307a1-d305-4097-aadc-95572e9e99e5", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Query classes for tree indices.\"\"\"\n\nfrom gpt_index.indices.query.tree.embedding_query import GPTTreeIndexEmbeddingQuery\nfrom gpt_index.indices.query.tree.leaf_query import GPTTreeIndexLeafQuery\nfrom gpt_index.indices.query.tree.retrieve_query import GPTTreeIndexRetQuery\n\n__all__ = [\n    \"GPTTreeIndexLeafQuery\",\n    \"GPTTreeIndexRetQuery\",\n    \"GPTTreeIndexEmbeddingQuery\",\n]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "f269b72b009c4da94d70b83a9b6b9f03af0345da", "node_info": null}, "1": {"text": "\"\"\"Query Tree using embedding similarity between query and node text.\"\"\"\n\nimport logging\nfrom typing import Any, Dict, List, Optional, Tuple\n\nfrom gpt_index.data_structs.data_structs import IndexGraph, Node\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.tree.leaf_query import GPTTreeIndexLeafQuery\nfrom gpt_index.indices.utils import get_sorted_node_list\nfrom gpt_index.prompts.prompts import TreeSelectMultiplePrompt, TreeSelectPrompt\n\n\nclass GPTTreeIndexEmbeddingQuery(GPTTreeIndexLeafQuery):\n    \"\"\"\n    GPT Tree Index embedding query.\n\n    This class traverses the index graph using the embedding similarity between the\n    query and the node text.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"embedding\")\n\n    Args:\n        query_template (Optional[TreeSelectPrompt]): Tree Select Query Prompt\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "index": 1, "child_indices": [], "ref_doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "node_info": null}, "2": {"text": "(Optional[TreeSelectPrompt]): Tree Select Query Prompt\n            (see :ref:`Prompt-Templates`).\n        query_template_multiple (Optional[TreeSelectMultiplePrompt]): Tree Select\n            Query Prompt (Multiple)\n            (see :ref:`Prompt-Templates`).\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        child_branch_factor (int): Number of child nodes to consider at each level.\n            If child_branch_factor is 1, then the query will only choose one child node\n         ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "index": 2, "child_indices": [], "ref_doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "node_info": null}, "3": {"text": "only choose one child node\n            to traverse for any given parent node.\n            If child_branch_factor is 2, then the query will choose two child nodes.\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexGraph,\n        query_template: Optional[TreeSelectPrompt] = None,\n        query_template_multiple: Optional[TreeSelectMultiplePrompt] = None,\n        child_branch_factor: int = 1,\n        embed_model: Optional[BaseEmbedding] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "index": 3, "child_indices": [], "ref_doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "node_info": null}, "4": {"text": "  \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            index_struct,\n            query_template=query_template,\n            query_template_multiple=query_template_multiple,\n            child_branch_factor=child_branch_factor,\n            embed_model=embed_model,\n            **kwargs,\n        )\n        self.child_branch_factor = child_branch_factor\n\n    def _query_level(\n        self,\n        cur_nodes: Dict[int, Node],\n        query_str: str,\n        level: int = 0,\n    ) -> str:\n        cur_node_list =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "index": 4, "child_indices": [], "ref_doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "node_info": null}, "5": {"text": "str:\n        cur_node_list = get_sorted_node_list(cur_nodes)\n\n        # Get the node with the highest similarity to the query\n        selected_node, selected_index = self._get_most_similar_node(\n            cur_node_list, query_str\n        )\n        logging.debug(\n            f\">[Level {level}] Node [{selected_index+1}] Summary text: \"\n            f\"{' '.join(selected_node.get_text().splitlines())}\"\n        )\n\n        # Get the response for the selected node\n        response = self._query_with_selected_node(selected_node, query_str, level=level)\n\n        return response\n\n    def _get_query_text_embedding_similarities(\n        self,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "index": 5, "child_indices": [], "ref_doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "node_info": null}, "6": {"text": "       self, query_str: str, nodes: List[Node]\n    ) -> List[float]:\n        \"\"\"\n        Get query text embedding similarity.\n\n        Cache the query embedding and the node text embedding.\n\n        \"\"\"\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n        similarities = []\n        for node in nodes:\n            if node.embedding is not None:\n                text_embedding = node.embedding\n            else:\n                text_embedding = self._embed_model.get_text_embedding(node.get_text())\n                node.embedding = text_embedding\n\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "index": 6, "child_indices": [], "ref_doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "node_info": null}, "7": {"text": "text_embedding\n\n            similarity = self._embed_model.similarity(query_embedding, text_embedding)\n            similarities.append(similarity)\n        return similarities\n\n    def _get_most_similar_node(\n        self, nodes: List[Node], query_str: str\n    ) -> Tuple[Node, int]:\n        \"\"\"Get the node with the highest similarity to the query.\"\"\"\n        similarities = self._get_query_text_embedding_similarities(query_str, nodes)\n\n        selected_index = similarities.index(max(similarities))\n\n        selected_node = nodes[similarities.index(max(similarities))]\n        return selected_node, selected_index\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "index": 7, "child_indices": [], "ref_doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "node_info": null}, "8": {"text": "\"\"\"Leaf query mechanism.\"\"\"\n\nimport logging\nfrom typing import Any, Dict, Optional, cast\n\nfrom gpt_index.data_structs.data_structs import IndexGraph, Node\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.response.builder import ResponseBuilder\nfrom gpt_index.indices.utils import (\n    extract_numbers_given_response,\n    get_sorted_node_list,\n    truncate_text,\n)\nfrom gpt_index.prompts.default_prompts import (\n    DEFAULT_QUERY_PROMPT,\n    DEFAULT_QUERY_PROMPT_MULTIPLE,\n)\nfrom gpt_index.prompts.prompts import TreeSelectMultiplePrompt, TreeSelectPrompt\nfrom gpt_index.response.schema import Response\n\n\nclass GPTTreeIndexLeafQuery(BaseGPTIndexQuery[IndexGraph]):\n    \"\"\"GPT Tree Index leaf query.\n\n    This class traverses the index graph and searches for a leaf node that", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 8, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "9": {"text": "class traverses the index graph and searches for a leaf node that can best\n    answer the query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    Args:\n        query_template (Optional[TreeSelectPrompt]): Tree Select Query Prompt\n            (see :ref:`Prompt-Templates`).\n        query_template_multiple (Optional[TreeSelectMultiplePrompt]): Tree Select\n            Query Prompt (Multiple)\n            (see :ref:`Prompt-Templates`).\n        child_branch_factor (int): Number of child nodes to consider at each level.\n            If child_branch_factor is 1, then the query will only choose one child node\n            to traverse for any given parent node.\n            If", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 9, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "10": {"text": "parent node.\n            If child_branch_factor is 2, then the query will choose two child nodes.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexGraph,\n        query_template: Optional[TreeSelectPrompt] = None,\n        query_template_multiple: Optional[TreeSelectMultiplePrompt] = None,\n        child_branch_factor: int = 1,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(index_struct, **kwargs)\n        self.query_template = query_template or DEFAULT_QUERY_PROMPT\n        self.query_template_multiple = (\n            query_template_multiple or DEFAULT_QUERY_PROMPT_MULTIPLE\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 10, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "11": {"text": "       )\n        self.child_branch_factor = child_branch_factor\n\n    def _query_with_selected_node(\n        self,\n        selected_node: Node,\n        query_str: str,\n        prev_response: Optional[str] = None,\n        level: int = 0,\n    ) -> str:\n        \"\"\"Get response for selected node.\n\n        If not leaf node, it will recursively call _query on the child nodes.\n        If prev_response is provided, we will update prev_response with the answer.\n\n        \"\"\"\n        if len(selected_node.child_indices) == 0:\n            response_builder = ResponseBuilder(\n                self._prompt_helper,\n              ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 11, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "12": {"text": "               self._llm_predictor,\n                self.text_qa_template,\n                self.refine_template,\n            )\n            self.response_builder.add_node(selected_node)\n            # use response builder to get answer from node\n            node_text, _ = self._get_text_from_node(\n                query_str, selected_node, level=level\n            )\n            cur_response = response_builder.get_response_over_chunks(\n                query_str, [node_text], prev_response=prev_response\n            )\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 12, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "13": {"text": "       )\n            logging.debug(f\">[Level {level}] Current answer response: {cur_response} \")\n        else:\n            cur_response = self._query_level(\n                {\n                    i: self.index_struct.all_nodes[i]\n                    for i in selected_node.child_indices\n                },\n                query_str,\n                level=level + 1,\n            )\n\n        if prev_response is None:\n            return cur_response\n        else:\n         ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 13, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "14": {"text": "    else:\n            context_msg = \"\\n\".join([selected_node.get_text(), cur_response])\n            cur_response, formatted_refine_prompt = self._llm_predictor.predict(\n                self.refine_template,\n                query_str=query_str,\n                existing_answer=prev_response,\n                context_msg=context_msg,\n            )\n\n            logging.debug(f\">[Level {level}] Refine prompt: {formatted_refine_prompt}\")\n            logging.debug(f\">[Level {level}] Current refined response: {cur_response} \")\n            return cur_response\n\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 14, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "15": {"text": "         return cur_response\n\n    def _query_level(\n        self,\n        cur_nodes: Dict[int, Node],\n        query_str: str,\n        level: int = 0,\n    ) -> str:\n        \"\"\"Answer a query recursively.\"\"\"\n        cur_node_list = get_sorted_node_list(cur_nodes)\n\n        if self.child_branch_factor == 1:\n            query_template = self.query_template.partial_format(\n                num_chunks=len(cur_node_list), query_str=query_str\n            )\n            numbered_node_text = self._prompt_helper.get_numbered_text_from_nodes(\n              ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 15, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "16": {"text": "               cur_node_list, prompt=query_template\n            )\n            response, formatted_query_prompt = self._llm_predictor.predict(\n                query_template,\n                context_list=numbered_node_text,\n            )\n        else:\n            query_template_multiple = self.query_template_multiple.partial_format(\n                num_chunks=len(cur_node_list),\n                query_str=query_str,\n                branching_factor=self.child_branch_factor,\n            )\n          ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 16, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "17": {"text": "    )\n            numbered_node_text = self._prompt_helper.get_numbered_text_from_nodes(\n                cur_node_list, prompt=query_template_multiple\n            )\n            response, formatted_query_prompt = self._llm_predictor.predict(\n                query_template_multiple,\n                context_list=numbered_node_text,\n            )\n\n        logging.debug(\n            f\">[Level {level}] current prompt template: {formatted_query_prompt}\"\n        )\n\n        numbers = extract_numbers_given_response(response, n=self.child_branch_factor)\n        if numbers", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 17, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "18": {"text": "       if numbers is None:\n            logging.debug(\n                f\">[Level {level}] Could not retrieve response - no numbers present\"\n            )\n            # just join text from current nodes as response\n            return response\n        result_response = None\n        for number_str in numbers:\n            number = int(number_str)\n            if number > len(cur_node_list):\n                logging.debug(\n                    f\">[Level {level}] Invalid response: {response} - \"\n                    f\"number {number} out of range\"\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 18, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "19": {"text": "{number} out of range\"\n                )\n                return response\n\n            # number is 1-indexed, so subtract 1\n            selected_node = cur_node_list[number - 1]\n\n            logging.info(\n                f\">[Level {level}] Selected node: \"\n                f\"[{number}]/[{','.join([str(int(n)) for n in numbers])}]\"\n            )\n            debug_str = \" \".join(selected_node.get_text().splitlines())\n            logging.debug(\n                f\">[Level {level}] Node \"\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 19, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "20": {"text": "Node \"\n                f\"[{number}] Summary text: \"\n                f\"{ truncate_text(debug_str, 100) }\"\n            )\n            result_response = self._query_with_selected_node(\n                selected_node,\n                query_str,\n                prev_response=result_response,\n                level=level,\n            )\n        # result_response should not be None\n        return cast(str, result_response)\n\n    def _query(self, query_str: str) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        # NOTE: this overrides the _query method", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 20, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "21": {"text": "     # NOTE: this overrides the _query method in the base class\n        logging.info(f\"> Starting query: {query_str}\")\n        response_str = self._query_level(\n            self.index_struct.root_nodes,\n            query_str,\n            level=0,\n        ).strip()\n        return Response(response_str, source_nodes=self.response_builder.get_sources())\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 21, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "22": {"text": "\"\"\"Retrieve query.\"\"\"\nimport logging\nfrom typing import List, Optional\n\nfrom gpt_index.data_structs.data_structs import IndexGraph, Node\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.utils import get_sorted_node_list\n\n\nclass GPTTreeIndexRetQuery(BaseGPTIndexQuery[IndexGraph]):\n    \"\"\"GPT Tree Index retrieve query.\n\n    This class directly retrieves the answer from the root nodes.\n\n    Unlike GPTTreeIndexLeafQuery, this class assumes the graph already stores\n    the answer (because it was constructed with a query_str), so it does not\n    attempt to parse information down the graph in order to synthesize an answer.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"retrieve\")\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]):", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/retrieve_query.py", "file_name": "retrieve_query.py"}, "index": 22, "child_indices": [], "ref_doc_id": "71b67e07dcb1494c785428d1ecd9d9d6ca5f168a", "node_info": null}, "23": {"text": "  text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n\n    \"\"\"\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        logging.info(f\"> Starting query: {query_str}\")\n        node_list = get_sorted_node_list(self.index_struct.root_nodes)\n        text_qa_template = self.text_qa_template.partial_format(query_str=query_str)\n        node_text = self._prompt_helper.get_text_from_nodes(\n            node_list,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/retrieve_query.py", "file_name": "retrieve_query.py"}, "index": 23, "child_indices": [], "ref_doc_id": "71b67e07dcb1494c785428d1ecd9d9d6ca5f168a", "node_info": null}, "24": {"text": "           node_list, prompt=text_qa_template\n        )\n        return [Node(text=node_text)]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/retrieve_query.py", "file_name": "retrieve_query.py"}, "index": 24, "child_indices": [], "ref_doc_id": "71b67e07dcb1494c785428d1ecd9d9d6ca5f168a", "node_info": null}, "25": {"text": "\"\"\"Summarize query.\"\"\"\n\nimport logging\nfrom typing import Any, List, Optional, cast\n\nfrom gpt_index.data_structs.data_structs import IndexGraph, Node\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.response.builder import ResponseMode\nfrom gpt_index.indices.utils import get_sorted_node_list\n\n\nclass GPTTreeIndexSummarizeQuery(BaseGPTIndexQuery[IndexGraph]):\n    \"\"\"GPT Tree Index summarize query.\n\n    This class builds a query-specific tree from leaf nodes to return a response.\n    Using this query mode means that the tree index doesn't need to be built\n    when initialized, since we rebuild the tree for each query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"summarize\")\n\n    Args:\n        text_qa_template", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/summarize_query.py", "file_name": "summarize_query.py"}, "index": 25, "child_indices": [], "ref_doc_id": "46f8def5a3bd788e7448ee6c0ec10c1522f2a425", "node_info": null}, "26": {"text": " Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexGraph,\n        num_children: int = 10,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        if \"response_mode\" in kwargs:\n            raise ValueError(\n                \"response_mode should not be specified for summarize query\"\n            )\n        response_kwargs = kwargs.pop(\"response_kwargs\", {})\n        response_kwargs.update(num_children=num_children)\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/summarize_query.py", "file_name": "summarize_query.py"}, "index": 26, "child_indices": [], "ref_doc_id": "46f8def5a3bd788e7448ee6c0ec10c1522f2a425", "node_info": null}, "27": {"text": "       super().__init__(\n            index_struct,\n            response_mode=ResponseMode.TREE_SUMMARIZE,\n            response_kwargs=response_kwargs,\n            **kwargs,\n        )\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        logging.info(f\"> Starting query: {query_str}\")\n        index_struct = cast(IndexGraph, self._index_struct)\n        sorted_node_list = get_sorted_node_list(index_struct.all_nodes)\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/summarize_query.py", "file_name": "summarize_query.py"}, "index": 27, "child_indices": [], "ref_doc_id": "46f8def5a3bd788e7448ee6c0ec10c1522f2a425", "node_info": null}, "28": {"text": "       return sorted_node_list\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/summarize_query.py", "file_name": "summarize_query.py"}, "index": 28, "child_indices": [], "ref_doc_id": "46f8def5a3bd788e7448ee6c0ec10c1522f2a425", "node_info": null}, "29": {"text": "This code file contains classes for tree indices query. It includes GPTTreeIndexEmbeddingQuery, GPTTreeIndexLeafQuery, and GPTTreeIndexRetQuery. GPTTreeIndexEmbeddingQuery traverses the index graph using the embedding similarity between the query and the node text. It has parameters such as query_template, query_template_multiple, text_qa_template, refine_template, child_branch_factor, and embed_model. GPTTreeIndexLeafQuery traverses the index graph and searches for a leaf node that can best answer the query. It has parameters such as query_template, query_template_multiple, and child_branch_factor. GPTTreeIndexRetQuery retrieves the nodes from the index graph using the query string. It has parameters such as query_template, query_template_multiple, text_qa_template, refine_template, and child_branch_factor.", "doc_id": null, "embedding": null, "extra_info": null, "index": 29, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "30": {"text": "The file leaf_query.py is part of the GPT Tree Index Query module. It is used to answer queries recursively by traversing the graph structure of the index. It uses a query template to prompt the user for a response, and then uses the response to select a node from the graph. It then uses the response builder to get the answer from the node, and if necessary, uses the refine template to refine the answer. It also has a _query method which overrides the base class _query method and is used to answer the query.", "doc_id": null, "embedding": null, "extra_info": null, "index": 30, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "31": {"text": "retrieve_query.py and summarize_query.py are two files in the gpt_index/indices/query/tree directory. The retrieve_query.py file contains a function that takes a text-qa template as an argument and returns a Node object with the text. The summarize_query.py file contains a class that builds a query-specific tree from leaf nodes to return a response. It also has an init function that sets the response mode to tree summarize and sets the response kwargs. It also has a _get_nodes_for_response function that takes a query string and a similarity tracker as arguments and returns a sorted list of nodes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 31, "child_indices": [24, 25, 26, 27, 28], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"29": {"text": "This code file contains classes for tree indices query. It includes GPTTreeIndexEmbeddingQuery, GPTTreeIndexLeafQuery, and GPTTreeIndexRetQuery. GPTTreeIndexEmbeddingQuery traverses the index graph using the embedding similarity between the query and the node text. It has parameters such as query_template, query_template_multiple, text_qa_template, refine_template, child_branch_factor, and embed_model. GPTTreeIndexLeafQuery traverses the index graph and searches for a leaf node that can best answer the query. It has parameters such as query_template, query_template_multiple, and child_branch_factor. GPTTreeIndexRetQuery retrieves the nodes from the index graph using the query string. It has parameters such as query_template, query_template_multiple, text_qa_template, refine_template, and child_branch_factor.", "doc_id": null, "embedding": null, "extra_info": null, "index": 29, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "30": {"text": "The file leaf_query.py is part of the GPT Tree Index Query module. It is used to answer queries recursively by traversing the graph structure of the index. It uses a query template to prompt the user for a response, and then uses the response to select a node from the graph. It then uses the response builder to get the answer from the node, and if necessary, uses the refine template to refine the answer. It also has a _query method which overrides the base class _query method and is used to answer the query.", "doc_id": null, "embedding": null, "extra_info": null, "index": 30, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "31": {"text": "retrieve_query.py and summarize_query.py are two files in the gpt_index/indices/query/tree directory. The retrieve_query.py file contains a function that takes a text-qa template as an argument and returns a Node object with the text. The summarize_query.py file contains a class that builds a query-specific tree from leaf nodes to return a response. It also has an init function that sets the response mode to tree summarize and sets the response kwargs. It also has a _get_nodes_for_response function that takes a query string and a similarity tracker as arguments and returns a sorted list of nodes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 31, "child_indices": [24, 25, 26, 27, 28], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"f269b72b009c4da94d70b83a9b6b9f03af0345da": {"text": "\"\"\"Query classes for tree indices.\"\"\"\n\nfrom gpt_index.indices.query.tree.embedding_query import GPTTreeIndexEmbeddingQuery\nfrom gpt_index.indices.query.tree.leaf_query import GPTTreeIndexLeafQuery\nfrom gpt_index.indices.query.tree.retrieve_query import GPTTreeIndexRetQuery\n\n__all__ = [\n    \"GPTTreeIndexLeafQuery\",\n    \"GPTTreeIndexRetQuery\",\n    \"GPTTreeIndexEmbeddingQuery\",\n]\n", "doc_id": "f269b72b009c4da94d70b83a9b6b9f03af0345da", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332": {"text": "\"\"\"Query Tree using embedding similarity between query and node text.\"\"\"\n\nimport logging\nfrom typing import Any, Dict, List, Optional, Tuple\n\nfrom gpt_index.data_structs.data_structs import IndexGraph, Node\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.tree.leaf_query import GPTTreeIndexLeafQuery\nfrom gpt_index.indices.utils import get_sorted_node_list\nfrom gpt_index.prompts.prompts import TreeSelectMultiplePrompt, TreeSelectPrompt\n\n\nclass GPTTreeIndexEmbeddingQuery(GPTTreeIndexLeafQuery):\n    \"\"\"\n    GPT Tree Index embedding query.\n\n    This class traverses the index graph using the embedding similarity between the\n    query and the node text.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"embedding\")\n\n    Args:\n        query_template (Optional[TreeSelectPrompt]): Tree Select Query Prompt\n            (see :ref:`Prompt-Templates`).\n        query_template_multiple (Optional[TreeSelectMultiplePrompt]): Tree Select\n            Query Prompt (Multiple)\n            (see :ref:`Prompt-Templates`).\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        child_branch_factor (int): Number of child nodes to consider at each level.\n            If child_branch_factor is 1, then the query will only choose one child node\n            to traverse for any given parent node.\n            If child_branch_factor is 2, then the query will choose two child nodes.\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexGraph,\n        query_template: Optional[TreeSelectPrompt] = None,\n        query_template_multiple: Optional[TreeSelectMultiplePrompt] = None,\n        child_branch_factor: int = 1,\n        embed_model: Optional[BaseEmbedding] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            index_struct,\n            query_template=query_template,\n            query_template_multiple=query_template_multiple,\n            child_branch_factor=child_branch_factor,\n            embed_model=embed_model,\n            **kwargs,\n        )\n        self.child_branch_factor = child_branch_factor\n\n    def _query_level(\n        self,\n        cur_nodes: Dict[int, Node],\n        query_str: str,\n        level: int = 0,\n    ) -> str:\n        cur_node_list = get_sorted_node_list(cur_nodes)\n\n        # Get the node with the highest similarity to the query\n        selected_node, selected_index = self._get_most_similar_node(\n            cur_node_list, query_str\n        )\n        logging.debug(\n            f\">[Level {level}] Node [{selected_index+1}] Summary text: \"\n            f\"{' '.join(selected_node.get_text().splitlines())}\"\n        )\n\n        # Get the response for the selected node\n        response = self._query_with_selected_node(selected_node, query_str, level=level)\n\n        return response\n\n    def _get_query_text_embedding_similarities(\n        self, query_str: str, nodes: List[Node]\n    ) -> List[float]:\n        \"\"\"\n        Get query text embedding similarity.\n\n        Cache the query embedding and the node text embedding.\n\n        \"\"\"\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n        similarities = []\n        for node in nodes:\n            if node.embedding is not None:\n                text_embedding = node.embedding\n            else:\n                text_embedding = self._embed_model.get_text_embedding(node.get_text())\n                node.embedding = text_embedding\n\n            similarity = self._embed_model.similarity(query_embedding, text_embedding)\n            similarities.append(similarity)\n        return similarities\n\n    def _get_most_similar_node(\n        self, nodes: List[Node], query_str: str\n    ) -> Tuple[Node, int]:\n        \"\"\"Get the node with the highest similarity to the query.\"\"\"\n        similarities = self._get_query_text_embedding_similarities(query_str, nodes)\n\n        selected_index = similarities.index(max(similarities))\n\n        selected_node = nodes[similarities.index(max(similarities))]\n        return selected_node, selected_index\n", "doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "__type__": "Document"}, "6317248da78605b9ee03504c5343c039423e3e4b": {"text": "\"\"\"Leaf query mechanism.\"\"\"\n\nimport logging\nfrom typing import Any, Dict, Optional, cast\n\nfrom gpt_index.data_structs.data_structs import IndexGraph, Node\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.response.builder import ResponseBuilder\nfrom gpt_index.indices.utils import (\n    extract_numbers_given_response,\n    get_sorted_node_list,\n    truncate_text,\n)\nfrom gpt_index.prompts.default_prompts import (\n    DEFAULT_QUERY_PROMPT,\n    DEFAULT_QUERY_PROMPT_MULTIPLE,\n)\nfrom gpt_index.prompts.prompts import TreeSelectMultiplePrompt, TreeSelectPrompt\nfrom gpt_index.response.schema import Response\n\n\nclass GPTTreeIndexLeafQuery(BaseGPTIndexQuery[IndexGraph]):\n    \"\"\"GPT Tree Index leaf query.\n\n    This class traverses the index graph and searches for a leaf node that can best\n    answer the query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    Args:\n        query_template (Optional[TreeSelectPrompt]): Tree Select Query Prompt\n            (see :ref:`Prompt-Templates`).\n        query_template_multiple (Optional[TreeSelectMultiplePrompt]): Tree Select\n            Query Prompt (Multiple)\n            (see :ref:`Prompt-Templates`).\n        child_branch_factor (int): Number of child nodes to consider at each level.\n            If child_branch_factor is 1, then the query will only choose one child node\n            to traverse for any given parent node.\n            If child_branch_factor is 2, then the query will choose two child nodes.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexGraph,\n        query_template: Optional[TreeSelectPrompt] = None,\n        query_template_multiple: Optional[TreeSelectMultiplePrompt] = None,\n        child_branch_factor: int = 1,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(index_struct, **kwargs)\n        self.query_template = query_template or DEFAULT_QUERY_PROMPT\n        self.query_template_multiple = (\n            query_template_multiple or DEFAULT_QUERY_PROMPT_MULTIPLE\n        )\n        self.child_branch_factor = child_branch_factor\n\n    def _query_with_selected_node(\n        self,\n        selected_node: Node,\n        query_str: str,\n        prev_response: Optional[str] = None,\n        level: int = 0,\n    ) -> str:\n        \"\"\"Get response for selected node.\n\n        If not leaf node, it will recursively call _query on the child nodes.\n        If prev_response is provided, we will update prev_response with the answer.\n\n        \"\"\"\n        if len(selected_node.child_indices) == 0:\n            response_builder = ResponseBuilder(\n                self._prompt_helper,\n                self._llm_predictor,\n                self.text_qa_template,\n                self.refine_template,\n            )\n            self.response_builder.add_node(selected_node)\n            # use response builder to get answer from node\n            node_text, _ = self._get_text_from_node(\n                query_str, selected_node, level=level\n            )\n            cur_response = response_builder.get_response_over_chunks(\n                query_str, [node_text], prev_response=prev_response\n            )\n            logging.debug(f\">[Level {level}] Current answer response: {cur_response} \")\n        else:\n            cur_response = self._query_level(\n                {\n                    i: self.index_struct.all_nodes[i]\n                    for i in selected_node.child_indices\n                },\n                query_str,\n                level=level + 1,\n            )\n\n        if prev_response is None:\n            return cur_response\n        else:\n            context_msg = \"\\n\".join([selected_node.get_text(), cur_response])\n            cur_response, formatted_refine_prompt = self._llm_predictor.predict(\n                self.refine_template,\n                query_str=query_str,\n                existing_answer=prev_response,\n                context_msg=context_msg,\n            )\n\n            logging.debug(f\">[Level {level}] Refine prompt: {formatted_refine_prompt}\")\n            logging.debug(f\">[Level {level}] Current refined response: {cur_response} \")\n            return cur_response\n\n    def _query_level(\n        self,\n        cur_nodes: Dict[int, Node],\n        query_str: str,\n        level: int = 0,\n    ) -> str:\n        \"\"\"Answer a query recursively.\"\"\"\n        cur_node_list = get_sorted_node_list(cur_nodes)\n\n        if self.child_branch_factor == 1:\n            query_template = self.query_template.partial_format(\n                num_chunks=len(cur_node_list), query_str=query_str\n            )\n            numbered_node_text = self._prompt_helper.get_numbered_text_from_nodes(\n                cur_node_list, prompt=query_template\n            )\n            response, formatted_query_prompt = self._llm_predictor.predict(\n                query_template,\n                context_list=numbered_node_text,\n            )\n        else:\n            query_template_multiple = self.query_template_multiple.partial_format(\n                num_chunks=len(cur_node_list),\n                query_str=query_str,\n                branching_factor=self.child_branch_factor,\n            )\n            numbered_node_text = self._prompt_helper.get_numbered_text_from_nodes(\n                cur_node_list, prompt=query_template_multiple\n            )\n            response, formatted_query_prompt = self._llm_predictor.predict(\n                query_template_multiple,\n                context_list=numbered_node_text,\n            )\n\n        logging.debug(\n            f\">[Level {level}] current prompt template: {formatted_query_prompt}\"\n        )\n\n        numbers = extract_numbers_given_response(response, n=self.child_branch_factor)\n        if numbers is None:\n            logging.debug(\n                f\">[Level {level}] Could not retrieve response - no numbers present\"\n            )\n            # just join text from current nodes as response\n            return response\n        result_response = None\n        for number_str in numbers:\n            number = int(number_str)\n            if number > len(cur_node_list):\n                logging.debug(\n                    f\">[Level {level}] Invalid response: {response} - \"\n                    f\"number {number} out of range\"\n                )\n                return response\n\n            # number is 1-indexed, so subtract 1\n            selected_node = cur_node_list[number - 1]\n\n            logging.info(\n                f\">[Level {level}] Selected node: \"\n                f\"[{number}]/[{','.join([str(int(n)) for n in numbers])}]\"\n            )\n            debug_str = \" \".join(selected_node.get_text().splitlines())\n            logging.debug(\n                f\">[Level {level}] Node \"\n                f\"[{number}] Summary text: \"\n                f\"{ truncate_text(debug_str, 100) }\"\n            )\n            result_response = self._query_with_selected_node(\n                selected_node,\n                query_str,\n                prev_response=result_response,\n                level=level,\n            )\n        # result_response should not be None\n        return cast(str, result_response)\n\n    def _query(self, query_str: str) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        # NOTE: this overrides the _query method in the base class\n        logging.info(f\"> Starting query: {query_str}\")\n        response_str = self._query_level(\n            self.index_struct.root_nodes,\n            query_str,\n            level=0,\n        ).strip()\n        return Response(response_str, source_nodes=self.response_builder.get_sources())\n", "doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "__type__": "Document"}, "71b67e07dcb1494c785428d1ecd9d9d6ca5f168a": {"text": "\"\"\"Retrieve query.\"\"\"\nimport logging\nfrom typing import List, Optional\n\nfrom gpt_index.data_structs.data_structs import IndexGraph, Node\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.utils import get_sorted_node_list\n\n\nclass GPTTreeIndexRetQuery(BaseGPTIndexQuery[IndexGraph]):\n    \"\"\"GPT Tree Index retrieve query.\n\n    This class directly retrieves the answer from the root nodes.\n\n    Unlike GPTTreeIndexLeafQuery, this class assumes the graph already stores\n    the answer (because it was constructed with a query_str), so it does not\n    attempt to parse information down the graph in order to synthesize an answer.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"retrieve\")\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n\n    \"\"\"\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        logging.info(f\"> Starting query: {query_str}\")\n        node_list = get_sorted_node_list(self.index_struct.root_nodes)\n        text_qa_template = self.text_qa_template.partial_format(query_str=query_str)\n        node_text = self._prompt_helper.get_text_from_nodes(\n            node_list, prompt=text_qa_template\n        )\n        return [Node(text=node_text)]\n", "doc_id": "71b67e07dcb1494c785428d1ecd9d9d6ca5f168a", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/retrieve_query.py", "file_name": "retrieve_query.py"}, "__type__": "Document"}, "46f8def5a3bd788e7448ee6c0ec10c1522f2a425": {"text": "\"\"\"Summarize query.\"\"\"\n\nimport logging\nfrom typing import Any, List, Optional, cast\n\nfrom gpt_index.data_structs.data_structs import IndexGraph, Node\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.response.builder import ResponseMode\nfrom gpt_index.indices.utils import get_sorted_node_list\n\n\nclass GPTTreeIndexSummarizeQuery(BaseGPTIndexQuery[IndexGraph]):\n    \"\"\"GPT Tree Index summarize query.\n\n    This class builds a query-specific tree from leaf nodes to return a response.\n    Using this query mode means that the tree index doesn't need to be built\n    when initialized, since we rebuild the tree for each query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"summarize\")\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexGraph,\n        num_children: int = 10,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        if \"response_mode\" in kwargs:\n            raise ValueError(\n                \"response_mode should not be specified for summarize query\"\n            )\n        response_kwargs = kwargs.pop(\"response_kwargs\", {})\n        response_kwargs.update(num_children=num_children)\n        super().__init__(\n            index_struct,\n            response_mode=ResponseMode.TREE_SUMMARIZE,\n            response_kwargs=response_kwargs,\n            **kwargs,\n        )\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        logging.info(f\"> Starting query: {query_str}\")\n        index_struct = cast(IndexGraph, self._index_struct)\n        sorted_node_list = get_sorted_node_list(index_struct.all_nodes)\n        return sorted_node_list\n", "doc_id": "46f8def5a3bd788e7448ee6c0ec10c1522f2a425", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/summarize_query.py", "file_name": "summarize_query.py"}, "__type__": "Document"}, "2e1307a1-d305-4097-aadc-95572e9e99e5": {"text": "\n__init__.py: Query classes for tree indices.\nembedding_query.py: Query Tree using embedding similarity between query and node text.\nleaf_query.py: Leaf query mechanism.", "doc_id": "2e1307a1-d305-4097-aadc-95572e9e99e5", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Query classes for tree indices.\"\"\"\n\nfrom gpt_index.indices.query.tree.embedding_query import GPTTreeIndexEmbeddingQuery\nfrom gpt_index.indices.query.tree.leaf_query import GPTTreeIndexLeafQuery\nfrom gpt_index.indices.query.tree.retrieve_query import GPTTreeIndexRetQuery\n\n__all__ = [\n    \"GPTTreeIndexLeafQuery\",\n    \"GPTTreeIndexRetQuery\",\n    \"GPTTreeIndexEmbeddingQuery\",\n]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "f269b72b009c4da94d70b83a9b6b9f03af0345da", "node_info": null}, "1": {"text": "\"\"\"Query Tree using embedding similarity between query and node text.\"\"\"\n\nimport logging\nfrom typing import Any, Dict, List, Optional, Tuple\n\nfrom gpt_index.data_structs.data_structs import IndexGraph, Node\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.tree.leaf_query import GPTTreeIndexLeafQuery\nfrom gpt_index.indices.utils import get_sorted_node_list\nfrom gpt_index.prompts.prompts import TreeSelectMultiplePrompt, TreeSelectPrompt\n\n\nclass GPTTreeIndexEmbeddingQuery(GPTTreeIndexLeafQuery):\n    \"\"\"\n    GPT Tree Index embedding query.\n\n    This class traverses the index graph using the embedding similarity between the\n    query and the node text.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"embedding\")\n\n    Args:\n        query_template (Optional[TreeSelectPrompt]): Tree Select Query Prompt\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "index": 1, "child_indices": [], "ref_doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "node_info": null}, "2": {"text": "(Optional[TreeSelectPrompt]): Tree Select Query Prompt\n            (see :ref:`Prompt-Templates`).\n        query_template_multiple (Optional[TreeSelectMultiplePrompt]): Tree Select\n            Query Prompt (Multiple)\n            (see :ref:`Prompt-Templates`).\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        child_branch_factor (int): Number of child nodes to consider at each level.\n            If child_branch_factor is 1, then the query will only choose one child node\n         ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "index": 2, "child_indices": [], "ref_doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "node_info": null}, "3": {"text": "only choose one child node\n            to traverse for any given parent node.\n            If child_branch_factor is 2, then the query will choose two child nodes.\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexGraph,\n        query_template: Optional[TreeSelectPrompt] = None,\n        query_template_multiple: Optional[TreeSelectMultiplePrompt] = None,\n        child_branch_factor: int = 1,\n        embed_model: Optional[BaseEmbedding] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "index": 3, "child_indices": [], "ref_doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "node_info": null}, "4": {"text": "  \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            index_struct,\n            query_template=query_template,\n            query_template_multiple=query_template_multiple,\n            child_branch_factor=child_branch_factor,\n            embed_model=embed_model,\n            **kwargs,\n        )\n        self.child_branch_factor = child_branch_factor\n\n    def _query_level(\n        self,\n        cur_nodes: Dict[int, Node],\n        query_str: str,\n        level: int = 0,\n    ) -> str:\n        cur_node_list =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "index": 4, "child_indices": [], "ref_doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "node_info": null}, "5": {"text": "str:\n        cur_node_list = get_sorted_node_list(cur_nodes)\n\n        # Get the node with the highest similarity to the query\n        selected_node, selected_index = self._get_most_similar_node(\n            cur_node_list, query_str\n        )\n        logging.debug(\n            f\">[Level {level}] Node [{selected_index+1}] Summary text: \"\n            f\"{' '.join(selected_node.get_text().splitlines())}\"\n        )\n\n        # Get the response for the selected node\n        response = self._query_with_selected_node(selected_node, query_str, level=level)\n\n        return response\n\n    def _get_query_text_embedding_similarities(\n        self,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "index": 5, "child_indices": [], "ref_doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "node_info": null}, "6": {"text": "       self, query_str: str, nodes: List[Node]\n    ) -> List[float]:\n        \"\"\"\n        Get query text embedding similarity.\n\n        Cache the query embedding and the node text embedding.\n\n        \"\"\"\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n        similarities = []\n        for node in nodes:\n            if node.embedding is not None:\n                text_embedding = node.embedding\n            else:\n                text_embedding = self._embed_model.get_text_embedding(node.get_text())\n                node.embedding = text_embedding\n\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "index": 6, "child_indices": [], "ref_doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "node_info": null}, "7": {"text": "text_embedding\n\n            similarity = self._embed_model.similarity(query_embedding, text_embedding)\n            similarities.append(similarity)\n        return similarities\n\n    def _get_most_similar_node(\n        self, nodes: List[Node], query_str: str\n    ) -> Tuple[Node, int]:\n        \"\"\"Get the node with the highest similarity to the query.\"\"\"\n        similarities = self._get_query_text_embedding_similarities(query_str, nodes)\n\n        selected_index = similarities.index(max(similarities))\n\n        selected_node = nodes[similarities.index(max(similarities))]\n        return selected_node, selected_index\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "index": 7, "child_indices": [], "ref_doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "node_info": null}, "8": {"text": "\"\"\"Leaf query mechanism.\"\"\"\n\nimport logging\nfrom typing import Any, Dict, Optional, cast\n\nfrom gpt_index.data_structs.data_structs import IndexGraph, Node\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.response.builder import ResponseBuilder\nfrom gpt_index.indices.utils import (\n    extract_numbers_given_response,\n    get_sorted_node_list,\n    truncate_text,\n)\nfrom gpt_index.prompts.default_prompts import (\n    DEFAULT_QUERY_PROMPT,\n    DEFAULT_QUERY_PROMPT_MULTIPLE,\n)\nfrom gpt_index.prompts.prompts import TreeSelectMultiplePrompt, TreeSelectPrompt\nfrom gpt_index.response.schema import Response\n\n\nclass GPTTreeIndexLeafQuery(BaseGPTIndexQuery[IndexGraph]):\n    \"\"\"GPT Tree Index leaf query.\n\n    This class traverses the index graph and searches for a leaf node that", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 8, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "9": {"text": "class traverses the index graph and searches for a leaf node that can best\n    answer the query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    Args:\n        query_template (Optional[TreeSelectPrompt]): Tree Select Query Prompt\n            (see :ref:`Prompt-Templates`).\n        query_template_multiple (Optional[TreeSelectMultiplePrompt]): Tree Select\n            Query Prompt (Multiple)\n            (see :ref:`Prompt-Templates`).\n        child_branch_factor (int): Number of child nodes to consider at each level.\n            If child_branch_factor is 1, then the query will only choose one child node\n            to traverse for any given parent node.\n            If", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 9, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "10": {"text": "parent node.\n            If child_branch_factor is 2, then the query will choose two child nodes.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexGraph,\n        query_template: Optional[TreeSelectPrompt] = None,\n        query_template_multiple: Optional[TreeSelectMultiplePrompt] = None,\n        child_branch_factor: int = 1,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(index_struct, **kwargs)\n        self.query_template = query_template or DEFAULT_QUERY_PROMPT\n        self.query_template_multiple = (\n            query_template_multiple or DEFAULT_QUERY_PROMPT_MULTIPLE\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 10, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "11": {"text": "       )\n        self.child_branch_factor = child_branch_factor\n\n    def _query_with_selected_node(\n        self,\n        selected_node: Node,\n        query_str: str,\n        prev_response: Optional[str] = None,\n        level: int = 0,\n    ) -> str:\n        \"\"\"Get response for selected node.\n\n        If not leaf node, it will recursively call _query on the child nodes.\n        If prev_response is provided, we will update prev_response with the answer.\n\n        \"\"\"\n        if len(selected_node.child_indices) == 0:\n            response_builder = ResponseBuilder(\n                self._prompt_helper,\n              ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 11, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "12": {"text": "               self._llm_predictor,\n                self.text_qa_template,\n                self.refine_template,\n            )\n            self.response_builder.add_node(selected_node)\n            # use response builder to get answer from node\n            node_text, _ = self._get_text_from_node(\n                query_str, selected_node, level=level\n            )\n            cur_response = response_builder.get_response_over_chunks(\n                query_str, [node_text], prev_response=prev_response\n            )\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 12, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "13": {"text": "       )\n            logging.debug(f\">[Level {level}] Current answer response: {cur_response} \")\n        else:\n            cur_response = self._query_level(\n                {\n                    i: self.index_struct.all_nodes[i]\n                    for i in selected_node.child_indices\n                },\n                query_str,\n                level=level + 1,\n            )\n\n        if prev_response is None:\n            return cur_response\n        else:\n         ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 13, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "14": {"text": "    else:\n            context_msg = \"\\n\".join([selected_node.get_text(), cur_response])\n            cur_response, formatted_refine_prompt = self._llm_predictor.predict(\n                self.refine_template,\n                query_str=query_str,\n                existing_answer=prev_response,\n                context_msg=context_msg,\n            )\n\n            logging.debug(f\">[Level {level}] Refine prompt: {formatted_refine_prompt}\")\n            logging.debug(f\">[Level {level}] Current refined response: {cur_response} \")\n            return cur_response\n\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 14, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "15": {"text": "         return cur_response\n\n    def _query_level(\n        self,\n        cur_nodes: Dict[int, Node],\n        query_str: str,\n        level: int = 0,\n    ) -> str:\n        \"\"\"Answer a query recursively.\"\"\"\n        cur_node_list = get_sorted_node_list(cur_nodes)\n\n        if self.child_branch_factor == 1:\n            query_template = self.query_template.partial_format(\n                num_chunks=len(cur_node_list), query_str=query_str\n            )\n            numbered_node_text = self._prompt_helper.get_numbered_text_from_nodes(\n              ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 15, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "16": {"text": "               cur_node_list, prompt=query_template\n            )\n            response, formatted_query_prompt = self._llm_predictor.predict(\n                query_template,\n                context_list=numbered_node_text,\n            )\n        else:\n            query_template_multiple = self.query_template_multiple.partial_format(\n                num_chunks=len(cur_node_list),\n                query_str=query_str,\n                branching_factor=self.child_branch_factor,\n            )\n          ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 16, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "17": {"text": "    )\n            numbered_node_text = self._prompt_helper.get_numbered_text_from_nodes(\n                cur_node_list, prompt=query_template_multiple\n            )\n            response, formatted_query_prompt = self._llm_predictor.predict(\n                query_template_multiple,\n                context_list=numbered_node_text,\n            )\n\n        logging.debug(\n            f\">[Level {level}] current prompt template: {formatted_query_prompt}\"\n        )\n\n        numbers = extract_numbers_given_response(response, n=self.child_branch_factor)\n        if numbers", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 17, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "18": {"text": "       if numbers is None:\n            logging.debug(\n                f\">[Level {level}] Could not retrieve response - no numbers present\"\n            )\n            # just join text from current nodes as response\n            return response\n        result_response = None\n        for number_str in numbers:\n            number = int(number_str)\n            if number > len(cur_node_list):\n                logging.debug(\n                    f\">[Level {level}] Invalid response: {response} - \"\n                    f\"number {number} out of range\"\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 18, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "19": {"text": "{number} out of range\"\n                )\n                return response\n\n            # number is 1-indexed, so subtract 1\n            selected_node = cur_node_list[number - 1]\n\n            logging.info(\n                f\">[Level {level}] Selected node: \"\n                f\"[{number}]/[{','.join([str(int(n)) for n in numbers])}]\"\n            )\n            debug_str = \" \".join(selected_node.get_text().splitlines())\n            logging.debug(\n                f\">[Level {level}] Node \"\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 19, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "20": {"text": "Node \"\n                f\"[{number}] Summary text: \"\n                f\"{ truncate_text(debug_str, 100) }\"\n            )\n            result_response = self._query_with_selected_node(\n                selected_node,\n                query_str,\n                prev_response=result_response,\n                level=level,\n            )\n        # result_response should not be None\n        return cast(str, result_response)\n\n    def _query(self, query_str: str) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        # NOTE: this overrides the _query method", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 20, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "21": {"text": "     # NOTE: this overrides the _query method in the base class\n        logging.info(f\"> Starting query: {query_str}\")\n        response_str = self._query_level(\n            self.index_struct.root_nodes,\n            query_str,\n            level=0,\n        ).strip()\n        return Response(response_str, source_nodes=self.response_builder.get_sources())\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "index": 21, "child_indices": [], "ref_doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "node_info": null}, "22": {"text": "\"\"\"Retrieve query.\"\"\"\nimport logging\nfrom typing import List, Optional\n\nfrom gpt_index.data_structs.data_structs import IndexGraph, Node\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.utils import get_sorted_node_list\n\n\nclass GPTTreeIndexRetQuery(BaseGPTIndexQuery[IndexGraph]):\n    \"\"\"GPT Tree Index retrieve query.\n\n    This class directly retrieves the answer from the root nodes.\n\n    Unlike GPTTreeIndexLeafQuery, this class assumes the graph already stores\n    the answer (because it was constructed with a query_str), so it does not\n    attempt to parse information down the graph in order to synthesize an answer.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"retrieve\")\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]):", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/retrieve_query.py", "file_name": "retrieve_query.py"}, "index": 22, "child_indices": [], "ref_doc_id": "71b67e07dcb1494c785428d1ecd9d9d6ca5f168a", "node_info": null}, "23": {"text": "  text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n\n    \"\"\"\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        logging.info(f\"> Starting query: {query_str}\")\n        node_list = get_sorted_node_list(self.index_struct.root_nodes)\n        text_qa_template = self.text_qa_template.partial_format(query_str=query_str)\n        node_text = self._prompt_helper.get_text_from_nodes(\n            node_list,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/retrieve_query.py", "file_name": "retrieve_query.py"}, "index": 23, "child_indices": [], "ref_doc_id": "71b67e07dcb1494c785428d1ecd9d9d6ca5f168a", "node_info": null}, "24": {"text": "           node_list, prompt=text_qa_template\n        )\n        return [Node(text=node_text)]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/retrieve_query.py", "file_name": "retrieve_query.py"}, "index": 24, "child_indices": [], "ref_doc_id": "71b67e07dcb1494c785428d1ecd9d9d6ca5f168a", "node_info": null}, "25": {"text": "\"\"\"Summarize query.\"\"\"\n\nimport logging\nfrom typing import Any, List, Optional, cast\n\nfrom gpt_index.data_structs.data_structs import IndexGraph, Node\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.response.builder import ResponseMode\nfrom gpt_index.indices.utils import get_sorted_node_list\n\n\nclass GPTTreeIndexSummarizeQuery(BaseGPTIndexQuery[IndexGraph]):\n    \"\"\"GPT Tree Index summarize query.\n\n    This class builds a query-specific tree from leaf nodes to return a response.\n    Using this query mode means that the tree index doesn't need to be built\n    when initialized, since we rebuild the tree for each query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"summarize\")\n\n    Args:\n        text_qa_template", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/summarize_query.py", "file_name": "summarize_query.py"}, "index": 25, "child_indices": [], "ref_doc_id": "46f8def5a3bd788e7448ee6c0ec10c1522f2a425", "node_info": null}, "26": {"text": " Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexGraph,\n        num_children: int = 10,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        if \"response_mode\" in kwargs:\n            raise ValueError(\n                \"response_mode should not be specified for summarize query\"\n            )\n        response_kwargs = kwargs.pop(\"response_kwargs\", {})\n        response_kwargs.update(num_children=num_children)\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/summarize_query.py", "file_name": "summarize_query.py"}, "index": 26, "child_indices": [], "ref_doc_id": "46f8def5a3bd788e7448ee6c0ec10c1522f2a425", "node_info": null}, "27": {"text": "       super().__init__(\n            index_struct,\n            response_mode=ResponseMode.TREE_SUMMARIZE,\n            response_kwargs=response_kwargs,\n            **kwargs,\n        )\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        logging.info(f\"> Starting query: {query_str}\")\n        index_struct = cast(IndexGraph, self._index_struct)\n        sorted_node_list = get_sorted_node_list(index_struct.all_nodes)\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/summarize_query.py", "file_name": "summarize_query.py"}, "index": 27, "child_indices": [], "ref_doc_id": "46f8def5a3bd788e7448ee6c0ec10c1522f2a425", "node_info": null}, "28": {"text": "       return sorted_node_list\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/summarize_query.py", "file_name": "summarize_query.py"}, "index": 28, "child_indices": [], "ref_doc_id": "46f8def5a3bd788e7448ee6c0ec10c1522f2a425", "node_info": null}, "29": {"text": "This code file contains classes for tree indices query. It includes GPTTreeIndexEmbeddingQuery, GPTTreeIndexLeafQuery, and GPTTreeIndexRetQuery. GPTTreeIndexEmbeddingQuery traverses the index graph using the embedding similarity between the query and the node text. It has parameters such as query_template, query_template_multiple, text_qa_template, refine_template, child_branch_factor, and embed_model. GPTTreeIndexLeafQuery traverses the index graph and searches for a leaf node that can best answer the query. It has parameters such as query_template, query_template_multiple, and child_branch_factor. GPTTreeIndexRetQuery retrieves the nodes from the index graph using the query string. It has parameters such as query_template, query_template_multiple, text_qa_template, refine_template, and child_branch_factor.", "doc_id": null, "embedding": null, "extra_info": null, "index": 29, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "30": {"text": "The file leaf_query.py is part of the GPT Tree Index Query module. It is used to answer queries recursively by traversing the graph structure of the index. It uses a query template to prompt the user for a response, and then uses the response to select a node from the graph. It then uses the response builder to get the answer from the node, and if necessary, uses the refine template to refine the answer. It also has a _query method which overrides the base class _query method and is used to answer the query.", "doc_id": null, "embedding": null, "extra_info": null, "index": 30, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "31": {"text": "retrieve_query.py and summarize_query.py are two files in the gpt_index/indices/query/tree directory. The retrieve_query.py file contains a function that takes a text-qa template as an argument and returns a Node object with the text. The summarize_query.py file contains a class that builds a query-specific tree from leaf nodes to return a response. It also has an init function that sets the response mode to tree summarize and sets the response kwargs. It also has a _get_nodes_for_response function that takes a query string and a similarity tracker as arguments and returns a sorted list of nodes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 31, "child_indices": [24, 25, 26, 27, 28], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"29": {"text": "This code file contains classes for tree indices query. It includes GPTTreeIndexEmbeddingQuery, GPTTreeIndexLeafQuery, and GPTTreeIndexRetQuery. GPTTreeIndexEmbeddingQuery traverses the index graph using the embedding similarity between the query and the node text. It has parameters such as query_template, query_template_multiple, text_qa_template, refine_template, child_branch_factor, and embed_model. GPTTreeIndexLeafQuery traverses the index graph and searches for a leaf node that can best answer the query. It has parameters such as query_template, query_template_multiple, and child_branch_factor. GPTTreeIndexRetQuery retrieves the nodes from the index graph using the query string. It has parameters such as query_template, query_template_multiple, text_qa_template, refine_template, and child_branch_factor.", "doc_id": null, "embedding": null, "extra_info": null, "index": 29, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "30": {"text": "The file leaf_query.py is part of the GPT Tree Index Query module. It is used to answer queries recursively by traversing the graph structure of the index. It uses a query template to prompt the user for a response, and then uses the response to select a node from the graph. It then uses the response builder to get the answer from the node, and if necessary, uses the refine template to refine the answer. It also has a _query method which overrides the base class _query method and is used to answer the query.", "doc_id": null, "embedding": null, "extra_info": null, "index": 30, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "31": {"text": "retrieve_query.py and summarize_query.py are two files in the gpt_index/indices/query/tree directory. The retrieve_query.py file contains a function that takes a text-qa template as an argument and returns a Node object with the text. The summarize_query.py file contains a class that builds a query-specific tree from leaf nodes to return a response. It also has an init function that sets the response mode to tree summarize and sets the response kwargs. It also has a _get_nodes_for_response function that takes a query string and a similarity tracker as arguments and returns a sorted list of nodes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 31, "child_indices": [24, 25, 26, 27, 28], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}