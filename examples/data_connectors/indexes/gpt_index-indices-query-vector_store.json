{"index_struct": {"text": "\nThe __init__.py file contains query classes for vector store indices. The base.py file contains a base vector store index query. The faiss.py file contains a default query for GPTFaissIndex, which queries an underlying Faiss index to retrieve top-k nodes by embedding similarity to the query. The pinecone.py file contains a Pinecone vector store index query, which queries an underlying Pinecone index to retrieve top-k nodes by embedding similarity to the query.\n\nsimple.py contains a class GPTSimpleVectorIndexQuery which initializes parameters and a method _get_nodes_for_response which queries the index and returns a list of nodes.\n\nweaviate.py contains a class GPTWeaviateIndexQuery which initializes parameters and a method _get_nodes_for_response which queries the index and returns a list of nodes. Additionally, the file contains logging functions to provide debugging information.", "doc_id": "e403ecd5-54f4-498c-828b-7fdbe3c79b6d", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Query classes for vector store indices.\"\"\"\n\nfrom gpt_index.indices.query.vector_store.faiss import GPTFaissIndexQuery\nfrom gpt_index.indices.query.vector_store.simple import GPTSimpleVectorIndexQuery\nfrom gpt_index.indices.query.vector_store.weaviate import GPTWeaviateIndexQuery\n\n__all__ = [\"GPTFaissIndexQuery\", \"GPTSimpleVectorIndexQuery\", \"GPTWeaviateIndexQuery\"]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "1dd47d1956669dbe44b595ae46e20924311f7be6", "node_info": null}, "1": {"text": "\"\"\"Base vector store index query.\"\"\"\n\n\nfrom typing import Any, Generic, Optional, TypeVar\n\nfrom gpt_index.data_structs.data_structs import IndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\n\nBID = TypeVar(\"BID\", bound=IndexStruct)\n\n\nclass BaseGPTVectorStoreIndexQuery(BaseGPTIndexQuery[BID], Generic[BID]):\n    \"\"\"Base vector store query.\"\"\"\n\n    def __init__(\n        self,\n        index_struct: BID,\n        embed_model: Optional[BaseEmbedding] = None,\n        similarity_top_k: Optional[int] = 1,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(index_struct=index_struct, embed_model=embed_model, **kwargs)\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/base.py", "file_name": "base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "c99a92ca905368d8cf79d8d42bfe270fa9e76733", "node_info": null}, "2": {"text": "embed_model=embed_model, **kwargs)\n        self.similarity_top_k = similarity_top_k\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/base.py", "file_name": "base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "c99a92ca905368d8cf79d8d42bfe270fa9e76733", "node_info": null}, "3": {"text": "\"\"\"Default query for GPTFaissIndex.\"\"\"\n\nimport logging\nfrom typing import Any, List, Optional, cast\n\nimport numpy as np\n\nfrom gpt_index.data_structs.data_structs import IndexDict, Node\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.query.vector_store.base import BaseGPTVectorStoreIndexQuery\nfrom gpt_index.indices.utils import truncate_text\n\n\nclass GPTFaissIndexQuery(BaseGPTVectorStoreIndexQuery[IndexDict]):\n    \"\"\"GPTFaissIndex query.\n\n    An embedding-based query for GPTFaissIndex, which queries\n    an underlying Faiss index to retrieve top-k nodes by\n    embedding similarity to the query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    Args:\n        text_qa_template", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 3, "child_indices": [], "ref_doc_id": "1ea526708a54837eb2d2594ce803150877deeec3", "node_info": null}, "4": {"text": " Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        faiss_index (faiss.Index): A Faiss Index object (required)\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        similarity_top_k (int): Number of similar nodes to retrieve.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexDict,\n        faiss_index: Optional[Any] = None,\n        embed_model: Optional[BaseEmbedding] =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 4, "child_indices": [], "ref_doc_id": "1ea526708a54837eb2d2594ce803150877deeec3", "node_info": null}, "5": {"text": "    embed_model: Optional[BaseEmbedding] = None,\n        similarity_top_k: Optional[int] = 1,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            index_struct=index_struct,\n            embed_model=embed_model,\n            similarity_top_k=similarity_top_k,\n            **kwargs,\n        )\n        if faiss_index is None:\n            raise ValueError(\"faiss_index cannot be None.\")\n        # NOTE: cast to Any for now\n        self._faiss_index = cast(Any, faiss_index)\n        self._faiss_index =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 5, "child_indices": [], "ref_doc_id": "1ea526708a54837eb2d2594ce803150877deeec3", "node_info": null}, "6": {"text": "       self._faiss_index = faiss_index\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n        query_embedding_np = np.array(query_embedding, dtype=\"float32\")[np.newaxis, :]\n        dists, indices = self._faiss_index.search(\n            query_embedding_np, self.similarity_top_k\n        )\n        dists = [d[0] for d in dists]\n        # if empty, then return an empty response\n        if", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 6, "child_indices": [], "ref_doc_id": "1ea526708a54837eb2d2594ce803150877deeec3", "node_info": null}, "7": {"text": "empty, then return an empty response\n        if len(indices) == 0:\n            return []\n\n        # returned dimension is 1 x k\n        node_idxs = list([str(i) for i in indices[0]])\n        top_k_nodes = self._index_struct.get_nodes(node_idxs)\n\n        if similarity_tracker is not None:\n            for node, similarity in zip(top_k_nodes, dists):\n                similarity_tracker.add(node, similarity)\n\n        if logging.getLogger(__name__).getEffectiveLevel() == logging.DEBUG:\n            fmt_txts = []\n            for node_idx, node_similarity, node in zip(node_idxs, dists,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 7, "child_indices": [], "ref_doc_id": "1ea526708a54837eb2d2594ce803150877deeec3", "node_info": null}, "8": {"text": "node in zip(node_idxs, dists, top_k_nodes):\n                fmt_txt = f\"> [Node {node_idx}] [Similarity score: \\\n                    {float(node_similarity):.6}] {truncate_text(node.get_text(), 100)}\"\n                fmt_txts.append(fmt_txt)\n            top_k_node_text = \"\\n\".join(fmt_txts)\n            logging.debug(f\"> Top {len(top_k_nodes)} nodes:\\n{top_k_node_text}\")\n\n        return top_k_nodes\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 8, "child_indices": [], "ref_doc_id": "1ea526708a54837eb2d2594ce803150877deeec3", "node_info": null}, "9": {"text": "\"\"\"Pinecone vector store index query.\"\"\"\nimport logging\nfrom typing import Any, Dict, List, Optional, cast\n\nfrom gpt_index.data_structs.data_structs import IndexDict, Node\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.query.vector_store.base import BaseGPTVectorStoreIndexQuery\nfrom gpt_index.indices.utils import truncate_text\n\n\nclass GPTPineconeIndexQuery(BaseGPTVectorStoreIndexQuery[IndexDict]):\n    \"\"\"GPTPineconeIndex query.\n\n    An embedding-based query for GPTPineconeIndex, which queries\n    an undelrying Pinecone index to retrieve top-k nodes by\n    embedding similarity to the query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]):", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 9, "child_indices": [], "ref_doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "node_info": null}, "10": {"text": "  text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        pinecone_index (pinecone.Index): A Pinecone Index object (required)\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        similarity_top_k (int): Number of similar nodes to retrieve.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexDict,\n        pinecone_index: Optional[Any] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 10, "child_indices": [], "ref_doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "node_info": null}, "11": {"text": "Optional[BaseEmbedding] = None,\n        similarity_top_k: Optional[int] = 1,\n        pinecone_kwargs: Optional[Dict] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            index_struct=index_struct,\n            embed_model=embed_model,\n            similarity_top_k=similarity_top_k,\n            **kwargs,\n        )\n        if pinecone_index is None:\n            raise ValueError(\"pinecone_index cannot be None.\")\n        # NOTE: cast to Any for now\n        self._pinecone_index = cast(Any, pinecone_index)\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 11, "child_indices": [], "ref_doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "node_info": null}, "12": {"text": "= cast(Any, pinecone_index)\n        self._pinecone_index = pinecone_index\n\n        self._pinecone_kwargs = pinecone_kwargs or {}\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n\n        response = self._pinecone_index.query(\n            query_embedding,\n            top_k=self.similarity_top_k,\n            include_values=True,\n            include_metadata=True,\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 12, "child_indices": [], "ref_doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "node_info": null}, "13": {"text": "           **self._pinecone_kwargs,\n        )\n\n        top_k_nodes = []\n        top_k_ids = []\n        top_k_scores = []\n        for match in response.matches:\n            text = match.metadata[\"text\"]\n            node = Node(text=text, extra_info=match.metadata)\n            top_k_ids.append(match.id)\n            top_k_nodes.append(node)\n            top_k_scores.append(match.score)\n            if similarity_tracker is not None:\n                similarity_tracker.add(node, match.score)\n\n        if", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 13, "child_indices": [], "ref_doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "node_info": null}, "14": {"text": "match.score)\n\n        if logging.getLogger(__name__).getEffectiveLevel() == logging.DEBUG:\n            fmt_txts = []\n            for node_idx, node_similarity, node in zip(\n                top_k_ids, top_k_scores, top_k_nodes\n            ):\n                fmt_txt = f\"> [Node {node_idx}] [Similarity score: \\\n                    {node_similarity:.6}] {truncate_text(node.get_text(), 100)}\"\n                fmt_txts.append(fmt_txt)\n            top_k_node_text = \"\\n\".join(fmt_txts)\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 14, "child_indices": [], "ref_doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "node_info": null}, "15": {"text": "           logging.debug(f\"> Top {len(top_k_nodes)} nodes:\\n{top_k_node_text}\")\n\n        return top_k_nodes\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 15, "child_indices": [], "ref_doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "node_info": null}, "16": {"text": "\"\"\"Qdrant vector store index query.\"\"\"\nimport logging\nfrom typing import Any, List, Optional, cast\n\nfrom gpt_index.data_structs import Node, QdrantIndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.query.vector_store.base import BaseGPTVectorStoreIndexQuery\nfrom gpt_index.indices.utils import truncate_text\n\n\nclass GPTQdrantIndexQuery(BaseGPTVectorStoreIndexQuery[QdrantIndexStruct]):\n    \"\"\"GPTQdrantIndex query.\n\n    An embedding-based query for GPTQdrantIndex, which queries\n    an undelrying Qdrant index to retrieve top-k nodes by\n    embedding similarity to the query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    Args:\n        text_qa_template", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 16, "child_indices": [], "ref_doc_id": "666d66f5854bdfaa296829a9de353eddb6f46e69", "node_info": null}, "17": {"text": " Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        similarity_top_k (int): Number of similar nodes to retrieve.\n        client (Optional[Any]): QdrantClient instance from `qdrant-client` package\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: QdrantIndexStruct,\n        embed_model: Optional[BaseEmbedding] = None,\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 17, "child_indices": [], "ref_doc_id": "666d66f5854bdfaa296829a9de353eddb6f46e69", "node_info": null}, "18": {"text": "= None,\n        similarity_top_k: int = 1,\n        client: Optional[Any] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            index_struct=index_struct,\n            embed_model=embed_model,\n            similarity_top_k=similarity_top_k,\n            **kwargs,\n        )\n\n        import_err_msg = (\n            \"`qdrant-client` package not found, please run `pip install qdrant-client`\"\n        )\n        try:\n            import qdrant_client  # noqa: F401\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 18, "child_indices": [], "ref_doc_id": "666d66f5854bdfaa296829a9de353eddb6f46e69", "node_info": null}, "19": {"text": "import qdrant_client  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n\n        if client is None:\n            raise ValueError(\"client cannot be None.\")\n\n        self._client = cast(qdrant_client.QdrantClient, client)\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        from qdrant_client.http.models.models import Payload\n\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n\n        response = self._client.search(\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 19, "child_indices": [], "ref_doc_id": "666d66f5854bdfaa296829a9de353eddb6f46e69", "node_info": null}, "20": {"text": "           collection_name=self.index_struct.get_collection_name(),\n            query_vector=query_embedding,\n            limit=cast(int, self.similarity_top_k),\n        )\n\n        logging.debug(f\"> Top {len(response)} nodes:\")\n\n        nodes = []\n        for point in response:\n            payload = cast(Payload, point.payload)\n            node = Node(\n                doc_id=payload.get(\"doc_id\"),\n                text=payload.get(\"text\"),\n            )\n            nodes.append(node)\n\n            if similarity_tracker", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 20, "child_indices": [], "ref_doc_id": "666d66f5854bdfaa296829a9de353eddb6f46e69", "node_info": null}, "21": {"text": "          if similarity_tracker is not None:\n                similarity_tracker.add(node, point.score)\n\n            logging.debug(\n                f\"> [Node {point.id}] [Similarity score: {point.score:.6}] \"\n                f\"{truncate_text(str(payload.get('text')), 100)}\"\n            )\n\n        return nodes\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 21, "child_indices": [], "ref_doc_id": "666d66f5854bdfaa296829a9de353eddb6f46e69", "node_info": null}, "22": {"text": "\"\"\"Default query for GPTSimpleVectorIndex.\"\"\"\nimport logging\nfrom typing import List, Optional\n\nfrom gpt_index.data_structs.data_structs import Node, SimpleIndexDict\nfrom gpt_index.indices.query.embedding_utils import (\n    SimilarityTracker,\n    get_top_k_embeddings,\n)\nfrom gpt_index.indices.query.vector_store.base import BaseGPTVectorStoreIndexQuery\nfrom gpt_index.indices.utils import truncate_text\n\n\nclass GPTSimpleVectorIndexQuery(BaseGPTVectorStoreIndexQuery[SimpleIndexDict]):\n    \"\"\"GPTSimpleVectorIndex query.\n\n    An embedding-based query for GPTSimpleVectorIndex, which queries\n    an underlying dict-based embedding store to retrieve top-k nodes by\n    embedding similarity to the query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    Args:\n        text_qa_template", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/simple.py", "file_name": "simple.py"}, "index": 22, "child_indices": [], "ref_doc_id": "33f9551e7c7f50285582555c35fb754a11a4c61c", "node_info": null}, "23": {"text": " Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        similarity_top_k (int): Number of similar nodes to retrieve.\n\n    \"\"\"\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        # TODO:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/simple.py", "file_name": "simple.py"}, "index": 23, "child_indices": [], "ref_doc_id": "33f9551e7c7f50285582555c35fb754a11a4c61c", "node_info": null}, "24": {"text": "response.\"\"\"\n        # TODO: consolidate with get_query_text_embedding_similarities\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n        items = self._index_struct.embedding_dict.items()\n        node_ids = [t[0] for t in items]\n        embeddings = [t[1] for t in items]\n\n        top_similarities, top_ids = get_top_k_embeddings(\n            self._embed_model,\n            query_embedding,\n            embeddings,\n            similarity_top_k=self.similarity_top_k,\n            embedding_ids=node_ids,\n        )\n        top_k_nodes =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/simple.py", "file_name": "simple.py"}, "index": 24, "child_indices": [], "ref_doc_id": "33f9551e7c7f50285582555c35fb754a11a4c61c", "node_info": null}, "25": {"text": ")\n        top_k_nodes = self._index_struct.get_nodes(top_ids)\n        if similarity_tracker is not None:\n            for node, similarity in zip(top_k_nodes, top_similarities):\n                similarity_tracker.add(node, similarity)\n\n        if logging.getLogger(__name__).getEffectiveLevel() == logging.DEBUG:\n            fmt_txts = []\n            for node_idx, node_similarity, node in zip(\n                top_ids, top_similarities, top_k_nodes\n            ):\n                fmt_txt = f\"> [Node {node_idx}] [Similarity score: \\\n              ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/simple.py", "file_name": "simple.py"}, "index": 25, "child_indices": [], "ref_doc_id": "33f9551e7c7f50285582555c35fb754a11a4c61c", "node_info": null}, "26": {"text": "\\\n                    {node_similarity:.6}] {truncate_text(node.get_text(), 100)}\"\n                fmt_txts.append(fmt_txt)\n            top_k_node_text = \"\\n\".join(fmt_txts)\n            logging.debug(f\"> Top {len(top_k_nodes)} nodes:\\n{top_k_node_text}\")\n\n        return top_k_nodes\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/simple.py", "file_name": "simple.py"}, "index": 26, "child_indices": [], "ref_doc_id": "33f9551e7c7f50285582555c35fb754a11a4c61c", "node_info": null}, "27": {"text": "\"\"\"Weaviate vector store index query.\"\"\"\n\n\nimport logging\nfrom typing import Any, List, Optional, cast\n\nfrom gpt_index.data_structs.data_structs import Node, WeaviateIndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.utils import truncate_text\nfrom gpt_index.readers.weaviate.data_structs import WeaviateNode\n\n\nclass GPTWeaviateIndexQuery(BaseGPTIndexQuery[WeaviateIndexStruct]):\n    \"\"\"Base vector store query.\"\"\"\n\n    def __init__(\n        self,\n        index_struct: WeaviateIndexStruct,\n        embed_model: Optional[BaseEmbedding] = None,\n        similarity_top_k: Optional[int] = 1,\n        weaviate_client:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 27, "child_indices": [], "ref_doc_id": "c74c84559cd6f32007980215425c5cdaeb0843b8", "node_info": null}, "28": {"text": "1,\n        weaviate_client: Optional[Any] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(index_struct=index_struct, embed_model=embed_model, **kwargs)\n        self.similarity_top_k = similarity_top_k\n        import_err_msg = (\n            \"`weaviate` package not found, please run `pip install weaviate-client`\"\n        )\n        try:\n            import weaviate  # noqa: F401\n            from weaviate import Client  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n        self.client =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 28, "child_indices": [], "ref_doc_id": "c74c84559cd6f32007980215425c5cdaeb0843b8", "node_info": null}, "29": {"text": "       self.client = cast(Client, weaviate_client)\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n        nodes = WeaviateNode.to_gpt_index_list(\n            self.client,\n            self._index_struct.get_class_prefix(),\n            vector=query_embedding,\n            object_limit=self.similarity_top_k,\n        )\n        nodes = nodes[:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 29, "child_indices": [], "ref_doc_id": "c74c84559cd6f32007980215425c5cdaeb0843b8", "node_info": null}, "30": {"text": ")\n        nodes = nodes[: self.similarity_top_k]\n        node_idxs = [str(i) for i in range(len(nodes))]\n\n        if logging.getLogger(__name__).getEffectiveLevel() == logging.DEBUG:\n            fmt_txts = []\n            for node_idx, node in zip(node_idxs, nodes):\n                fmt_txt = f\"> [Node {node_idx}] {truncate_text(node.get_text(), 100)}\"\n                fmt_txts.append(fmt_txt)\n            top_k_node_text = \"\\n\".join(fmt_txts)\n            logging.debug(f\"> Top {len(nodes)}", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 30, "child_indices": [], "ref_doc_id": "c74c84559cd6f32007980215425c5cdaeb0843b8", "node_info": null}, "31": {"text": "  logging.debug(f\"> Top {len(nodes)} nodes:\\n{top_k_node_text}\")\n        return nodes\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 31, "child_indices": [], "ref_doc_id": "c74c84559cd6f32007980215425c5cdaeb0843b8", "node_info": null}, "32": {"text": "This code file contains classes for vector store indices. It includes the base class BaseGPTVectorStoreIndexQuery, which initializes parameters for the query, and two subclasses, GPTFaissIndexQuery and GPTPineconeIndexQuery, which query an underlying Faiss or Pinecone index to retrieve top-k nodes by embedding similarity to the query. The classes contain methods for retrieving nodes for the response, and logging debug information.", "doc_id": null, "embedding": null, "extra_info": null, "index": 32, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "33": {"text": "The file pinecone.py is a part of the GPT-Index package, which is used to query an underlying Qdrant index to retrieve top-k nodes by embedding similarity to the query. It contains a class GPTQdrantIndexQuery which initializes parameters and a method _get_nodes_for_response which queries the index and returns a list of nodes. The file qdrant.py is a part of the GPT-Index package, which is used to query an underlying dict-based embedding store to retrieve top-k nodes by embedding similarity to the query. It contains a class GPTSimpleVectorIndexQuery which initializes parameters and a method _get_nodes_for_response which queries the index and returns a list of nodes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 33, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "34": {"text": "The file simple.py and weaviate.py are part of the GPT Index Query Vector Store. The purpose of these files is to provide a way to query a vector store index and return the top k nodes that are most similar to the query string. The files contain functions to embed the query string, calculate the similarity between the query string and the nodes in the index, and return the top k nodes. Additionally, the files contain logging functions to provide debugging information.", "doc_id": null, "embedding": null, "extra_info": null, "index": 34, "child_indices": [24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"32": {"text": "This code file contains classes for vector store indices. It includes the base class BaseGPTVectorStoreIndexQuery, which initializes parameters for the query, and two subclasses, GPTFaissIndexQuery and GPTPineconeIndexQuery, which query an underlying Faiss or Pinecone index to retrieve top-k nodes by embedding similarity to the query. The classes contain methods for retrieving nodes for the response, and logging debug information.", "doc_id": null, "embedding": null, "extra_info": null, "index": 32, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "33": {"text": "The file pinecone.py is a part of the GPT-Index package, which is used to query an underlying Qdrant index to retrieve top-k nodes by embedding similarity to the query. It contains a class GPTQdrantIndexQuery which initializes parameters and a method _get_nodes_for_response which queries the index and returns a list of nodes. The file qdrant.py is a part of the GPT-Index package, which is used to query an underlying dict-based embedding store to retrieve top-k nodes by embedding similarity to the query. It contains a class GPTSimpleVectorIndexQuery which initializes parameters and a method _get_nodes_for_response which queries the index and returns a list of nodes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 33, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "34": {"text": "The file simple.py and weaviate.py are part of the GPT Index Query Vector Store. The purpose of these files is to provide a way to query a vector store index and return the top k nodes that are most similar to the query string. The files contain functions to embed the query string, calculate the similarity between the query string and the nodes in the index, and return the top k nodes. Additionally, the files contain logging functions to provide debugging information.", "doc_id": null, "embedding": null, "extra_info": null, "index": 34, "child_indices": [24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"1dd47d1956669dbe44b595ae46e20924311f7be6": {"text": "\"\"\"Query classes for vector store indices.\"\"\"\n\nfrom gpt_index.indices.query.vector_store.faiss import GPTFaissIndexQuery\nfrom gpt_index.indices.query.vector_store.simple import GPTSimpleVectorIndexQuery\nfrom gpt_index.indices.query.vector_store.weaviate import GPTWeaviateIndexQuery\n\n__all__ = [\"GPTFaissIndexQuery\", \"GPTSimpleVectorIndexQuery\", \"GPTWeaviateIndexQuery\"]\n", "doc_id": "1dd47d1956669dbe44b595ae46e20924311f7be6", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "c99a92ca905368d8cf79d8d42bfe270fa9e76733": {"text": "\"\"\"Base vector store index query.\"\"\"\n\n\nfrom typing import Any, Generic, Optional, TypeVar\n\nfrom gpt_index.data_structs.data_structs import IndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\n\nBID = TypeVar(\"BID\", bound=IndexStruct)\n\n\nclass BaseGPTVectorStoreIndexQuery(BaseGPTIndexQuery[BID], Generic[BID]):\n    \"\"\"Base vector store query.\"\"\"\n\n    def __init__(\n        self,\n        index_struct: BID,\n        embed_model: Optional[BaseEmbedding] = None,\n        similarity_top_k: Optional[int] = 1,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(index_struct=index_struct, embed_model=embed_model, **kwargs)\n        self.similarity_top_k = similarity_top_k\n", "doc_id": "c99a92ca905368d8cf79d8d42bfe270fa9e76733", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/base.py", "file_name": "base.py"}, "__type__": "Document"}, "1ea526708a54837eb2d2594ce803150877deeec3": {"text": "\"\"\"Default query for GPTFaissIndex.\"\"\"\n\nimport logging\nfrom typing import Any, List, Optional, cast\n\nimport numpy as np\n\nfrom gpt_index.data_structs.data_structs import IndexDict, Node\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.query.vector_store.base import BaseGPTVectorStoreIndexQuery\nfrom gpt_index.indices.utils import truncate_text\n\n\nclass GPTFaissIndexQuery(BaseGPTVectorStoreIndexQuery[IndexDict]):\n    \"\"\"GPTFaissIndex query.\n\n    An embedding-based query for GPTFaissIndex, which queries\n    an underlying Faiss index to retrieve top-k nodes by\n    embedding similarity to the query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        faiss_index (faiss.Index): A Faiss Index object (required)\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        similarity_top_k (int): Number of similar nodes to retrieve.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexDict,\n        faiss_index: Optional[Any] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        similarity_top_k: Optional[int] = 1,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            index_struct=index_struct,\n            embed_model=embed_model,\n            similarity_top_k=similarity_top_k,\n            **kwargs,\n        )\n        if faiss_index is None:\n            raise ValueError(\"faiss_index cannot be None.\")\n        # NOTE: cast to Any for now\n        self._faiss_index = cast(Any, faiss_index)\n        self._faiss_index = faiss_index\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n        query_embedding_np = np.array(query_embedding, dtype=\"float32\")[np.newaxis, :]\n        dists, indices = self._faiss_index.search(\n            query_embedding_np, self.similarity_top_k\n        )\n        dists = [d[0] for d in dists]\n        # if empty, then return an empty response\n        if len(indices) == 0:\n            return []\n\n        # returned dimension is 1 x k\n        node_idxs = list([str(i) for i in indices[0]])\n        top_k_nodes = self._index_struct.get_nodes(node_idxs)\n\n        if similarity_tracker is not None:\n            for node, similarity in zip(top_k_nodes, dists):\n                similarity_tracker.add(node, similarity)\n\n        if logging.getLogger(__name__).getEffectiveLevel() == logging.DEBUG:\n            fmt_txts = []\n            for node_idx, node_similarity, node in zip(node_idxs, dists, top_k_nodes):\n                fmt_txt = f\"> [Node {node_idx}] [Similarity score: \\\n                    {float(node_similarity):.6}] {truncate_text(node.get_text(), 100)}\"\n                fmt_txts.append(fmt_txt)\n            top_k_node_text = \"\\n\".join(fmt_txts)\n            logging.debug(f\"> Top {len(top_k_nodes)} nodes:\\n{top_k_node_text}\")\n\n        return top_k_nodes\n", "doc_id": "1ea526708a54837eb2d2594ce803150877deeec3", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/faiss.py", "file_name": "faiss.py"}, "__type__": "Document"}, "0260639a9de8f896a7e32e927fa8f257c442953b": {"text": "\"\"\"Pinecone vector store index query.\"\"\"\nimport logging\nfrom typing import Any, Dict, List, Optional, cast\n\nfrom gpt_index.data_structs.data_structs import IndexDict, Node\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.query.vector_store.base import BaseGPTVectorStoreIndexQuery\nfrom gpt_index.indices.utils import truncate_text\n\n\nclass GPTPineconeIndexQuery(BaseGPTVectorStoreIndexQuery[IndexDict]):\n    \"\"\"GPTPineconeIndex query.\n\n    An embedding-based query for GPTPineconeIndex, which queries\n    an undelrying Pinecone index to retrieve top-k nodes by\n    embedding similarity to the query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        pinecone_index (pinecone.Index): A Pinecone Index object (required)\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        similarity_top_k (int): Number of similar nodes to retrieve.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexDict,\n        pinecone_index: Optional[Any] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        similarity_top_k: Optional[int] = 1,\n        pinecone_kwargs: Optional[Dict] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            index_struct=index_struct,\n            embed_model=embed_model,\n            similarity_top_k=similarity_top_k,\n            **kwargs,\n        )\n        if pinecone_index is None:\n            raise ValueError(\"pinecone_index cannot be None.\")\n        # NOTE: cast to Any for now\n        self._pinecone_index = cast(Any, pinecone_index)\n        self._pinecone_index = pinecone_index\n\n        self._pinecone_kwargs = pinecone_kwargs or {}\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n\n        response = self._pinecone_index.query(\n            query_embedding,\n            top_k=self.similarity_top_k,\n            include_values=True,\n            include_metadata=True,\n            **self._pinecone_kwargs,\n        )\n\n        top_k_nodes = []\n        top_k_ids = []\n        top_k_scores = []\n        for match in response.matches:\n            text = match.metadata[\"text\"]\n            node = Node(text=text, extra_info=match.metadata)\n            top_k_ids.append(match.id)\n            top_k_nodes.append(node)\n            top_k_scores.append(match.score)\n            if similarity_tracker is not None:\n                similarity_tracker.add(node, match.score)\n\n        if logging.getLogger(__name__).getEffectiveLevel() == logging.DEBUG:\n            fmt_txts = []\n            for node_idx, node_similarity, node in zip(\n                top_k_ids, top_k_scores, top_k_nodes\n            ):\n                fmt_txt = f\"> [Node {node_idx}] [Similarity score: \\\n                    {node_similarity:.6}] {truncate_text(node.get_text(), 100)}\"\n                fmt_txts.append(fmt_txt)\n            top_k_node_text = \"\\n\".join(fmt_txts)\n            logging.debug(f\"> Top {len(top_k_nodes)} nodes:\\n{top_k_node_text}\")\n\n        return top_k_nodes\n", "doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "__type__": "Document"}, "666d66f5854bdfaa296829a9de353eddb6f46e69": {"text": "\"\"\"Qdrant vector store index query.\"\"\"\nimport logging\nfrom typing import Any, List, Optional, cast\n\nfrom gpt_index.data_structs import Node, QdrantIndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.query.vector_store.base import BaseGPTVectorStoreIndexQuery\nfrom gpt_index.indices.utils import truncate_text\n\n\nclass GPTQdrantIndexQuery(BaseGPTVectorStoreIndexQuery[QdrantIndexStruct]):\n    \"\"\"GPTQdrantIndex query.\n\n    An embedding-based query for GPTQdrantIndex, which queries\n    an undelrying Qdrant index to retrieve top-k nodes by\n    embedding similarity to the query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        similarity_top_k (int): Number of similar nodes to retrieve.\n        client (Optional[Any]): QdrantClient instance from `qdrant-client` package\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: QdrantIndexStruct,\n        embed_model: Optional[BaseEmbedding] = None,\n        similarity_top_k: int = 1,\n        client: Optional[Any] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            index_struct=index_struct,\n            embed_model=embed_model,\n            similarity_top_k=similarity_top_k,\n            **kwargs,\n        )\n\n        import_err_msg = (\n            \"`qdrant-client` package not found, please run `pip install qdrant-client`\"\n        )\n        try:\n            import qdrant_client  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n\n        if client is None:\n            raise ValueError(\"client cannot be None.\")\n\n        self._client = cast(qdrant_client.QdrantClient, client)\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        from qdrant_client.http.models.models import Payload\n\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n\n        response = self._client.search(\n            collection_name=self.index_struct.get_collection_name(),\n            query_vector=query_embedding,\n            limit=cast(int, self.similarity_top_k),\n        )\n\n        logging.debug(f\"> Top {len(response)} nodes:\")\n\n        nodes = []\n        for point in response:\n            payload = cast(Payload, point.payload)\n            node = Node(\n                doc_id=payload.get(\"doc_id\"),\n                text=payload.get(\"text\"),\n            )\n            nodes.append(node)\n\n            if similarity_tracker is not None:\n                similarity_tracker.add(node, point.score)\n\n            logging.debug(\n                f\"> [Node {point.id}] [Similarity score: {point.score:.6}] \"\n                f\"{truncate_text(str(payload.get('text')), 100)}\"\n            )\n\n        return nodes\n", "doc_id": "666d66f5854bdfaa296829a9de353eddb6f46e69", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/qdrant.py", "file_name": "qdrant.py"}, "__type__": "Document"}, "33f9551e7c7f50285582555c35fb754a11a4c61c": {"text": "\"\"\"Default query for GPTSimpleVectorIndex.\"\"\"\nimport logging\nfrom typing import List, Optional\n\nfrom gpt_index.data_structs.data_structs import Node, SimpleIndexDict\nfrom gpt_index.indices.query.embedding_utils import (\n    SimilarityTracker,\n    get_top_k_embeddings,\n)\nfrom gpt_index.indices.query.vector_store.base import BaseGPTVectorStoreIndexQuery\nfrom gpt_index.indices.utils import truncate_text\n\n\nclass GPTSimpleVectorIndexQuery(BaseGPTVectorStoreIndexQuery[SimpleIndexDict]):\n    \"\"\"GPTSimpleVectorIndex query.\n\n    An embedding-based query for GPTSimpleVectorIndex, which queries\n    an underlying dict-based embedding store to retrieve top-k nodes by\n    embedding similarity to the query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        similarity_top_k (int): Number of similar nodes to retrieve.\n\n    \"\"\"\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        # TODO: consolidate with get_query_text_embedding_similarities\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n        items = self._index_struct.embedding_dict.items()\n        node_ids = [t[0] for t in items]\n        embeddings = [t[1] for t in items]\n\n        top_similarities, top_ids = get_top_k_embeddings(\n            self._embed_model,\n            query_embedding,\n            embeddings,\n            similarity_top_k=self.similarity_top_k,\n            embedding_ids=node_ids,\n        )\n        top_k_nodes = self._index_struct.get_nodes(top_ids)\n        if similarity_tracker is not None:\n            for node, similarity in zip(top_k_nodes, top_similarities):\n                similarity_tracker.add(node, similarity)\n\n        if logging.getLogger(__name__).getEffectiveLevel() == logging.DEBUG:\n            fmt_txts = []\n            for node_idx, node_similarity, node in zip(\n                top_ids, top_similarities, top_k_nodes\n            ):\n                fmt_txt = f\"> [Node {node_idx}] [Similarity score: \\\n                    {node_similarity:.6}] {truncate_text(node.get_text(), 100)}\"\n                fmt_txts.append(fmt_txt)\n            top_k_node_text = \"\\n\".join(fmt_txts)\n            logging.debug(f\"> Top {len(top_k_nodes)} nodes:\\n{top_k_node_text}\")\n\n        return top_k_nodes\n", "doc_id": "33f9551e7c7f50285582555c35fb754a11a4c61c", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/simple.py", "file_name": "simple.py"}, "__type__": "Document"}, "c74c84559cd6f32007980215425c5cdaeb0843b8": {"text": "\"\"\"Weaviate vector store index query.\"\"\"\n\n\nimport logging\nfrom typing import Any, List, Optional, cast\n\nfrom gpt_index.data_structs.data_structs import Node, WeaviateIndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.utils import truncate_text\nfrom gpt_index.readers.weaviate.data_structs import WeaviateNode\n\n\nclass GPTWeaviateIndexQuery(BaseGPTIndexQuery[WeaviateIndexStruct]):\n    \"\"\"Base vector store query.\"\"\"\n\n    def __init__(\n        self,\n        index_struct: WeaviateIndexStruct,\n        embed_model: Optional[BaseEmbedding] = None,\n        similarity_top_k: Optional[int] = 1,\n        weaviate_client: Optional[Any] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(index_struct=index_struct, embed_model=embed_model, **kwargs)\n        self.similarity_top_k = similarity_top_k\n        import_err_msg = (\n            \"`weaviate` package not found, please run `pip install weaviate-client`\"\n        )\n        try:\n            import weaviate  # noqa: F401\n            from weaviate import Client  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n        self.client = cast(Client, weaviate_client)\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n        nodes = WeaviateNode.to_gpt_index_list(\n            self.client,\n            self._index_struct.get_class_prefix(),\n            vector=query_embedding,\n            object_limit=self.similarity_top_k,\n        )\n        nodes = nodes[: self.similarity_top_k]\n        node_idxs = [str(i) for i in range(len(nodes))]\n\n        if logging.getLogger(__name__).getEffectiveLevel() == logging.DEBUG:\n            fmt_txts = []\n            for node_idx, node in zip(node_idxs, nodes):\n                fmt_txt = f\"> [Node {node_idx}] {truncate_text(node.get_text(), 100)}\"\n                fmt_txts.append(fmt_txt)\n            top_k_node_text = \"\\n\".join(fmt_txts)\n            logging.debug(f\"> Top {len(nodes)} nodes:\\n{top_k_node_text}\")\n        return nodes\n", "doc_id": "c74c84559cd6f32007980215425c5cdaeb0843b8", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/weaviate.py", "file_name": "weaviate.py"}, "__type__": "Document"}, "e403ecd5-54f4-498c-828b-7fdbe3c79b6d": {"text": "\nThe __init__.py file contains query classes for vector store indices. The base.py file contains a base vector store index query. The faiss.py file contains a default query for GPTFaissIndex, which queries an underlying Faiss index to retrieve top-k nodes by embedding similarity to the query. The pinecone.py file contains a Pinecone vector store index query, which queries an underlying Pinecone index to retrieve top-k nodes by embedding similarity to the query.\n\nsimple.py contains a class GPTSimpleVectorIndexQuery which initializes parameters and a method _get_nodes_for_response which queries the index and returns a list of nodes.\n\nweaviate.py contains a class GPTWeaviateIndexQuery which initializes parameters and a method _get_nodes_for_response which queries the index and returns a list of nodes. Additionally, the file contains logging functions to provide debugging information.", "doc_id": "e403ecd5-54f4-498c-828b-7fdbe3c79b6d", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Query classes for vector store indices.\"\"\"\n\nfrom gpt_index.indices.query.vector_store.faiss import GPTFaissIndexQuery\nfrom gpt_index.indices.query.vector_store.simple import GPTSimpleVectorIndexQuery\nfrom gpt_index.indices.query.vector_store.weaviate import GPTWeaviateIndexQuery\n\n__all__ = [\"GPTFaissIndexQuery\", \"GPTSimpleVectorIndexQuery\", \"GPTWeaviateIndexQuery\"]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "1dd47d1956669dbe44b595ae46e20924311f7be6", "node_info": null}, "1": {"text": "\"\"\"Base vector store index query.\"\"\"\n\n\nfrom typing import Any, Generic, Optional, TypeVar\n\nfrom gpt_index.data_structs.data_structs import IndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\n\nBID = TypeVar(\"BID\", bound=IndexStruct)\n\n\nclass BaseGPTVectorStoreIndexQuery(BaseGPTIndexQuery[BID], Generic[BID]):\n    \"\"\"Base vector store query.\"\"\"\n\n    def __init__(\n        self,\n        index_struct: BID,\n        embed_model: Optional[BaseEmbedding] = None,\n        similarity_top_k: Optional[int] = 1,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(index_struct=index_struct, embed_model=embed_model, **kwargs)\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/base.py", "file_name": "base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "c99a92ca905368d8cf79d8d42bfe270fa9e76733", "node_info": null}, "2": {"text": "embed_model=embed_model, **kwargs)\n        self.similarity_top_k = similarity_top_k\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/base.py", "file_name": "base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "c99a92ca905368d8cf79d8d42bfe270fa9e76733", "node_info": null}, "3": {"text": "\"\"\"Default query for GPTFaissIndex.\"\"\"\n\nimport logging\nfrom typing import Any, List, Optional, cast\n\nimport numpy as np\n\nfrom gpt_index.data_structs.data_structs import IndexDict, Node\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.query.vector_store.base import BaseGPTVectorStoreIndexQuery\nfrom gpt_index.indices.utils import truncate_text\n\n\nclass GPTFaissIndexQuery(BaseGPTVectorStoreIndexQuery[IndexDict]):\n    \"\"\"GPTFaissIndex query.\n\n    An embedding-based query for GPTFaissIndex, which queries\n    an underlying Faiss index to retrieve top-k nodes by\n    embedding similarity to the query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    Args:\n        text_qa_template", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 3, "child_indices": [], "ref_doc_id": "1ea526708a54837eb2d2594ce803150877deeec3", "node_info": null}, "4": {"text": " Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        faiss_index (faiss.Index): A Faiss Index object (required)\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        similarity_top_k (int): Number of similar nodes to retrieve.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexDict,\n        faiss_index: Optional[Any] = None,\n        embed_model: Optional[BaseEmbedding] =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 4, "child_indices": [], "ref_doc_id": "1ea526708a54837eb2d2594ce803150877deeec3", "node_info": null}, "5": {"text": "    embed_model: Optional[BaseEmbedding] = None,\n        similarity_top_k: Optional[int] = 1,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            index_struct=index_struct,\n            embed_model=embed_model,\n            similarity_top_k=similarity_top_k,\n            **kwargs,\n        )\n        if faiss_index is None:\n            raise ValueError(\"faiss_index cannot be None.\")\n        # NOTE: cast to Any for now\n        self._faiss_index = cast(Any, faiss_index)\n        self._faiss_index =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 5, "child_indices": [], "ref_doc_id": "1ea526708a54837eb2d2594ce803150877deeec3", "node_info": null}, "6": {"text": "       self._faiss_index = faiss_index\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n        query_embedding_np = np.array(query_embedding, dtype=\"float32\")[np.newaxis, :]\n        dists, indices = self._faiss_index.search(\n            query_embedding_np, self.similarity_top_k\n        )\n        dists = [d[0] for d in dists]\n        # if empty, then return an empty response\n        if", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 6, "child_indices": [], "ref_doc_id": "1ea526708a54837eb2d2594ce803150877deeec3", "node_info": null}, "7": {"text": "empty, then return an empty response\n        if len(indices) == 0:\n            return []\n\n        # returned dimension is 1 x k\n        node_idxs = list([str(i) for i in indices[0]])\n        top_k_nodes = self._index_struct.get_nodes(node_idxs)\n\n        if similarity_tracker is not None:\n            for node, similarity in zip(top_k_nodes, dists):\n                similarity_tracker.add(node, similarity)\n\n        if logging.getLogger(__name__).getEffectiveLevel() == logging.DEBUG:\n            fmt_txts = []\n            for node_idx, node_similarity, node in zip(node_idxs, dists,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 7, "child_indices": [], "ref_doc_id": "1ea526708a54837eb2d2594ce803150877deeec3", "node_info": null}, "8": {"text": "node in zip(node_idxs, dists, top_k_nodes):\n                fmt_txt = f\"> [Node {node_idx}] [Similarity score: \\\n                    {float(node_similarity):.6}] {truncate_text(node.get_text(), 100)}\"\n                fmt_txts.append(fmt_txt)\n            top_k_node_text = \"\\n\".join(fmt_txts)\n            logging.debug(f\"> Top {len(top_k_nodes)} nodes:\\n{top_k_node_text}\")\n\n        return top_k_nodes\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 8, "child_indices": [], "ref_doc_id": "1ea526708a54837eb2d2594ce803150877deeec3", "node_info": null}, "9": {"text": "\"\"\"Pinecone vector store index query.\"\"\"\nimport logging\nfrom typing import Any, Dict, List, Optional, cast\n\nfrom gpt_index.data_structs.data_structs import IndexDict, Node\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.query.vector_store.base import BaseGPTVectorStoreIndexQuery\nfrom gpt_index.indices.utils import truncate_text\n\n\nclass GPTPineconeIndexQuery(BaseGPTVectorStoreIndexQuery[IndexDict]):\n    \"\"\"GPTPineconeIndex query.\n\n    An embedding-based query for GPTPineconeIndex, which queries\n    an undelrying Pinecone index to retrieve top-k nodes by\n    embedding similarity to the query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]):", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 9, "child_indices": [], "ref_doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "node_info": null}, "10": {"text": "  text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        pinecone_index (pinecone.Index): A Pinecone Index object (required)\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        similarity_top_k (int): Number of similar nodes to retrieve.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexDict,\n        pinecone_index: Optional[Any] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 10, "child_indices": [], "ref_doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "node_info": null}, "11": {"text": "Optional[BaseEmbedding] = None,\n        similarity_top_k: Optional[int] = 1,\n        pinecone_kwargs: Optional[Dict] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            index_struct=index_struct,\n            embed_model=embed_model,\n            similarity_top_k=similarity_top_k,\n            **kwargs,\n        )\n        if pinecone_index is None:\n            raise ValueError(\"pinecone_index cannot be None.\")\n        # NOTE: cast to Any for now\n        self._pinecone_index = cast(Any, pinecone_index)\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 11, "child_indices": [], "ref_doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "node_info": null}, "12": {"text": "= cast(Any, pinecone_index)\n        self._pinecone_index = pinecone_index\n\n        self._pinecone_kwargs = pinecone_kwargs or {}\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n\n        response = self._pinecone_index.query(\n            query_embedding,\n            top_k=self.similarity_top_k,\n            include_values=True,\n            include_metadata=True,\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 12, "child_indices": [], "ref_doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "node_info": null}, "13": {"text": "           **self._pinecone_kwargs,\n        )\n\n        top_k_nodes = []\n        top_k_ids = []\n        top_k_scores = []\n        for match in response.matches:\n            text = match.metadata[\"text\"]\n            node = Node(text=text, extra_info=match.metadata)\n            top_k_ids.append(match.id)\n            top_k_nodes.append(node)\n            top_k_scores.append(match.score)\n            if similarity_tracker is not None:\n                similarity_tracker.add(node, match.score)\n\n        if", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 13, "child_indices": [], "ref_doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "node_info": null}, "14": {"text": "match.score)\n\n        if logging.getLogger(__name__).getEffectiveLevel() == logging.DEBUG:\n            fmt_txts = []\n            for node_idx, node_similarity, node in zip(\n                top_k_ids, top_k_scores, top_k_nodes\n            ):\n                fmt_txt = f\"> [Node {node_idx}] [Similarity score: \\\n                    {node_similarity:.6}] {truncate_text(node.get_text(), 100)}\"\n                fmt_txts.append(fmt_txt)\n            top_k_node_text = \"\\n\".join(fmt_txts)\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 14, "child_indices": [], "ref_doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "node_info": null}, "15": {"text": "           logging.debug(f\"> Top {len(top_k_nodes)} nodes:\\n{top_k_node_text}\")\n\n        return top_k_nodes\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 15, "child_indices": [], "ref_doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "node_info": null}, "16": {"text": "\"\"\"Qdrant vector store index query.\"\"\"\nimport logging\nfrom typing import Any, List, Optional, cast\n\nfrom gpt_index.data_structs import Node, QdrantIndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.query.vector_store.base import BaseGPTVectorStoreIndexQuery\nfrom gpt_index.indices.utils import truncate_text\n\n\nclass GPTQdrantIndexQuery(BaseGPTVectorStoreIndexQuery[QdrantIndexStruct]):\n    \"\"\"GPTQdrantIndex query.\n\n    An embedding-based query for GPTQdrantIndex, which queries\n    an undelrying Qdrant index to retrieve top-k nodes by\n    embedding similarity to the query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    Args:\n        text_qa_template", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 16, "child_indices": [], "ref_doc_id": "666d66f5854bdfaa296829a9de353eddb6f46e69", "node_info": null}, "17": {"text": " Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        similarity_top_k (int): Number of similar nodes to retrieve.\n        client (Optional[Any]): QdrantClient instance from `qdrant-client` package\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: QdrantIndexStruct,\n        embed_model: Optional[BaseEmbedding] = None,\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 17, "child_indices": [], "ref_doc_id": "666d66f5854bdfaa296829a9de353eddb6f46e69", "node_info": null}, "18": {"text": "= None,\n        similarity_top_k: int = 1,\n        client: Optional[Any] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            index_struct=index_struct,\n            embed_model=embed_model,\n            similarity_top_k=similarity_top_k,\n            **kwargs,\n        )\n\n        import_err_msg = (\n            \"`qdrant-client` package not found, please run `pip install qdrant-client`\"\n        )\n        try:\n            import qdrant_client  # noqa: F401\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 18, "child_indices": [], "ref_doc_id": "666d66f5854bdfaa296829a9de353eddb6f46e69", "node_info": null}, "19": {"text": "import qdrant_client  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n\n        if client is None:\n            raise ValueError(\"client cannot be None.\")\n\n        self._client = cast(qdrant_client.QdrantClient, client)\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        from qdrant_client.http.models.models import Payload\n\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n\n        response = self._client.search(\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 19, "child_indices": [], "ref_doc_id": "666d66f5854bdfaa296829a9de353eddb6f46e69", "node_info": null}, "20": {"text": "           collection_name=self.index_struct.get_collection_name(),\n            query_vector=query_embedding,\n            limit=cast(int, self.similarity_top_k),\n        )\n\n        logging.debug(f\"> Top {len(response)} nodes:\")\n\n        nodes = []\n        for point in response:\n            payload = cast(Payload, point.payload)\n            node = Node(\n                doc_id=payload.get(\"doc_id\"),\n                text=payload.get(\"text\"),\n            )\n            nodes.append(node)\n\n            if similarity_tracker", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 20, "child_indices": [], "ref_doc_id": "666d66f5854bdfaa296829a9de353eddb6f46e69", "node_info": null}, "21": {"text": "          if similarity_tracker is not None:\n                similarity_tracker.add(node, point.score)\n\n            logging.debug(\n                f\"> [Node {point.id}] [Similarity score: {point.score:.6}] \"\n                f\"{truncate_text(str(payload.get('text')), 100)}\"\n            )\n\n        return nodes\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 21, "child_indices": [], "ref_doc_id": "666d66f5854bdfaa296829a9de353eddb6f46e69", "node_info": null}, "22": {"text": "\"\"\"Default query for GPTSimpleVectorIndex.\"\"\"\nimport logging\nfrom typing import List, Optional\n\nfrom gpt_index.data_structs.data_structs import Node, SimpleIndexDict\nfrom gpt_index.indices.query.embedding_utils import (\n    SimilarityTracker,\n    get_top_k_embeddings,\n)\nfrom gpt_index.indices.query.vector_store.base import BaseGPTVectorStoreIndexQuery\nfrom gpt_index.indices.utils import truncate_text\n\n\nclass GPTSimpleVectorIndexQuery(BaseGPTVectorStoreIndexQuery[SimpleIndexDict]):\n    \"\"\"GPTSimpleVectorIndex query.\n\n    An embedding-based query for GPTSimpleVectorIndex, which queries\n    an underlying dict-based embedding store to retrieve top-k nodes by\n    embedding similarity to the query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    Args:\n        text_qa_template", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/simple.py", "file_name": "simple.py"}, "index": 22, "child_indices": [], "ref_doc_id": "33f9551e7c7f50285582555c35fb754a11a4c61c", "node_info": null}, "23": {"text": " Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        similarity_top_k (int): Number of similar nodes to retrieve.\n\n    \"\"\"\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        # TODO:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/simple.py", "file_name": "simple.py"}, "index": 23, "child_indices": [], "ref_doc_id": "33f9551e7c7f50285582555c35fb754a11a4c61c", "node_info": null}, "24": {"text": "response.\"\"\"\n        # TODO: consolidate with get_query_text_embedding_similarities\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n        items = self._index_struct.embedding_dict.items()\n        node_ids = [t[0] for t in items]\n        embeddings = [t[1] for t in items]\n\n        top_similarities, top_ids = get_top_k_embeddings(\n            self._embed_model,\n            query_embedding,\n            embeddings,\n            similarity_top_k=self.similarity_top_k,\n            embedding_ids=node_ids,\n        )\n        top_k_nodes =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/simple.py", "file_name": "simple.py"}, "index": 24, "child_indices": [], "ref_doc_id": "33f9551e7c7f50285582555c35fb754a11a4c61c", "node_info": null}, "25": {"text": ")\n        top_k_nodes = self._index_struct.get_nodes(top_ids)\n        if similarity_tracker is not None:\n            for node, similarity in zip(top_k_nodes, top_similarities):\n                similarity_tracker.add(node, similarity)\n\n        if logging.getLogger(__name__).getEffectiveLevel() == logging.DEBUG:\n            fmt_txts = []\n            for node_idx, node_similarity, node in zip(\n                top_ids, top_similarities, top_k_nodes\n            ):\n                fmt_txt = f\"> [Node {node_idx}] [Similarity score: \\\n              ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/simple.py", "file_name": "simple.py"}, "index": 25, "child_indices": [], "ref_doc_id": "33f9551e7c7f50285582555c35fb754a11a4c61c", "node_info": null}, "26": {"text": "\\\n                    {node_similarity:.6}] {truncate_text(node.get_text(), 100)}\"\n                fmt_txts.append(fmt_txt)\n            top_k_node_text = \"\\n\".join(fmt_txts)\n            logging.debug(f\"> Top {len(top_k_nodes)} nodes:\\n{top_k_node_text}\")\n\n        return top_k_nodes\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/simple.py", "file_name": "simple.py"}, "index": 26, "child_indices": [], "ref_doc_id": "33f9551e7c7f50285582555c35fb754a11a4c61c", "node_info": null}, "27": {"text": "\"\"\"Weaviate vector store index query.\"\"\"\n\n\nimport logging\nfrom typing import Any, List, Optional, cast\n\nfrom gpt_index.data_structs.data_structs import Node, WeaviateIndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.utils import truncate_text\nfrom gpt_index.readers.weaviate.data_structs import WeaviateNode\n\n\nclass GPTWeaviateIndexQuery(BaseGPTIndexQuery[WeaviateIndexStruct]):\n    \"\"\"Base vector store query.\"\"\"\n\n    def __init__(\n        self,\n        index_struct: WeaviateIndexStruct,\n        embed_model: Optional[BaseEmbedding] = None,\n        similarity_top_k: Optional[int] = 1,\n        weaviate_client:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 27, "child_indices": [], "ref_doc_id": "c74c84559cd6f32007980215425c5cdaeb0843b8", "node_info": null}, "28": {"text": "1,\n        weaviate_client: Optional[Any] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(index_struct=index_struct, embed_model=embed_model, **kwargs)\n        self.similarity_top_k = similarity_top_k\n        import_err_msg = (\n            \"`weaviate` package not found, please run `pip install weaviate-client`\"\n        )\n        try:\n            import weaviate  # noqa: F401\n            from weaviate import Client  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n        self.client =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 28, "child_indices": [], "ref_doc_id": "c74c84559cd6f32007980215425c5cdaeb0843b8", "node_info": null}, "29": {"text": "       self.client = cast(Client, weaviate_client)\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n        nodes = WeaviateNode.to_gpt_index_list(\n            self.client,\n            self._index_struct.get_class_prefix(),\n            vector=query_embedding,\n            object_limit=self.similarity_top_k,\n        )\n        nodes = nodes[:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 29, "child_indices": [], "ref_doc_id": "c74c84559cd6f32007980215425c5cdaeb0843b8", "node_info": null}, "30": {"text": ")\n        nodes = nodes[: self.similarity_top_k]\n        node_idxs = [str(i) for i in range(len(nodes))]\n\n        if logging.getLogger(__name__).getEffectiveLevel() == logging.DEBUG:\n            fmt_txts = []\n            for node_idx, node in zip(node_idxs, nodes):\n                fmt_txt = f\"> [Node {node_idx}] {truncate_text(node.get_text(), 100)}\"\n                fmt_txts.append(fmt_txt)\n            top_k_node_text = \"\\n\".join(fmt_txts)\n            logging.debug(f\"> Top {len(nodes)}", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 30, "child_indices": [], "ref_doc_id": "c74c84559cd6f32007980215425c5cdaeb0843b8", "node_info": null}, "31": {"text": "  logging.debug(f\"> Top {len(nodes)} nodes:\\n{top_k_node_text}\")\n        return nodes\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 31, "child_indices": [], "ref_doc_id": "c74c84559cd6f32007980215425c5cdaeb0843b8", "node_info": null}, "32": {"text": "This code file contains classes for vector store indices. It includes the base class BaseGPTVectorStoreIndexQuery, which initializes parameters for the query, and two subclasses, GPTFaissIndexQuery and GPTPineconeIndexQuery, which query an underlying Faiss or Pinecone index to retrieve top-k nodes by embedding similarity to the query. The classes contain methods for retrieving nodes for the response, and logging debug information.", "doc_id": null, "embedding": null, "extra_info": null, "index": 32, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "33": {"text": "The file pinecone.py is a part of the GPT-Index package, which is used to query an underlying Qdrant index to retrieve top-k nodes by embedding similarity to the query. It contains a class GPTQdrantIndexQuery which initializes parameters and a method _get_nodes_for_response which queries the index and returns a list of nodes. The file qdrant.py is a part of the GPT-Index package, which is used to query an underlying dict-based embedding store to retrieve top-k nodes by embedding similarity to the query. It contains a class GPTSimpleVectorIndexQuery which initializes parameters and a method _get_nodes_for_response which queries the index and returns a list of nodes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 33, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "34": {"text": "The file simple.py and weaviate.py are part of the GPT Index Query Vector Store. The purpose of these files is to provide a way to query a vector store index and return the top k nodes that are most similar to the query string. The files contain functions to embed the query string, calculate the similarity between the query string and the nodes in the index, and return the top k nodes. Additionally, the files contain logging functions to provide debugging information.", "doc_id": null, "embedding": null, "extra_info": null, "index": 34, "child_indices": [24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"32": {"text": "This code file contains classes for vector store indices. It includes the base class BaseGPTVectorStoreIndexQuery, which initializes parameters for the query, and two subclasses, GPTFaissIndexQuery and GPTPineconeIndexQuery, which query an underlying Faiss or Pinecone index to retrieve top-k nodes by embedding similarity to the query. The classes contain methods for retrieving nodes for the response, and logging debug information.", "doc_id": null, "embedding": null, "extra_info": null, "index": 32, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "33": {"text": "The file pinecone.py is a part of the GPT-Index package, which is used to query an underlying Qdrant index to retrieve top-k nodes by embedding similarity to the query. It contains a class GPTQdrantIndexQuery which initializes parameters and a method _get_nodes_for_response which queries the index and returns a list of nodes. The file qdrant.py is a part of the GPT-Index package, which is used to query an underlying dict-based embedding store to retrieve top-k nodes by embedding similarity to the query. It contains a class GPTSimpleVectorIndexQuery which initializes parameters and a method _get_nodes_for_response which queries the index and returns a list of nodes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 33, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "34": {"text": "The file simple.py and weaviate.py are part of the GPT Index Query Vector Store. The purpose of these files is to provide a way to query a vector store index and return the top k nodes that are most similar to the query string. The files contain functions to embed the query string, calculate the similarity between the query string and the nodes in the index, and return the top k nodes. Additionally, the files contain logging functions to provide debugging information.", "doc_id": null, "embedding": null, "extra_info": null, "index": 34, "child_indices": [24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}