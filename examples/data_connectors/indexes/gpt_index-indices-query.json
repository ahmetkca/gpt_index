{"index_struct": {"text": "\nThese documents do not have summaries.", "doc_id": "c2931efb-92ef-4f0f-9916-ee27d69d4d2d", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "node_info": null}, "1": {"text": "\"\"\"Base query classes.\"\"\"\n\nimport logging\nimport re\nfrom abc import abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Dict, Generic, List, Optional, Tuple, TypeVar, cast\n\nfrom gpt_index.data_structs.data_structs import IndexStruct, Node\nfrom gpt_index.docstore import DocumentStore\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.embeddings.openai import OpenAIEmbedding\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.response.builder import ResponseBuilder, ResponseMode, TextChunk\nfrom gpt_index.indices.utils import truncate_text\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.default_prompts import (\n    DEFAULT_REFINE_PROMPT,\n    DEFAULT_TEXT_QA_PROMPT,\n)\nfrom gpt_index.prompts.prompts import", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "2": {"text": "gpt_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\nfrom gpt_index.response.schema import Response\nfrom gpt_index.token_counter.token_counter import llm_token_counter\n\nIS = TypeVar(\"IS\", bound=IndexStruct)\n\n\n@dataclass\nclass BaseQueryRunner:\n    \"\"\"Base query runner.\"\"\"\n\n    @abstractmethod\n    def query(self, query: str, index_struct: IndexStruct) -> Response:\n        \"\"\"Schedule a query.\"\"\"\n        raise NotImplementedError(\"Not implemented yet.\")\n\n\nclass BaseGPTIndexQuery(Generic[IS]):\n    \"\"\"Base GPT Index Query.\n\n    Helper class that is used to query an index. Can be called within `query`\n    method of a BaseGPTIndex object, or instantiated independently.\n\n    Args:\n        llm_predictor (LLMPredictor): Optional LLMPredictor object. If not provided,\n            will use the default LLMPredictor", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "3": {"text": "        will use the default LLMPredictor (text-davinci-003)\n        prompt_helper (PromptHelper): Optional PromptHelper object. If not provided,\n            will use the default PromptHelper.\n        required_keywords (List[str]): Optional list of keywords that must be present\n            in nodes. Can be used to query most indices (tree index is an exception).\n        exclude_keywords (List[str]): Optional list of keywords that must not be\n            present in nodes. Can be used to query most indices (tree index is an\n            exception).\n        response_mode (ResponseMode): Optional ResponseMode. If not provided, will\n            use the default ResponseMode.\n        text_qa_template (QuestionAnswerPrompt): Optional QuestionAnswerPrompt object.\n            If not", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "4": {"text": "object.\n            If not provided, will use the default QuestionAnswerPrompt.\n        refine_template (RefinePrompt): Optional RefinePrompt object. If not provided,\n            will use the default RefinePrompt.\n        include_summary (bool): Optional bool. If True, will also use the summary\n            text of the index when generating a response (the summary text can be set\n            through `index.set_text(\"<text>\")`).\n        similarity_cutoff (float): Optional float. If set, will filter out nodes with\n            similarity below this cutoff threshold when computing the response\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IS,\n        # TODO: pass from superclass\n        llm_predictor: Optional[LLMPredictor] =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 4, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "5": {"text": " llm_predictor: Optional[LLMPredictor] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        docstore: Optional[DocumentStore] = None,\n        query_runner: Optional[BaseQueryRunner] = None,\n        required_keywords: Optional[List[str]] = None,\n        exclude_keywords: Optional[List[str]] = None,\n        response_mode: ResponseMode = ResponseMode.DEFAULT,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        refine_template: Optional[RefinePrompt] = None,\n        include_summary: bool = False,\n        response_kwargs: Optional[Dict] = None,\n        similarity_cutoff: Optional[float] = None,\n    ) -> None:\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 5, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "6": {"text": "= None,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        if index_struct is None:\n            raise ValueError(\"index_struct must be provided.\")\n        self._validate_index_struct(index_struct)\n        self._index_struct = index_struct\n        self._llm_predictor = llm_predictor or LLMPredictor()\n        # NOTE: the embed_model isn't used in all indices\n        self._embed_model = embed_model or OpenAIEmbedding()\n        self._docstore = docstore\n        self._query_runner = query_runner\n        # TODO: make this a required param\n        if prompt_helper is None:\n            raise ValueError(\"prompt_helper must be provided.\")\n        self._prompt_helper", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 6, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "7": {"text": "       self._prompt_helper = cast(PromptHelper, prompt_helper)\n\n        self._required_keywords = required_keywords\n        self._exclude_keywords = exclude_keywords\n        self._response_mode = ResponseMode(response_mode)\n\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        self.refine_template = refine_template or DEFAULT_REFINE_PROMPT\n        self._include_summary = include_summary\n\n        self._response_kwargs = response_kwargs or {}\n        self.response_builder = ResponseBuilder(\n            self._prompt_helper,\n            self._llm_predictor,\n            self.text_qa_template,\n            self.refine_template,\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 7, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "8": {"text": "        self.refine_template,\n        )\n\n        self.similarity_cutoff = similarity_cutoff\n\n    def _should_use_node(\n        self, node: Node, similarity_tracker: Optional[SimilarityTracker] = None\n    ) -> bool:\n        \"\"\"Run node through filters to determine if it should be used.\"\"\"\n        words = re.findall(r\"\\w+\", node.get_text())\n        if self._required_keywords is not None:\n            for w in self._required_keywords:\n                if w not in words:\n                    return False\n\n        if self._exclude_keywords is not None:\n            for w in self._exclude_keywords:\n             ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 8, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "9": {"text": "               if w in words:\n                    return False\n\n        sim_cutoff_exists = (\n            similarity_tracker is not None and self.similarity_cutoff is not None\n        )\n\n        if sim_cutoff_exists:\n            similarity = cast(SimilarityTracker, similarity_tracker).find(node)\n            if similarity is None:\n                return False\n            if cast(float, similarity) < cast(float, self.similarity_cutoff):\n                return False\n\n        return True\n\n    def _get_text_from_node(\n        self,\n        query_str: str,\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 9, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "10": {"text": "     query_str: str,\n        node: Node,\n        level: Optional[int] = None,\n    ) -> Tuple[TextChunk, Optional[Response]]:\n        \"\"\"Query a given node.\n\n        If node references a given document, then return the document.\n        If node references a given index, then query the index.\n\n        \"\"\"\n        level_str = \"\" if level is None else f\"[Level {level}]\"\n        fmt_text_chunk = truncate_text(node.get_text(), 50)\n        logging.debug(f\">{level_str} Searching in chunk: {fmt_text_chunk}\")\n\n        is_index_struct = False\n        # if self._query_runner is not None, assume we want to do a recursive\n        # query. In order to not perform a recursive query, make sure\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 10, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "11": {"text": "not perform a recursive query, make sure\n        # _query_runner is None.\n        if (\n            self._query_runner is not None\n            and node.ref_doc_id is not None\n            and self._docstore is not None\n        ):\n            doc = self._docstore.get_document(node.ref_doc_id, raise_error=True)\n            if isinstance(doc, IndexStruct):\n                is_index_struct = True\n\n        if is_index_struct:\n            query_runner = cast(BaseQueryRunner, self._query_runner)\n            response = query_runner.query(query_str, cast(IndexStruct, doc))\n            return TextChunk(str(response),", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 11, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "12": {"text": "       return TextChunk(str(response), is_answer=True), response\n        else:\n            text = node.get_text()\n            return TextChunk(text), None\n\n    @property\n    def index_struct(self) -> IS:\n        \"\"\"Get the index struct.\"\"\"\n        return self._index_struct\n\n    def _validate_index_struct(self, index_struct: IS) -> None:\n        \"\"\"Validate the index struct.\"\"\"\n        pass\n\n    def _give_response_for_nodes(\n        self, query_str: str, text_chunks: List[TextChunk]\n    ) -> str:\n        \"\"\"Give response for nodes.\"\"\"\n        self.response_builder.reset()\n        for text in text_chunks:\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 12, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "13": {"text": "           self.response_builder.add_text_chunks([text])\n        response = self.response_builder.get_response(\n            query_str,\n            mode=self._response_mode,\n            **self._response_kwargs,\n        )\n\n        return response or \"\"\n\n    def get_nodes_and_similarities_for_response(\n        self, query_str: str\n    ) -> List[Tuple[Node, Optional[float]]]:\n        \"\"\"Get list of tuples of node and similarity for response.\n\n        First part of the tuple is the node.\n        Second part of tuple is the distance from query to the node.\n        If not applicable, it's None.\n        \"\"\"\n        similarity_tracker = SimilarityTracker()\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 13, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "14": {"text": " similarity_tracker = SimilarityTracker()\n        nodes = self._get_nodes_for_response(\n            query_str, similarity_tracker=similarity_tracker\n        )\n        nodes = [\n            node for node in nodes if self._should_use_node(node, similarity_tracker)\n        ]\n\n        # TODO: create a `display` method to allow subclasses to print the Node\n        return similarity_tracker.get_zipped_nodes(nodes)\n\n    @abstractmethod\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n\n    def _query(self,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 14, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "15": {"text": "nodes for response.\"\"\"\n\n    def _query(self, query_str: str) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        # TODO: remove _query and just use query\n        tuples = self.get_nodes_and_similarities_for_response(query_str)\n        node_texts = []\n        for node, similarity in tuples:\n            text, response = self._get_text_from_node(query_str, node)\n            self.response_builder.add_node(node, similarity=similarity)\n            if response is not None:\n                # these are source nodes from within this node (when it's an index)\n                for source_node in response.source_nodes:\n                   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 15, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "16": {"text": "                self.response_builder.add_source_node(source_node)\n            node_texts.append(text)\n\n        if self._response_mode != ResponseMode.NO_TEXT:\n            response_str = self._give_response_for_nodes(query_str, node_texts)\n        else:\n            response_str = None\n\n        return Response(response_str, source_nodes=self.response_builder.get_sources())\n\n    @llm_token_counter(\"query\")\n    def query(self, query_str: str) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        response = self._query(query_str)\n        # if include_summary is True, then include summary text in answer\n        # summary text is set through `set_text` on the underlying index.\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 16, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "17": {"text": "on the underlying index.\n        # TODO: refactor response builder to be in the __init__\n        if self._response_mode != ResponseMode.NO_TEXT and self._include_summary:\n            response_builder = ResponseBuilder(\n                self._prompt_helper,\n                self._llm_predictor,\n                self.text_qa_template,\n                self.refine_template,\n                texts=[TextChunk(self._index_struct.get_text())],\n            )\n            # NOTE: use create and refine for now (default response mode)\n            response.response = response_builder.get_response(\n              ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 17, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "18": {"text": "               query_str,\n                mode=self._response_mode,\n                prev_response=response.response,\n            )\n\n        return response\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 18, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "19": {"text": "\"\"\"Embedding utils for queries.\"\"\"\n\nfrom typing import Dict, List, Optional, Tuple\n\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.embeddings.openai import BaseEmbedding\n\n\ndef get_top_k_embeddings(\n    embed_model: BaseEmbedding,\n    query_embedding: List[float],\n    embeddings: List[List[float]],\n    similarity_top_k: Optional[int] = None,\n    embedding_ids: Optional[List] = None,\n) -> Tuple[List[float], List]:\n    \"\"\"Get top nodes by similarity to the query.\"\"\"\n    if embedding_ids is None:\n        embedding_ids = [i for i in range(len(embeddings))]\n\n    similarities = []\n    for emb in embeddings:\n        similarity = embed_model.similarity(query_embedding, emb)\n        similarities.append(similarity)\n\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/embedding_utils.py", "file_name": "embedding_utils.py"}, "index": 19, "child_indices": [], "ref_doc_id": "f0bab417b4bf90ea4c52f882a52fbf041839974c", "node_info": null}, "20": {"text": "     similarities.append(similarity)\n\n    sorted_tups = sorted(\n        zip(similarities, embedding_ids), key=lambda x: x[0], reverse=True\n    )\n    similarity_top_k = similarity_top_k or len(sorted_tups)\n    result_tups = sorted_tups[:similarity_top_k]\n\n    result_similarities = [s for s, _ in result_tups]\n    result_ids = [n for _, n in result_tups]\n\n    return result_similarities, result_ids\n\n\nclass SimilarityTracker:\n    \"\"\"Helper class to manage node similarities during lifecycle of a single query.\"\"\"\n\n    # TODO: smarter way to store this information\n    lookup: Dict[str, float] = {}\n\n    def _hash(self, node: Node) -> str:\n        \"\"\"Generate a unique key for each node.\"\"\"\n        # TODO: Better way to get unique", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/embedding_utils.py", "file_name": "embedding_utils.py"}, "index": 20, "child_indices": [], "ref_doc_id": "f0bab417b4bf90ea4c52f882a52fbf041839974c", "node_info": null}, "21": {"text": "      # TODO: Better way to get unique identifier of a node\n        return str(abs(hash(node.get_text())))\n\n    def add(self, node: Node, similarity: float) -> None:\n        \"\"\"Add a node and its similarity score.\"\"\"\n        node_hash = self._hash(node)\n        self.lookup[node_hash] = similarity\n\n    def find(self, node: Node) -> Optional[float]:\n        \"\"\"Find a node's similarity score.\"\"\"\n        node_hash = self._hash(node)\n        if node_hash not in self.lookup:\n            return None\n        return self.lookup[node_hash]\n\n    def get_zipped_nodes(self, nodes: List[Node]) -> List[Tuple[Node, Optional[float]]]:\n        \"\"\"Get a zipped list of nodes and their", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/embedding_utils.py", "file_name": "embedding_utils.py"}, "index": 21, "child_indices": [], "ref_doc_id": "f0bab417b4bf90ea4c52f882a52fbf041839974c", "node_info": null}, "22": {"text": "     \"\"\"Get a zipped list of nodes and their corresponding scores.\"\"\"\n        similarities = [self.find(node) for node in nodes]\n        return list(zip(nodes, similarities))\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/embedding_utils.py", "file_name": "embedding_utils.py"}, "index": 22, "child_indices": [], "ref_doc_id": "f0bab417b4bf90ea4c52f882a52fbf041839974c", "node_info": null}, "23": {"text": "\"\"\"Query runner.\"\"\"\n\nfrom typing import Any, Dict, List, Optional, Union, cast\n\nfrom gpt_index.data_structs.data_structs import IndexStruct\nfrom gpt_index.docstore import DocumentStore\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.query.base import BaseQueryRunner\nfrom gpt_index.indices.query.schema import QueryConfig, QueryMode\nfrom gpt_index.indices.registry import IndexRegistry\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.response.schema import Response\n\n# TMP: refactor query config type\nQUERY_CONFIG_TYPE = Union[Dict, QueryConfig]\n\n\nclass QueryRunner(BaseQueryRunner):\n    \"\"\"Tool to take in a query request and perform a query with the right classes.\n\n    Higher-level wrapper over a given query.\n\n    \"\"\"\n\n    def __init__(\n        self,\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/query_runner.py", "file_name": "query_runner.py"}, "index": 23, "child_indices": [], "ref_doc_id": "266fe6e986b73943246d77bc2f38623ad4ca18c7", "node_info": null}, "24": {"text": "      self,\n        llm_predictor: LLMPredictor,\n        prompt_helper: PromptHelper,\n        embed_model: BaseEmbedding,\n        docstore: DocumentStore,\n        index_registry: IndexRegistry,\n        query_configs: Optional[List[QUERY_CONFIG_TYPE]] = None,\n        recursive: bool = False,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        config_dict: Dict[str, QueryConfig] = {}\n        if query_configs is None or len(query_configs) == 0:\n            query_config_objs: List[QueryConfig] = []\n        elif isinstance(query_configs[0], Dict):\n            query_config_objs = [\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/query_runner.py", "file_name": "query_runner.py"}, "index": 24, "child_indices": [], "ref_doc_id": "266fe6e986b73943246d77bc2f38623ad4ca18c7", "node_info": null}, "25": {"text": " query_config_objs = [\n                QueryConfig.from_dict(cast(Dict, qc)) for qc in query_configs\n            ]\n        else:\n            query_config_objs = [cast(QueryConfig, q) for q in query_configs]\n\n        for qc in query_config_objs:\n            config_dict[qc.index_struct_type] = qc\n\n        self._config_dict = config_dict\n        self._llm_predictor = llm_predictor\n        self._prompt_helper = prompt_helper\n        self._embed_model = embed_model\n        self._docstore = docstore\n        self._index_registry = index_registry\n        self._recursive = recursive\n\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/query_runner.py", "file_name": "query_runner.py"}, "index": 25, "child_indices": [], "ref_doc_id": "266fe6e986b73943246d77bc2f38623ad4ca18c7", "node_info": null}, "26": {"text": "      self._recursive = recursive\n\n    def _get_query_kwargs(self, config: QueryConfig) -> Dict[str, Any]:\n        \"\"\"Get query kwargs.\n\n        Also update with default arguments if not present.\n\n        \"\"\"\n        query_kwargs = {k: v for k, v in config.query_kwargs.items()}\n        if \"prompt_helper\" not in query_kwargs:\n            query_kwargs[\"prompt_helper\"] = self._prompt_helper\n        if \"llm_predictor\" not in query_kwargs:\n            query_kwargs[\"llm_predictor\"] = self._llm_predictor\n        if \"embed_model\" not in query_kwargs:\n            query_kwargs[\"embed_model\"] = self._embed_model\n        return", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/query_runner.py", "file_name": "query_runner.py"}, "index": 26, "child_indices": [], "ref_doc_id": "266fe6e986b73943246d77bc2f38623ad4ca18c7", "node_info": null}, "27": {"text": "= self._embed_model\n        return query_kwargs\n\n    def query(self, query_str: str, index_struct: IndexStruct) -> Response:\n        \"\"\"Run query.\"\"\"\n        index_struct_type = index_struct.get_type()\n        if index_struct_type not in self._config_dict:\n            config = QueryConfig(\n                index_struct_type=index_struct_type, query_mode=QueryMode.DEFAULT\n            )\n        else:\n            config = self._config_dict[index_struct_type]\n        mode = config.query_mode\n\n        query_cls = self._index_registry.type_to_query[index_struct_type][mode]\n        # if recursive, pass self as query_runner to each individual query\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/query_runner.py", "file_name": "query_runner.py"}, "index": 27, "child_indices": [], "ref_doc_id": "266fe6e986b73943246d77bc2f38623ad4ca18c7", "node_info": null}, "28": {"text": "self as query_runner to each individual query\n        query_runner = self if self._recursive else None\n        query_kwargs = self._get_query_kwargs(config)\n        query_obj = query_cls(\n            index_struct,\n            **query_kwargs,\n            query_runner=query_runner,\n            docstore=self._docstore,\n        )\n\n        return query_obj.query(query_str)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/query_runner.py", "file_name": "query_runner.py"}, "index": 28, "child_indices": [], "ref_doc_id": "266fe6e986b73943246d77bc2f38623ad4ca18c7", "node_info": null}, "29": {"text": "\"\"\"Query Configuration Schema.\n\nThis schema is used under the hood for all queries, but is primarily\nexposed for recursive queries over composable indices.\n\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Any, Dict\n\nfrom dataclasses_json import DataClassJsonMixin\n\n\nclass QueryMode(str, Enum):\n    \"\"\"Query mode enum.\n\n    Can be passed as the enum struct, or as the underlying string.\n\n    Attributes:\n        DEFAULT (\"default\"): Default query mode.\n        RETRIEVE (\"retrieve\"): Retrieve mode.\n        EMBEDDING (\"embedding\"): Embedding mode.\n        SUMMARIZE (\"summarize\"): Summarize mode. Used for hierarchical\n            summarization in the tree index.\n        SIMPLE (\"simple\"): Simple mode. Used for keyword extraction.\n        RAKE (\"rake\"): RAKE mode. Used for", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/schema.py", "file_name": "schema.py"}, "index": 29, "child_indices": [], "ref_doc_id": "d7ed8486721f6f32f5c8ed69fbdce569a77b5ed0", "node_info": null}, "30": {"text": "    RAKE (\"rake\"): RAKE mode. Used for keyword extraction.\n        RECURSIVE (\"recursive\"): Recursive mode. Used to recursively query\n            over composed indices.\n\n    \"\"\"\n\n    DEFAULT = \"default\"\n    # a special \"retrieve\" query for tree index that retrieves that top nodes\n    RETRIEVE = \"retrieve\"\n    # embedding-based query\n    EMBEDDING = \"embedding\"\n\n    # to hierarchically summarize using tree\n    SUMMARIZE = \"summarize\"\n\n    # for keyword extractor\n    SIMPLE = \"simple\"\n    RAKE = \"rake\"\n\n    # recursive queries (composable queries)\n    # NOTE: deprecated\n    RECURSIVE = \"recursive\"\n\n    # for sql queries\n    SQL = \"sql\"\n\n\n@dataclass\nclass QueryConfig(DataClassJsonMixin):\n    \"\"\"Query config.\n\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/schema.py", "file_name": "schema.py"}, "index": 30, "child_indices": [], "ref_doc_id": "d7ed8486721f6f32f5c8ed69fbdce569a77b5ed0", "node_info": null}, "31": {"text": "   \"\"\"Query config.\n\n    Used under the hood for all queries.\n    The user must explicitly specify a list of query config objects is passed during\n    a query call to define configurations for each individual subindex within an\n    overall composed index.\n\n    The user may choose to specify either the query config objects directly,\n    or as a list of JSON dictionaries. For instance, the following are equivalent:\n\n    .. code-block:: python\n\n        # using JSON dictionaries\n        query_configs = [\n            {\n                \"index_struct_type\": \"tree\",\n                \"query_mode\": \"default\",\n                \"query_kwargs\": {\n                    \"child_branch_factor\": 2\n                }\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/schema.py", "file_name": "schema.py"}, "index": 31, "child_indices": [], "ref_doc_id": "d7ed8486721f6f32f5c8ed69fbdce569a77b5ed0", "node_info": null}, "32": {"text": "             }\n            },\n            ...\n        ]\n        response = index.query(\n            \"<query_str>\", mode=\"recursive\", query_configs=query_configs\n        )\n\n    .. code-block:: python\n\n        query_configs = [\n            QueryConfig(\n                index_struct_type=IndexStructType.TREE,\n                query_mode=QueryMode.DEFAULT,\n                query_kwargs={\n                    \"child_branch_factor\": 2\n                }\n            )\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/schema.py", "file_name": "schema.py"}, "index": 32, "child_indices": [], "ref_doc_id": "d7ed8486721f6f32f5c8ed69fbdce569a77b5ed0", "node_info": null}, "33": {"text": "        )\n            ...\n        ]\n        response = index.query(\n            \"<query_str>\", mode=\"recursive\", query_configs=query_configs\n        )\n\n\n    Args:\n        index_struct_type (IndexStructType): The type of index struct.\n        query_mode (QueryMode): The query mode.\n        query_kwargs (Dict[str, Any], optional): The query kwargs. Defaults to {}.\n\n    \"\"\"\n\n    # index_struct_type: IndexStructType\n    index_struct_type: str\n    query_mode: QueryMode\n    query_kwargs: Dict[str, Any] = field(default_factory=dict)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/schema.py", "file_name": "schema.py"}, "index": 33, "child_indices": [], "ref_doc_id": "d7ed8486721f6f32f5c8ed69fbdce569a77b5ed0", "node_info": null}, "34": {"text": "This code file is the base query class for the GPT Index. It is used to query an index and can be called within the query method of a BaseGPTIndex object, or instantiated independently. It takes in parameters such as an LLMPredictor, PromptHelper, embed_model, docstore, query_runner, required_keywords, exclude_keywords, response_mode, text_qa_template, refine_template, include_summary, response_kwargs, and similarity_cutoff. It also has methods such as _should_use_node and _get_text_from_node which are used to filter out nodes and query a given node respectively.", "doc_id": null, "embedding": null, "extra_info": null, "index": 34, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "35": {"text": "The file base.py from the gpt_index/indices/query/ directory is a Python file that contains the code for the BaseQueryRunner class. This class is used to answer queries and provide responses. It contains methods to validate the index struct, get nodes and similarities for response, query, and get top k embeddings. It also contains a SimilarityTracker class which is used to manage node similarities during the lifecycle of a single query. The QueryRunner class is a higher-level wrapper over a given query and is used to take in a query request and perform a query with the right classes. It contains methods to initialize the query runner, query, and get the response.", "doc_id": null, "embedding": null, "extra_info": null, "index": 35, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "36": {"text": "query_runner.py is a file in the gpt_index/indices/query/ directory that is used to run queries on a given index. It takes in a list of query configurations, which are defined in the schema.py file. It then uses the query configurations to create a query object, which is then used to run the query. The query object can be configured to use a prompt helper, llm predictor, embed model, and docstore. It also supports recursive queries, which can be used to query over composed indices.", "doc_id": null, "embedding": null, "extra_info": null, "index": 36, "child_indices": [32, 33, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"34": {"text": "This code file is the base query class for the GPT Index. It is used to query an index and can be called within the query method of a BaseGPTIndex object, or instantiated independently. It takes in parameters such as an LLMPredictor, PromptHelper, embed_model, docstore, query_runner, required_keywords, exclude_keywords, response_mode, text_qa_template, refine_template, include_summary, response_kwargs, and similarity_cutoff. It also has methods such as _should_use_node and _get_text_from_node which are used to filter out nodes and query a given node respectively.", "doc_id": null, "embedding": null, "extra_info": null, "index": 34, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "35": {"text": "The file base.py from the gpt_index/indices/query/ directory is a Python file that contains the code for the BaseQueryRunner class. This class is used to answer queries and provide responses. It contains methods to validate the index struct, get nodes and similarities for response, query, and get top k embeddings. It also contains a SimilarityTracker class which is used to manage node similarities during the lifecycle of a single query. The QueryRunner class is a higher-level wrapper over a given query and is used to take in a query request and perform a query with the right classes. It contains methods to initialize the query runner, query, and get the response.", "doc_id": null, "embedding": null, "extra_info": null, "index": 35, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "36": {"text": "query_runner.py is a file in the gpt_index/indices/query/ directory that is used to run queries on a given index. It takes in a list of query configurations, which are defined in the schema.py file. It then uses the query configurations to create a query object, which is then used to run the query. The query object can be configured to use a prompt helper, llm predictor, embed model, and docstore. It also supports recursive queries, which can be used to query over composed indices.", "doc_id": null, "embedding": null, "extra_info": null, "index": 36, "child_indices": [32, 33, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"1d4640565ae2765d9ca96a509dc9809217f62f2f": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "642b4f3bbe0ec820cf0734cec152c1766a7d0725": {"text": "\"\"\"Base query classes.\"\"\"\n\nimport logging\nimport re\nfrom abc import abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Dict, Generic, List, Optional, Tuple, TypeVar, cast\n\nfrom gpt_index.data_structs.data_structs import IndexStruct, Node\nfrom gpt_index.docstore import DocumentStore\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.embeddings.openai import OpenAIEmbedding\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.response.builder import ResponseBuilder, ResponseMode, TextChunk\nfrom gpt_index.indices.utils import truncate_text\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.default_prompts import (\n    DEFAULT_REFINE_PROMPT,\n    DEFAULT_TEXT_QA_PROMPT,\n)\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\nfrom gpt_index.response.schema import Response\nfrom gpt_index.token_counter.token_counter import llm_token_counter\n\nIS = TypeVar(\"IS\", bound=IndexStruct)\n\n\n@dataclass\nclass BaseQueryRunner:\n    \"\"\"Base query runner.\"\"\"\n\n    @abstractmethod\n    def query(self, query: str, index_struct: IndexStruct) -> Response:\n        \"\"\"Schedule a query.\"\"\"\n        raise NotImplementedError(\"Not implemented yet.\")\n\n\nclass BaseGPTIndexQuery(Generic[IS]):\n    \"\"\"Base GPT Index Query.\n\n    Helper class that is used to query an index. Can be called within `query`\n    method of a BaseGPTIndex object, or instantiated independently.\n\n    Args:\n        llm_predictor (LLMPredictor): Optional LLMPredictor object. If not provided,\n            will use the default LLMPredictor (text-davinci-003)\n        prompt_helper (PromptHelper): Optional PromptHelper object. If not provided,\n            will use the default PromptHelper.\n        required_keywords (List[str]): Optional list of keywords that must be present\n            in nodes. Can be used to query most indices (tree index is an exception).\n        exclude_keywords (List[str]): Optional list of keywords that must not be\n            present in nodes. Can be used to query most indices (tree index is an\n            exception).\n        response_mode (ResponseMode): Optional ResponseMode. If not provided, will\n            use the default ResponseMode.\n        text_qa_template (QuestionAnswerPrompt): Optional QuestionAnswerPrompt object.\n            If not provided, will use the default QuestionAnswerPrompt.\n        refine_template (RefinePrompt): Optional RefinePrompt object. If not provided,\n            will use the default RefinePrompt.\n        include_summary (bool): Optional bool. If True, will also use the summary\n            text of the index when generating a response (the summary text can be set\n            through `index.set_text(\"<text>\")`).\n        similarity_cutoff (float): Optional float. If set, will filter out nodes with\n            similarity below this cutoff threshold when computing the response\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IS,\n        # TODO: pass from superclass\n        llm_predictor: Optional[LLMPredictor] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        docstore: Optional[DocumentStore] = None,\n        query_runner: Optional[BaseQueryRunner] = None,\n        required_keywords: Optional[List[str]] = None,\n        exclude_keywords: Optional[List[str]] = None,\n        response_mode: ResponseMode = ResponseMode.DEFAULT,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        refine_template: Optional[RefinePrompt] = None,\n        include_summary: bool = False,\n        response_kwargs: Optional[Dict] = None,\n        similarity_cutoff: Optional[float] = None,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        if index_struct is None:\n            raise ValueError(\"index_struct must be provided.\")\n        self._validate_index_struct(index_struct)\n        self._index_struct = index_struct\n        self._llm_predictor = llm_predictor or LLMPredictor()\n        # NOTE: the embed_model isn't used in all indices\n        self._embed_model = embed_model or OpenAIEmbedding()\n        self._docstore = docstore\n        self._query_runner = query_runner\n        # TODO: make this a required param\n        if prompt_helper is None:\n            raise ValueError(\"prompt_helper must be provided.\")\n        self._prompt_helper = cast(PromptHelper, prompt_helper)\n\n        self._required_keywords = required_keywords\n        self._exclude_keywords = exclude_keywords\n        self._response_mode = ResponseMode(response_mode)\n\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        self.refine_template = refine_template or DEFAULT_REFINE_PROMPT\n        self._include_summary = include_summary\n\n        self._response_kwargs = response_kwargs or {}\n        self.response_builder = ResponseBuilder(\n            self._prompt_helper,\n            self._llm_predictor,\n            self.text_qa_template,\n            self.refine_template,\n        )\n\n        self.similarity_cutoff = similarity_cutoff\n\n    def _should_use_node(\n        self, node: Node, similarity_tracker: Optional[SimilarityTracker] = None\n    ) -> bool:\n        \"\"\"Run node through filters to determine if it should be used.\"\"\"\n        words = re.findall(r\"\\w+\", node.get_text())\n        if self._required_keywords is not None:\n            for w in self._required_keywords:\n                if w not in words:\n                    return False\n\n        if self._exclude_keywords is not None:\n            for w in self._exclude_keywords:\n                if w in words:\n                    return False\n\n        sim_cutoff_exists = (\n            similarity_tracker is not None and self.similarity_cutoff is not None\n        )\n\n        if sim_cutoff_exists:\n            similarity = cast(SimilarityTracker, similarity_tracker).find(node)\n            if similarity is None:\n                return False\n            if cast(float, similarity) < cast(float, self.similarity_cutoff):\n                return False\n\n        return True\n\n    def _get_text_from_node(\n        self,\n        query_str: str,\n        node: Node,\n        level: Optional[int] = None,\n    ) -> Tuple[TextChunk, Optional[Response]]:\n        \"\"\"Query a given node.\n\n        If node references a given document, then return the document.\n        If node references a given index, then query the index.\n\n        \"\"\"\n        level_str = \"\" if level is None else f\"[Level {level}]\"\n        fmt_text_chunk = truncate_text(node.get_text(), 50)\n        logging.debug(f\">{level_str} Searching in chunk: {fmt_text_chunk}\")\n\n        is_index_struct = False\n        # if self._query_runner is not None, assume we want to do a recursive\n        # query. In order to not perform a recursive query, make sure\n        # _query_runner is None.\n        if (\n            self._query_runner is not None\n            and node.ref_doc_id is not None\n            and self._docstore is not None\n        ):\n            doc = self._docstore.get_document(node.ref_doc_id, raise_error=True)\n            if isinstance(doc, IndexStruct):\n                is_index_struct = True\n\n        if is_index_struct:\n            query_runner = cast(BaseQueryRunner, self._query_runner)\n            response = query_runner.query(query_str, cast(IndexStruct, doc))\n            return TextChunk(str(response), is_answer=True), response\n        else:\n            text = node.get_text()\n            return TextChunk(text), None\n\n    @property\n    def index_struct(self) -> IS:\n        \"\"\"Get the index struct.\"\"\"\n        return self._index_struct\n\n    def _validate_index_struct(self, index_struct: IS) -> None:\n        \"\"\"Validate the index struct.\"\"\"\n        pass\n\n    def _give_response_for_nodes(\n        self, query_str: str, text_chunks: List[TextChunk]\n    ) -> str:\n        \"\"\"Give response for nodes.\"\"\"\n        self.response_builder.reset()\n        for text in text_chunks:\n            self.response_builder.add_text_chunks([text])\n        response = self.response_builder.get_response(\n            query_str,\n            mode=self._response_mode,\n            **self._response_kwargs,\n        )\n\n        return response or \"\"\n\n    def get_nodes_and_similarities_for_response(\n        self, query_str: str\n    ) -> List[Tuple[Node, Optional[float]]]:\n        \"\"\"Get list of tuples of node and similarity for response.\n\n        First part of the tuple is the node.\n        Second part of tuple is the distance from query to the node.\n        If not applicable, it's None.\n        \"\"\"\n        similarity_tracker = SimilarityTracker()\n        nodes = self._get_nodes_for_response(\n            query_str, similarity_tracker=similarity_tracker\n        )\n        nodes = [\n            node for node in nodes if self._should_use_node(node, similarity_tracker)\n        ]\n\n        # TODO: create a `display` method to allow subclasses to print the Node\n        return similarity_tracker.get_zipped_nodes(nodes)\n\n    @abstractmethod\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n\n    def _query(self, query_str: str) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        # TODO: remove _query and just use query\n        tuples = self.get_nodes_and_similarities_for_response(query_str)\n        node_texts = []\n        for node, similarity in tuples:\n            text, response = self._get_text_from_node(query_str, node)\n            self.response_builder.add_node(node, similarity=similarity)\n            if response is not None:\n                # these are source nodes from within this node (when it's an index)\n                for source_node in response.source_nodes:\n                    self.response_builder.add_source_node(source_node)\n            node_texts.append(text)\n\n        if self._response_mode != ResponseMode.NO_TEXT:\n            response_str = self._give_response_for_nodes(query_str, node_texts)\n        else:\n            response_str = None\n\n        return Response(response_str, source_nodes=self.response_builder.get_sources())\n\n    @llm_token_counter(\"query\")\n    def query(self, query_str: str) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        response = self._query(query_str)\n        # if include_summary is True, then include summary text in answer\n        # summary text is set through `set_text` on the underlying index.\n        # TODO: refactor response builder to be in the __init__\n        if self._response_mode != ResponseMode.NO_TEXT and self._include_summary:\n            response_builder = ResponseBuilder(\n                self._prompt_helper,\n                self._llm_predictor,\n                self.text_qa_template,\n                self.refine_template,\n                texts=[TextChunk(self._index_struct.get_text())],\n            )\n            # NOTE: use create and refine for now (default response mode)\n            response.response = response_builder.get_response(\n                query_str,\n                mode=self._response_mode,\n                prev_response=response.response,\n            )\n\n        return response\n", "doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "__type__": "Document"}, "f0bab417b4bf90ea4c52f882a52fbf041839974c": {"text": "\"\"\"Embedding utils for queries.\"\"\"\n\nfrom typing import Dict, List, Optional, Tuple\n\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.embeddings.openai import BaseEmbedding\n\n\ndef get_top_k_embeddings(\n    embed_model: BaseEmbedding,\n    query_embedding: List[float],\n    embeddings: List[List[float]],\n    similarity_top_k: Optional[int] = None,\n    embedding_ids: Optional[List] = None,\n) -> Tuple[List[float], List]:\n    \"\"\"Get top nodes by similarity to the query.\"\"\"\n    if embedding_ids is None:\n        embedding_ids = [i for i in range(len(embeddings))]\n\n    similarities = []\n    for emb in embeddings:\n        similarity = embed_model.similarity(query_embedding, emb)\n        similarities.append(similarity)\n\n    sorted_tups = sorted(\n        zip(similarities, embedding_ids), key=lambda x: x[0], reverse=True\n    )\n    similarity_top_k = similarity_top_k or len(sorted_tups)\n    result_tups = sorted_tups[:similarity_top_k]\n\n    result_similarities = [s for s, _ in result_tups]\n    result_ids = [n for _, n in result_tups]\n\n    return result_similarities, result_ids\n\n\nclass SimilarityTracker:\n    \"\"\"Helper class to manage node similarities during lifecycle of a single query.\"\"\"\n\n    # TODO: smarter way to store this information\n    lookup: Dict[str, float] = {}\n\n    def _hash(self, node: Node) -> str:\n        \"\"\"Generate a unique key for each node.\"\"\"\n        # TODO: Better way to get unique identifier of a node\n        return str(abs(hash(node.get_text())))\n\n    def add(self, node: Node, similarity: float) -> None:\n        \"\"\"Add a node and its similarity score.\"\"\"\n        node_hash = self._hash(node)\n        self.lookup[node_hash] = similarity\n\n    def find(self, node: Node) -> Optional[float]:\n        \"\"\"Find a node's similarity score.\"\"\"\n        node_hash = self._hash(node)\n        if node_hash not in self.lookup:\n            return None\n        return self.lookup[node_hash]\n\n    def get_zipped_nodes(self, nodes: List[Node]) -> List[Tuple[Node, Optional[float]]]:\n        \"\"\"Get a zipped list of nodes and their corresponding scores.\"\"\"\n        similarities = [self.find(node) for node in nodes]\n        return list(zip(nodes, similarities))\n", "doc_id": "f0bab417b4bf90ea4c52f882a52fbf041839974c", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/embedding_utils.py", "file_name": "embedding_utils.py"}, "__type__": "Document"}, "266fe6e986b73943246d77bc2f38623ad4ca18c7": {"text": "\"\"\"Query runner.\"\"\"\n\nfrom typing import Any, Dict, List, Optional, Union, cast\n\nfrom gpt_index.data_structs.data_structs import IndexStruct\nfrom gpt_index.docstore import DocumentStore\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.query.base import BaseQueryRunner\nfrom gpt_index.indices.query.schema import QueryConfig, QueryMode\nfrom gpt_index.indices.registry import IndexRegistry\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.response.schema import Response\n\n# TMP: refactor query config type\nQUERY_CONFIG_TYPE = Union[Dict, QueryConfig]\n\n\nclass QueryRunner(BaseQueryRunner):\n    \"\"\"Tool to take in a query request and perform a query with the right classes.\n\n    Higher-level wrapper over a given query.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        llm_predictor: LLMPredictor,\n        prompt_helper: PromptHelper,\n        embed_model: BaseEmbedding,\n        docstore: DocumentStore,\n        index_registry: IndexRegistry,\n        query_configs: Optional[List[QUERY_CONFIG_TYPE]] = None,\n        recursive: bool = False,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        config_dict: Dict[str, QueryConfig] = {}\n        if query_configs is None or len(query_configs) == 0:\n            query_config_objs: List[QueryConfig] = []\n        elif isinstance(query_configs[0], Dict):\n            query_config_objs = [\n                QueryConfig.from_dict(cast(Dict, qc)) for qc in query_configs\n            ]\n        else:\n            query_config_objs = [cast(QueryConfig, q) for q in query_configs]\n\n        for qc in query_config_objs:\n            config_dict[qc.index_struct_type] = qc\n\n        self._config_dict = config_dict\n        self._llm_predictor = llm_predictor\n        self._prompt_helper = prompt_helper\n        self._embed_model = embed_model\n        self._docstore = docstore\n        self._index_registry = index_registry\n        self._recursive = recursive\n\n    def _get_query_kwargs(self, config: QueryConfig) -> Dict[str, Any]:\n        \"\"\"Get query kwargs.\n\n        Also update with default arguments if not present.\n\n        \"\"\"\n        query_kwargs = {k: v for k, v in config.query_kwargs.items()}\n        if \"prompt_helper\" not in query_kwargs:\n            query_kwargs[\"prompt_helper\"] = self._prompt_helper\n        if \"llm_predictor\" not in query_kwargs:\n            query_kwargs[\"llm_predictor\"] = self._llm_predictor\n        if \"embed_model\" not in query_kwargs:\n            query_kwargs[\"embed_model\"] = self._embed_model\n        return query_kwargs\n\n    def query(self, query_str: str, index_struct: IndexStruct) -> Response:\n        \"\"\"Run query.\"\"\"\n        index_struct_type = index_struct.get_type()\n        if index_struct_type not in self._config_dict:\n            config = QueryConfig(\n                index_struct_type=index_struct_type, query_mode=QueryMode.DEFAULT\n            )\n        else:\n            config = self._config_dict[index_struct_type]\n        mode = config.query_mode\n\n        query_cls = self._index_registry.type_to_query[index_struct_type][mode]\n        # if recursive, pass self as query_runner to each individual query\n        query_runner = self if self._recursive else None\n        query_kwargs = self._get_query_kwargs(config)\n        query_obj = query_cls(\n            index_struct,\n            **query_kwargs,\n            query_runner=query_runner,\n            docstore=self._docstore,\n        )\n\n        return query_obj.query(query_str)\n", "doc_id": "266fe6e986b73943246d77bc2f38623ad4ca18c7", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/query_runner.py", "file_name": "query_runner.py"}, "__type__": "Document"}, "d7ed8486721f6f32f5c8ed69fbdce569a77b5ed0": {"text": "\"\"\"Query Configuration Schema.\n\nThis schema is used under the hood for all queries, but is primarily\nexposed for recursive queries over composable indices.\n\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Any, Dict\n\nfrom dataclasses_json import DataClassJsonMixin\n\n\nclass QueryMode(str, Enum):\n    \"\"\"Query mode enum.\n\n    Can be passed as the enum struct, or as the underlying string.\n\n    Attributes:\n        DEFAULT (\"default\"): Default query mode.\n        RETRIEVE (\"retrieve\"): Retrieve mode.\n        EMBEDDING (\"embedding\"): Embedding mode.\n        SUMMARIZE (\"summarize\"): Summarize mode. Used for hierarchical\n            summarization in the tree index.\n        SIMPLE (\"simple\"): Simple mode. Used for keyword extraction.\n        RAKE (\"rake\"): RAKE mode. Used for keyword extraction.\n        RECURSIVE (\"recursive\"): Recursive mode. Used to recursively query\n            over composed indices.\n\n    \"\"\"\n\n    DEFAULT = \"default\"\n    # a special \"retrieve\" query for tree index that retrieves that top nodes\n    RETRIEVE = \"retrieve\"\n    # embedding-based query\n    EMBEDDING = \"embedding\"\n\n    # to hierarchically summarize using tree\n    SUMMARIZE = \"summarize\"\n\n    # for keyword extractor\n    SIMPLE = \"simple\"\n    RAKE = \"rake\"\n\n    # recursive queries (composable queries)\n    # NOTE: deprecated\n    RECURSIVE = \"recursive\"\n\n    # for sql queries\n    SQL = \"sql\"\n\n\n@dataclass\nclass QueryConfig(DataClassJsonMixin):\n    \"\"\"Query config.\n\n    Used under the hood for all queries.\n    The user must explicitly specify a list of query config objects is passed during\n    a query call to define configurations for each individual subindex within an\n    overall composed index.\n\n    The user may choose to specify either the query config objects directly,\n    or as a list of JSON dictionaries. For instance, the following are equivalent:\n\n    .. code-block:: python\n\n        # using JSON dictionaries\n        query_configs = [\n            {\n                \"index_struct_type\": \"tree\",\n                \"query_mode\": \"default\",\n                \"query_kwargs\": {\n                    \"child_branch_factor\": 2\n                }\n            },\n            ...\n        ]\n        response = index.query(\n            \"<query_str>\", mode=\"recursive\", query_configs=query_configs\n        )\n\n    .. code-block:: python\n\n        query_configs = [\n            QueryConfig(\n                index_struct_type=IndexStructType.TREE,\n                query_mode=QueryMode.DEFAULT,\n                query_kwargs={\n                    \"child_branch_factor\": 2\n                }\n            )\n            ...\n        ]\n        response = index.query(\n            \"<query_str>\", mode=\"recursive\", query_configs=query_configs\n        )\n\n\n    Args:\n        index_struct_type (IndexStructType): The type of index struct.\n        query_mode (QueryMode): The query mode.\n        query_kwargs (Dict[str, Any], optional): The query kwargs. Defaults to {}.\n\n    \"\"\"\n\n    # index_struct_type: IndexStructType\n    index_struct_type: str\n    query_mode: QueryMode\n    query_kwargs: Dict[str, Any] = field(default_factory=dict)\n", "doc_id": "d7ed8486721f6f32f5c8ed69fbdce569a77b5ed0", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/schema.py", "file_name": "schema.py"}, "__type__": "Document"}, "c2931efb-92ef-4f0f-9916-ee27d69d4d2d": {"text": "\nThese documents do not have summaries.", "doc_id": "c2931efb-92ef-4f0f-9916-ee27d69d4d2d", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "node_info": null}, "1": {"text": "\"\"\"Base query classes.\"\"\"\n\nimport logging\nimport re\nfrom abc import abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Dict, Generic, List, Optional, Tuple, TypeVar, cast\n\nfrom gpt_index.data_structs.data_structs import IndexStruct, Node\nfrom gpt_index.docstore import DocumentStore\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.embeddings.openai import OpenAIEmbedding\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.response.builder import ResponseBuilder, ResponseMode, TextChunk\nfrom gpt_index.indices.utils import truncate_text\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.default_prompts import (\n    DEFAULT_REFINE_PROMPT,\n    DEFAULT_TEXT_QA_PROMPT,\n)\nfrom gpt_index.prompts.prompts import", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "2": {"text": "gpt_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\nfrom gpt_index.response.schema import Response\nfrom gpt_index.token_counter.token_counter import llm_token_counter\n\nIS = TypeVar(\"IS\", bound=IndexStruct)\n\n\n@dataclass\nclass BaseQueryRunner:\n    \"\"\"Base query runner.\"\"\"\n\n    @abstractmethod\n    def query(self, query: str, index_struct: IndexStruct) -> Response:\n        \"\"\"Schedule a query.\"\"\"\n        raise NotImplementedError(\"Not implemented yet.\")\n\n\nclass BaseGPTIndexQuery(Generic[IS]):\n    \"\"\"Base GPT Index Query.\n\n    Helper class that is used to query an index. Can be called within `query`\n    method of a BaseGPTIndex object, or instantiated independently.\n\n    Args:\n        llm_predictor (LLMPredictor): Optional LLMPredictor object. If not provided,\n            will use the default LLMPredictor", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "3": {"text": "        will use the default LLMPredictor (text-davinci-003)\n        prompt_helper (PromptHelper): Optional PromptHelper object. If not provided,\n            will use the default PromptHelper.\n        required_keywords (List[str]): Optional list of keywords that must be present\n            in nodes. Can be used to query most indices (tree index is an exception).\n        exclude_keywords (List[str]): Optional list of keywords that must not be\n            present in nodes. Can be used to query most indices (tree index is an\n            exception).\n        response_mode (ResponseMode): Optional ResponseMode. If not provided, will\n            use the default ResponseMode.\n        text_qa_template (QuestionAnswerPrompt): Optional QuestionAnswerPrompt object.\n            If not", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "4": {"text": "object.\n            If not provided, will use the default QuestionAnswerPrompt.\n        refine_template (RefinePrompt): Optional RefinePrompt object. If not provided,\n            will use the default RefinePrompt.\n        include_summary (bool): Optional bool. If True, will also use the summary\n            text of the index when generating a response (the summary text can be set\n            through `index.set_text(\"<text>\")`).\n        similarity_cutoff (float): Optional float. If set, will filter out nodes with\n            similarity below this cutoff threshold when computing the response\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IS,\n        # TODO: pass from superclass\n        llm_predictor: Optional[LLMPredictor] =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 4, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "5": {"text": " llm_predictor: Optional[LLMPredictor] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        docstore: Optional[DocumentStore] = None,\n        query_runner: Optional[BaseQueryRunner] = None,\n        required_keywords: Optional[List[str]] = None,\n        exclude_keywords: Optional[List[str]] = None,\n        response_mode: ResponseMode = ResponseMode.DEFAULT,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        refine_template: Optional[RefinePrompt] = None,\n        include_summary: bool = False,\n        response_kwargs: Optional[Dict] = None,\n        similarity_cutoff: Optional[float] = None,\n    ) -> None:\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 5, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "6": {"text": "= None,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        if index_struct is None:\n            raise ValueError(\"index_struct must be provided.\")\n        self._validate_index_struct(index_struct)\n        self._index_struct = index_struct\n        self._llm_predictor = llm_predictor or LLMPredictor()\n        # NOTE: the embed_model isn't used in all indices\n        self._embed_model = embed_model or OpenAIEmbedding()\n        self._docstore = docstore\n        self._query_runner = query_runner\n        # TODO: make this a required param\n        if prompt_helper is None:\n            raise ValueError(\"prompt_helper must be provided.\")\n        self._prompt_helper", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 6, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "7": {"text": "       self._prompt_helper = cast(PromptHelper, prompt_helper)\n\n        self._required_keywords = required_keywords\n        self._exclude_keywords = exclude_keywords\n        self._response_mode = ResponseMode(response_mode)\n\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        self.refine_template = refine_template or DEFAULT_REFINE_PROMPT\n        self._include_summary = include_summary\n\n        self._response_kwargs = response_kwargs or {}\n        self.response_builder = ResponseBuilder(\n            self._prompt_helper,\n            self._llm_predictor,\n            self.text_qa_template,\n            self.refine_template,\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 7, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "8": {"text": "        self.refine_template,\n        )\n\n        self.similarity_cutoff = similarity_cutoff\n\n    def _should_use_node(\n        self, node: Node, similarity_tracker: Optional[SimilarityTracker] = None\n    ) -> bool:\n        \"\"\"Run node through filters to determine if it should be used.\"\"\"\n        words = re.findall(r\"\\w+\", node.get_text())\n        if self._required_keywords is not None:\n            for w in self._required_keywords:\n                if w not in words:\n                    return False\n\n        if self._exclude_keywords is not None:\n            for w in self._exclude_keywords:\n             ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 8, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "9": {"text": "               if w in words:\n                    return False\n\n        sim_cutoff_exists = (\n            similarity_tracker is not None and self.similarity_cutoff is not None\n        )\n\n        if sim_cutoff_exists:\n            similarity = cast(SimilarityTracker, similarity_tracker).find(node)\n            if similarity is None:\n                return False\n            if cast(float, similarity) < cast(float, self.similarity_cutoff):\n                return False\n\n        return True\n\n    def _get_text_from_node(\n        self,\n        query_str: str,\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 9, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "10": {"text": "     query_str: str,\n        node: Node,\n        level: Optional[int] = None,\n    ) -> Tuple[TextChunk, Optional[Response]]:\n        \"\"\"Query a given node.\n\n        If node references a given document, then return the document.\n        If node references a given index, then query the index.\n\n        \"\"\"\n        level_str = \"\" if level is None else f\"[Level {level}]\"\n        fmt_text_chunk = truncate_text(node.get_text(), 50)\n        logging.debug(f\">{level_str} Searching in chunk: {fmt_text_chunk}\")\n\n        is_index_struct = False\n        # if self._query_runner is not None, assume we want to do a recursive\n        # query. In order to not perform a recursive query, make sure\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 10, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "11": {"text": "not perform a recursive query, make sure\n        # _query_runner is None.\n        if (\n            self._query_runner is not None\n            and node.ref_doc_id is not None\n            and self._docstore is not None\n        ):\n            doc = self._docstore.get_document(node.ref_doc_id, raise_error=True)\n            if isinstance(doc, IndexStruct):\n                is_index_struct = True\n\n        if is_index_struct:\n            query_runner = cast(BaseQueryRunner, self._query_runner)\n            response = query_runner.query(query_str, cast(IndexStruct, doc))\n            return TextChunk(str(response),", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 11, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "12": {"text": "       return TextChunk(str(response), is_answer=True), response\n        else:\n            text = node.get_text()\n            return TextChunk(text), None\n\n    @property\n    def index_struct(self) -> IS:\n        \"\"\"Get the index struct.\"\"\"\n        return self._index_struct\n\n    def _validate_index_struct(self, index_struct: IS) -> None:\n        \"\"\"Validate the index struct.\"\"\"\n        pass\n\n    def _give_response_for_nodes(\n        self, query_str: str, text_chunks: List[TextChunk]\n    ) -> str:\n        \"\"\"Give response for nodes.\"\"\"\n        self.response_builder.reset()\n        for text in text_chunks:\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 12, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "13": {"text": "           self.response_builder.add_text_chunks([text])\n        response = self.response_builder.get_response(\n            query_str,\n            mode=self._response_mode,\n            **self._response_kwargs,\n        )\n\n        return response or \"\"\n\n    def get_nodes_and_similarities_for_response(\n        self, query_str: str\n    ) -> List[Tuple[Node, Optional[float]]]:\n        \"\"\"Get list of tuples of node and similarity for response.\n\n        First part of the tuple is the node.\n        Second part of tuple is the distance from query to the node.\n        If not applicable, it's None.\n        \"\"\"\n        similarity_tracker = SimilarityTracker()\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 13, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "14": {"text": " similarity_tracker = SimilarityTracker()\n        nodes = self._get_nodes_for_response(\n            query_str, similarity_tracker=similarity_tracker\n        )\n        nodes = [\n            node for node in nodes if self._should_use_node(node, similarity_tracker)\n        ]\n\n        # TODO: create a `display` method to allow subclasses to print the Node\n        return similarity_tracker.get_zipped_nodes(nodes)\n\n    @abstractmethod\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n\n    def _query(self,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 14, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "15": {"text": "nodes for response.\"\"\"\n\n    def _query(self, query_str: str) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        # TODO: remove _query and just use query\n        tuples = self.get_nodes_and_similarities_for_response(query_str)\n        node_texts = []\n        for node, similarity in tuples:\n            text, response = self._get_text_from_node(query_str, node)\n            self.response_builder.add_node(node, similarity=similarity)\n            if response is not None:\n                # these are source nodes from within this node (when it's an index)\n                for source_node in response.source_nodes:\n                   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 15, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "16": {"text": "                self.response_builder.add_source_node(source_node)\n            node_texts.append(text)\n\n        if self._response_mode != ResponseMode.NO_TEXT:\n            response_str = self._give_response_for_nodes(query_str, node_texts)\n        else:\n            response_str = None\n\n        return Response(response_str, source_nodes=self.response_builder.get_sources())\n\n    @llm_token_counter(\"query\")\n    def query(self, query_str: str) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        response = self._query(query_str)\n        # if include_summary is True, then include summary text in answer\n        # summary text is set through `set_text` on the underlying index.\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 16, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "17": {"text": "on the underlying index.\n        # TODO: refactor response builder to be in the __init__\n        if self._response_mode != ResponseMode.NO_TEXT and self._include_summary:\n            response_builder = ResponseBuilder(\n                self._prompt_helper,\n                self._llm_predictor,\n                self.text_qa_template,\n                self.refine_template,\n                texts=[TextChunk(self._index_struct.get_text())],\n            )\n            # NOTE: use create and refine for now (default response mode)\n            response.response = response_builder.get_response(\n              ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 17, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "18": {"text": "               query_str,\n                mode=self._response_mode,\n                prev_response=response.response,\n            )\n\n        return response\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 18, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "19": {"text": "\"\"\"Embedding utils for queries.\"\"\"\n\nfrom typing import Dict, List, Optional, Tuple\n\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.embeddings.openai import BaseEmbedding\n\n\ndef get_top_k_embeddings(\n    embed_model: BaseEmbedding,\n    query_embedding: List[float],\n    embeddings: List[List[float]],\n    similarity_top_k: Optional[int] = None,\n    embedding_ids: Optional[List] = None,\n) -> Tuple[List[float], List]:\n    \"\"\"Get top nodes by similarity to the query.\"\"\"\n    if embedding_ids is None:\n        embedding_ids = [i for i in range(len(embeddings))]\n\n    similarities = []\n    for emb in embeddings:\n        similarity = embed_model.similarity(query_embedding, emb)\n        similarities.append(similarity)\n\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/embedding_utils.py", "file_name": "embedding_utils.py"}, "index": 19, "child_indices": [], "ref_doc_id": "f0bab417b4bf90ea4c52f882a52fbf041839974c", "node_info": null}, "20": {"text": "     similarities.append(similarity)\n\n    sorted_tups = sorted(\n        zip(similarities, embedding_ids), key=lambda x: x[0], reverse=True\n    )\n    similarity_top_k = similarity_top_k or len(sorted_tups)\n    result_tups = sorted_tups[:similarity_top_k]\n\n    result_similarities = [s for s, _ in result_tups]\n    result_ids = [n for _, n in result_tups]\n\n    return result_similarities, result_ids\n\n\nclass SimilarityTracker:\n    \"\"\"Helper class to manage node similarities during lifecycle of a single query.\"\"\"\n\n    # TODO: smarter way to store this information\n    lookup: Dict[str, float] = {}\n\n    def _hash(self, node: Node) -> str:\n        \"\"\"Generate a unique key for each node.\"\"\"\n        # TODO: Better way to get unique", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/embedding_utils.py", "file_name": "embedding_utils.py"}, "index": 20, "child_indices": [], "ref_doc_id": "f0bab417b4bf90ea4c52f882a52fbf041839974c", "node_info": null}, "21": {"text": "      # TODO: Better way to get unique identifier of a node\n        return str(abs(hash(node.get_text())))\n\n    def add(self, node: Node, similarity: float) -> None:\n        \"\"\"Add a node and its similarity score.\"\"\"\n        node_hash = self._hash(node)\n        self.lookup[node_hash] = similarity\n\n    def find(self, node: Node) -> Optional[float]:\n        \"\"\"Find a node's similarity score.\"\"\"\n        node_hash = self._hash(node)\n        if node_hash not in self.lookup:\n            return None\n        return self.lookup[node_hash]\n\n    def get_zipped_nodes(self, nodes: List[Node]) -> List[Tuple[Node, Optional[float]]]:\n        \"\"\"Get a zipped list of nodes and their", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/embedding_utils.py", "file_name": "embedding_utils.py"}, "index": 21, "child_indices": [], "ref_doc_id": "f0bab417b4bf90ea4c52f882a52fbf041839974c", "node_info": null}, "22": {"text": "     \"\"\"Get a zipped list of nodes and their corresponding scores.\"\"\"\n        similarities = [self.find(node) for node in nodes]\n        return list(zip(nodes, similarities))\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/embedding_utils.py", "file_name": "embedding_utils.py"}, "index": 22, "child_indices": [], "ref_doc_id": "f0bab417b4bf90ea4c52f882a52fbf041839974c", "node_info": null}, "23": {"text": "\"\"\"Query runner.\"\"\"\n\nfrom typing import Any, Dict, List, Optional, Union, cast\n\nfrom gpt_index.data_structs.data_structs import IndexStruct\nfrom gpt_index.docstore import DocumentStore\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.query.base import BaseQueryRunner\nfrom gpt_index.indices.query.schema import QueryConfig, QueryMode\nfrom gpt_index.indices.registry import IndexRegistry\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.response.schema import Response\n\n# TMP: refactor query config type\nQUERY_CONFIG_TYPE = Union[Dict, QueryConfig]\n\n\nclass QueryRunner(BaseQueryRunner):\n    \"\"\"Tool to take in a query request and perform a query with the right classes.\n\n    Higher-level wrapper over a given query.\n\n    \"\"\"\n\n    def __init__(\n        self,\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/query_runner.py", "file_name": "query_runner.py"}, "index": 23, "child_indices": [], "ref_doc_id": "266fe6e986b73943246d77bc2f38623ad4ca18c7", "node_info": null}, "24": {"text": "      self,\n        llm_predictor: LLMPredictor,\n        prompt_helper: PromptHelper,\n        embed_model: BaseEmbedding,\n        docstore: DocumentStore,\n        index_registry: IndexRegistry,\n        query_configs: Optional[List[QUERY_CONFIG_TYPE]] = None,\n        recursive: bool = False,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        config_dict: Dict[str, QueryConfig] = {}\n        if query_configs is None or len(query_configs) == 0:\n            query_config_objs: List[QueryConfig] = []\n        elif isinstance(query_configs[0], Dict):\n            query_config_objs = [\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/query_runner.py", "file_name": "query_runner.py"}, "index": 24, "child_indices": [], "ref_doc_id": "266fe6e986b73943246d77bc2f38623ad4ca18c7", "node_info": null}, "25": {"text": " query_config_objs = [\n                QueryConfig.from_dict(cast(Dict, qc)) for qc in query_configs\n            ]\n        else:\n            query_config_objs = [cast(QueryConfig, q) for q in query_configs]\n\n        for qc in query_config_objs:\n            config_dict[qc.index_struct_type] = qc\n\n        self._config_dict = config_dict\n        self._llm_predictor = llm_predictor\n        self._prompt_helper = prompt_helper\n        self._embed_model = embed_model\n        self._docstore = docstore\n        self._index_registry = index_registry\n        self._recursive = recursive\n\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/query_runner.py", "file_name": "query_runner.py"}, "index": 25, "child_indices": [], "ref_doc_id": "266fe6e986b73943246d77bc2f38623ad4ca18c7", "node_info": null}, "26": {"text": "      self._recursive = recursive\n\n    def _get_query_kwargs(self, config: QueryConfig) -> Dict[str, Any]:\n        \"\"\"Get query kwargs.\n\n        Also update with default arguments if not present.\n\n        \"\"\"\n        query_kwargs = {k: v for k, v in config.query_kwargs.items()}\n        if \"prompt_helper\" not in query_kwargs:\n            query_kwargs[\"prompt_helper\"] = self._prompt_helper\n        if \"llm_predictor\" not in query_kwargs:\n            query_kwargs[\"llm_predictor\"] = self._llm_predictor\n        if \"embed_model\" not in query_kwargs:\n            query_kwargs[\"embed_model\"] = self._embed_model\n        return", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/query_runner.py", "file_name": "query_runner.py"}, "index": 26, "child_indices": [], "ref_doc_id": "266fe6e986b73943246d77bc2f38623ad4ca18c7", "node_info": null}, "27": {"text": "= self._embed_model\n        return query_kwargs\n\n    def query(self, query_str: str, index_struct: IndexStruct) -> Response:\n        \"\"\"Run query.\"\"\"\n        index_struct_type = index_struct.get_type()\n        if index_struct_type not in self._config_dict:\n            config = QueryConfig(\n                index_struct_type=index_struct_type, query_mode=QueryMode.DEFAULT\n            )\n        else:\n            config = self._config_dict[index_struct_type]\n        mode = config.query_mode\n\n        query_cls = self._index_registry.type_to_query[index_struct_type][mode]\n        # if recursive, pass self as query_runner to each individual query\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/query_runner.py", "file_name": "query_runner.py"}, "index": 27, "child_indices": [], "ref_doc_id": "266fe6e986b73943246d77bc2f38623ad4ca18c7", "node_info": null}, "28": {"text": "self as query_runner to each individual query\n        query_runner = self if self._recursive else None\n        query_kwargs = self._get_query_kwargs(config)\n        query_obj = query_cls(\n            index_struct,\n            **query_kwargs,\n            query_runner=query_runner,\n            docstore=self._docstore,\n        )\n\n        return query_obj.query(query_str)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/query_runner.py", "file_name": "query_runner.py"}, "index": 28, "child_indices": [], "ref_doc_id": "266fe6e986b73943246d77bc2f38623ad4ca18c7", "node_info": null}, "29": {"text": "\"\"\"Query Configuration Schema.\n\nThis schema is used under the hood for all queries, but is primarily\nexposed for recursive queries over composable indices.\n\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Any, Dict\n\nfrom dataclasses_json import DataClassJsonMixin\n\n\nclass QueryMode(str, Enum):\n    \"\"\"Query mode enum.\n\n    Can be passed as the enum struct, or as the underlying string.\n\n    Attributes:\n        DEFAULT (\"default\"): Default query mode.\n        RETRIEVE (\"retrieve\"): Retrieve mode.\n        EMBEDDING (\"embedding\"): Embedding mode.\n        SUMMARIZE (\"summarize\"): Summarize mode. Used for hierarchical\n            summarization in the tree index.\n        SIMPLE (\"simple\"): Simple mode. Used for keyword extraction.\n        RAKE (\"rake\"): RAKE mode. Used for", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/schema.py", "file_name": "schema.py"}, "index": 29, "child_indices": [], "ref_doc_id": "d7ed8486721f6f32f5c8ed69fbdce569a77b5ed0", "node_info": null}, "30": {"text": "    RAKE (\"rake\"): RAKE mode. Used for keyword extraction.\n        RECURSIVE (\"recursive\"): Recursive mode. Used to recursively query\n            over composed indices.\n\n    \"\"\"\n\n    DEFAULT = \"default\"\n    # a special \"retrieve\" query for tree index that retrieves that top nodes\n    RETRIEVE = \"retrieve\"\n    # embedding-based query\n    EMBEDDING = \"embedding\"\n\n    # to hierarchically summarize using tree\n    SUMMARIZE = \"summarize\"\n\n    # for keyword extractor\n    SIMPLE = \"simple\"\n    RAKE = \"rake\"\n\n    # recursive queries (composable queries)\n    # NOTE: deprecated\n    RECURSIVE = \"recursive\"\n\n    # for sql queries\n    SQL = \"sql\"\n\n\n@dataclass\nclass QueryConfig(DataClassJsonMixin):\n    \"\"\"Query config.\n\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/schema.py", "file_name": "schema.py"}, "index": 30, "child_indices": [], "ref_doc_id": "d7ed8486721f6f32f5c8ed69fbdce569a77b5ed0", "node_info": null}, "31": {"text": "   \"\"\"Query config.\n\n    Used under the hood for all queries.\n    The user must explicitly specify a list of query config objects is passed during\n    a query call to define configurations for each individual subindex within an\n    overall composed index.\n\n    The user may choose to specify either the query config objects directly,\n    or as a list of JSON dictionaries. For instance, the following are equivalent:\n\n    .. code-block:: python\n\n        # using JSON dictionaries\n        query_configs = [\n            {\n                \"index_struct_type\": \"tree\",\n                \"query_mode\": \"default\",\n                \"query_kwargs\": {\n                    \"child_branch_factor\": 2\n                }\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/schema.py", "file_name": "schema.py"}, "index": 31, "child_indices": [], "ref_doc_id": "d7ed8486721f6f32f5c8ed69fbdce569a77b5ed0", "node_info": null}, "32": {"text": "             }\n            },\n            ...\n        ]\n        response = index.query(\n            \"<query_str>\", mode=\"recursive\", query_configs=query_configs\n        )\n\n    .. code-block:: python\n\n        query_configs = [\n            QueryConfig(\n                index_struct_type=IndexStructType.TREE,\n                query_mode=QueryMode.DEFAULT,\n                query_kwargs={\n                    \"child_branch_factor\": 2\n                }\n            )\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/schema.py", "file_name": "schema.py"}, "index": 32, "child_indices": [], "ref_doc_id": "d7ed8486721f6f32f5c8ed69fbdce569a77b5ed0", "node_info": null}, "33": {"text": "        )\n            ...\n        ]\n        response = index.query(\n            \"<query_str>\", mode=\"recursive\", query_configs=query_configs\n        )\n\n\n    Args:\n        index_struct_type (IndexStructType): The type of index struct.\n        query_mode (QueryMode): The query mode.\n        query_kwargs (Dict[str, Any], optional): The query kwargs. Defaults to {}.\n\n    \"\"\"\n\n    # index_struct_type: IndexStructType\n    index_struct_type: str\n    query_mode: QueryMode\n    query_kwargs: Dict[str, Any] = field(default_factory=dict)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/schema.py", "file_name": "schema.py"}, "index": 33, "child_indices": [], "ref_doc_id": "d7ed8486721f6f32f5c8ed69fbdce569a77b5ed0", "node_info": null}, "34": {"text": "This code file is the base query class for the GPT Index. It is used to query an index and can be called within the query method of a BaseGPTIndex object, or instantiated independently. It takes in parameters such as an LLMPredictor, PromptHelper, embed_model, docstore, query_runner, required_keywords, exclude_keywords, response_mode, text_qa_template, refine_template, include_summary, response_kwargs, and similarity_cutoff. It also has methods such as _should_use_node and _get_text_from_node which are used to filter out nodes and query a given node respectively.", "doc_id": null, "embedding": null, "extra_info": null, "index": 34, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "35": {"text": "The file base.py from the gpt_index/indices/query/ directory is a Python file that contains the code for the BaseQueryRunner class. This class is used to answer queries and provide responses. It contains methods to validate the index struct, get nodes and similarities for response, query, and get top k embeddings. It also contains a SimilarityTracker class which is used to manage node similarities during the lifecycle of a single query. The QueryRunner class is a higher-level wrapper over a given query and is used to take in a query request and perform a query with the right classes. It contains methods to initialize the query runner, query, and get the response.", "doc_id": null, "embedding": null, "extra_info": null, "index": 35, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "36": {"text": "query_runner.py is a file in the gpt_index/indices/query/ directory that is used to run queries on a given index. It takes in a list of query configurations, which are defined in the schema.py file. It then uses the query configurations to create a query object, which is then used to run the query. The query object can be configured to use a prompt helper, llm predictor, embed model, and docstore. It also supports recursive queries, which can be used to query over composed indices.", "doc_id": null, "embedding": null, "extra_info": null, "index": 36, "child_indices": [32, 33, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"34": {"text": "This code file is the base query class for the GPT Index. It is used to query an index and can be called within the query method of a BaseGPTIndex object, or instantiated independently. It takes in parameters such as an LLMPredictor, PromptHelper, embed_model, docstore, query_runner, required_keywords, exclude_keywords, response_mode, text_qa_template, refine_template, include_summary, response_kwargs, and similarity_cutoff. It also has methods such as _should_use_node and _get_text_from_node which are used to filter out nodes and query a given node respectively.", "doc_id": null, "embedding": null, "extra_info": null, "index": 34, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "35": {"text": "The file base.py from the gpt_index/indices/query/ directory is a Python file that contains the code for the BaseQueryRunner class. This class is used to answer queries and provide responses. It contains methods to validate the index struct, get nodes and similarities for response, query, and get top k embeddings. It also contains a SimilarityTracker class which is used to manage node similarities during the lifecycle of a single query. The QueryRunner class is a higher-level wrapper over a given query and is used to take in a query request and perform a query with the right classes. It contains methods to initialize the query runner, query, and get the response.", "doc_id": null, "embedding": null, "extra_info": null, "index": 35, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "36": {"text": "query_runner.py is a file in the gpt_index/indices/query/ directory that is used to run queries on a given index. It takes in a list of query configurations, which are defined in the schema.py file. It then uses the query configurations to create a query object, which is then used to run the query. The query object can be configured to use a prompt helper, llm predictor, embed model, and docstore. It also supports recursive queries, which can be used to query over composed indices.", "doc_id": null, "embedding": null, "extra_info": null, "index": 36, "child_indices": [32, 33, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}