{"index_struct": {"text": "\nThese documents provide a response builder class that enables users to generate responses to queries using the GPT indexing system. The response builder class supports different modes, including stuffing chunks into prompt, creating and refining separately over each chunk, and tree summarization. It also provides functions for adding text chunks, resetting, adding nodes, getting sources, refining responses, and giving responses.", "doc_id": "2f7094ac-300c-43a6-ad85-1790c25509e0", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "node_info": null}, "1": {"text": "\"\"\"Response builder class.\n\nThis class provides general functions for taking in a set of text\nand generating a response.\n\nWill support different modes, from 1) stuffing chunks into prompt,\n2) create and refine separately over each chunk, 3) tree summarization.\n\n\"\"\"\nimport logging\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional\n\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.indices.common.tree.base import GPTTreeIndexBuilder\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.utils import get_sorted_node_list, truncate_text\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt, SummaryPrompt\nfrom gpt_index.response.schema import SourceNode\nfrom gpt_index.utils import temp_set_attrs\n\n\nclass ResponseMode(str, Enum):\n    \"\"\"Response", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 1, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "2": {"text": "ResponseMode(str, Enum):\n    \"\"\"Response modes.\"\"\"\n\n    DEFAULT = \"default\"\n    COMPACT = \"compact\"\n    TREE_SUMMARIZE = \"tree_summarize\"\n    NO_TEXT = \"no_text\"\n\n\n@dataclass\nclass TextChunk:\n    \"\"\"Response chunk.\"\"\"\n\n    text: str\n    # Whether this chunk is already a response\n    is_answer: bool = False\n\n\nclass ResponseBuilder:\n    \"\"\"Response builder class.\"\"\"\n\n    def __init__(\n        self,\n        prompt_helper: PromptHelper,\n        llm_predictor: LLMPredictor,\n        text_qa_template: QuestionAnswerPrompt,\n        refine_template: RefinePrompt,\n        texts: Optional[List[TextChunk]] = None,\n        nodes: Optional[List[Node]] = None,\n    ) -> None:\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 2, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "3": {"text": "= None,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self.prompt_helper = prompt_helper\n        self.llm_predictor = llm_predictor\n        self.text_qa_template = text_qa_template\n        self.refine_template = refine_template\n        self._texts = texts or []\n        nodes = nodes or []\n        self.source_nodes: List[SourceNode] = SourceNode.from_nodes(nodes)\n\n    def add_text_chunks(self, text_chunks: List[TextChunk]) -> None:\n        \"\"\"Add text chunk.\"\"\"\n        self._texts.extend(text_chunks)\n\n    def reset(self) -> None:\n        \"\"\"Clear text chunks.\"\"\"\n        self._texts = []\n\n    def", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 3, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "4": {"text": "     self._texts = []\n\n    def add_node(self, node: Node, similarity: Optional[float] = None) -> None:\n        \"\"\"Add node.\"\"\"\n        self.source_nodes.append(SourceNode.from_node(node, similarity=similarity))\n\n    def add_source_node(self, source_node: SourceNode) -> None:\n        \"\"\"Add source node directly.\"\"\"\n        self.source_nodes.append(source_node)\n\n    def get_sources(self) -> List[SourceNode]:\n        \"\"\"Get sources.\"\"\"\n        return self.source_nodes\n\n    def refine_response_single(\n        self,\n        response: str,\n        query_str: str,\n        text_chunk: str,\n    ) -> str:\n        \"\"\"Refine response.\"\"\"\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 4, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "5": {"text": "  \"\"\"Refine response.\"\"\"\n        fmt_text_chunk = truncate_text(text_chunk, 50)\n        logging.debug(f\"> Refine context: {fmt_text_chunk}\")\n        # NOTE: partial format refine template with query_str and existing_answer here\n        refine_template = self.refine_template.partial_format(\n            query_str=query_str, existing_answer=response\n        )\n        refine_text_splitter = self.prompt_helper.get_text_splitter_given_prompt(\n            refine_template, 1\n        )\n        text_chunks = refine_text_splitter.split_text(text_chunk)\n        for cur_text_chunk in text_chunks:\n            response, _ =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 5, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "6": {"text": "           response, _ = self.llm_predictor.predict(\n                refine_template,\n                context_msg=cur_text_chunk,\n            )\n            logging.debug(f\"> Refined response: {response}\")\n        return response\n\n    def give_response_single(\n        self,\n        query_str: str,\n        text_chunk: str,\n    ) -> str:\n        \"\"\"Give response given a query and a corresponding text chunk.\"\"\"\n        text_qa_template = self.text_qa_template.partial_format(query_str=query_str)\n        qa_text_splitter = self.prompt_helper.get_text_splitter_given_prompt(\n          ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 6, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "7": {"text": "           text_qa_template, 1\n        )\n        text_chunks = qa_text_splitter.split_text(text_chunk)\n        response = None\n        # TODO: consolidate with loop in get_response_default\n        for cur_text_chunk in text_chunks:\n            if response is None:\n                response, _ = self.llm_predictor.predict(\n                    text_qa_template,\n                    context_str=cur_text_chunk,\n                )\n                logging.debug(f\"> Initial response: {response}\")\n            else:\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 7, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "8": {"text": "      else:\n                response = self.refine_response_single(\n                    response,\n                    query_str,\n                    cur_text_chunk,\n                )\n        return response or \"\"\n\n    def get_response_over_chunks(\n        self,\n        query_str: str,\n        text_chunks: List[TextChunk],\n        prev_response: Optional[str] = None,\n    ) -> str:\n        \"\"\"Give response over chunks.\"\"\"\n        response = None\n        for text_chunk in text_chunks:\n            if prev_response is None:\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 8, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "9": {"text": "        if prev_response is None:\n                # if this is the first chunk, and text chunk already\n                # is an answer, then return it\n                if text_chunk.is_answer:\n                    response = text_chunk.text\n                # otherwise give response\n                else:\n                    response = self.give_response_single(\n                        query_str,\n                        text_chunk.text,\n                    )\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 9, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "10": {"text": "   )\n            else:\n                response = self.refine_response_single(\n                    prev_response, query_str, text_chunk.text\n                )\n            prev_response = response\n        return response or \"Empty Response\"\n\n    def _get_response_default(\n        self, query_str: str, prev_response: Optional[str]\n    ) -> str:\n        return self.get_response_over_chunks(\n            query_str, self._texts, prev_response=prev_response\n        )\n\n    def _get_response_compact(\n        self, query_str: str, prev_response: Optional[str]\n    ) -> str:\n        \"\"\"Get compact", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 10, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "11": {"text": ") -> str:\n        \"\"\"Get compact response.\"\"\"\n        # use prompt helper to fix compact text_chunks under the prompt limitation\n        max_prompt = self.prompt_helper.get_biggest_prompt(\n            [self.text_qa_template, self.refine_template]\n        )\n        with temp_set_attrs(self.prompt_helper, use_chunk_size_limit=False):\n            new_texts = self.prompt_helper.compact_text_chunks(\n                max_prompt, [t.text for t in self._texts]\n            )\n            new_text_chunks = [TextChunk(text=t) for t in new_texts]\n            response =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 11, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "12": {"text": "           response = self.get_response_over_chunks(\n                query_str, new_text_chunks, prev_response=prev_response\n            )\n        return response\n\n    def _get_response_tree_summarize(\n        self,\n        query_str: str,\n        prev_response: Optional[str],\n        num_children: int = 10,\n    ) -> str:\n        \"\"\"Get tree summarize response.\"\"\"\n        text_qa_template = self.text_qa_template.partial_format(query_str=query_str)\n        summary_template = SummaryPrompt.from_prompt(text_qa_template)\n\n        # first join all the text chunks into a single text\n        all_text = \"\\n\\n\".join([t.text for t in", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 12, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "13": {"text": "= \"\\n\\n\".join([t.text for t in self._texts])\n        # then get text splitter\n        text_splitter = self.prompt_helper.get_text_splitter_given_prompt(\n            summary_template, num_children\n        )\n        text_chunks = text_splitter.split_text(all_text)\n        all_nodes: Dict[int, Node] = {\n            i: Node(text=t) for i, t in enumerate(text_chunks)\n        }\n\n        index_builder = GPTTreeIndexBuilder(\n            num_children,\n            summary_template,\n            self.llm_predictor,\n            self.prompt_helper,\n        )\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 13, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "14": {"text": "       )\n        root_nodes = index_builder.build_index_from_nodes(all_nodes, all_nodes)\n        node_list = get_sorted_node_list(root_nodes)\n        node_text = self.prompt_helper.get_text_from_nodes(\n            node_list, prompt=text_qa_template\n        )\n        response = self.get_response_over_chunks(\n            query_str,\n            [TextChunk(node_text)],\n            prev_response=prev_response,\n        )\n        return response or \"Empty Response\"\n\n    def get_response(\n        self,\n        query_str: str,\n        prev_response: Optional[str] =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 14, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "15": {"text": "       prev_response: Optional[str] = None,\n        mode: ResponseMode = ResponseMode.DEFAULT,\n        **response_kwargs: Any,\n    ) -> str:\n        \"\"\"Get response.\"\"\"\n        if mode == ResponseMode.DEFAULT:\n            return self._get_response_default(query_str, prev_response)\n        elif mode == ResponseMode.COMPACT:\n            return self._get_response_compact(query_str, prev_response)\n        elif mode == ResponseMode.TREE_SUMMARIZE:\n            return self._get_response_tree_summarize(\n                query_str, prev_response, **response_kwargs\n            )\n        else:\n            raise", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 15, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "16": {"text": " else:\n            raise ValueError(f\"Invalid mode: {mode}\")\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 16, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "17": {"text": "This code file contains the ResponseBuilder class, which provides general functions for taking in a set of text and generating a response. It supports different modes, such as stuffing chunks into prompt, creating and refining separately over each chunk, and tree summarization. It has functions for adding text chunks, resetting, adding nodes, getting sources, refining response, giving response, and getting response over chunks. It also has functions for getting response in default and compact modes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 17, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "18": {"text": "The file builder.py is part of the GPT indexing system. It contains functions that are used to generate responses to queries. The main function, get_response(), takes a query string and a response mode as parameters and returns a response. Depending on the response mode, the function calls one of three other functions to generate the response: _get_response_default(), _get_response_compact(), or _get_response_tree_summarize(). These functions use the text_qa_template, SummaryPrompt, GPTTreeIndexBuilder, and prompt_helper to generate the response.", "doc_id": null, "embedding": null, "extra_info": null, "index": 18, "child_indices": [12, 13, 14, 15, 16], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"17": {"text": "This code file contains the ResponseBuilder class, which provides general functions for taking in a set of text and generating a response. It supports different modes, such as stuffing chunks into prompt, creating and refining separately over each chunk, and tree summarization. It has functions for adding text chunks, resetting, adding nodes, getting sources, refining response, giving response, and getting response over chunks. It also has functions for getting response in default and compact modes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 17, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "18": {"text": "The file builder.py is part of the GPT indexing system. It contains functions that are used to generate responses to queries. The main function, get_response(), takes a query string and a response mode as parameters and returns a response. Depending on the response mode, the function calls one of three other functions to generate the response: _get_response_default(), _get_response_compact(), or _get_response_tree_summarize(). These functions use the text_qa_template, SummaryPrompt, GPTTreeIndexBuilder, and prompt_helper to generate the response.", "doc_id": null, "embedding": null, "extra_info": null, "index": 18, "child_indices": [12, 13, 14, 15, 16], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"1d4640565ae2765d9ca96a509dc9809217f62f2f": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "6b5adc8eec796f223f385f4911e32df83efc6785": {"text": "\"\"\"Response builder class.\n\nThis class provides general functions for taking in a set of text\nand generating a response.\n\nWill support different modes, from 1) stuffing chunks into prompt,\n2) create and refine separately over each chunk, 3) tree summarization.\n\n\"\"\"\nimport logging\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional\n\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.indices.common.tree.base import GPTTreeIndexBuilder\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.utils import get_sorted_node_list, truncate_text\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt, SummaryPrompt\nfrom gpt_index.response.schema import SourceNode\nfrom gpt_index.utils import temp_set_attrs\n\n\nclass ResponseMode(str, Enum):\n    \"\"\"Response modes.\"\"\"\n\n    DEFAULT = \"default\"\n    COMPACT = \"compact\"\n    TREE_SUMMARIZE = \"tree_summarize\"\n    NO_TEXT = \"no_text\"\n\n\n@dataclass\nclass TextChunk:\n    \"\"\"Response chunk.\"\"\"\n\n    text: str\n    # Whether this chunk is already a response\n    is_answer: bool = False\n\n\nclass ResponseBuilder:\n    \"\"\"Response builder class.\"\"\"\n\n    def __init__(\n        self,\n        prompt_helper: PromptHelper,\n        llm_predictor: LLMPredictor,\n        text_qa_template: QuestionAnswerPrompt,\n        refine_template: RefinePrompt,\n        texts: Optional[List[TextChunk]] = None,\n        nodes: Optional[List[Node]] = None,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self.prompt_helper = prompt_helper\n        self.llm_predictor = llm_predictor\n        self.text_qa_template = text_qa_template\n        self.refine_template = refine_template\n        self._texts = texts or []\n        nodes = nodes or []\n        self.source_nodes: List[SourceNode] = SourceNode.from_nodes(nodes)\n\n    def add_text_chunks(self, text_chunks: List[TextChunk]) -> None:\n        \"\"\"Add text chunk.\"\"\"\n        self._texts.extend(text_chunks)\n\n    def reset(self) -> None:\n        \"\"\"Clear text chunks.\"\"\"\n        self._texts = []\n\n    def add_node(self, node: Node, similarity: Optional[float] = None) -> None:\n        \"\"\"Add node.\"\"\"\n        self.source_nodes.append(SourceNode.from_node(node, similarity=similarity))\n\n    def add_source_node(self, source_node: SourceNode) -> None:\n        \"\"\"Add source node directly.\"\"\"\n        self.source_nodes.append(source_node)\n\n    def get_sources(self) -> List[SourceNode]:\n        \"\"\"Get sources.\"\"\"\n        return self.source_nodes\n\n    def refine_response_single(\n        self,\n        response: str,\n        query_str: str,\n        text_chunk: str,\n    ) -> str:\n        \"\"\"Refine response.\"\"\"\n        fmt_text_chunk = truncate_text(text_chunk, 50)\n        logging.debug(f\"> Refine context: {fmt_text_chunk}\")\n        # NOTE: partial format refine template with query_str and existing_answer here\n        refine_template = self.refine_template.partial_format(\n            query_str=query_str, existing_answer=response\n        )\n        refine_text_splitter = self.prompt_helper.get_text_splitter_given_prompt(\n            refine_template, 1\n        )\n        text_chunks = refine_text_splitter.split_text(text_chunk)\n        for cur_text_chunk in text_chunks:\n            response, _ = self.llm_predictor.predict(\n                refine_template,\n                context_msg=cur_text_chunk,\n            )\n            logging.debug(f\"> Refined response: {response}\")\n        return response\n\n    def give_response_single(\n        self,\n        query_str: str,\n        text_chunk: str,\n    ) -> str:\n        \"\"\"Give response given a query and a corresponding text chunk.\"\"\"\n        text_qa_template = self.text_qa_template.partial_format(query_str=query_str)\n        qa_text_splitter = self.prompt_helper.get_text_splitter_given_prompt(\n            text_qa_template, 1\n        )\n        text_chunks = qa_text_splitter.split_text(text_chunk)\n        response = None\n        # TODO: consolidate with loop in get_response_default\n        for cur_text_chunk in text_chunks:\n            if response is None:\n                response, _ = self.llm_predictor.predict(\n                    text_qa_template,\n                    context_str=cur_text_chunk,\n                )\n                logging.debug(f\"> Initial response: {response}\")\n            else:\n                response = self.refine_response_single(\n                    response,\n                    query_str,\n                    cur_text_chunk,\n                )\n        return response or \"\"\n\n    def get_response_over_chunks(\n        self,\n        query_str: str,\n        text_chunks: List[TextChunk],\n        prev_response: Optional[str] = None,\n    ) -> str:\n        \"\"\"Give response over chunks.\"\"\"\n        response = None\n        for text_chunk in text_chunks:\n            if prev_response is None:\n                # if this is the first chunk, and text chunk already\n                # is an answer, then return it\n                if text_chunk.is_answer:\n                    response = text_chunk.text\n                # otherwise give response\n                else:\n                    response = self.give_response_single(\n                        query_str,\n                        text_chunk.text,\n                    )\n            else:\n                response = self.refine_response_single(\n                    prev_response, query_str, text_chunk.text\n                )\n            prev_response = response\n        return response or \"Empty Response\"\n\n    def _get_response_default(\n        self, query_str: str, prev_response: Optional[str]\n    ) -> str:\n        return self.get_response_over_chunks(\n            query_str, self._texts, prev_response=prev_response\n        )\n\n    def _get_response_compact(\n        self, query_str: str, prev_response: Optional[str]\n    ) -> str:\n        \"\"\"Get compact response.\"\"\"\n        # use prompt helper to fix compact text_chunks under the prompt limitation\n        max_prompt = self.prompt_helper.get_biggest_prompt(\n            [self.text_qa_template, self.refine_template]\n        )\n        with temp_set_attrs(self.prompt_helper, use_chunk_size_limit=False):\n            new_texts = self.prompt_helper.compact_text_chunks(\n                max_prompt, [t.text for t in self._texts]\n            )\n            new_text_chunks = [TextChunk(text=t) for t in new_texts]\n            response = self.get_response_over_chunks(\n                query_str, new_text_chunks, prev_response=prev_response\n            )\n        return response\n\n    def _get_response_tree_summarize(\n        self,\n        query_str: str,\n        prev_response: Optional[str],\n        num_children: int = 10,\n    ) -> str:\n        \"\"\"Get tree summarize response.\"\"\"\n        text_qa_template = self.text_qa_template.partial_format(query_str=query_str)\n        summary_template = SummaryPrompt.from_prompt(text_qa_template)\n\n        # first join all the text chunks into a single text\n        all_text = \"\\n\\n\".join([t.text for t in self._texts])\n        # then get text splitter\n        text_splitter = self.prompt_helper.get_text_splitter_given_prompt(\n            summary_template, num_children\n        )\n        text_chunks = text_splitter.split_text(all_text)\n        all_nodes: Dict[int, Node] = {\n            i: Node(text=t) for i, t in enumerate(text_chunks)\n        }\n\n        index_builder = GPTTreeIndexBuilder(\n            num_children,\n            summary_template,\n            self.llm_predictor,\n            self.prompt_helper,\n        )\n        root_nodes = index_builder.build_index_from_nodes(all_nodes, all_nodes)\n        node_list = get_sorted_node_list(root_nodes)\n        node_text = self.prompt_helper.get_text_from_nodes(\n            node_list, prompt=text_qa_template\n        )\n        response = self.get_response_over_chunks(\n            query_str,\n            [TextChunk(node_text)],\n            prev_response=prev_response,\n        )\n        return response or \"Empty Response\"\n\n    def get_response(\n        self,\n        query_str: str,\n        prev_response: Optional[str] = None,\n        mode: ResponseMode = ResponseMode.DEFAULT,\n        **response_kwargs: Any,\n    ) -> str:\n        \"\"\"Get response.\"\"\"\n        if mode == ResponseMode.DEFAULT:\n            return self._get_response_default(query_str, prev_response)\n        elif mode == ResponseMode.COMPACT:\n            return self._get_response_compact(query_str, prev_response)\n        elif mode == ResponseMode.TREE_SUMMARIZE:\n            return self._get_response_tree_summarize(\n                query_str, prev_response, **response_kwargs\n            )\n        else:\n            raise ValueError(f\"Invalid mode: {mode}\")\n", "doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "__type__": "Document"}, "2f7094ac-300c-43a6-ad85-1790c25509e0": {"text": "\nThese documents provide a response builder class that enables users to generate responses to queries using the GPT indexing system. The response builder class supports different modes, including stuffing chunks into prompt, creating and refining separately over each chunk, and tree summarization. It also provides functions for adding text chunks, resetting, adding nodes, getting sources, refining responses, and giving responses.", "doc_id": "2f7094ac-300c-43a6-ad85-1790c25509e0", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "node_info": null}, "1": {"text": "\"\"\"Response builder class.\n\nThis class provides general functions for taking in a set of text\nand generating a response.\n\nWill support different modes, from 1) stuffing chunks into prompt,\n2) create and refine separately over each chunk, 3) tree summarization.\n\n\"\"\"\nimport logging\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional\n\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.indices.common.tree.base import GPTTreeIndexBuilder\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.utils import get_sorted_node_list, truncate_text\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt, SummaryPrompt\nfrom gpt_index.response.schema import SourceNode\nfrom gpt_index.utils import temp_set_attrs\n\n\nclass ResponseMode(str, Enum):\n    \"\"\"Response", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 1, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "2": {"text": "ResponseMode(str, Enum):\n    \"\"\"Response modes.\"\"\"\n\n    DEFAULT = \"default\"\n    COMPACT = \"compact\"\n    TREE_SUMMARIZE = \"tree_summarize\"\n    NO_TEXT = \"no_text\"\n\n\n@dataclass\nclass TextChunk:\n    \"\"\"Response chunk.\"\"\"\n\n    text: str\n    # Whether this chunk is already a response\n    is_answer: bool = False\n\n\nclass ResponseBuilder:\n    \"\"\"Response builder class.\"\"\"\n\n    def __init__(\n        self,\n        prompt_helper: PromptHelper,\n        llm_predictor: LLMPredictor,\n        text_qa_template: QuestionAnswerPrompt,\n        refine_template: RefinePrompt,\n        texts: Optional[List[TextChunk]] = None,\n        nodes: Optional[List[Node]] = None,\n    ) -> None:\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 2, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "3": {"text": "= None,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self.prompt_helper = prompt_helper\n        self.llm_predictor = llm_predictor\n        self.text_qa_template = text_qa_template\n        self.refine_template = refine_template\n        self._texts = texts or []\n        nodes = nodes or []\n        self.source_nodes: List[SourceNode] = SourceNode.from_nodes(nodes)\n\n    def add_text_chunks(self, text_chunks: List[TextChunk]) -> None:\n        \"\"\"Add text chunk.\"\"\"\n        self._texts.extend(text_chunks)\n\n    def reset(self) -> None:\n        \"\"\"Clear text chunks.\"\"\"\n        self._texts = []\n\n    def", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 3, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "4": {"text": "     self._texts = []\n\n    def add_node(self, node: Node, similarity: Optional[float] = None) -> None:\n        \"\"\"Add node.\"\"\"\n        self.source_nodes.append(SourceNode.from_node(node, similarity=similarity))\n\n    def add_source_node(self, source_node: SourceNode) -> None:\n        \"\"\"Add source node directly.\"\"\"\n        self.source_nodes.append(source_node)\n\n    def get_sources(self) -> List[SourceNode]:\n        \"\"\"Get sources.\"\"\"\n        return self.source_nodes\n\n    def refine_response_single(\n        self,\n        response: str,\n        query_str: str,\n        text_chunk: str,\n    ) -> str:\n        \"\"\"Refine response.\"\"\"\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 4, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "5": {"text": "  \"\"\"Refine response.\"\"\"\n        fmt_text_chunk = truncate_text(text_chunk, 50)\n        logging.debug(f\"> Refine context: {fmt_text_chunk}\")\n        # NOTE: partial format refine template with query_str and existing_answer here\n        refine_template = self.refine_template.partial_format(\n            query_str=query_str, existing_answer=response\n        )\n        refine_text_splitter = self.prompt_helper.get_text_splitter_given_prompt(\n            refine_template, 1\n        )\n        text_chunks = refine_text_splitter.split_text(text_chunk)\n        for cur_text_chunk in text_chunks:\n            response, _ =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 5, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "6": {"text": "           response, _ = self.llm_predictor.predict(\n                refine_template,\n                context_msg=cur_text_chunk,\n            )\n            logging.debug(f\"> Refined response: {response}\")\n        return response\n\n    def give_response_single(\n        self,\n        query_str: str,\n        text_chunk: str,\n    ) -> str:\n        \"\"\"Give response given a query and a corresponding text chunk.\"\"\"\n        text_qa_template = self.text_qa_template.partial_format(query_str=query_str)\n        qa_text_splitter = self.prompt_helper.get_text_splitter_given_prompt(\n          ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 6, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "7": {"text": "           text_qa_template, 1\n        )\n        text_chunks = qa_text_splitter.split_text(text_chunk)\n        response = None\n        # TODO: consolidate with loop in get_response_default\n        for cur_text_chunk in text_chunks:\n            if response is None:\n                response, _ = self.llm_predictor.predict(\n                    text_qa_template,\n                    context_str=cur_text_chunk,\n                )\n                logging.debug(f\"> Initial response: {response}\")\n            else:\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 7, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "8": {"text": "      else:\n                response = self.refine_response_single(\n                    response,\n                    query_str,\n                    cur_text_chunk,\n                )\n        return response or \"\"\n\n    def get_response_over_chunks(\n        self,\n        query_str: str,\n        text_chunks: List[TextChunk],\n        prev_response: Optional[str] = None,\n    ) -> str:\n        \"\"\"Give response over chunks.\"\"\"\n        response = None\n        for text_chunk in text_chunks:\n            if prev_response is None:\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 8, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "9": {"text": "        if prev_response is None:\n                # if this is the first chunk, and text chunk already\n                # is an answer, then return it\n                if text_chunk.is_answer:\n                    response = text_chunk.text\n                # otherwise give response\n                else:\n                    response = self.give_response_single(\n                        query_str,\n                        text_chunk.text,\n                    )\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 9, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "10": {"text": "   )\n            else:\n                response = self.refine_response_single(\n                    prev_response, query_str, text_chunk.text\n                )\n            prev_response = response\n        return response or \"Empty Response\"\n\n    def _get_response_default(\n        self, query_str: str, prev_response: Optional[str]\n    ) -> str:\n        return self.get_response_over_chunks(\n            query_str, self._texts, prev_response=prev_response\n        )\n\n    def _get_response_compact(\n        self, query_str: str, prev_response: Optional[str]\n    ) -> str:\n        \"\"\"Get compact", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 10, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "11": {"text": ") -> str:\n        \"\"\"Get compact response.\"\"\"\n        # use prompt helper to fix compact text_chunks under the prompt limitation\n        max_prompt = self.prompt_helper.get_biggest_prompt(\n            [self.text_qa_template, self.refine_template]\n        )\n        with temp_set_attrs(self.prompt_helper, use_chunk_size_limit=False):\n            new_texts = self.prompt_helper.compact_text_chunks(\n                max_prompt, [t.text for t in self._texts]\n            )\n            new_text_chunks = [TextChunk(text=t) for t in new_texts]\n            response =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 11, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "12": {"text": "           response = self.get_response_over_chunks(\n                query_str, new_text_chunks, prev_response=prev_response\n            )\n        return response\n\n    def _get_response_tree_summarize(\n        self,\n        query_str: str,\n        prev_response: Optional[str],\n        num_children: int = 10,\n    ) -> str:\n        \"\"\"Get tree summarize response.\"\"\"\n        text_qa_template = self.text_qa_template.partial_format(query_str=query_str)\n        summary_template = SummaryPrompt.from_prompt(text_qa_template)\n\n        # first join all the text chunks into a single text\n        all_text = \"\\n\\n\".join([t.text for t in", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 12, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "13": {"text": "= \"\\n\\n\".join([t.text for t in self._texts])\n        # then get text splitter\n        text_splitter = self.prompt_helper.get_text_splitter_given_prompt(\n            summary_template, num_children\n        )\n        text_chunks = text_splitter.split_text(all_text)\n        all_nodes: Dict[int, Node] = {\n            i: Node(text=t) for i, t in enumerate(text_chunks)\n        }\n\n        index_builder = GPTTreeIndexBuilder(\n            num_children,\n            summary_template,\n            self.llm_predictor,\n            self.prompt_helper,\n        )\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 13, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "14": {"text": "       )\n        root_nodes = index_builder.build_index_from_nodes(all_nodes, all_nodes)\n        node_list = get_sorted_node_list(root_nodes)\n        node_text = self.prompt_helper.get_text_from_nodes(\n            node_list, prompt=text_qa_template\n        )\n        response = self.get_response_over_chunks(\n            query_str,\n            [TextChunk(node_text)],\n            prev_response=prev_response,\n        )\n        return response or \"Empty Response\"\n\n    def get_response(\n        self,\n        query_str: str,\n        prev_response: Optional[str] =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 14, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "15": {"text": "       prev_response: Optional[str] = None,\n        mode: ResponseMode = ResponseMode.DEFAULT,\n        **response_kwargs: Any,\n    ) -> str:\n        \"\"\"Get response.\"\"\"\n        if mode == ResponseMode.DEFAULT:\n            return self._get_response_default(query_str, prev_response)\n        elif mode == ResponseMode.COMPACT:\n            return self._get_response_compact(query_str, prev_response)\n        elif mode == ResponseMode.TREE_SUMMARIZE:\n            return self._get_response_tree_summarize(\n                query_str, prev_response, **response_kwargs\n            )\n        else:\n            raise", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 15, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "16": {"text": " else:\n            raise ValueError(f\"Invalid mode: {mode}\")\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "index": 16, "child_indices": [], "ref_doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "node_info": null}, "17": {"text": "This code file contains the ResponseBuilder class, which provides general functions for taking in a set of text and generating a response. It supports different modes, such as stuffing chunks into prompt, creating and refining separately over each chunk, and tree summarization. It has functions for adding text chunks, resetting, adding nodes, getting sources, refining response, giving response, and getting response over chunks. It also has functions for getting response in default and compact modes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 17, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "18": {"text": "The file builder.py is part of the GPT indexing system. It contains functions that are used to generate responses to queries. The main function, get_response(), takes a query string and a response mode as parameters and returns a response. Depending on the response mode, the function calls one of three other functions to generate the response: _get_response_default(), _get_response_compact(), or _get_response_tree_summarize(). These functions use the text_qa_template, SummaryPrompt, GPTTreeIndexBuilder, and prompt_helper to generate the response.", "doc_id": null, "embedding": null, "extra_info": null, "index": 18, "child_indices": [12, 13, 14, 15, 16], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"17": {"text": "This code file contains the ResponseBuilder class, which provides general functions for taking in a set of text and generating a response. It supports different modes, such as stuffing chunks into prompt, creating and refining separately over each chunk, and tree summarization. It has functions for adding text chunks, resetting, adding nodes, getting sources, refining response, giving response, and getting response over chunks. It also has functions for getting response in default and compact modes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 17, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "18": {"text": "The file builder.py is part of the GPT indexing system. It contains functions that are used to generate responses to queries. The main function, get_response(), takes a query string and a response mode as parameters and returns a response. Depending on the response mode, the function calls one of three other functions to generate the response: _get_response_default(), _get_response_compact(), or _get_response_tree_summarize(). These functions use the text_qa_template, SummaryPrompt, GPTTreeIndexBuilder, and prompt_helper to generate the response.", "doc_id": null, "embedding": null, "extra_info": null, "index": 18, "child_indices": [12, 13, 14, 15, 16], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}