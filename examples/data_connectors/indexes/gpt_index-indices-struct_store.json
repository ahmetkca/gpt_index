{"index_struct": {"text": "\n__init__.py defines the GPTSQLStructStoreIndex class and its parameters. base.py contains methods for inserting documents into the index, validating fields, and building the index from documents. The GPTSQLStructStoreIndex class uses a SQL database to store data and allows users to query the data using either raw SQL queries or natural language queries. The BaseGPTStructStoreIndex class is used to create a structured store index from unstructured text, and the GPTSQLStructStoreIndex class is used to create a SQLite structured store index.", "doc_id": "2b40a09d-8012-4f25-a438-cdac65cf7926", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Structured store indices.\"\"\"\n\nfrom gpt_index.indices.struct_store.sql import GPTSQLStructStoreIndex\n\n__all__ = [\"GPTSQLStructStoreIndex\"]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "3a3d24fbc1ebb0b17b6173f66959d30be00ce4b0", "node_info": null}, "1": {"text": "\"\"\"Struct store.\"\"\"\n\nimport logging\nimport re\nfrom abc import abstractmethod\nfrom typing import Any, Callable, Dict, Generic, Optional, Sequence, TypeVar\n\nfrom gpt_index.data_structs.table import BaseStructTable, StructDatapoint\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.utils import truncate_text\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_SCHEMA_EXTRACT_PROMPT\nfrom gpt_index.prompts.prompts import SchemaExtractPrompt\nfrom gpt_index.schema import BaseDocument\n\nBST = TypeVar(\"BST\", bound=BaseStructTable)\n\n\ndef default_output_parser(output: str) -> Optional[Dict[str, Any]]:\n    \"\"\"Parse output of schema extraction.\n\n    Attempt to parse the following format from the default prompt:\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "2": {"text": "  Attempt to parse the following format from the default prompt:\n    field1: <value>, field2: <value>, ...\n\n    \"\"\"\n    tups = output.split(\"\\n\")\n\n    fields = {}\n    for tup in tups:\n        if \":\" in tup:\n            tokens = tup.split(\":\")\n            field = re.sub(r\"\\W+\", \"\", tokens[0])\n            value = re.sub(r\"\\W+\", \"\", tokens[1])\n            fields[field] = value\n    return fields\n\n\nOUTPUT_PARSER_TYPE = Callable[[str], Optional[Dict[str, Any]]]\n\n\nclass BaseGPTStructStoreIndex(BaseGPTIndex[BST], Generic[BST]):\n    \"\"\"Base GPT Struct Store Index.\"\"\"\n\n    def __init__(\n        self,\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "3": {"text": "      self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[BST] = None,\n        schema_extract_prompt: Optional[SchemaExtractPrompt] = None,\n        output_parser: Optional[OUTPUT_PARSER_TYPE] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self.schema_extract_prompt = (\n            schema_extract_prompt or DEFAULT_SCHEMA_EXTRACT_PROMPT\n        )\n        self.output_parser = output_parser or default_output_parser\n        super().__init__(\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "4": {"text": "           documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            **kwargs,\n        )\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.schema_extract_prompt, 1\n        )\n\n    @abstractmethod\n    def _insert_datapoint(self, datapoint: StructDatapoint) -> None:\n        \"\"\"Insert datapoint into index.\"\"\"\n\n    @abstractmethod\n    def _get_col_types_map(self) -> Dict[str, type]:\n        \"\"\"Get col types map for schema.\"\"\"\n\n    def _clean_and_validate_fields(self, fields: Dict[str, Any]) ->", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 4, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "5": {"text": "fields: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate fields with col_types_map.\"\"\"\n        new_fields = {}\n        col_types_map = self._get_col_types_map()\n        for field, value in fields.items():\n            clean_value = value\n            if field not in col_types_map:\n                continue\n            # if expected type is int or float, try to convert value to int or float\n            expected_type = col_types_map[field]\n            if expected_type == int:\n                try:\n                    clean_value = int(value)\n                except", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 5, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "6": {"text": "               except ValueError:\n                    continue\n            elif expected_type == float:\n                try:\n                    clean_value = float(value)\n                except ValueError:\n                    continue\n            else:\n                if len(value) == 0:\n                    continue\n                if not isinstance(value, col_types_map[field]):\n                    continue\n            new_fields[field] = clean_value\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 6, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "7": {"text": "  new_fields[field] = clean_value\n        return new_fields\n\n    @abstractmethod\n    def _get_schema_text(self) -> str:\n        \"\"\"Get schema text for extracting relevant info from unstructured text.\"\"\"\n\n    def _add_document_to_index(\n        self,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        text_chunks = text_splitter.split_text(document.get_text())\n        fields = {}\n        for i, text_chunk in enumerate(text_chunks):\n            fmt_text_chunk = truncate_text(text_chunk, 50)\n            logging.info(f\"> Adding chunk {i}:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 7, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "8": {"text": "   logging.info(f\"> Adding chunk {i}: {fmt_text_chunk}\")\n            # if embedding specified in document, pass it to the Node\n            schema_text = self._get_schema_text()\n            response_str, _ = self._llm_predictor.predict(\n                self.schema_extract_prompt,\n                text=text_chunk,\n                schema=schema_text,\n            )\n            cur_fields = self.output_parser(response_str)\n            if cur_fields is None:\n                continue\n            # validate fields with col_types_map\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 8, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "9": {"text": "validate fields with col_types_map\n            new_cur_fields = self._clean_and_validate_fields(cur_fields)\n            fields.update(new_cur_fields)\n\n        struct_datapoint = StructDatapoint(fields)\n        if struct_datapoint is not None:\n            self._insert_datapoint(struct_datapoint)\n            logging.debug(f\"> Added datapoint: {fields}\")\n\n    def _build_index_from_documents(self, documents: Sequence[BaseDocument]) -> BST:\n        \"\"\"Build index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.schema_extract_prompt, 1\n        )\n        index_struct =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 9, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "10": {"text": "   )\n        index_struct = self.index_struct_cls()\n        for d in documents:\n            self._add_document_to_index(d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(document, self._text_splitter)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        raise NotImplementedError(\"Delete not implemented for Struct Store Index.\")\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 10, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "11": {"text": "\"\"\"SQLite structured store.\"\"\"\n\nfrom typing import Any, Dict, List, Optional, Sequence, Type, cast\n\nfrom sqlalchemy import Table\n\nfrom gpt_index.data_structs.table import SQLStructTable, StructDatapoint\nfrom gpt_index.indices.base import DOCUMENTS_INPUT\nfrom gpt_index.indices.common.struct_store.base import SQLContextBuilder\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.struct_store.sql import (\n    GPTNLStructStoreIndexQuery,\n    GPTSQLStructStoreIndexQuery,\n)\nfrom gpt_index.indices.struct_store.base import BaseGPTStructStoreIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.sql_wrapper import SQLDatabase\nfrom gpt_index.schema import BaseDocument\n\n\nclass GPTSQLStructStoreIndex(BaseGPTStructStoreIndex[SQLStructTable]):\n    \"\"\"Base GPT SQL", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/sql.py", "file_name": "sql.py"}, "index": 11, "child_indices": [], "ref_doc_id": "9b52c03318a639e578f5784857f4fa6403f1c7e2", "node_info": null}, "12": {"text": "   \"\"\"Base GPT SQL Struct Store Index.\n\n    The GPTSQLStructStoreIndex is an index that uses a SQL database\n    under the hood. During index construction, the data can be inferred\n    from unstructured documents given a schema extract prompt,\n    or it can be pre-loaded in the database.\n\n    During query time, the user can either specify a raw SQL query\n    or a natural language query to retrieve their data.\n\n    Args:\n        sql_database (Optional[SQLDatabase]): SQL database to use,\n            including table names to specify.\n            See :ref:`Ref-Struct-Store` for more details.\n        table_name (Optional[str]): Name of the table to use\n            for extracting data.\n            Either table_name or table must be specified.\n        table (Optional[Table]): SQLAlchemy Table object to use.\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/sql.py", "file_name": "sql.py"}, "index": 12, "child_indices": [], "ref_doc_id": "9b52c03318a639e578f5784857f4fa6403f1c7e2", "node_info": null}, "13": {"text": "SQLAlchemy Table object to use.\n            Specifying the Table object explicitly, instead of\n            the table name, allows you to pass in a view.\n            Either table_name or table must be specified.\n        table_context_dict (Optional[Dict[str, str]]): Optional table context to use.\n            If specified,\n            sql_context_builder and context_documents cannot be specified.\n        sql_context_builder (Optional[SQLContextBuilder]): SQL context builder.\n            If specified, the context builder will be used to build\n            context for the specified table, which will then be used during\n            query-time. Also if specified, context_documents must be specified,\n            and table_context cannot be specified.\n        context_documents_dict", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/sql.py", "file_name": "sql.py"}, "index": 13, "child_indices": [], "ref_doc_id": "9b52c03318a639e578f5784857f4fa6403f1c7e2", "node_info": null}, "14": {"text": "specified.\n        context_documents_dict (Optional[Dict[str, List[BaseDocument]]]):\n            Optional context\n            documents to inform the sql_context_builder. Must be specified if\n            sql_context_builder is specified. Cannot be specified if table_context\n            is specified.\n\n    \"\"\"\n\n    index_struct_cls = SQLStructTable\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[SQLStructTable] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        sql_database: Optional[SQLDatabase] = None,\n        table_name: Optional[str] = None,\n        table: Optional[Table] = None,\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/sql.py", "file_name": "sql.py"}, "index": 14, "child_indices": [], "ref_doc_id": "9b52c03318a639e578f5784857f4fa6403f1c7e2", "node_info": null}, "15": {"text": "    table: Optional[Table] = None,\n        ref_doc_id_column: Optional[str] = None,\n        table_context_dict: Optional[Dict[str, str]] = None,\n        sql_context_builder: Optional[SQLContextBuilder] = None,\n        context_documents_dict: Optional[Dict[str, List[BaseDocument]]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        # currently the user must specify a table info\n        if table_name is None and table is None:\n            raise ValueError(\"table_name must be specified\")\n        self.table_name = table_name or cast(Table, table).name\n        if sql_database is None:\n            raise ValueError(\"sql_database must be specified\")\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/sql.py", "file_name": "sql.py"}, "index": 15, "child_indices": [], "ref_doc_id": "9b52c03318a639e578f5784857f4fa6403f1c7e2", "node_info": null}, "16": {"text": "must be specified\")\n        self.sql_database = sql_database\n        if table is None:\n            table = self.sql_database.metadata_obj.tables[table_name]\n        # if ref_doc_id_column is specified, then we need to check that\n        # it is a valid column in the table\n        col_names = [c.name for c in table.c]\n        if ref_doc_id_column is not None and ref_doc_id_column not in col_names:\n            raise ValueError(\n                f\"ref_doc_id_column {ref_doc_id_column} not in table {table_name}\"\n            )\n        self.ref_doc_id_column = ref_doc_id_column\n        # then store python types of each column\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/sql.py", "file_name": "sql.py"}, "index": 16, "child_indices": [], "ref_doc_id": "9b52c03318a639e578f5784857f4fa6403f1c7e2", "node_info": null}, "17": {"text": "# then store python types of each column\n        self._col_types_map: Dict[str, type] = {\n            c.name: table.c[c.name].type.python_type for c in table.c\n        }\n\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            **kwargs,\n        )\n\n        # if context builder is specified, then add to context_dict\n        if table_context_dict is not None and (\n            sql_context_builder is not None or context_documents_dict is not None\n        ):\n            raise ValueError(\n         ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/sql.py", "file_name": "sql.py"}, "index": 17, "child_indices": [], "ref_doc_id": "9b52c03318a639e578f5784857f4fa6403f1c7e2", "node_info": null}, "18": {"text": "  raise ValueError(\n                \"Cannot specify both table_context_dict and \"\n                \"sql_context_builder/context_documents_dict\"\n            )\n        if sql_context_builder is not None:\n            if context_documents_dict is None:\n                raise ValueError(\n                    \"context_documents_dict must be specified if \"\n                    \"sql_context_builder is specified\"\n                )\n            context_documents_dict = cast(\n                Dict[str, List[BaseDocument]], context_documents_dict\n            )\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/sql.py", "file_name": "sql.py"}, "index": 18, "child_indices": [], "ref_doc_id": "9b52c03318a639e578f5784857f4fa6403f1c7e2", "node_info": null}, "19": {"text": "           )\n            context_dict: Dict[\n                str, str\n            ] = sql_context_builder.build_all_context_from_documents(\n                context_documents_dict\n            )\n        elif table_context_dict is not None:\n            context_dict = table_context_dict\n        else:\n            context_dict = {}\n\n        # validate context_dict keys are valid table names\n        context_keys = set(context_dict.keys())\n        if not context_keys.issubset(set(self.sql_database.get_table_names())):\n            raise ValueError(\n            ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/sql.py", "file_name": "sql.py"}, "index": 19, "child_indices": [], "ref_doc_id": "9b52c03318a639e578f5784857f4fa6403f1c7e2", "node_info": null}, "20": {"text": "ValueError(\n                \"Invalid context table names: \"\n                f\"{context_keys - set(self.sql_database.get_table_names())}\"\n            )\n\n        self._index_struct.context_dict.update(context_dict)\n        self._sql_context_builder = sql_context_builder\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTNLStructStoreIndexQuery,\n            QueryMode.SQL: GPTSQLStructStoreIndexQuery,\n        }\n\n    def _get_col_types_map(self) -> Dict[str, type]:\n        \"\"\"Get col types map for schema.\"\"\"\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/sql.py", "file_name": "sql.py"}, "index": 20, "child_indices": [], "ref_doc_id": "9b52c03318a639e578f5784857f4fa6403f1c7e2", "node_info": null}, "21": {"text": "    \"\"\"Get col types map for schema.\"\"\"\n        return self._col_types_map\n\n    def _get_schema_text(self) -> str:\n        \"\"\"Insert datapoint into index.\"\"\"\n        return self.sql_database.get_single_table_info(self.table_name)\n\n    def _insert_datapoint(self, datapoint: StructDatapoint) -> None:\n        \"\"\"Insert datapoint into index.\"\"\"\n        datapoint_dict = datapoint.to_dict()[\"fields\"]\n        self.sql_database.insert_into_table(\n            self.table_name, cast(Dict[Any, Any], datapoint_dict)\n        )\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Any) -> None:\n        \"\"\"Preprocess query.\n\n        This allows subclasses to pass in additional query", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/sql.py", "file_name": "sql.py"}, "index": 21, "child_indices": [], "ref_doc_id": "9b52c03318a639e578f5784857f4fa6403f1c7e2", "node_info": null}, "22": {"text": "      This allows subclasses to pass in additional query kwargs\n        to query, for instance arguments that are shared between the\n        index and the query class. By default, this does nothing.\n        This also allows subclasses to do validation.\n\n        \"\"\"\n        super()._preprocess_query(mode, query_kwargs)\n        # pass along sql_database, table_name\n        query_kwargs[\"sql_database\"] = self.sql_database\n        if mode == QueryMode.DEFAULT:\n            query_kwargs[\"ref_doc_id_column\"] = self.ref_doc_id_column\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/sql.py", "file_name": "sql.py"}, "index": 22, "child_indices": [], "ref_doc_id": "9b52c03318a639e578f5784857f4fa6403f1c7e2", "node_info": null}, "23": {"text": "\nThis code file contains the code for the BaseGPTStructStoreIndex class, which is used to create a structured store index from unstructured text. It includes methods for inserting documents into the index, validating fields, and building the index from documents. It also includes the GPTSQLStructStoreIndex class, which is used to create a SQLite structured store index. It includes methods for creating a SQLContextBuilder, creating a GPTSQLStructStoreIndexQuery, and creating a SQLDatabase.", "doc_id": null, "embedding": null, "extra_info": null, "index": 23, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "24": {"text": "The GPTSQLStructStoreIndex is an index that uses a SQL database under the hood. It allows users to specify a raw SQL query or a natural language query to retrieve their data. It requires a SQLDatabase, table_name, and table to be specified. It also allows for a ref_doc_id_column, table_context_dict, sql_context_builder, and context_documents_dict to be specified. It stores python types of each column and allows for additional query kwargs to be passed in. It also allows for validation of the query kwargs.", "doc_id": null, "embedding": null, "extra_info": null, "index": 24, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"23": {"text": "\nThis code file contains the code for the BaseGPTStructStoreIndex class, which is used to create a structured store index from unstructured text. It includes methods for inserting documents into the index, validating fields, and building the index from documents. It also includes the GPTSQLStructStoreIndex class, which is used to create a SQLite structured store index. It includes methods for creating a SQLContextBuilder, creating a GPTSQLStructStoreIndexQuery, and creating a SQLDatabase.", "doc_id": null, "embedding": null, "extra_info": null, "index": 23, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "24": {"text": "The GPTSQLStructStoreIndex is an index that uses a SQL database under the hood. It allows users to specify a raw SQL query or a natural language query to retrieve their data. It requires a SQLDatabase, table_name, and table to be specified. It also allows for a ref_doc_id_column, table_context_dict, sql_context_builder, and context_documents_dict to be specified. It stores python types of each column and allows for additional query kwargs to be passed in. It also allows for validation of the query kwargs.", "doc_id": null, "embedding": null, "extra_info": null, "index": 24, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"3a3d24fbc1ebb0b17b6173f66959d30be00ce4b0": {"text": "\"\"\"Structured store indices.\"\"\"\n\nfrom gpt_index.indices.struct_store.sql import GPTSQLStructStoreIndex\n\n__all__ = [\"GPTSQLStructStoreIndex\"]\n", "doc_id": "3a3d24fbc1ebb0b17b6173f66959d30be00ce4b0", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "ed3819ef94005896d68caab2ee933ca64cb2f62c": {"text": "\"\"\"Struct store.\"\"\"\n\nimport logging\nimport re\nfrom abc import abstractmethod\nfrom typing import Any, Callable, Dict, Generic, Optional, Sequence, TypeVar\n\nfrom gpt_index.data_structs.table import BaseStructTable, StructDatapoint\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.utils import truncate_text\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_SCHEMA_EXTRACT_PROMPT\nfrom gpt_index.prompts.prompts import SchemaExtractPrompt\nfrom gpt_index.schema import BaseDocument\n\nBST = TypeVar(\"BST\", bound=BaseStructTable)\n\n\ndef default_output_parser(output: str) -> Optional[Dict[str, Any]]:\n    \"\"\"Parse output of schema extraction.\n\n    Attempt to parse the following format from the default prompt:\n    field1: <value>, field2: <value>, ...\n\n    \"\"\"\n    tups = output.split(\"\\n\")\n\n    fields = {}\n    for tup in tups:\n        if \":\" in tup:\n            tokens = tup.split(\":\")\n            field = re.sub(r\"\\W+\", \"\", tokens[0])\n            value = re.sub(r\"\\W+\", \"\", tokens[1])\n            fields[field] = value\n    return fields\n\n\nOUTPUT_PARSER_TYPE = Callable[[str], Optional[Dict[str, Any]]]\n\n\nclass BaseGPTStructStoreIndex(BaseGPTIndex[BST], Generic[BST]):\n    \"\"\"Base GPT Struct Store Index.\"\"\"\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[BST] = None,\n        schema_extract_prompt: Optional[SchemaExtractPrompt] = None,\n        output_parser: Optional[OUTPUT_PARSER_TYPE] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self.schema_extract_prompt = (\n            schema_extract_prompt or DEFAULT_SCHEMA_EXTRACT_PROMPT\n        )\n        self.output_parser = output_parser or default_output_parser\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            **kwargs,\n        )\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.schema_extract_prompt, 1\n        )\n\n    @abstractmethod\n    def _insert_datapoint(self, datapoint: StructDatapoint) -> None:\n        \"\"\"Insert datapoint into index.\"\"\"\n\n    @abstractmethod\n    def _get_col_types_map(self) -> Dict[str, type]:\n        \"\"\"Get col types map for schema.\"\"\"\n\n    def _clean_and_validate_fields(self, fields: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate fields with col_types_map.\"\"\"\n        new_fields = {}\n        col_types_map = self._get_col_types_map()\n        for field, value in fields.items():\n            clean_value = value\n            if field not in col_types_map:\n                continue\n            # if expected type is int or float, try to convert value to int or float\n            expected_type = col_types_map[field]\n            if expected_type == int:\n                try:\n                    clean_value = int(value)\n                except ValueError:\n                    continue\n            elif expected_type == float:\n                try:\n                    clean_value = float(value)\n                except ValueError:\n                    continue\n            else:\n                if len(value) == 0:\n                    continue\n                if not isinstance(value, col_types_map[field]):\n                    continue\n            new_fields[field] = clean_value\n        return new_fields\n\n    @abstractmethod\n    def _get_schema_text(self) -> str:\n        \"\"\"Get schema text for extracting relevant info from unstructured text.\"\"\"\n\n    def _add_document_to_index(\n        self,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        text_chunks = text_splitter.split_text(document.get_text())\n        fields = {}\n        for i, text_chunk in enumerate(text_chunks):\n            fmt_text_chunk = truncate_text(text_chunk, 50)\n            logging.info(f\"> Adding chunk {i}: {fmt_text_chunk}\")\n            # if embedding specified in document, pass it to the Node\n            schema_text = self._get_schema_text()\n            response_str, _ = self._llm_predictor.predict(\n                self.schema_extract_prompt,\n                text=text_chunk,\n                schema=schema_text,\n            )\n            cur_fields = self.output_parser(response_str)\n            if cur_fields is None:\n                continue\n            # validate fields with col_types_map\n            new_cur_fields = self._clean_and_validate_fields(cur_fields)\n            fields.update(new_cur_fields)\n\n        struct_datapoint = StructDatapoint(fields)\n        if struct_datapoint is not None:\n            self._insert_datapoint(struct_datapoint)\n            logging.debug(f\"> Added datapoint: {fields}\")\n\n    def _build_index_from_documents(self, documents: Sequence[BaseDocument]) -> BST:\n        \"\"\"Build index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.schema_extract_prompt, 1\n        )\n        index_struct = self.index_struct_cls()\n        for d in documents:\n            self._add_document_to_index(d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(document, self._text_splitter)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        raise NotImplementedError(\"Delete not implemented for Struct Store Index.\")\n", "doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "__type__": "Document"}, "9b52c03318a639e578f5784857f4fa6403f1c7e2": {"text": "\"\"\"SQLite structured store.\"\"\"\n\nfrom typing import Any, Dict, List, Optional, Sequence, Type, cast\n\nfrom sqlalchemy import Table\n\nfrom gpt_index.data_structs.table import SQLStructTable, StructDatapoint\nfrom gpt_index.indices.base import DOCUMENTS_INPUT\nfrom gpt_index.indices.common.struct_store.base import SQLContextBuilder\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.struct_store.sql import (\n    GPTNLStructStoreIndexQuery,\n    GPTSQLStructStoreIndexQuery,\n)\nfrom gpt_index.indices.struct_store.base import BaseGPTStructStoreIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.sql_wrapper import SQLDatabase\nfrom gpt_index.schema import BaseDocument\n\n\nclass GPTSQLStructStoreIndex(BaseGPTStructStoreIndex[SQLStructTable]):\n    \"\"\"Base GPT SQL Struct Store Index.\n\n    The GPTSQLStructStoreIndex is an index that uses a SQL database\n    under the hood. During index construction, the data can be inferred\n    from unstructured documents given a schema extract prompt,\n    or it can be pre-loaded in the database.\n\n    During query time, the user can either specify a raw SQL query\n    or a natural language query to retrieve their data.\n\n    Args:\n        sql_database (Optional[SQLDatabase]): SQL database to use,\n            including table names to specify.\n            See :ref:`Ref-Struct-Store` for more details.\n        table_name (Optional[str]): Name of the table to use\n            for extracting data.\n            Either table_name or table must be specified.\n        table (Optional[Table]): SQLAlchemy Table object to use.\n            Specifying the Table object explicitly, instead of\n            the table name, allows you to pass in a view.\n            Either table_name or table must be specified.\n        table_context_dict (Optional[Dict[str, str]]): Optional table context to use.\n            If specified,\n            sql_context_builder and context_documents cannot be specified.\n        sql_context_builder (Optional[SQLContextBuilder]): SQL context builder.\n            If specified, the context builder will be used to build\n            context for the specified table, which will then be used during\n            query-time. Also if specified, context_documents must be specified,\n            and table_context cannot be specified.\n        context_documents_dict (Optional[Dict[str, List[BaseDocument]]]):\n            Optional context\n            documents to inform the sql_context_builder. Must be specified if\n            sql_context_builder is specified. Cannot be specified if table_context\n            is specified.\n\n    \"\"\"\n\n    index_struct_cls = SQLStructTable\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[SQLStructTable] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        sql_database: Optional[SQLDatabase] = None,\n        table_name: Optional[str] = None,\n        table: Optional[Table] = None,\n        ref_doc_id_column: Optional[str] = None,\n        table_context_dict: Optional[Dict[str, str]] = None,\n        sql_context_builder: Optional[SQLContextBuilder] = None,\n        context_documents_dict: Optional[Dict[str, List[BaseDocument]]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        # currently the user must specify a table info\n        if table_name is None and table is None:\n            raise ValueError(\"table_name must be specified\")\n        self.table_name = table_name or cast(Table, table).name\n        if sql_database is None:\n            raise ValueError(\"sql_database must be specified\")\n        self.sql_database = sql_database\n        if table is None:\n            table = self.sql_database.metadata_obj.tables[table_name]\n        # if ref_doc_id_column is specified, then we need to check that\n        # it is a valid column in the table\n        col_names = [c.name for c in table.c]\n        if ref_doc_id_column is not None and ref_doc_id_column not in col_names:\n            raise ValueError(\n                f\"ref_doc_id_column {ref_doc_id_column} not in table {table_name}\"\n            )\n        self.ref_doc_id_column = ref_doc_id_column\n        # then store python types of each column\n        self._col_types_map: Dict[str, type] = {\n            c.name: table.c[c.name].type.python_type for c in table.c\n        }\n\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            **kwargs,\n        )\n\n        # if context builder is specified, then add to context_dict\n        if table_context_dict is not None and (\n            sql_context_builder is not None or context_documents_dict is not None\n        ):\n            raise ValueError(\n                \"Cannot specify both table_context_dict and \"\n                \"sql_context_builder/context_documents_dict\"\n            )\n        if sql_context_builder is not None:\n            if context_documents_dict is None:\n                raise ValueError(\n                    \"context_documents_dict must be specified if \"\n                    \"sql_context_builder is specified\"\n                )\n            context_documents_dict = cast(\n                Dict[str, List[BaseDocument]], context_documents_dict\n            )\n            context_dict: Dict[\n                str, str\n            ] = sql_context_builder.build_all_context_from_documents(\n                context_documents_dict\n            )\n        elif table_context_dict is not None:\n            context_dict = table_context_dict\n        else:\n            context_dict = {}\n\n        # validate context_dict keys are valid table names\n        context_keys = set(context_dict.keys())\n        if not context_keys.issubset(set(self.sql_database.get_table_names())):\n            raise ValueError(\n                \"Invalid context table names: \"\n                f\"{context_keys - set(self.sql_database.get_table_names())}\"\n            )\n\n        self._index_struct.context_dict.update(context_dict)\n        self._sql_context_builder = sql_context_builder\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTNLStructStoreIndexQuery,\n            QueryMode.SQL: GPTSQLStructStoreIndexQuery,\n        }\n\n    def _get_col_types_map(self) -> Dict[str, type]:\n        \"\"\"Get col types map for schema.\"\"\"\n        return self._col_types_map\n\n    def _get_schema_text(self) -> str:\n        \"\"\"Insert datapoint into index.\"\"\"\n        return self.sql_database.get_single_table_info(self.table_name)\n\n    def _insert_datapoint(self, datapoint: StructDatapoint) -> None:\n        \"\"\"Insert datapoint into index.\"\"\"\n        datapoint_dict = datapoint.to_dict()[\"fields\"]\n        self.sql_database.insert_into_table(\n            self.table_name, cast(Dict[Any, Any], datapoint_dict)\n        )\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Any) -> None:\n        \"\"\"Preprocess query.\n\n        This allows subclasses to pass in additional query kwargs\n        to query, for instance arguments that are shared between the\n        index and the query class. By default, this does nothing.\n        This also allows subclasses to do validation.\n\n        \"\"\"\n        super()._preprocess_query(mode, query_kwargs)\n        # pass along sql_database, table_name\n        query_kwargs[\"sql_database\"] = self.sql_database\n        if mode == QueryMode.DEFAULT:\n            query_kwargs[\"ref_doc_id_column\"] = self.ref_doc_id_column\n", "doc_id": "9b52c03318a639e578f5784857f4fa6403f1c7e2", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/sql.py", "file_name": "sql.py"}, "__type__": "Document"}, "2b40a09d-8012-4f25-a438-cdac65cf7926": {"text": "\n__init__.py defines the GPTSQLStructStoreIndex class and its parameters. base.py contains methods for inserting documents into the index, validating fields, and building the index from documents. The GPTSQLStructStoreIndex class uses a SQL database to store data and allows users to query the data using either raw SQL queries or natural language queries. The BaseGPTStructStoreIndex class is used to create a structured store index from unstructured text, and the GPTSQLStructStoreIndex class is used to create a SQLite structured store index.", "doc_id": "2b40a09d-8012-4f25-a438-cdac65cf7926", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Structured store indices.\"\"\"\n\nfrom gpt_index.indices.struct_store.sql import GPTSQLStructStoreIndex\n\n__all__ = [\"GPTSQLStructStoreIndex\"]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "3a3d24fbc1ebb0b17b6173f66959d30be00ce4b0", "node_info": null}, "1": {"text": "\"\"\"Struct store.\"\"\"\n\nimport logging\nimport re\nfrom abc import abstractmethod\nfrom typing import Any, Callable, Dict, Generic, Optional, Sequence, TypeVar\n\nfrom gpt_index.data_structs.table import BaseStructTable, StructDatapoint\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.utils import truncate_text\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_SCHEMA_EXTRACT_PROMPT\nfrom gpt_index.prompts.prompts import SchemaExtractPrompt\nfrom gpt_index.schema import BaseDocument\n\nBST = TypeVar(\"BST\", bound=BaseStructTable)\n\n\ndef default_output_parser(output: str) -> Optional[Dict[str, Any]]:\n    \"\"\"Parse output of schema extraction.\n\n    Attempt to parse the following format from the default prompt:\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "2": {"text": "  Attempt to parse the following format from the default prompt:\n    field1: <value>, field2: <value>, ...\n\n    \"\"\"\n    tups = output.split(\"\\n\")\n\n    fields = {}\n    for tup in tups:\n        if \":\" in tup:\n            tokens = tup.split(\":\")\n            field = re.sub(r\"\\W+\", \"\", tokens[0])\n            value = re.sub(r\"\\W+\", \"\", tokens[1])\n            fields[field] = value\n    return fields\n\n\nOUTPUT_PARSER_TYPE = Callable[[str], Optional[Dict[str, Any]]]\n\n\nclass BaseGPTStructStoreIndex(BaseGPTIndex[BST], Generic[BST]):\n    \"\"\"Base GPT Struct Store Index.\"\"\"\n\n    def __init__(\n        self,\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "3": {"text": "      self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[BST] = None,\n        schema_extract_prompt: Optional[SchemaExtractPrompt] = None,\n        output_parser: Optional[OUTPUT_PARSER_TYPE] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self.schema_extract_prompt = (\n            schema_extract_prompt or DEFAULT_SCHEMA_EXTRACT_PROMPT\n        )\n        self.output_parser = output_parser or default_output_parser\n        super().__init__(\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "4": {"text": "           documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            **kwargs,\n        )\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.schema_extract_prompt, 1\n        )\n\n    @abstractmethod\n    def _insert_datapoint(self, datapoint: StructDatapoint) -> None:\n        \"\"\"Insert datapoint into index.\"\"\"\n\n    @abstractmethod\n    def _get_col_types_map(self) -> Dict[str, type]:\n        \"\"\"Get col types map for schema.\"\"\"\n\n    def _clean_and_validate_fields(self, fields: Dict[str, Any]) ->", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 4, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "5": {"text": "fields: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate fields with col_types_map.\"\"\"\n        new_fields = {}\n        col_types_map = self._get_col_types_map()\n        for field, value in fields.items():\n            clean_value = value\n            if field not in col_types_map:\n                continue\n            # if expected type is int or float, try to convert value to int or float\n            expected_type = col_types_map[field]\n            if expected_type == int:\n                try:\n                    clean_value = int(value)\n                except", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 5, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "6": {"text": "               except ValueError:\n                    continue\n            elif expected_type == float:\n                try:\n                    clean_value = float(value)\n                except ValueError:\n                    continue\n            else:\n                if len(value) == 0:\n                    continue\n                if not isinstance(value, col_types_map[field]):\n                    continue\n            new_fields[field] = clean_value\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 6, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "7": {"text": "  new_fields[field] = clean_value\n        return new_fields\n\n    @abstractmethod\n    def _get_schema_text(self) -> str:\n        \"\"\"Get schema text for extracting relevant info from unstructured text.\"\"\"\n\n    def _add_document_to_index(\n        self,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        text_chunks = text_splitter.split_text(document.get_text())\n        fields = {}\n        for i, text_chunk in enumerate(text_chunks):\n            fmt_text_chunk = truncate_text(text_chunk, 50)\n            logging.info(f\"> Adding chunk {i}:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 7, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "8": {"text": "   logging.info(f\"> Adding chunk {i}: {fmt_text_chunk}\")\n            # if embedding specified in document, pass it to the Node\n            schema_text = self._get_schema_text()\n            response_str, _ = self._llm_predictor.predict(\n                self.schema_extract_prompt,\n                text=text_chunk,\n                schema=schema_text,\n            )\n            cur_fields = self.output_parser(response_str)\n            if cur_fields is None:\n                continue\n            # validate fields with col_types_map\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 8, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "9": {"text": "validate fields with col_types_map\n            new_cur_fields = self._clean_and_validate_fields(cur_fields)\n            fields.update(new_cur_fields)\n\n        struct_datapoint = StructDatapoint(fields)\n        if struct_datapoint is not None:\n            self._insert_datapoint(struct_datapoint)\n            logging.debug(f\"> Added datapoint: {fields}\")\n\n    def _build_index_from_documents(self, documents: Sequence[BaseDocument]) -> BST:\n        \"\"\"Build index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.schema_extract_prompt, 1\n        )\n        index_struct =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 9, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "10": {"text": "   )\n        index_struct = self.index_struct_cls()\n        for d in documents:\n            self._add_document_to_index(d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(document, self._text_splitter)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        raise NotImplementedError(\"Delete not implemented for Struct Store Index.\")\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 10, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "11": {"text": "\"\"\"SQLite structured store.\"\"\"\n\nfrom typing import Any, Dict, List, Optional, Sequence, Type, cast\n\nfrom sqlalchemy import Table\n\nfrom gpt_index.data_structs.table import SQLStructTable, StructDatapoint\nfrom gpt_index.indices.base import DOCUMENTS_INPUT\nfrom gpt_index.indices.common.struct_store.base import SQLContextBuilder\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.struct_store.sql import (\n    GPTNLStructStoreIndexQuery,\n    GPTSQLStructStoreIndexQuery,\n)\nfrom gpt_index.indices.struct_store.base import BaseGPTStructStoreIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.sql_wrapper import SQLDatabase\nfrom gpt_index.schema import BaseDocument\n\n\nclass GPTSQLStructStoreIndex(BaseGPTStructStoreIndex[SQLStructTable]):\n    \"\"\"Base GPT SQL", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/sql.py", "file_name": "sql.py"}, "index": 11, "child_indices": [], "ref_doc_id": "9b52c03318a639e578f5784857f4fa6403f1c7e2", "node_info": null}, "12": {"text": "   \"\"\"Base GPT SQL Struct Store Index.\n\n    The GPTSQLStructStoreIndex is an index that uses a SQL database\n    under the hood. During index construction, the data can be inferred\n    from unstructured documents given a schema extract prompt,\n    or it can be pre-loaded in the database.\n\n    During query time, the user can either specify a raw SQL query\n    or a natural language query to retrieve their data.\n\n    Args:\n        sql_database (Optional[SQLDatabase]): SQL database to use,\n            including table names to specify.\n            See :ref:`Ref-Struct-Store` for more details.\n        table_name (Optional[str]): Name of the table to use\n            for extracting data.\n            Either table_name or table must be specified.\n        table (Optional[Table]): SQLAlchemy Table object to use.\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/sql.py", "file_name": "sql.py"}, "index": 12, "child_indices": [], "ref_doc_id": "9b52c03318a639e578f5784857f4fa6403f1c7e2", "node_info": null}, "13": {"text": "SQLAlchemy Table object to use.\n            Specifying the Table object explicitly, instead of\n            the table name, allows you to pass in a view.\n            Either table_name or table must be specified.\n        table_context_dict (Optional[Dict[str, str]]): Optional table context to use.\n            If specified,\n            sql_context_builder and context_documents cannot be specified.\n        sql_context_builder (Optional[SQLContextBuilder]): SQL context builder.\n            If specified, the context builder will be used to build\n            context for the specified table, which will then be used during\n            query-time. Also if specified, context_documents must be specified,\n            and table_context cannot be specified.\n        context_documents_dict", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/sql.py", "file_name": "sql.py"}, "index": 13, "child_indices": [], "ref_doc_id": "9b52c03318a639e578f5784857f4fa6403f1c7e2", "node_info": null}, "14": {"text": "specified.\n        context_documents_dict (Optional[Dict[str, List[BaseDocument]]]):\n            Optional context\n            documents to inform the sql_context_builder. Must be specified if\n            sql_context_builder is specified. Cannot be specified if table_context\n            is specified.\n\n    \"\"\"\n\n    index_struct_cls = SQLStructTable\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[SQLStructTable] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        sql_database: Optional[SQLDatabase] = None,\n        table_name: Optional[str] = None,\n        table: Optional[Table] = None,\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/sql.py", "file_name": "sql.py"}, "index": 14, "child_indices": [], "ref_doc_id": "9b52c03318a639e578f5784857f4fa6403f1c7e2", "node_info": null}, "15": {"text": "    table: Optional[Table] = None,\n        ref_doc_id_column: Optional[str] = None,\n        table_context_dict: Optional[Dict[str, str]] = None,\n        sql_context_builder: Optional[SQLContextBuilder] = None,\n        context_documents_dict: Optional[Dict[str, List[BaseDocument]]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        # currently the user must specify a table info\n        if table_name is None and table is None:\n            raise ValueError(\"table_name must be specified\")\n        self.table_name = table_name or cast(Table, table).name\n        if sql_database is None:\n            raise ValueError(\"sql_database must be specified\")\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/sql.py", "file_name": "sql.py"}, "index": 15, "child_indices": [], "ref_doc_id": "9b52c03318a639e578f5784857f4fa6403f1c7e2", "node_info": null}, "16": {"text": "must be specified\")\n        self.sql_database = sql_database\n        if table is None:\n            table = self.sql_database.metadata_obj.tables[table_name]\n        # if ref_doc_id_column is specified, then we need to check that\n        # it is a valid column in the table\n        col_names = [c.name for c in table.c]\n        if ref_doc_id_column is not None and ref_doc_id_column not in col_names:\n            raise ValueError(\n                f\"ref_doc_id_column {ref_doc_id_column} not in table {table_name}\"\n            )\n        self.ref_doc_id_column = ref_doc_id_column\n        # then store python types of each column\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/sql.py", "file_name": "sql.py"}, "index": 16, "child_indices": [], "ref_doc_id": "9b52c03318a639e578f5784857f4fa6403f1c7e2", "node_info": null}, "17": {"text": "# then store python types of each column\n        self._col_types_map: Dict[str, type] = {\n            c.name: table.c[c.name].type.python_type for c in table.c\n        }\n\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            **kwargs,\n        )\n\n        # if context builder is specified, then add to context_dict\n        if table_context_dict is not None and (\n            sql_context_builder is not None or context_documents_dict is not None\n        ):\n            raise ValueError(\n         ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/sql.py", "file_name": "sql.py"}, "index": 17, "child_indices": [], "ref_doc_id": "9b52c03318a639e578f5784857f4fa6403f1c7e2", "node_info": null}, "18": {"text": "  raise ValueError(\n                \"Cannot specify both table_context_dict and \"\n                \"sql_context_builder/context_documents_dict\"\n            )\n        if sql_context_builder is not None:\n            if context_documents_dict is None:\n                raise ValueError(\n                    \"context_documents_dict must be specified if \"\n                    \"sql_context_builder is specified\"\n                )\n            context_documents_dict = cast(\n                Dict[str, List[BaseDocument]], context_documents_dict\n            )\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/sql.py", "file_name": "sql.py"}, "index": 18, "child_indices": [], "ref_doc_id": "9b52c03318a639e578f5784857f4fa6403f1c7e2", "node_info": null}, "19": {"text": "           )\n            context_dict: Dict[\n                str, str\n            ] = sql_context_builder.build_all_context_from_documents(\n                context_documents_dict\n            )\n        elif table_context_dict is not None:\n            context_dict = table_context_dict\n        else:\n            context_dict = {}\n\n        # validate context_dict keys are valid table names\n        context_keys = set(context_dict.keys())\n        if not context_keys.issubset(set(self.sql_database.get_table_names())):\n            raise ValueError(\n            ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/sql.py", "file_name": "sql.py"}, "index": 19, "child_indices": [], "ref_doc_id": "9b52c03318a639e578f5784857f4fa6403f1c7e2", "node_info": null}, "20": {"text": "ValueError(\n                \"Invalid context table names: \"\n                f\"{context_keys - set(self.sql_database.get_table_names())}\"\n            )\n\n        self._index_struct.context_dict.update(context_dict)\n        self._sql_context_builder = sql_context_builder\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTNLStructStoreIndexQuery,\n            QueryMode.SQL: GPTSQLStructStoreIndexQuery,\n        }\n\n    def _get_col_types_map(self) -> Dict[str, type]:\n        \"\"\"Get col types map for schema.\"\"\"\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/sql.py", "file_name": "sql.py"}, "index": 20, "child_indices": [], "ref_doc_id": "9b52c03318a639e578f5784857f4fa6403f1c7e2", "node_info": null}, "21": {"text": "    \"\"\"Get col types map for schema.\"\"\"\n        return self._col_types_map\n\n    def _get_schema_text(self) -> str:\n        \"\"\"Insert datapoint into index.\"\"\"\n        return self.sql_database.get_single_table_info(self.table_name)\n\n    def _insert_datapoint(self, datapoint: StructDatapoint) -> None:\n        \"\"\"Insert datapoint into index.\"\"\"\n        datapoint_dict = datapoint.to_dict()[\"fields\"]\n        self.sql_database.insert_into_table(\n            self.table_name, cast(Dict[Any, Any], datapoint_dict)\n        )\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Any) -> None:\n        \"\"\"Preprocess query.\n\n        This allows subclasses to pass in additional query", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/sql.py", "file_name": "sql.py"}, "index": 21, "child_indices": [], "ref_doc_id": "9b52c03318a639e578f5784857f4fa6403f1c7e2", "node_info": null}, "22": {"text": "      This allows subclasses to pass in additional query kwargs\n        to query, for instance arguments that are shared between the\n        index and the query class. By default, this does nothing.\n        This also allows subclasses to do validation.\n\n        \"\"\"\n        super()._preprocess_query(mode, query_kwargs)\n        # pass along sql_database, table_name\n        query_kwargs[\"sql_database\"] = self.sql_database\n        if mode == QueryMode.DEFAULT:\n            query_kwargs[\"ref_doc_id_column\"] = self.ref_doc_id_column\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/sql.py", "file_name": "sql.py"}, "index": 22, "child_indices": [], "ref_doc_id": "9b52c03318a639e578f5784857f4fa6403f1c7e2", "node_info": null}, "23": {"text": "\nThis code file contains the code for the BaseGPTStructStoreIndex class, which is used to create a structured store index from unstructured text. It includes methods for inserting documents into the index, validating fields, and building the index from documents. It also includes the GPTSQLStructStoreIndex class, which is used to create a SQLite structured store index. It includes methods for creating a SQLContextBuilder, creating a GPTSQLStructStoreIndexQuery, and creating a SQLDatabase.", "doc_id": null, "embedding": null, "extra_info": null, "index": 23, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "24": {"text": "The GPTSQLStructStoreIndex is an index that uses a SQL database under the hood. It allows users to specify a raw SQL query or a natural language query to retrieve their data. It requires a SQLDatabase, table_name, and table to be specified. It also allows for a ref_doc_id_column, table_context_dict, sql_context_builder, and context_documents_dict to be specified. It stores python types of each column and allows for additional query kwargs to be passed in. It also allows for validation of the query kwargs.", "doc_id": null, "embedding": null, "extra_info": null, "index": 24, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"23": {"text": "\nThis code file contains the code for the BaseGPTStructStoreIndex class, which is used to create a structured store index from unstructured text. It includes methods for inserting documents into the index, validating fields, and building the index from documents. It also includes the GPTSQLStructStoreIndex class, which is used to create a SQLite structured store index. It includes methods for creating a SQLContextBuilder, creating a GPTSQLStructStoreIndexQuery, and creating a SQLDatabase.", "doc_id": null, "embedding": null, "extra_info": null, "index": 23, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "24": {"text": "The GPTSQLStructStoreIndex is an index that uses a SQL database under the hood. It allows users to specify a raw SQL query or a natural language query to retrieve their data. It requires a SQLDatabase, table_name, and table to be specified. It also allows for a ref_doc_id_column, table_context_dict, sql_context_builder, and context_documents_dict to be specified. It stores python types of each column and allows for additional query kwargs to be passed in. It also allows for validation of the query kwargs.", "doc_id": null, "embedding": null, "extra_info": null, "index": 24, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}