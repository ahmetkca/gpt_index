{"index_struct": {"text": "\nThe summaries of these documents describe a GPT Index library that contains code for vector-store based data structures, including classes for the base vector store index, GPT Faiss Index, GPT Pinecone Index, GPT Qdrant Index, GPTSimpleVectorIndex, and GPTWeaviateIndex. These classes are used to store nodes that are keyed by embeddings, and those embeddings are stored within the respective vector store. During index construction, the document texts are chunked up, converted to nodes with text, and encoded in document embeddings stored within the vector store. During query time, the index uses the vector store to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes.", "doc_id": "0f0bfe98-caa8-4c66-af7f-c2fbab872177", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Vector-store based data structures.\"\"\"\n\nfrom gpt_index.indices.vector_store.faiss import GPTFaissIndex\nfrom gpt_index.indices.vector_store.pinecone import GPTPineconeIndex\nfrom gpt_index.indices.vector_store.qdrant import GPTQdrantIndex\nfrom gpt_index.indices.vector_store.simple import GPTSimpleVectorIndex\nfrom gpt_index.indices.vector_store.weaviate import GPTWeaviateIndex\n\n__all__ = [\n    \"GPTFaissIndex\",\n    \"GPTSimpleVectorIndex\",\n    \"GPTWeaviateIndex\",\n    \"GPTPineconeIndex\",\n    \"GPTQdrantIndex\",\n]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "095e81a317c46ee013bad276c00ee6523870019c", "node_info": null}, "1": {"text": "\"\"\"Base vector store index.\n\nAn index that that is built on top of an existing vector store.\n\n\"\"\"\n\nfrom abc import abstractmethod\nfrom typing import Any, Generic, Optional, Sequence, TypeVar\n\nfrom gpt_index.data_structs.data_structs import IndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.schema import BaseDocument\n\nBID = TypeVar(\"BID\", bound=IndexStruct)\n\n\nclass BaseGPTVectorStoreIndex(BaseGPTIndex[BID], Generic[BID]):\n    \"\"\"Base GPT Vector Store Index.\n\n    Args:\n        text_qa_template", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/base.py", "file_name": "base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "e7576a555defa75d31ec9e716c927ee9b9e97d8f", "node_info": null}, "2": {"text": " Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n    \"\"\"\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[BID] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/base.py", "file_name": "base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "e7576a555defa75d31ec9e716c927ee9b9e97d8f", "node_info": null}, "3": {"text": "  \"\"\"Initialize params.\"\"\"\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            embed_model=embed_model,\n            **kwargs,\n        )\n        # NOTE: when building the vector store index, text_qa_template is not partially\n        # formatted because we don't know the query ahead of time.\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/base.py", "file_name": "base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "e7576a555defa75d31ec9e716c927ee9b9e97d8f", "node_info": null}, "4": {"text": "1\n        )\n\n    @abstractmethod\n    def _add_document_to_index(\n        self,\n        index_struct: BID,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n\n    def _build_index_from_documents(self, documents: Sequence[BaseDocument]) -> BID:\n        \"\"\"Build index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n        index_struct = self.index_struct_cls()\n        for d in documents:\n            self._add_document_to_index(index_struct, d,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/base.py", "file_name": "base.py"}, "index": 4, "child_indices": [], "ref_doc_id": "e7576a555defa75d31ec9e716c927ee9b9e97d8f", "node_info": null}, "5": {"text": "self._add_document_to_index(index_struct, d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(self._index_struct, document, self._text_splitter)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/base.py", "file_name": "base.py"}, "index": 5, "child_indices": [], "ref_doc_id": "e7576a555defa75d31ec9e716c927ee9b9e97d8f", "node_info": null}, "6": {"text": "\"\"\"Faiss Vector store index.\n\nAn index that that is built on top of an existing vector store.\n\n\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence, Type, cast\n\nimport numpy as np\n\nfrom gpt_index.data_structs.data_structs import IndexDict\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.vector_store.faiss import GPTFaissIndexQuery\nfrom gpt_index.indices.vector_store.base import BaseGPTVectorStoreIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.schema import BaseDocument\n\n\nclass", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 6, "child_indices": [], "ref_doc_id": "f211883caf76a36c2a3c024807976f40924947fc", "node_info": null}, "7": {"text": "gpt_index.schema import BaseDocument\n\n\nclass GPTFaissIndex(BaseGPTVectorStoreIndex[IndexDict]):\n    \"\"\"GPT Faiss Index.\n\n    The GPTFaissIndex is a data structure where nodes are keyed by\n    embeddings, and those embeddings are stored within a Faiss index.\n    During index construction, the document texts are chunked up,\n    converted to nodes with text; they are then encoded in\n    document embeddings stored within Faiss.\n\n    During query time, the index uses Faiss to query for the top\n    k most similar nodes, and synthesizes an answer from the\n    retrieved nodes.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        faiss_index (faiss.Index): A Faiss Index object (required). Note: the index\n          ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 7, "child_indices": [], "ref_doc_id": "f211883caf76a36c2a3c024807976f40924947fc", "node_info": null}, "8": {"text": "Note: the index\n            will be reset during index construction.\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n    \"\"\"\n\n    index_struct_cls = IndexDict\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[IndexDict] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        faiss_index: Optional[Any] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 8, "child_indices": [], "ref_doc_id": "f211883caf76a36c2a3c024807976f40924947fc", "node_info": null}, "9": {"text": "       \"\"\"Initialize params.\"\"\"\n        import_err_msg = \"\"\"\n            `faiss` package not found. For instructions on\n            how to install `faiss` please visit\n            https://github.com/facebookresearch/faiss/wiki/Installing-Faiss\n        \"\"\"\n        try:\n            import faiss  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n\n        if faiss_index is None:\n            raise ValueError(\"faiss_index cannot be None.\")\n        if documents is not None and faiss_index.ntotal > 0:\n            raise ValueError(\n                \"If building a", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 9, "child_indices": [], "ref_doc_id": "f211883caf76a36c2a3c024807976f40924947fc", "node_info": null}, "10": {"text": "            \"If building a GPTFaissIndex from scratch, faiss_index must be empty.\"\n            )\n        self._faiss_index = cast(faiss.Index, faiss_index)\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            text_qa_template=text_qa_template,\n            llm_predictor=llm_predictor,\n            embed_model=embed_model,\n            **kwargs,\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 10, "child_indices": [], "ref_doc_id": "f211883caf76a36c2a3c024807976f40924947fc", "node_info": null}, "11": {"text": "       return {\n            QueryMode.DEFAULT: GPTFaissIndexQuery,\n            QueryMode.EMBEDDING: GPTFaissIndexQuery,\n        }\n\n    def _add_document_to_index(\n        self,\n        index_struct: IndexDict,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        nodes = self._get_nodes_from_document(document, text_splitter)\n        for n in nodes:\n            # add to FAISS\n            # NOTE: embeddings won't be stored in Node but rather in underlying\n            # Faiss store\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 11, "child_indices": [], "ref_doc_id": "f211883caf76a36c2a3c024807976f40924947fc", "node_info": null}, "12": {"text": "# Faiss store\n            if n.embedding is None:\n                text_embedding = self._embed_model.get_text_embedding(n.get_text())\n            else:\n                text_embedding = n.embedding\n\n            text_embedding_np = np.array(text_embedding, dtype=\"float32\")[np.newaxis, :]\n            new_id = str(self._faiss_index.ntotal)\n            self._faiss_index.add(text_embedding_np)\n\n            # add to index\n            index_struct.add_node(n, text_id=new_id)\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Any) -> None:\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 12, "child_indices": [], "ref_doc_id": "f211883caf76a36c2a3c024807976f40924947fc", "node_info": null}, "13": {"text": "Any) -> None:\n        \"\"\"Preprocess query.\n\n        This allows subclasses to pass in additional query kwargs\n        to query, for instance arguments that are shared between the\n        index and the query class. By default, this does nothing.\n        This also allows subclasses to do validation.\n\n        \"\"\"\n        super()._preprocess_query(mode, query_kwargs)\n        # pass along faiss_index\n        query_kwargs[\"faiss_index\"] = self._faiss_index\n\n    @classmethod\n    def load_from_disk(\n        cls, save_path: str, faiss_index_save_path: Optional[str] = None, **kwargs: Any\n    ) -> \"BaseGPTIndex\":\n        \"\"\"Load index from disk.\n\n        This method loads the index from a JSON file stored on disk. The index data\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 13, "child_indices": [], "ref_doc_id": "f211883caf76a36c2a3c024807976f40924947fc", "node_info": null}, "14": {"text": "file stored on disk. The index data\n        structure itself is preserved completely. If the index is defined over\n        subindices, those subindices will also be preserved (and subindices of\n        those subindices, etc.).\n        In GPTFaissIndex, we allow user to specify an additional\n        `faiss_index_save_path` to load faiss index from a file - that\n        way, the user does not have to recreate the faiss index outside\n        of this class.\n\n        Args:\n            save_path (str): The save_path of the file.\n            faiss_index_save_path (Optional[str]): The save_path of the\n                Faiss index file. If not specified, the Faiss index\n                will not be saved to disk.\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 14, "child_indices": [], "ref_doc_id": "f211883caf76a36c2a3c024807976f40924947fc", "node_info": null}, "15": {"text": "       will not be saved to disk.\n            **kwargs: Additional kwargs to pass to the index constructor.\n\n        Returns:\n            BaseGPTIndex: The loaded index.\n\n        \"\"\"\n        if faiss_index_save_path is not None:\n            import faiss\n\n            faiss_index = faiss.read_index(faiss_index_save_path)\n            return super().load_from_disk(save_path, faiss_index=faiss_index, **kwargs)\n        else:\n            return super().load_from_disk(save_path, **kwargs)\n\n    def save_to_disk(\n        self,\n        save_path: str,\n        faiss_index_save_path:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 15, "child_indices": [], "ref_doc_id": "f211883caf76a36c2a3c024807976f40924947fc", "node_info": null}, "16": {"text": "       faiss_index_save_path: Optional[str] = None,\n        **save_kwargs: Any,\n    ) -> None:\n        \"\"\"Save to file.\n\n        This method stores the index into a JSON file stored on disk.\n        In GPTFaissIndex, we allow user to specify an additional\n        `faiss_index_save_path` to save the faiss index to a file - that\n        way, the user can pass in the same argument in\n        `GPTFaissIndex.load_from_disk` without having to recreate\n        the Faiss index outside of this class.\n\n        Args:\n            save_path (str): The save_path of the file.\n            faiss_index_save_path (Optional[str]): The save_path of the\n                Faiss", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 16, "child_indices": [], "ref_doc_id": "f211883caf76a36c2a3c024807976f40924947fc", "node_info": null}, "17": {"text": "              Faiss index file. If not specified, the Faiss index\n                will not be saved to disk.\n\n        \"\"\"\n        super().save_to_disk(save_path, **save_kwargs)\n\n        if faiss_index_save_path is not None:\n            import faiss\n\n            faiss.write_index(self._faiss_index, faiss_index_save_path)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        raise NotImplementedError(\"Delete not yet implemented for Faiss index.\")\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 17, "child_indices": [], "ref_doc_id": "f211883caf76a36c2a3c024807976f40924947fc", "node_info": null}, "18": {"text": "\"\"\"Pinecone Vector store index.\n\nAn index that that is built on top of an existing vector store.\n\n\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence, Type, cast\n\nfrom gpt_index.data_structs.data_structs import PineconeIndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.vector_store.pinecone import GPTPineconeIndexQuery\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.schema import BaseDocument\nfrom gpt_index.utils import", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 18, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "19": {"text": "import BaseDocument\nfrom gpt_index.utils import get_new_id\n\n\nclass GPTPineconeIndex(BaseGPTIndex[PineconeIndexStruct]):\n    \"\"\"GPT Pinecone Index.\n\n    The GPTPineconeIndex is a data structure where nodes are keyed by\n    embeddings, and those embeddings are stored within a Pinecone index.\n    During index construction, the document texts are chunked up,\n    converted to nodes with text; they are then encoded in\n    document embeddings stored within Pinecone.\n\n    During query time, the index uses Pinecone to query for the top\n    k most similar nodes, and synthesizes an answer from the\n    retrieved nodes.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 19, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "20": {"text": "Embedding model to use for\n            embedding similarity.\n        chunk_size_limit (int): Maximum number of tokens per chunk. NOTE:\n            in Pinecone the default is 2048 due to metadata size restrictions.\n    \"\"\"\n\n    index_struct_cls = PineconeIndexStruct\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[PineconeIndexStruct] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        pinecone_index: Optional[Any] = None,\n        chunk_size_limit: int = 2048,\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 20, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "21": {"text": "   chunk_size_limit: int = 2048,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        import_err_msg = (\n            \"`pinecone` package not found, please run `pip install pinecone-client`\"\n        )\n        try:\n            import pinecone  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n        self._pinecone_index = cast(pinecone.Index, pinecone_index)\n\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        super().__init__(\n            documents=documents,\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 21, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "22": {"text": "           index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            embed_model=embed_model,\n            chunk_size_limit=chunk_size_limit,\n            **kwargs,\n        )\n        # NOTE: when building the vector store index, text_qa_template is not partially\n        # formatted because we don't know the query ahead of time.\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 22, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "23": {"text": "      \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTPineconeIndexQuery,\n            QueryMode.EMBEDDING: GPTPineconeIndexQuery,\n        }\n\n    def _add_document_to_index(\n        self,\n        index_struct: PineconeIndexStruct,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        nodes = self._get_nodes_from_document(document, text_splitter)\n        for n in nodes:\n            if n.embedding is None:\n                text_embedding =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 23, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "24": {"text": "           text_embedding = self._embed_model.get_text_embedding(n.get_text())\n            else:\n                text_embedding = n.embedding\n\n            while True:\n                new_id = get_new_id(set())\n                result = self._pinecone_index.fetch([new_id])\n                if len(result[\"vectors\"]) == 0:\n                    break\n\n            metadata = {\n                \"text\": n.get_text(),\n                \"doc_id\": document.get_doc_id(),\n            }\n\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 24, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "25": {"text": "           }\n\n            self._pinecone_index.upsert([(new_id, text_embedding, metadata)])\n\n    def _build_index_from_documents(\n        self, documents: Sequence[BaseDocument]\n    ) -> PineconeIndexStruct:\n        \"\"\"Build index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n        index_struct = self.index_struct_cls()\n        for d in documents:\n            self._add_document_to_index(index_struct, d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 25, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "26": {"text": "**insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(self._index_struct, document, self._text_splitter)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        # delete by filtering on the doc_id metadata\n        self._pinecone_index.delete(filter={\"doc_id\": {\"$eq\": doc_id}})\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Any) -> None:\n        \"\"\"Query mode to class.\"\"\"\n        super()._preprocess_query(mode, query_kwargs)\n        # pass along pinecone client and info\n        query_kwargs[\"pinecone_index\"] = self._pinecone_index\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 26, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "27": {"text": "\"\"\"Qdrant vector store index.\n\nAn index that is built on top of an existing Qdrant collection.\n\n\"\"\"\nfrom typing import Any, Dict, Optional, Sequence, Type, cast\n\nfrom gpt_index.data_structs.data_structs import QdrantIndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.vector_store.qdrant import GPTQdrantIndexQuery\nfrom gpt_index.indices.vector_store.base import BaseGPTVectorStoreIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 27, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "28": {"text": "import QuestionAnswerPrompt\nfrom gpt_index.schema import BaseDocument\nfrom gpt_index.utils import get_new_id\n\n\nclass GPTQdrantIndex(BaseGPTVectorStoreIndex[QdrantIndexStruct]):\n    \"\"\"GPT Qdrant Index.\n\n    The GPTQdrantIndex is a data structure where nodes are keyed by\n    embeddings, and those embeddings are stored within a Qdrant collection.\n    During index construction, the document texts are chunked up,\n    converted to nodes with text; they are then encoded in\n    document embeddings stored within Qdrant.\n\n    During query time, the index uses Qdrant to query for the top\n    k most similar nodes, and synthesizes an answer from the\n    retrieved nodes.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 28, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "29": {"text": "       embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        client (Optional[Any]): QdrantClient instance from `qdrant-client` package\n        collection_name: (Optional[str]): name of the Qdrant collection\n    \"\"\"\n\n    index_struct_cls = QdrantIndexStruct\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[QdrantIndexStruct] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        client: Optional[Any] = None,\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 29, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "30": {"text": "client: Optional[Any] = None,\n        collection_name: Optional[str] = None,\n        **kwargs: Any\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        import_err_msg = (\n            \"`qdrant-client` package not found, please run `pip install qdrant-client`\"\n        )\n        try:\n            import qdrant_client  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n\n        if client is None:\n            raise ValueError(\"client cannot be None.\")\n\n        if collection_name is None and index_struct is not None:\n            collection_name = index_struct.collection_name\n        if", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 30, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "31": {"text": "index_struct.collection_name\n        if collection_name is None:\n            raise ValueError(\"collection_name cannot be None.\")\n\n        self._client = cast(qdrant_client.QdrantClient, client)\n        self._collection_name = collection_name\n        self._collection_initialized = self._collection_exists(collection_name)\n\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        super().__init__(\n            documents,\n            index_struct,\n            text_qa_template,\n            llm_predictor,\n            embed_model,\n            **kwargs\n        )\n        # NOTE:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 31, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "32": {"text": "    )\n        # NOTE: when building the vector store index, text_qa_template is not partially\n        # formatted because we don't know the query ahead of time.\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTQdrantIndexQuery,\n            QueryMode.EMBEDDING: GPTQdrantIndexQuery,\n        }\n\n    def _add_document_to_index(\n        self,\n        index_struct:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 32, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "33": {"text": "  self,\n        index_struct: QdrantIndexStruct,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        from qdrant_client.http import models as rest\n        from qdrant_client.http.exceptions import UnexpectedResponse\n\n        nodes = self._get_nodes_from_document(document, text_splitter)\n        for n in nodes:\n            if n.embedding is None:\n                text_embedding = self._embed_model.get_text_embedding(n.get_text())\n            else:\n                text_embedding = n.embedding\n\n            collection_name =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 33, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "34": {"text": "           collection_name = index_struct.get_collection_name()\n\n            # Create the Qdrant collection, if it does not exist yet\n            if not self._collection_initialized:\n                self._create_collection(\n                    collection_name=collection_name,\n                    vector_size=len(text_embedding),\n                )\n                self._collection_initialized = True\n\n            while True:\n                new_id = get_new_id(set())\n                try:\n                   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 34, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "35": {"text": "                self._client.http.points_api.get_point(\n                        collection_name=collection_name, id=new_id\n                    )\n                except UnexpectedResponse:\n                    break\n\n            payload = {\n                \"doc_id\": document.get_doc_id(),\n                \"text\": n.get_text(),\n                \"index\": n.index,\n            }\n\n            self._client.upsert(\n                collection_name=collection_name,\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 35, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "36": {"text": "      collection_name=collection_name,\n                points=[\n                    rest.PointStruct(\n                        id=new_id,\n                        vector=text_embedding,\n                        payload=payload,\n                    )\n                ],\n            )\n\n    def _build_index_from_documents(\n        self, documents: Sequence[BaseDocument]\n    ) -> QdrantIndexStruct:\n        \"\"\"Build index from documents.\"\"\"\n        text_splitter =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 36, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "37": {"text": "       text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n        index_struct = self.index_struct_cls(collection_name=self._collection_name)\n        for d in documents:\n            self._add_document_to_index(index_struct, d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(self.index_struct, document, self._text_splitter)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        from", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 37, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "38": {"text": " \"\"\"Delete a document.\"\"\"\n        from qdrant_client.http import models as rest\n\n        self._client.delete(\n            collection_name=self._collection_name,\n            points_selector=rest.Filter(\n                must=[\n                    rest.FieldCondition(\n                        key=\"doc_id\", match=rest.MatchValue(value=doc_id)\n                    )\n                ]\n            ),\n        )\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Any) -> None:\n        \"\"\"Query mode to class.\"\"\"\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 38, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "39": {"text": " \"\"\"Query mode to class.\"\"\"\n        super()._preprocess_query(mode, query_kwargs)\n        # Pass along Qdrant client instance\n        query_kwargs[\"client\"] = self._client\n\n    def _create_collection(self, collection_name: str, vector_size: int) -> None:\n        \"\"\"Create a Qdrant collection.\"\"\"\n        from qdrant_client.http import models as rest\n\n        self._client.recreate_collection(\n            collection_name=collection_name,\n            vectors_config=rest.VectorParams(\n                size=vector_size,\n                distance=rest.Distance.COSINE,\n            ),\n        )\n\n    def _collection_exists(self, collection_name: str) ->", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 39, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "40": {"text": "def _collection_exists(self, collection_name: str) -> bool:\n        from qdrant_client.http.exceptions import UnexpectedResponse\n\n        try:\n            response = self._client.http.collections_api.get_collection(collection_name)\n            return response.result is not None\n        except UnexpectedResponse:\n            return False\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 40, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "41": {"text": "\"\"\"Simple vector store index.\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence, Type\n\nfrom gpt_index.data_structs.data_structs import SimpleIndexDict\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.vector_store.simple import GPTSimpleVectorIndexQuery\nfrom gpt_index.indices.vector_store.base import BaseGPTVectorStoreIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.schema import BaseDocument\nfrom gpt_index.utils import get_new_id\n\n\nclass GPTSimpleVectorIndex(BaseGPTVectorStoreIndex[SimpleIndexDict]):\n    \"\"\"GPT Simple Vector", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/simple.py", "file_name": "simple.py"}, "index": 41, "child_indices": [], "ref_doc_id": "b901531911d54b57fd5436cb636d971e68fea306", "node_info": null}, "42": {"text": "   \"\"\"GPT Simple Vector Index.\n\n    The GPTSimpleVectorIndex is a data structure where nodes are keyed by\n    embeddings, and those embeddings are stored within a simple dictionary.\n    During index construction, the document texts are chunked up,\n    converted to nodes with text; they are then encoded in\n    document embeddings stored within the dict.\n\n    During query time, the index uses the dict to query for the top\n    k most similar nodes, and synthesizes an answer from the\n    retrieved nodes.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n    \"\"\"\n\n    index_struct_cls = SimpleIndexDict\n\n    def __init__(\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/simple.py", "file_name": "simple.py"}, "index": 42, "child_indices": [], "ref_doc_id": "b901531911d54b57fd5436cb636d971e68fea306", "node_info": null}, "43": {"text": "= SimpleIndexDict\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[SimpleIndexDict] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            text_qa_template=text_qa_template,\n            llm_predictor=llm_predictor,\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/simple.py", "file_name": "simple.py"}, "index": 43, "child_indices": [], "ref_doc_id": "b901531911d54b57fd5436cb636d971e68fea306", "node_info": null}, "44": {"text": "           embed_model=embed_model,\n            **kwargs,\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTSimpleVectorIndexQuery,\n            QueryMode.EMBEDDING: GPTSimpleVectorIndexQuery,\n        }\n\n    def _add_document_to_index(\n        self,\n        index_struct: SimpleIndexDict,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        nodes =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/simple.py", "file_name": "simple.py"}, "index": 44, "child_indices": [], "ref_doc_id": "b901531911d54b57fd5436cb636d971e68fea306", "node_info": null}, "45": {"text": "document to index.\"\"\"\n        nodes = self._get_nodes_from_document(document, text_splitter)\n        for n in nodes:\n            # add to in-memory dict\n            # NOTE: embeddings won't be stored in Node but rather in underlying\n            # Faiss store\n            if n.embedding is None:\n                text_embedding = self._embed_model.get_text_embedding(n.get_text())\n            else:\n                text_embedding = n.embedding\n            new_id = get_new_id(set(index_struct.nodes_dict.keys()))\n\n            # add to index\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/simple.py", "file_name": "simple.py"}, "index": 45, "child_indices": [], "ref_doc_id": "b901531911d54b57fd5436cb636d971e68fea306", "node_info": null}, "46": {"text": "# add to index\n            index_struct.add_node(n, text_id=new_id)\n            # TODO: deprecate\n            index_struct.add_to_embedding_dict(new_id, text_embedding)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        text_ids_to_delete = set()\n        int_ids_to_delete = set()\n        for text_id, int_id in self.index_struct.id_map.items():\n            node = self.index_struct.nodes_dict[int_id]\n            if node.ref_doc_id != doc_id:\n                continue\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/simple.py", "file_name": "simple.py"}, "index": 46, "child_indices": [], "ref_doc_id": "b901531911d54b57fd5436cb636d971e68fea306", "node_info": null}, "47": {"text": "   continue\n            text_ids_to_delete.add(text_id)\n            int_ids_to_delete.add(int_id)\n\n        for int_id, text_id in zip(int_ids_to_delete, text_ids_to_delete):\n            del self.index_struct.nodes_dict[int_id]\n            del self.index_struct.id_map[text_id]\n            del self.index_struct.embedding_dict[text_id]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/simple.py", "file_name": "simple.py"}, "index": 47, "child_indices": [], "ref_doc_id": "b901531911d54b57fd5436cb636d971e68fea306", "node_info": null}, "48": {"text": "\"\"\"Weaviate Vector store index.\n\nAn index that that is built on top of an existing vector store.\n\n\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence, Type, cast\n\nfrom gpt_index.data_structs.data_structs import WeaviateIndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.vector_store.weaviate import GPTWeaviateIndexQuery\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.readers.weaviate.data_structs import", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 48, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "49": {"text": "import WeaviateNode\nfrom gpt_index.readers.weaviate.utils import get_default_class_prefix\nfrom gpt_index.schema import BaseDocument\n\n\nclass GPTWeaviateIndex(BaseGPTIndex[WeaviateIndexStruct]):\n    \"\"\"GPT Weaviate Index.\n\n    The GPTWeaviateIndex is a data structure where nodes are keyed by\n    embeddings, and those embeddings are stored within a Weaviate index.\n    During index construction, the document texts are chunked up,\n    converted to nodes with text; they are then encoded in\n    document embeddings stored within Weaviate.\n\n    During query time, the index uses Weaviate to query for the top\n    k most similar nodes, and synthesizes an answer from the\n    retrieved nodes.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 49, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "50": {"text": "(see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n    \"\"\"\n\n    index_struct_cls = WeaviateIndexStruct\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[WeaviateIndexStruct] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        weaviate_client: Optional[Any] = None,\n        class_prefix: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 50, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "51": {"text": "**kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        import_err_msg = (\n            \"`weaviate` package not found, please run `pip install weaviate-client`\"\n        )\n        try:\n            import weaviate  # noqa: F401\n            from weaviate import Client  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n\n        self.client = cast(Client, weaviate_client)\n        if index_struct is not None:\n            if class_prefix is not None:\n                raise ValueError(\n                    \"class_prefix", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 51, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "52": {"text": "            \"class_prefix must be None when index_struct is not None.\"\n                )\n            self.class_prefix = index_struct.get_class_prefix()\n        else:\n            self.class_prefix = class_prefix or get_default_class_prefix()\n        # try to create schema\n        WeaviateNode.create_schema(self.client, self.class_prefix)\n\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 52, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "53": {"text": "           embed_model=embed_model,\n            **kwargs,\n        )\n        # NOTE: when building the vector store index, text_qa_template is not partially\n        # formatted because we don't know the query ahead of time.\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTWeaviateIndexQuery,\n            QueryMode.EMBEDDING: GPTWeaviateIndexQuery,\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 53, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "54": {"text": "GPTWeaviateIndexQuery,\n        }\n\n    def _add_document_to_index(\n        self,\n        index_struct: WeaviateIndexStruct,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        nodes = self._get_nodes_from_document(document, text_splitter)\n        for n in nodes:\n            if n.embedding is None:\n                n.embedding = self._embed_model.get_text_embedding(n.get_text())\n            WeaviateNode.from_gpt_index(self.client, n, index_struct.get_class_prefix())\n\n    def _build_index_from_documents(\n        self,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 54, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "55": {"text": "       self, documents: Sequence[BaseDocument]\n    ) -> WeaviateIndexStruct:\n        \"\"\"Build index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n        index_struct = self.index_struct_cls(class_prefix=self.class_prefix)\n        for d in documents:\n            self._add_document_to_index(index_struct, d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(self._index_struct, document, self._text_splitter)\n\n    def", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 55, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "56": {"text": "document, self._text_splitter)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        WeaviateNode.delete_document(self.client, doc_id, self.class_prefix)\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Any) -> None:\n        \"\"\"Query mode to class.\"\"\"\n        super()._preprocess_query(mode, query_kwargs)\n        # pass along weaviate client and info\n        query_kwargs[\"weaviate_client\"] = self.client\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 56, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "57": {"text": "This code file contains code for vector-store based data structures. It includes classes for the base vector store index, GPT Faiss Index, and other related classes. The base vector store index initializes parameters and has methods for adding documents to the index, building the index from documents, and inserting documents. The GPT Faiss Index is a data structure where nodes are keyed by embeddings, and those embeddings are stored within a Faiss index. During index construction, the document texts are chunked up, converted to nodes with text; they are then encoded in document embeddings stored within Faiss. During query time, the index uses Faiss to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 57, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "58": {"text": "The file faiss.py is a part of the GPT Index library and is used to create a Faiss index. It contains methods for preprocessing queries, loading and saving the index from/to disk, and deleting documents from the index. The GPTPineconeIndex class in the file pinecone.py is used to create a Pinecone vector store index. It contains methods for initializing parameters, getting the query map, and adding documents to the index.", "doc_id": null, "embedding": null, "extra_info": null, "index": 58, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "59": {"text": "The file pinecone.py is a vector store index that is built on top of an existing Pinecone collection. It is used to store nodes that are keyed by embeddings, and those embeddings are stored within the Pinecone collection. During index construction, the document texts are chunked up, converted to nodes with text, and then encoded in document embeddings stored within Pinecone. During query time, the index uses Pinecone to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes. The file qdrant.py is a vector store index that is built on top of an existing Qdrant collection. It is used to store nodes that are keyed by embeddings, and those embeddings are stored within the Qdrant collection. During index construction, the document texts are chunked up, converted to nodes with text, and then encoded in document embeddings stored within Qdrant. During query time, the index uses Qdrant to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 59, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}, "60": {"text": "The GPTSimpleVectorIndex is a data structure where nodes are keyed by embeddings, and those embeddings are stored within a simple dictionary. During index construction, the document texts are chunked up, converted to nodes with text; they are then encoded in document embeddings stored within the dict. During query time, the index uses the dict to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes. The index is initialized with documents, an index structure, a text-qa template, an LLM predictor, and an embedding model. It has a get_query_map() method which returns a dictionary of query modes and their corresponding query classes. The _add_document_to_index() method adds documents to the index structure, and the _delete() method deletes documents from the index structure.", "doc_id": null, "embedding": null, "extra_info": null, "index": 60, "child_indices": [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], "ref_doc_id": null, "node_info": null}, "61": {"text": "The GPTWeaviateIndex is a data structure that uses an existing vector store to store nodes keyed by embeddings. During index construction, the document texts are chunked up, converted to nodes with text, and encoded in document embeddings stored within Weaviate. During query time, the index uses Weaviate to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes. The GPTWeaviateIndex class initializes parameters, creates a schema, and provides methods for adding, deleting, and querying documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 61, "child_indices": [48, 49, 50, 51, 52, 53, 54, 55, 56], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"57": {"text": "This code file contains code for vector-store based data structures. It includes classes for the base vector store index, GPT Faiss Index, and other related classes. The base vector store index initializes parameters and has methods for adding documents to the index, building the index from documents, and inserting documents. The GPT Faiss Index is a data structure where nodes are keyed by embeddings, and those embeddings are stored within a Faiss index. During index construction, the document texts are chunked up, converted to nodes with text; they are then encoded in document embeddings stored within Faiss. During query time, the index uses Faiss to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 57, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "58": {"text": "The file faiss.py is a part of the GPT Index library and is used to create a Faiss index. It contains methods for preprocessing queries, loading and saving the index from/to disk, and deleting documents from the index. The GPTPineconeIndex class in the file pinecone.py is used to create a Pinecone vector store index. It contains methods for initializing parameters, getting the query map, and adding documents to the index.", "doc_id": null, "embedding": null, "extra_info": null, "index": 58, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "59": {"text": "The file pinecone.py is a vector store index that is built on top of an existing Pinecone collection. It is used to store nodes that are keyed by embeddings, and those embeddings are stored within the Pinecone collection. During index construction, the document texts are chunked up, converted to nodes with text, and then encoded in document embeddings stored within Pinecone. During query time, the index uses Pinecone to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes. The file qdrant.py is a vector store index that is built on top of an existing Qdrant collection. It is used to store nodes that are keyed by embeddings, and those embeddings are stored within the Qdrant collection. During index construction, the document texts are chunked up, converted to nodes with text, and then encoded in document embeddings stored within Qdrant. During query time, the index uses Qdrant to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 59, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}, "60": {"text": "The GPTSimpleVectorIndex is a data structure where nodes are keyed by embeddings, and those embeddings are stored within a simple dictionary. During index construction, the document texts are chunked up, converted to nodes with text; they are then encoded in document embeddings stored within the dict. During query time, the index uses the dict to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes. The index is initialized with documents, an index structure, a text-qa template, an LLM predictor, and an embedding model. It has a get_query_map() method which returns a dictionary of query modes and their corresponding query classes. The _add_document_to_index() method adds documents to the index structure, and the _delete() method deletes documents from the index structure.", "doc_id": null, "embedding": null, "extra_info": null, "index": 60, "child_indices": [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], "ref_doc_id": null, "node_info": null}, "61": {"text": "The GPTWeaviateIndex is a data structure that uses an existing vector store to store nodes keyed by embeddings. During index construction, the document texts are chunked up, converted to nodes with text, and encoded in document embeddings stored within Weaviate. During query time, the index uses Weaviate to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes. The GPTWeaviateIndex class initializes parameters, creates a schema, and provides methods for adding, deleting, and querying documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 61, "child_indices": [48, 49, 50, 51, 52, 53, 54, 55, 56], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"095e81a317c46ee013bad276c00ee6523870019c": {"text": "\"\"\"Vector-store based data structures.\"\"\"\n\nfrom gpt_index.indices.vector_store.faiss import GPTFaissIndex\nfrom gpt_index.indices.vector_store.pinecone import GPTPineconeIndex\nfrom gpt_index.indices.vector_store.qdrant import GPTQdrantIndex\nfrom gpt_index.indices.vector_store.simple import GPTSimpleVectorIndex\nfrom gpt_index.indices.vector_store.weaviate import GPTWeaviateIndex\n\n__all__ = [\n    \"GPTFaissIndex\",\n    \"GPTSimpleVectorIndex\",\n    \"GPTWeaviateIndex\",\n    \"GPTPineconeIndex\",\n    \"GPTQdrantIndex\",\n]\n", "doc_id": "095e81a317c46ee013bad276c00ee6523870019c", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "e7576a555defa75d31ec9e716c927ee9b9e97d8f": {"text": "\"\"\"Base vector store index.\n\nAn index that that is built on top of an existing vector store.\n\n\"\"\"\n\nfrom abc import abstractmethod\nfrom typing import Any, Generic, Optional, Sequence, TypeVar\n\nfrom gpt_index.data_structs.data_structs import IndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.schema import BaseDocument\n\nBID = TypeVar(\"BID\", bound=IndexStruct)\n\n\nclass BaseGPTVectorStoreIndex(BaseGPTIndex[BID], Generic[BID]):\n    \"\"\"Base GPT Vector Store Index.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n    \"\"\"\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[BID] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            embed_model=embed_model,\n            **kwargs,\n        )\n        # NOTE: when building the vector store index, text_qa_template is not partially\n        # formatted because we don't know the query ahead of time.\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n\n    @abstractmethod\n    def _add_document_to_index(\n        self,\n        index_struct: BID,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n\n    def _build_index_from_documents(self, documents: Sequence[BaseDocument]) -> BID:\n        \"\"\"Build index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n        index_struct = self.index_struct_cls()\n        for d in documents:\n            self._add_document_to_index(index_struct, d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(self._index_struct, document, self._text_splitter)\n", "doc_id": "e7576a555defa75d31ec9e716c927ee9b9e97d8f", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/base.py", "file_name": "base.py"}, "__type__": "Document"}, "f211883caf76a36c2a3c024807976f40924947fc": {"text": "\"\"\"Faiss Vector store index.\n\nAn index that that is built on top of an existing vector store.\n\n\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence, Type, cast\n\nimport numpy as np\n\nfrom gpt_index.data_structs.data_structs import IndexDict\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.vector_store.faiss import GPTFaissIndexQuery\nfrom gpt_index.indices.vector_store.base import BaseGPTVectorStoreIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.schema import BaseDocument\n\n\nclass GPTFaissIndex(BaseGPTVectorStoreIndex[IndexDict]):\n    \"\"\"GPT Faiss Index.\n\n    The GPTFaissIndex is a data structure where nodes are keyed by\n    embeddings, and those embeddings are stored within a Faiss index.\n    During index construction, the document texts are chunked up,\n    converted to nodes with text; they are then encoded in\n    document embeddings stored within Faiss.\n\n    During query time, the index uses Faiss to query for the top\n    k most similar nodes, and synthesizes an answer from the\n    retrieved nodes.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        faiss_index (faiss.Index): A Faiss Index object (required). Note: the index\n            will be reset during index construction.\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n    \"\"\"\n\n    index_struct_cls = IndexDict\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[IndexDict] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        faiss_index: Optional[Any] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        import_err_msg = \"\"\"\n            `faiss` package not found. For instructions on\n            how to install `faiss` please visit\n            https://github.com/facebookresearch/faiss/wiki/Installing-Faiss\n        \"\"\"\n        try:\n            import faiss  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n\n        if faiss_index is None:\n            raise ValueError(\"faiss_index cannot be None.\")\n        if documents is not None and faiss_index.ntotal > 0:\n            raise ValueError(\n                \"If building a GPTFaissIndex from scratch, faiss_index must be empty.\"\n            )\n        self._faiss_index = cast(faiss.Index, faiss_index)\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            text_qa_template=text_qa_template,\n            llm_predictor=llm_predictor,\n            embed_model=embed_model,\n            **kwargs,\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTFaissIndexQuery,\n            QueryMode.EMBEDDING: GPTFaissIndexQuery,\n        }\n\n    def _add_document_to_index(\n        self,\n        index_struct: IndexDict,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        nodes = self._get_nodes_from_document(document, text_splitter)\n        for n in nodes:\n            # add to FAISS\n            # NOTE: embeddings won't be stored in Node but rather in underlying\n            # Faiss store\n            if n.embedding is None:\n                text_embedding = self._embed_model.get_text_embedding(n.get_text())\n            else:\n                text_embedding = n.embedding\n\n            text_embedding_np = np.array(text_embedding, dtype=\"float32\")[np.newaxis, :]\n            new_id = str(self._faiss_index.ntotal)\n            self._faiss_index.add(text_embedding_np)\n\n            # add to index\n            index_struct.add_node(n, text_id=new_id)\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Any) -> None:\n        \"\"\"Preprocess query.\n\n        This allows subclasses to pass in additional query kwargs\n        to query, for instance arguments that are shared between the\n        index and the query class. By default, this does nothing.\n        This also allows subclasses to do validation.\n\n        \"\"\"\n        super()._preprocess_query(mode, query_kwargs)\n        # pass along faiss_index\n        query_kwargs[\"faiss_index\"] = self._faiss_index\n\n    @classmethod\n    def load_from_disk(\n        cls, save_path: str, faiss_index_save_path: Optional[str] = None, **kwargs: Any\n    ) -> \"BaseGPTIndex\":\n        \"\"\"Load index from disk.\n\n        This method loads the index from a JSON file stored on disk. The index data\n        structure itself is preserved completely. If the index is defined over\n        subindices, those subindices will also be preserved (and subindices of\n        those subindices, etc.).\n        In GPTFaissIndex, we allow user to specify an additional\n        `faiss_index_save_path` to load faiss index from a file - that\n        way, the user does not have to recreate the faiss index outside\n        of this class.\n\n        Args:\n            save_path (str): The save_path of the file.\n            faiss_index_save_path (Optional[str]): The save_path of the\n                Faiss index file. If not specified, the Faiss index\n                will not be saved to disk.\n            **kwargs: Additional kwargs to pass to the index constructor.\n\n        Returns:\n            BaseGPTIndex: The loaded index.\n\n        \"\"\"\n        if faiss_index_save_path is not None:\n            import faiss\n\n            faiss_index = faiss.read_index(faiss_index_save_path)\n            return super().load_from_disk(save_path, faiss_index=faiss_index, **kwargs)\n        else:\n            return super().load_from_disk(save_path, **kwargs)\n\n    def save_to_disk(\n        self,\n        save_path: str,\n        faiss_index_save_path: Optional[str] = None,\n        **save_kwargs: Any,\n    ) -> None:\n        \"\"\"Save to file.\n\n        This method stores the index into a JSON file stored on disk.\n        In GPTFaissIndex, we allow user to specify an additional\n        `faiss_index_save_path` to save the faiss index to a file - that\n        way, the user can pass in the same argument in\n        `GPTFaissIndex.load_from_disk` without having to recreate\n        the Faiss index outside of this class.\n\n        Args:\n            save_path (str): The save_path of the file.\n            faiss_index_save_path (Optional[str]): The save_path of the\n                Faiss index file. If not specified, the Faiss index\n                will not be saved to disk.\n\n        \"\"\"\n        super().save_to_disk(save_path, **save_kwargs)\n\n        if faiss_index_save_path is not None:\n            import faiss\n\n            faiss.write_index(self._faiss_index, faiss_index_save_path)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        raise NotImplementedError(\"Delete not yet implemented for Faiss index.\")\n", "doc_id": "f211883caf76a36c2a3c024807976f40924947fc", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/faiss.py", "file_name": "faiss.py"}, "__type__": "Document"}, "b51addbca1a5a149e748be38f9cfe1d28e5577c3": {"text": "\"\"\"Pinecone Vector store index.\n\nAn index that that is built on top of an existing vector store.\n\n\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence, Type, cast\n\nfrom gpt_index.data_structs.data_structs import PineconeIndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.vector_store.pinecone import GPTPineconeIndexQuery\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.schema import BaseDocument\nfrom gpt_index.utils import get_new_id\n\n\nclass GPTPineconeIndex(BaseGPTIndex[PineconeIndexStruct]):\n    \"\"\"GPT Pinecone Index.\n\n    The GPTPineconeIndex is a data structure where nodes are keyed by\n    embeddings, and those embeddings are stored within a Pinecone index.\n    During index construction, the document texts are chunked up,\n    converted to nodes with text; they are then encoded in\n    document embeddings stored within Pinecone.\n\n    During query time, the index uses Pinecone to query for the top\n    k most similar nodes, and synthesizes an answer from the\n    retrieved nodes.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        chunk_size_limit (int): Maximum number of tokens per chunk. NOTE:\n            in Pinecone the default is 2048 due to metadata size restrictions.\n    \"\"\"\n\n    index_struct_cls = PineconeIndexStruct\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[PineconeIndexStruct] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        pinecone_index: Optional[Any] = None,\n        chunk_size_limit: int = 2048,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        import_err_msg = (\n            \"`pinecone` package not found, please run `pip install pinecone-client`\"\n        )\n        try:\n            import pinecone  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n        self._pinecone_index = cast(pinecone.Index, pinecone_index)\n\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            embed_model=embed_model,\n            chunk_size_limit=chunk_size_limit,\n            **kwargs,\n        )\n        # NOTE: when building the vector store index, text_qa_template is not partially\n        # formatted because we don't know the query ahead of time.\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTPineconeIndexQuery,\n            QueryMode.EMBEDDING: GPTPineconeIndexQuery,\n        }\n\n    def _add_document_to_index(\n        self,\n        index_struct: PineconeIndexStruct,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        nodes = self._get_nodes_from_document(document, text_splitter)\n        for n in nodes:\n            if n.embedding is None:\n                text_embedding = self._embed_model.get_text_embedding(n.get_text())\n            else:\n                text_embedding = n.embedding\n\n            while True:\n                new_id = get_new_id(set())\n                result = self._pinecone_index.fetch([new_id])\n                if len(result[\"vectors\"]) == 0:\n                    break\n\n            metadata = {\n                \"text\": n.get_text(),\n                \"doc_id\": document.get_doc_id(),\n            }\n\n            self._pinecone_index.upsert([(new_id, text_embedding, metadata)])\n\n    def _build_index_from_documents(\n        self, documents: Sequence[BaseDocument]\n    ) -> PineconeIndexStruct:\n        \"\"\"Build index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n        index_struct = self.index_struct_cls()\n        for d in documents:\n            self._add_document_to_index(index_struct, d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(self._index_struct, document, self._text_splitter)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        # delete by filtering on the doc_id metadata\n        self._pinecone_index.delete(filter={\"doc_id\": {\"$eq\": doc_id}})\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Any) -> None:\n        \"\"\"Query mode to class.\"\"\"\n        super()._preprocess_query(mode, query_kwargs)\n        # pass along pinecone client and info\n        query_kwargs[\"pinecone_index\"] = self._pinecone_index\n", "doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "__type__": "Document"}, "bf1118e9a20657b90f4e5cbb9ae924330db5f50f": {"text": "\"\"\"Qdrant vector store index.\n\nAn index that is built on top of an existing Qdrant collection.\n\n\"\"\"\nfrom typing import Any, Dict, Optional, Sequence, Type, cast\n\nfrom gpt_index.data_structs.data_structs import QdrantIndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.vector_store.qdrant import GPTQdrantIndexQuery\nfrom gpt_index.indices.vector_store.base import BaseGPTVectorStoreIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.schema import BaseDocument\nfrom gpt_index.utils import get_new_id\n\n\nclass GPTQdrantIndex(BaseGPTVectorStoreIndex[QdrantIndexStruct]):\n    \"\"\"GPT Qdrant Index.\n\n    The GPTQdrantIndex is a data structure where nodes are keyed by\n    embeddings, and those embeddings are stored within a Qdrant collection.\n    During index construction, the document texts are chunked up,\n    converted to nodes with text; they are then encoded in\n    document embeddings stored within Qdrant.\n\n    During query time, the index uses Qdrant to query for the top\n    k most similar nodes, and synthesizes an answer from the\n    retrieved nodes.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        client (Optional[Any]): QdrantClient instance from `qdrant-client` package\n        collection_name: (Optional[str]): name of the Qdrant collection\n    \"\"\"\n\n    index_struct_cls = QdrantIndexStruct\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[QdrantIndexStruct] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        client: Optional[Any] = None,\n        collection_name: Optional[str] = None,\n        **kwargs: Any\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        import_err_msg = (\n            \"`qdrant-client` package not found, please run `pip install qdrant-client`\"\n        )\n        try:\n            import qdrant_client  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n\n        if client is None:\n            raise ValueError(\"client cannot be None.\")\n\n        if collection_name is None and index_struct is not None:\n            collection_name = index_struct.collection_name\n        if collection_name is None:\n            raise ValueError(\"collection_name cannot be None.\")\n\n        self._client = cast(qdrant_client.QdrantClient, client)\n        self._collection_name = collection_name\n        self._collection_initialized = self._collection_exists(collection_name)\n\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        super().__init__(\n            documents,\n            index_struct,\n            text_qa_template,\n            llm_predictor,\n            embed_model,\n            **kwargs\n        )\n        # NOTE: when building the vector store index, text_qa_template is not partially\n        # formatted because we don't know the query ahead of time.\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTQdrantIndexQuery,\n            QueryMode.EMBEDDING: GPTQdrantIndexQuery,\n        }\n\n    def _add_document_to_index(\n        self,\n        index_struct: QdrantIndexStruct,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        from qdrant_client.http import models as rest\n        from qdrant_client.http.exceptions import UnexpectedResponse\n\n        nodes = self._get_nodes_from_document(document, text_splitter)\n        for n in nodes:\n            if n.embedding is None:\n                text_embedding = self._embed_model.get_text_embedding(n.get_text())\n            else:\n                text_embedding = n.embedding\n\n            collection_name = index_struct.get_collection_name()\n\n            # Create the Qdrant collection, if it does not exist yet\n            if not self._collection_initialized:\n                self._create_collection(\n                    collection_name=collection_name,\n                    vector_size=len(text_embedding),\n                )\n                self._collection_initialized = True\n\n            while True:\n                new_id = get_new_id(set())\n                try:\n                    self._client.http.points_api.get_point(\n                        collection_name=collection_name, id=new_id\n                    )\n                except UnexpectedResponse:\n                    break\n\n            payload = {\n                \"doc_id\": document.get_doc_id(),\n                \"text\": n.get_text(),\n                \"index\": n.index,\n            }\n\n            self._client.upsert(\n                collection_name=collection_name,\n                points=[\n                    rest.PointStruct(\n                        id=new_id,\n                        vector=text_embedding,\n                        payload=payload,\n                    )\n                ],\n            )\n\n    def _build_index_from_documents(\n        self, documents: Sequence[BaseDocument]\n    ) -> QdrantIndexStruct:\n        \"\"\"Build index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n        index_struct = self.index_struct_cls(collection_name=self._collection_name)\n        for d in documents:\n            self._add_document_to_index(index_struct, d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(self.index_struct, document, self._text_splitter)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        from qdrant_client.http import models as rest\n\n        self._client.delete(\n            collection_name=self._collection_name,\n            points_selector=rest.Filter(\n                must=[\n                    rest.FieldCondition(\n                        key=\"doc_id\", match=rest.MatchValue(value=doc_id)\n                    )\n                ]\n            ),\n        )\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Any) -> None:\n        \"\"\"Query mode to class.\"\"\"\n        super()._preprocess_query(mode, query_kwargs)\n        # Pass along Qdrant client instance\n        query_kwargs[\"client\"] = self._client\n\n    def _create_collection(self, collection_name: str, vector_size: int) -> None:\n        \"\"\"Create a Qdrant collection.\"\"\"\n        from qdrant_client.http import models as rest\n\n        self._client.recreate_collection(\n            collection_name=collection_name,\n            vectors_config=rest.VectorParams(\n                size=vector_size,\n                distance=rest.Distance.COSINE,\n            ),\n        )\n\n    def _collection_exists(self, collection_name: str) -> bool:\n        from qdrant_client.http.exceptions import UnexpectedResponse\n\n        try:\n            response = self._client.http.collections_api.get_collection(collection_name)\n            return response.result is not None\n        except UnexpectedResponse:\n            return False\n", "doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "__type__": "Document"}, "b901531911d54b57fd5436cb636d971e68fea306": {"text": "\"\"\"Simple vector store index.\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence, Type\n\nfrom gpt_index.data_structs.data_structs import SimpleIndexDict\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.vector_store.simple import GPTSimpleVectorIndexQuery\nfrom gpt_index.indices.vector_store.base import BaseGPTVectorStoreIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.schema import BaseDocument\nfrom gpt_index.utils import get_new_id\n\n\nclass GPTSimpleVectorIndex(BaseGPTVectorStoreIndex[SimpleIndexDict]):\n    \"\"\"GPT Simple Vector Index.\n\n    The GPTSimpleVectorIndex is a data structure where nodes are keyed by\n    embeddings, and those embeddings are stored within a simple dictionary.\n    During index construction, the document texts are chunked up,\n    converted to nodes with text; they are then encoded in\n    document embeddings stored within the dict.\n\n    During query time, the index uses the dict to query for the top\n    k most similar nodes, and synthesizes an answer from the\n    retrieved nodes.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n    \"\"\"\n\n    index_struct_cls = SimpleIndexDict\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[SimpleIndexDict] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            text_qa_template=text_qa_template,\n            llm_predictor=llm_predictor,\n            embed_model=embed_model,\n            **kwargs,\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTSimpleVectorIndexQuery,\n            QueryMode.EMBEDDING: GPTSimpleVectorIndexQuery,\n        }\n\n    def _add_document_to_index(\n        self,\n        index_struct: SimpleIndexDict,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        nodes = self._get_nodes_from_document(document, text_splitter)\n        for n in nodes:\n            # add to in-memory dict\n            # NOTE: embeddings won't be stored in Node but rather in underlying\n            # Faiss store\n            if n.embedding is None:\n                text_embedding = self._embed_model.get_text_embedding(n.get_text())\n            else:\n                text_embedding = n.embedding\n            new_id = get_new_id(set(index_struct.nodes_dict.keys()))\n\n            # add to index\n            index_struct.add_node(n, text_id=new_id)\n            # TODO: deprecate\n            index_struct.add_to_embedding_dict(new_id, text_embedding)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        text_ids_to_delete = set()\n        int_ids_to_delete = set()\n        for text_id, int_id in self.index_struct.id_map.items():\n            node = self.index_struct.nodes_dict[int_id]\n            if node.ref_doc_id != doc_id:\n                continue\n            text_ids_to_delete.add(text_id)\n            int_ids_to_delete.add(int_id)\n\n        for int_id, text_id in zip(int_ids_to_delete, text_ids_to_delete):\n            del self.index_struct.nodes_dict[int_id]\n            del self.index_struct.id_map[text_id]\n            del self.index_struct.embedding_dict[text_id]\n", "doc_id": "b901531911d54b57fd5436cb636d971e68fea306", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/simple.py", "file_name": "simple.py"}, "__type__": "Document"}, "9c650f2a0631e9cc1398df985038f660d8b10030": {"text": "\"\"\"Weaviate Vector store index.\n\nAn index that that is built on top of an existing vector store.\n\n\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence, Type, cast\n\nfrom gpt_index.data_structs.data_structs import WeaviateIndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.vector_store.weaviate import GPTWeaviateIndexQuery\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.readers.weaviate.data_structs import WeaviateNode\nfrom gpt_index.readers.weaviate.utils import get_default_class_prefix\nfrom gpt_index.schema import BaseDocument\n\n\nclass GPTWeaviateIndex(BaseGPTIndex[WeaviateIndexStruct]):\n    \"\"\"GPT Weaviate Index.\n\n    The GPTWeaviateIndex is a data structure where nodes are keyed by\n    embeddings, and those embeddings are stored within a Weaviate index.\n    During index construction, the document texts are chunked up,\n    converted to nodes with text; they are then encoded in\n    document embeddings stored within Weaviate.\n\n    During query time, the index uses Weaviate to query for the top\n    k most similar nodes, and synthesizes an answer from the\n    retrieved nodes.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n    \"\"\"\n\n    index_struct_cls = WeaviateIndexStruct\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[WeaviateIndexStruct] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        weaviate_client: Optional[Any] = None,\n        class_prefix: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        import_err_msg = (\n            \"`weaviate` package not found, please run `pip install weaviate-client`\"\n        )\n        try:\n            import weaviate  # noqa: F401\n            from weaviate import Client  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n\n        self.client = cast(Client, weaviate_client)\n        if index_struct is not None:\n            if class_prefix is not None:\n                raise ValueError(\n                    \"class_prefix must be None when index_struct is not None.\"\n                )\n            self.class_prefix = index_struct.get_class_prefix()\n        else:\n            self.class_prefix = class_prefix or get_default_class_prefix()\n        # try to create schema\n        WeaviateNode.create_schema(self.client, self.class_prefix)\n\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            embed_model=embed_model,\n            **kwargs,\n        )\n        # NOTE: when building the vector store index, text_qa_template is not partially\n        # formatted because we don't know the query ahead of time.\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTWeaviateIndexQuery,\n            QueryMode.EMBEDDING: GPTWeaviateIndexQuery,\n        }\n\n    def _add_document_to_index(\n        self,\n        index_struct: WeaviateIndexStruct,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        nodes = self._get_nodes_from_document(document, text_splitter)\n        for n in nodes:\n            if n.embedding is None:\n                n.embedding = self._embed_model.get_text_embedding(n.get_text())\n            WeaviateNode.from_gpt_index(self.client, n, index_struct.get_class_prefix())\n\n    def _build_index_from_documents(\n        self, documents: Sequence[BaseDocument]\n    ) -> WeaviateIndexStruct:\n        \"\"\"Build index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n        index_struct = self.index_struct_cls(class_prefix=self.class_prefix)\n        for d in documents:\n            self._add_document_to_index(index_struct, d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(self._index_struct, document, self._text_splitter)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        WeaviateNode.delete_document(self.client, doc_id, self.class_prefix)\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Any) -> None:\n        \"\"\"Query mode to class.\"\"\"\n        super()._preprocess_query(mode, query_kwargs)\n        # pass along weaviate client and info\n        query_kwargs[\"weaviate_client\"] = self.client\n", "doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "__type__": "Document"}, "0f0bfe98-caa8-4c66-af7f-c2fbab872177": {"text": "\nThe summaries of these documents describe a GPT Index library that contains code for vector-store based data structures, including classes for the base vector store index, GPT Faiss Index, GPT Pinecone Index, GPT Qdrant Index, GPTSimpleVectorIndex, and GPTWeaviateIndex. These classes are used to store nodes that are keyed by embeddings, and those embeddings are stored within the respective vector store. During index construction, the document texts are chunked up, converted to nodes with text, and encoded in document embeddings stored within the vector store. During query time, the index uses the vector store to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes.", "doc_id": "0f0bfe98-caa8-4c66-af7f-c2fbab872177", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Vector-store based data structures.\"\"\"\n\nfrom gpt_index.indices.vector_store.faiss import GPTFaissIndex\nfrom gpt_index.indices.vector_store.pinecone import GPTPineconeIndex\nfrom gpt_index.indices.vector_store.qdrant import GPTQdrantIndex\nfrom gpt_index.indices.vector_store.simple import GPTSimpleVectorIndex\nfrom gpt_index.indices.vector_store.weaviate import GPTWeaviateIndex\n\n__all__ = [\n    \"GPTFaissIndex\",\n    \"GPTSimpleVectorIndex\",\n    \"GPTWeaviateIndex\",\n    \"GPTPineconeIndex\",\n    \"GPTQdrantIndex\",\n]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "095e81a317c46ee013bad276c00ee6523870019c", "node_info": null}, "1": {"text": "\"\"\"Base vector store index.\n\nAn index that that is built on top of an existing vector store.\n\n\"\"\"\n\nfrom abc import abstractmethod\nfrom typing import Any, Generic, Optional, Sequence, TypeVar\n\nfrom gpt_index.data_structs.data_structs import IndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.schema import BaseDocument\n\nBID = TypeVar(\"BID\", bound=IndexStruct)\n\n\nclass BaseGPTVectorStoreIndex(BaseGPTIndex[BID], Generic[BID]):\n    \"\"\"Base GPT Vector Store Index.\n\n    Args:\n        text_qa_template", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/base.py", "file_name": "base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "e7576a555defa75d31ec9e716c927ee9b9e97d8f", "node_info": null}, "2": {"text": " Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n    \"\"\"\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[BID] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/base.py", "file_name": "base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "e7576a555defa75d31ec9e716c927ee9b9e97d8f", "node_info": null}, "3": {"text": "  \"\"\"Initialize params.\"\"\"\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            embed_model=embed_model,\n            **kwargs,\n        )\n        # NOTE: when building the vector store index, text_qa_template is not partially\n        # formatted because we don't know the query ahead of time.\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/base.py", "file_name": "base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "e7576a555defa75d31ec9e716c927ee9b9e97d8f", "node_info": null}, "4": {"text": "1\n        )\n\n    @abstractmethod\n    def _add_document_to_index(\n        self,\n        index_struct: BID,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n\n    def _build_index_from_documents(self, documents: Sequence[BaseDocument]) -> BID:\n        \"\"\"Build index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n        index_struct = self.index_struct_cls()\n        for d in documents:\n            self._add_document_to_index(index_struct, d,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/base.py", "file_name": "base.py"}, "index": 4, "child_indices": [], "ref_doc_id": "e7576a555defa75d31ec9e716c927ee9b9e97d8f", "node_info": null}, "5": {"text": "self._add_document_to_index(index_struct, d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(self._index_struct, document, self._text_splitter)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/base.py", "file_name": "base.py"}, "index": 5, "child_indices": [], "ref_doc_id": "e7576a555defa75d31ec9e716c927ee9b9e97d8f", "node_info": null}, "6": {"text": "\"\"\"Faiss Vector store index.\n\nAn index that that is built on top of an existing vector store.\n\n\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence, Type, cast\n\nimport numpy as np\n\nfrom gpt_index.data_structs.data_structs import IndexDict\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.vector_store.faiss import GPTFaissIndexQuery\nfrom gpt_index.indices.vector_store.base import BaseGPTVectorStoreIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.schema import BaseDocument\n\n\nclass", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 6, "child_indices": [], "ref_doc_id": "f211883caf76a36c2a3c024807976f40924947fc", "node_info": null}, "7": {"text": "gpt_index.schema import BaseDocument\n\n\nclass GPTFaissIndex(BaseGPTVectorStoreIndex[IndexDict]):\n    \"\"\"GPT Faiss Index.\n\n    The GPTFaissIndex is a data structure where nodes are keyed by\n    embeddings, and those embeddings are stored within a Faiss index.\n    During index construction, the document texts are chunked up,\n    converted to nodes with text; they are then encoded in\n    document embeddings stored within Faiss.\n\n    During query time, the index uses Faiss to query for the top\n    k most similar nodes, and synthesizes an answer from the\n    retrieved nodes.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        faiss_index (faiss.Index): A Faiss Index object (required). Note: the index\n          ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 7, "child_indices": [], "ref_doc_id": "f211883caf76a36c2a3c024807976f40924947fc", "node_info": null}, "8": {"text": "Note: the index\n            will be reset during index construction.\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n    \"\"\"\n\n    index_struct_cls = IndexDict\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[IndexDict] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        faiss_index: Optional[Any] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 8, "child_indices": [], "ref_doc_id": "f211883caf76a36c2a3c024807976f40924947fc", "node_info": null}, "9": {"text": "       \"\"\"Initialize params.\"\"\"\n        import_err_msg = \"\"\"\n            `faiss` package not found. For instructions on\n            how to install `faiss` please visit\n            https://github.com/facebookresearch/faiss/wiki/Installing-Faiss\n        \"\"\"\n        try:\n            import faiss  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n\n        if faiss_index is None:\n            raise ValueError(\"faiss_index cannot be None.\")\n        if documents is not None and faiss_index.ntotal > 0:\n            raise ValueError(\n                \"If building a", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 9, "child_indices": [], "ref_doc_id": "f211883caf76a36c2a3c024807976f40924947fc", "node_info": null}, "10": {"text": "            \"If building a GPTFaissIndex from scratch, faiss_index must be empty.\"\n            )\n        self._faiss_index = cast(faiss.Index, faiss_index)\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            text_qa_template=text_qa_template,\n            llm_predictor=llm_predictor,\n            embed_model=embed_model,\n            **kwargs,\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 10, "child_indices": [], "ref_doc_id": "f211883caf76a36c2a3c024807976f40924947fc", "node_info": null}, "11": {"text": "       return {\n            QueryMode.DEFAULT: GPTFaissIndexQuery,\n            QueryMode.EMBEDDING: GPTFaissIndexQuery,\n        }\n\n    def _add_document_to_index(\n        self,\n        index_struct: IndexDict,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        nodes = self._get_nodes_from_document(document, text_splitter)\n        for n in nodes:\n            # add to FAISS\n            # NOTE: embeddings won't be stored in Node but rather in underlying\n            # Faiss store\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 11, "child_indices": [], "ref_doc_id": "f211883caf76a36c2a3c024807976f40924947fc", "node_info": null}, "12": {"text": "# Faiss store\n            if n.embedding is None:\n                text_embedding = self._embed_model.get_text_embedding(n.get_text())\n            else:\n                text_embedding = n.embedding\n\n            text_embedding_np = np.array(text_embedding, dtype=\"float32\")[np.newaxis, :]\n            new_id = str(self._faiss_index.ntotal)\n            self._faiss_index.add(text_embedding_np)\n\n            # add to index\n            index_struct.add_node(n, text_id=new_id)\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Any) -> None:\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 12, "child_indices": [], "ref_doc_id": "f211883caf76a36c2a3c024807976f40924947fc", "node_info": null}, "13": {"text": "Any) -> None:\n        \"\"\"Preprocess query.\n\n        This allows subclasses to pass in additional query kwargs\n        to query, for instance arguments that are shared between the\n        index and the query class. By default, this does nothing.\n        This also allows subclasses to do validation.\n\n        \"\"\"\n        super()._preprocess_query(mode, query_kwargs)\n        # pass along faiss_index\n        query_kwargs[\"faiss_index\"] = self._faiss_index\n\n    @classmethod\n    def load_from_disk(\n        cls, save_path: str, faiss_index_save_path: Optional[str] = None, **kwargs: Any\n    ) -> \"BaseGPTIndex\":\n        \"\"\"Load index from disk.\n\n        This method loads the index from a JSON file stored on disk. The index data\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 13, "child_indices": [], "ref_doc_id": "f211883caf76a36c2a3c024807976f40924947fc", "node_info": null}, "14": {"text": "file stored on disk. The index data\n        structure itself is preserved completely. If the index is defined over\n        subindices, those subindices will also be preserved (and subindices of\n        those subindices, etc.).\n        In GPTFaissIndex, we allow user to specify an additional\n        `faiss_index_save_path` to load faiss index from a file - that\n        way, the user does not have to recreate the faiss index outside\n        of this class.\n\n        Args:\n            save_path (str): The save_path of the file.\n            faiss_index_save_path (Optional[str]): The save_path of the\n                Faiss index file. If not specified, the Faiss index\n                will not be saved to disk.\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 14, "child_indices": [], "ref_doc_id": "f211883caf76a36c2a3c024807976f40924947fc", "node_info": null}, "15": {"text": "       will not be saved to disk.\n            **kwargs: Additional kwargs to pass to the index constructor.\n\n        Returns:\n            BaseGPTIndex: The loaded index.\n\n        \"\"\"\n        if faiss_index_save_path is not None:\n            import faiss\n\n            faiss_index = faiss.read_index(faiss_index_save_path)\n            return super().load_from_disk(save_path, faiss_index=faiss_index, **kwargs)\n        else:\n            return super().load_from_disk(save_path, **kwargs)\n\n    def save_to_disk(\n        self,\n        save_path: str,\n        faiss_index_save_path:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 15, "child_indices": [], "ref_doc_id": "f211883caf76a36c2a3c024807976f40924947fc", "node_info": null}, "16": {"text": "       faiss_index_save_path: Optional[str] = None,\n        **save_kwargs: Any,\n    ) -> None:\n        \"\"\"Save to file.\n\n        This method stores the index into a JSON file stored on disk.\n        In GPTFaissIndex, we allow user to specify an additional\n        `faiss_index_save_path` to save the faiss index to a file - that\n        way, the user can pass in the same argument in\n        `GPTFaissIndex.load_from_disk` without having to recreate\n        the Faiss index outside of this class.\n\n        Args:\n            save_path (str): The save_path of the file.\n            faiss_index_save_path (Optional[str]): The save_path of the\n                Faiss", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 16, "child_indices": [], "ref_doc_id": "f211883caf76a36c2a3c024807976f40924947fc", "node_info": null}, "17": {"text": "              Faiss index file. If not specified, the Faiss index\n                will not be saved to disk.\n\n        \"\"\"\n        super().save_to_disk(save_path, **save_kwargs)\n\n        if faiss_index_save_path is not None:\n            import faiss\n\n            faiss.write_index(self._faiss_index, faiss_index_save_path)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        raise NotImplementedError(\"Delete not yet implemented for Faiss index.\")\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 17, "child_indices": [], "ref_doc_id": "f211883caf76a36c2a3c024807976f40924947fc", "node_info": null}, "18": {"text": "\"\"\"Pinecone Vector store index.\n\nAn index that that is built on top of an existing vector store.\n\n\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence, Type, cast\n\nfrom gpt_index.data_structs.data_structs import PineconeIndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.vector_store.pinecone import GPTPineconeIndexQuery\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.schema import BaseDocument\nfrom gpt_index.utils import", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 18, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "19": {"text": "import BaseDocument\nfrom gpt_index.utils import get_new_id\n\n\nclass GPTPineconeIndex(BaseGPTIndex[PineconeIndexStruct]):\n    \"\"\"GPT Pinecone Index.\n\n    The GPTPineconeIndex is a data structure where nodes are keyed by\n    embeddings, and those embeddings are stored within a Pinecone index.\n    During index construction, the document texts are chunked up,\n    converted to nodes with text; they are then encoded in\n    document embeddings stored within Pinecone.\n\n    During query time, the index uses Pinecone to query for the top\n    k most similar nodes, and synthesizes an answer from the\n    retrieved nodes.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 19, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "20": {"text": "Embedding model to use for\n            embedding similarity.\n        chunk_size_limit (int): Maximum number of tokens per chunk. NOTE:\n            in Pinecone the default is 2048 due to metadata size restrictions.\n    \"\"\"\n\n    index_struct_cls = PineconeIndexStruct\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[PineconeIndexStruct] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        pinecone_index: Optional[Any] = None,\n        chunk_size_limit: int = 2048,\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 20, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "21": {"text": "   chunk_size_limit: int = 2048,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        import_err_msg = (\n            \"`pinecone` package not found, please run `pip install pinecone-client`\"\n        )\n        try:\n            import pinecone  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n        self._pinecone_index = cast(pinecone.Index, pinecone_index)\n\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        super().__init__(\n            documents=documents,\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 21, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "22": {"text": "           index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            embed_model=embed_model,\n            chunk_size_limit=chunk_size_limit,\n            **kwargs,\n        )\n        # NOTE: when building the vector store index, text_qa_template is not partially\n        # formatted because we don't know the query ahead of time.\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 22, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "23": {"text": "      \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTPineconeIndexQuery,\n            QueryMode.EMBEDDING: GPTPineconeIndexQuery,\n        }\n\n    def _add_document_to_index(\n        self,\n        index_struct: PineconeIndexStruct,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        nodes = self._get_nodes_from_document(document, text_splitter)\n        for n in nodes:\n            if n.embedding is None:\n                text_embedding =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 23, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "24": {"text": "           text_embedding = self._embed_model.get_text_embedding(n.get_text())\n            else:\n                text_embedding = n.embedding\n\n            while True:\n                new_id = get_new_id(set())\n                result = self._pinecone_index.fetch([new_id])\n                if len(result[\"vectors\"]) == 0:\n                    break\n\n            metadata = {\n                \"text\": n.get_text(),\n                \"doc_id\": document.get_doc_id(),\n            }\n\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 24, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "25": {"text": "           }\n\n            self._pinecone_index.upsert([(new_id, text_embedding, metadata)])\n\n    def _build_index_from_documents(\n        self, documents: Sequence[BaseDocument]\n    ) -> PineconeIndexStruct:\n        \"\"\"Build index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n        index_struct = self.index_struct_cls()\n        for d in documents:\n            self._add_document_to_index(index_struct, d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 25, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "26": {"text": "**insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(self._index_struct, document, self._text_splitter)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        # delete by filtering on the doc_id metadata\n        self._pinecone_index.delete(filter={\"doc_id\": {\"$eq\": doc_id}})\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Any) -> None:\n        \"\"\"Query mode to class.\"\"\"\n        super()._preprocess_query(mode, query_kwargs)\n        # pass along pinecone client and info\n        query_kwargs[\"pinecone_index\"] = self._pinecone_index\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 26, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "27": {"text": "\"\"\"Qdrant vector store index.\n\nAn index that is built on top of an existing Qdrant collection.\n\n\"\"\"\nfrom typing import Any, Dict, Optional, Sequence, Type, cast\n\nfrom gpt_index.data_structs.data_structs import QdrantIndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.vector_store.qdrant import GPTQdrantIndexQuery\nfrom gpt_index.indices.vector_store.base import BaseGPTVectorStoreIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 27, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "28": {"text": "import QuestionAnswerPrompt\nfrom gpt_index.schema import BaseDocument\nfrom gpt_index.utils import get_new_id\n\n\nclass GPTQdrantIndex(BaseGPTVectorStoreIndex[QdrantIndexStruct]):\n    \"\"\"GPT Qdrant Index.\n\n    The GPTQdrantIndex is a data structure where nodes are keyed by\n    embeddings, and those embeddings are stored within a Qdrant collection.\n    During index construction, the document texts are chunked up,\n    converted to nodes with text; they are then encoded in\n    document embeddings stored within Qdrant.\n\n    During query time, the index uses Qdrant to query for the top\n    k most similar nodes, and synthesizes an answer from the\n    retrieved nodes.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 28, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "29": {"text": "       embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        client (Optional[Any]): QdrantClient instance from `qdrant-client` package\n        collection_name: (Optional[str]): name of the Qdrant collection\n    \"\"\"\n\n    index_struct_cls = QdrantIndexStruct\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[QdrantIndexStruct] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        client: Optional[Any] = None,\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 29, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "30": {"text": "client: Optional[Any] = None,\n        collection_name: Optional[str] = None,\n        **kwargs: Any\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        import_err_msg = (\n            \"`qdrant-client` package not found, please run `pip install qdrant-client`\"\n        )\n        try:\n            import qdrant_client  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n\n        if client is None:\n            raise ValueError(\"client cannot be None.\")\n\n        if collection_name is None and index_struct is not None:\n            collection_name = index_struct.collection_name\n        if", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 30, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "31": {"text": "index_struct.collection_name\n        if collection_name is None:\n            raise ValueError(\"collection_name cannot be None.\")\n\n        self._client = cast(qdrant_client.QdrantClient, client)\n        self._collection_name = collection_name\n        self._collection_initialized = self._collection_exists(collection_name)\n\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        super().__init__(\n            documents,\n            index_struct,\n            text_qa_template,\n            llm_predictor,\n            embed_model,\n            **kwargs\n        )\n        # NOTE:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 31, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "32": {"text": "    )\n        # NOTE: when building the vector store index, text_qa_template is not partially\n        # formatted because we don't know the query ahead of time.\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTQdrantIndexQuery,\n            QueryMode.EMBEDDING: GPTQdrantIndexQuery,\n        }\n\n    def _add_document_to_index(\n        self,\n        index_struct:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 32, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "33": {"text": "  self,\n        index_struct: QdrantIndexStruct,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        from qdrant_client.http import models as rest\n        from qdrant_client.http.exceptions import UnexpectedResponse\n\n        nodes = self._get_nodes_from_document(document, text_splitter)\n        for n in nodes:\n            if n.embedding is None:\n                text_embedding = self._embed_model.get_text_embedding(n.get_text())\n            else:\n                text_embedding = n.embedding\n\n            collection_name =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 33, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "34": {"text": "           collection_name = index_struct.get_collection_name()\n\n            # Create the Qdrant collection, if it does not exist yet\n            if not self._collection_initialized:\n                self._create_collection(\n                    collection_name=collection_name,\n                    vector_size=len(text_embedding),\n                )\n                self._collection_initialized = True\n\n            while True:\n                new_id = get_new_id(set())\n                try:\n                   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 34, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "35": {"text": "                self._client.http.points_api.get_point(\n                        collection_name=collection_name, id=new_id\n                    )\n                except UnexpectedResponse:\n                    break\n\n            payload = {\n                \"doc_id\": document.get_doc_id(),\n                \"text\": n.get_text(),\n                \"index\": n.index,\n            }\n\n            self._client.upsert(\n                collection_name=collection_name,\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 35, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "36": {"text": "      collection_name=collection_name,\n                points=[\n                    rest.PointStruct(\n                        id=new_id,\n                        vector=text_embedding,\n                        payload=payload,\n                    )\n                ],\n            )\n\n    def _build_index_from_documents(\n        self, documents: Sequence[BaseDocument]\n    ) -> QdrantIndexStruct:\n        \"\"\"Build index from documents.\"\"\"\n        text_splitter =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 36, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "37": {"text": "       text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n        index_struct = self.index_struct_cls(collection_name=self._collection_name)\n        for d in documents:\n            self._add_document_to_index(index_struct, d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(self.index_struct, document, self._text_splitter)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        from", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 37, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "38": {"text": " \"\"\"Delete a document.\"\"\"\n        from qdrant_client.http import models as rest\n\n        self._client.delete(\n            collection_name=self._collection_name,\n            points_selector=rest.Filter(\n                must=[\n                    rest.FieldCondition(\n                        key=\"doc_id\", match=rest.MatchValue(value=doc_id)\n                    )\n                ]\n            ),\n        )\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Any) -> None:\n        \"\"\"Query mode to class.\"\"\"\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 38, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "39": {"text": " \"\"\"Query mode to class.\"\"\"\n        super()._preprocess_query(mode, query_kwargs)\n        # Pass along Qdrant client instance\n        query_kwargs[\"client\"] = self._client\n\n    def _create_collection(self, collection_name: str, vector_size: int) -> None:\n        \"\"\"Create a Qdrant collection.\"\"\"\n        from qdrant_client.http import models as rest\n\n        self._client.recreate_collection(\n            collection_name=collection_name,\n            vectors_config=rest.VectorParams(\n                size=vector_size,\n                distance=rest.Distance.COSINE,\n            ),\n        )\n\n    def _collection_exists(self, collection_name: str) ->", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 39, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "40": {"text": "def _collection_exists(self, collection_name: str) -> bool:\n        from qdrant_client.http.exceptions import UnexpectedResponse\n\n        try:\n            response = self._client.http.collections_api.get_collection(collection_name)\n            return response.result is not None\n        except UnexpectedResponse:\n            return False\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "index": 40, "child_indices": [], "ref_doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "node_info": null}, "41": {"text": "\"\"\"Simple vector store index.\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence, Type\n\nfrom gpt_index.data_structs.data_structs import SimpleIndexDict\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.vector_store.simple import GPTSimpleVectorIndexQuery\nfrom gpt_index.indices.vector_store.base import BaseGPTVectorStoreIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.schema import BaseDocument\nfrom gpt_index.utils import get_new_id\n\n\nclass GPTSimpleVectorIndex(BaseGPTVectorStoreIndex[SimpleIndexDict]):\n    \"\"\"GPT Simple Vector", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/simple.py", "file_name": "simple.py"}, "index": 41, "child_indices": [], "ref_doc_id": "b901531911d54b57fd5436cb636d971e68fea306", "node_info": null}, "42": {"text": "   \"\"\"GPT Simple Vector Index.\n\n    The GPTSimpleVectorIndex is a data structure where nodes are keyed by\n    embeddings, and those embeddings are stored within a simple dictionary.\n    During index construction, the document texts are chunked up,\n    converted to nodes with text; they are then encoded in\n    document embeddings stored within the dict.\n\n    During query time, the index uses the dict to query for the top\n    k most similar nodes, and synthesizes an answer from the\n    retrieved nodes.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n    \"\"\"\n\n    index_struct_cls = SimpleIndexDict\n\n    def __init__(\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/simple.py", "file_name": "simple.py"}, "index": 42, "child_indices": [], "ref_doc_id": "b901531911d54b57fd5436cb636d971e68fea306", "node_info": null}, "43": {"text": "= SimpleIndexDict\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[SimpleIndexDict] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            text_qa_template=text_qa_template,\n            llm_predictor=llm_predictor,\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/simple.py", "file_name": "simple.py"}, "index": 43, "child_indices": [], "ref_doc_id": "b901531911d54b57fd5436cb636d971e68fea306", "node_info": null}, "44": {"text": "           embed_model=embed_model,\n            **kwargs,\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTSimpleVectorIndexQuery,\n            QueryMode.EMBEDDING: GPTSimpleVectorIndexQuery,\n        }\n\n    def _add_document_to_index(\n        self,\n        index_struct: SimpleIndexDict,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        nodes =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/simple.py", "file_name": "simple.py"}, "index": 44, "child_indices": [], "ref_doc_id": "b901531911d54b57fd5436cb636d971e68fea306", "node_info": null}, "45": {"text": "document to index.\"\"\"\n        nodes = self._get_nodes_from_document(document, text_splitter)\n        for n in nodes:\n            # add to in-memory dict\n            # NOTE: embeddings won't be stored in Node but rather in underlying\n            # Faiss store\n            if n.embedding is None:\n                text_embedding = self._embed_model.get_text_embedding(n.get_text())\n            else:\n                text_embedding = n.embedding\n            new_id = get_new_id(set(index_struct.nodes_dict.keys()))\n\n            # add to index\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/simple.py", "file_name": "simple.py"}, "index": 45, "child_indices": [], "ref_doc_id": "b901531911d54b57fd5436cb636d971e68fea306", "node_info": null}, "46": {"text": "# add to index\n            index_struct.add_node(n, text_id=new_id)\n            # TODO: deprecate\n            index_struct.add_to_embedding_dict(new_id, text_embedding)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        text_ids_to_delete = set()\n        int_ids_to_delete = set()\n        for text_id, int_id in self.index_struct.id_map.items():\n            node = self.index_struct.nodes_dict[int_id]\n            if node.ref_doc_id != doc_id:\n                continue\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/simple.py", "file_name": "simple.py"}, "index": 46, "child_indices": [], "ref_doc_id": "b901531911d54b57fd5436cb636d971e68fea306", "node_info": null}, "47": {"text": "   continue\n            text_ids_to_delete.add(text_id)\n            int_ids_to_delete.add(int_id)\n\n        for int_id, text_id in zip(int_ids_to_delete, text_ids_to_delete):\n            del self.index_struct.nodes_dict[int_id]\n            del self.index_struct.id_map[text_id]\n            del self.index_struct.embedding_dict[text_id]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/simple.py", "file_name": "simple.py"}, "index": 47, "child_indices": [], "ref_doc_id": "b901531911d54b57fd5436cb636d971e68fea306", "node_info": null}, "48": {"text": "\"\"\"Weaviate Vector store index.\n\nAn index that that is built on top of an existing vector store.\n\n\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence, Type, cast\n\nfrom gpt_index.data_structs.data_structs import WeaviateIndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.vector_store.weaviate import GPTWeaviateIndexQuery\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.readers.weaviate.data_structs import", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 48, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "49": {"text": "import WeaviateNode\nfrom gpt_index.readers.weaviate.utils import get_default_class_prefix\nfrom gpt_index.schema import BaseDocument\n\n\nclass GPTWeaviateIndex(BaseGPTIndex[WeaviateIndexStruct]):\n    \"\"\"GPT Weaviate Index.\n\n    The GPTWeaviateIndex is a data structure where nodes are keyed by\n    embeddings, and those embeddings are stored within a Weaviate index.\n    During index construction, the document texts are chunked up,\n    converted to nodes with text; they are then encoded in\n    document embeddings stored within Weaviate.\n\n    During query time, the index uses Weaviate to query for the top\n    k most similar nodes, and synthesizes an answer from the\n    retrieved nodes.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 49, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "50": {"text": "(see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n    \"\"\"\n\n    index_struct_cls = WeaviateIndexStruct\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[WeaviateIndexStruct] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        weaviate_client: Optional[Any] = None,\n        class_prefix: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 50, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "51": {"text": "**kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        import_err_msg = (\n            \"`weaviate` package not found, please run `pip install weaviate-client`\"\n        )\n        try:\n            import weaviate  # noqa: F401\n            from weaviate import Client  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n\n        self.client = cast(Client, weaviate_client)\n        if index_struct is not None:\n            if class_prefix is not None:\n                raise ValueError(\n                    \"class_prefix", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 51, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "52": {"text": "            \"class_prefix must be None when index_struct is not None.\"\n                )\n            self.class_prefix = index_struct.get_class_prefix()\n        else:\n            self.class_prefix = class_prefix or get_default_class_prefix()\n        # try to create schema\n        WeaviateNode.create_schema(self.client, self.class_prefix)\n\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 52, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "53": {"text": "           embed_model=embed_model,\n            **kwargs,\n        )\n        # NOTE: when building the vector store index, text_qa_template is not partially\n        # formatted because we don't know the query ahead of time.\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTWeaviateIndexQuery,\n            QueryMode.EMBEDDING: GPTWeaviateIndexQuery,\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 53, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "54": {"text": "GPTWeaviateIndexQuery,\n        }\n\n    def _add_document_to_index(\n        self,\n        index_struct: WeaviateIndexStruct,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        nodes = self._get_nodes_from_document(document, text_splitter)\n        for n in nodes:\n            if n.embedding is None:\n                n.embedding = self._embed_model.get_text_embedding(n.get_text())\n            WeaviateNode.from_gpt_index(self.client, n, index_struct.get_class_prefix())\n\n    def _build_index_from_documents(\n        self,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 54, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "55": {"text": "       self, documents: Sequence[BaseDocument]\n    ) -> WeaviateIndexStruct:\n        \"\"\"Build index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n        index_struct = self.index_struct_cls(class_prefix=self.class_prefix)\n        for d in documents:\n            self._add_document_to_index(index_struct, d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(self._index_struct, document, self._text_splitter)\n\n    def", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 55, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "56": {"text": "document, self._text_splitter)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        WeaviateNode.delete_document(self.client, doc_id, self.class_prefix)\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Any) -> None:\n        \"\"\"Query mode to class.\"\"\"\n        super()._preprocess_query(mode, query_kwargs)\n        # pass along weaviate client and info\n        query_kwargs[\"weaviate_client\"] = self.client\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 56, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "57": {"text": "This code file contains code for vector-store based data structures. It includes classes for the base vector store index, GPT Faiss Index, and other related classes. The base vector store index initializes parameters and has methods for adding documents to the index, building the index from documents, and inserting documents. The GPT Faiss Index is a data structure where nodes are keyed by embeddings, and those embeddings are stored within a Faiss index. During index construction, the document texts are chunked up, converted to nodes with text; they are then encoded in document embeddings stored within Faiss. During query time, the index uses Faiss to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 57, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "58": {"text": "The file faiss.py is a part of the GPT Index library and is used to create a Faiss index. It contains methods for preprocessing queries, loading and saving the index from/to disk, and deleting documents from the index. The GPTPineconeIndex class in the file pinecone.py is used to create a Pinecone vector store index. It contains methods for initializing parameters, getting the query map, and adding documents to the index.", "doc_id": null, "embedding": null, "extra_info": null, "index": 58, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "59": {"text": "The file pinecone.py is a vector store index that is built on top of an existing Pinecone collection. It is used to store nodes that are keyed by embeddings, and those embeddings are stored within the Pinecone collection. During index construction, the document texts are chunked up, converted to nodes with text, and then encoded in document embeddings stored within Pinecone. During query time, the index uses Pinecone to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes. The file qdrant.py is a vector store index that is built on top of an existing Qdrant collection. It is used to store nodes that are keyed by embeddings, and those embeddings are stored within the Qdrant collection. During index construction, the document texts are chunked up, converted to nodes with text, and then encoded in document embeddings stored within Qdrant. During query time, the index uses Qdrant to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 59, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}, "60": {"text": "The GPTSimpleVectorIndex is a data structure where nodes are keyed by embeddings, and those embeddings are stored within a simple dictionary. During index construction, the document texts are chunked up, converted to nodes with text; they are then encoded in document embeddings stored within the dict. During query time, the index uses the dict to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes. The index is initialized with documents, an index structure, a text-qa template, an LLM predictor, and an embedding model. It has a get_query_map() method which returns a dictionary of query modes and their corresponding query classes. The _add_document_to_index() method adds documents to the index structure, and the _delete() method deletes documents from the index structure.", "doc_id": null, "embedding": null, "extra_info": null, "index": 60, "child_indices": [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], "ref_doc_id": null, "node_info": null}, "61": {"text": "The GPTWeaviateIndex is a data structure that uses an existing vector store to store nodes keyed by embeddings. During index construction, the document texts are chunked up, converted to nodes with text, and encoded in document embeddings stored within Weaviate. During query time, the index uses Weaviate to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes. The GPTWeaviateIndex class initializes parameters, creates a schema, and provides methods for adding, deleting, and querying documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 61, "child_indices": [48, 49, 50, 51, 52, 53, 54, 55, 56], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"57": {"text": "This code file contains code for vector-store based data structures. It includes classes for the base vector store index, GPT Faiss Index, and other related classes. The base vector store index initializes parameters and has methods for adding documents to the index, building the index from documents, and inserting documents. The GPT Faiss Index is a data structure where nodes are keyed by embeddings, and those embeddings are stored within a Faiss index. During index construction, the document texts are chunked up, converted to nodes with text; they are then encoded in document embeddings stored within Faiss. During query time, the index uses Faiss to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 57, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "58": {"text": "The file faiss.py is a part of the GPT Index library and is used to create a Faiss index. It contains methods for preprocessing queries, loading and saving the index from/to disk, and deleting documents from the index. The GPTPineconeIndex class in the file pinecone.py is used to create a Pinecone vector store index. It contains methods for initializing parameters, getting the query map, and adding documents to the index.", "doc_id": null, "embedding": null, "extra_info": null, "index": 58, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "59": {"text": "The file pinecone.py is a vector store index that is built on top of an existing Pinecone collection. It is used to store nodes that are keyed by embeddings, and those embeddings are stored within the Pinecone collection. During index construction, the document texts are chunked up, converted to nodes with text, and then encoded in document embeddings stored within Pinecone. During query time, the index uses Pinecone to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes. The file qdrant.py is a vector store index that is built on top of an existing Qdrant collection. It is used to store nodes that are keyed by embeddings, and those embeddings are stored within the Qdrant collection. During index construction, the document texts are chunked up, converted to nodes with text, and then encoded in document embeddings stored within Qdrant. During query time, the index uses Qdrant to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 59, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}, "60": {"text": "The GPTSimpleVectorIndex is a data structure where nodes are keyed by embeddings, and those embeddings are stored within a simple dictionary. During index construction, the document texts are chunked up, converted to nodes with text; they are then encoded in document embeddings stored within the dict. During query time, the index uses the dict to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes. The index is initialized with documents, an index structure, a text-qa template, an LLM predictor, and an embedding model. It has a get_query_map() method which returns a dictionary of query modes and their corresponding query classes. The _add_document_to_index() method adds documents to the index structure, and the _delete() method deletes documents from the index structure.", "doc_id": null, "embedding": null, "extra_info": null, "index": 60, "child_indices": [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], "ref_doc_id": null, "node_info": null}, "61": {"text": "The GPTWeaviateIndex is a data structure that uses an existing vector store to store nodes keyed by embeddings. During index construction, the document texts are chunked up, converted to nodes with text, and encoded in document embeddings stored within Weaviate. During query time, the index uses Weaviate to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes. The GPTWeaviateIndex class initializes parameters, creates a schema, and provides methods for adding, deleting, and querying documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 61, "child_indices": [48, 49, 50, 51, 52, 53, 54, 55, 56], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}