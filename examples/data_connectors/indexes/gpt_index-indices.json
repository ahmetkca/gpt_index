{"index_struct": {"text": "\n__init__.py:\nThis document provides a summary of the data structures used in GPT Index, including the keyword table, list, tree, and vector store indices.\n\nbase.py:\nThis document provides a summary of the base index classes, including parameters such as documents, LLM predictor, embed model, docstore, index registry, prompt helper, chunk size limit, and include extra info. It also includes methods such as _update_index_registry_and_docstore, _process_documents, and _validate_documents.\n\nprompt_helper.py:\nThis document provides a summary of the PromptHelper class, which provides methods for creating a text splitter given a prompt, getting the biggest prompt from a list of prompts, and getting text from nodes in a tree-structured index. It also provides methods for compacting text chunks and getting text from nodes in the format of a numbered list.\n\nregistry.py:\nThis document provides a summary of the Registry class, which provides methods for registering and retrieving indices, models, and tokenizers. It also provides methods for getting the list of registered indices, models, and tokenizers, as well as methods for getting", "doc_id": "3a35f0ed-9595-47b0-9600-e63edd1d18e3", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"GPT Index data structures.\"\"\"\n\n# indices\nfrom gpt_index.indices.keyword_table.base import GPTKeywordTableIndex\nfrom gpt_index.indices.keyword_table.rake_base import GPTRAKEKeywordTableIndex\nfrom gpt_index.indices.keyword_table.simple_base import GPTSimpleKeywordTableIndex\nfrom gpt_index.indices.list.base import GPTListIndex\nfrom gpt_index.indices.tree.base import GPTTreeIndex\nfrom gpt_index.indices.vector_store.faiss import GPTFaissIndex\n\n__all__ = [\n    \"GPTKeywordTableIndex\",\n    \"GPTSimpleKeywordTableIndex\",\n    \"GPTRAKEKeywordTableIndex\",\n    \"GPTListIndex\",\n    \"GPTTreeIndex\",\n    \"GPTFaissIndex\",\n]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "e7bbbf6f654fcec74653151ee1feeea4b388c65b", "node_info": null}, "1": {"text": "\"\"\"Base index classes.\"\"\"\nimport json\nimport logging\nfrom abc import abstractmethod\nfrom typing import (\n    Any,\n    Dict,\n    Generic,\n    List,\n    Optional,\n    Sequence,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom gpt_index.data_structs.data_structs import IndexStruct, Node\nfrom gpt_index.docstore import DOC_TYPE, DocumentStore\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.embeddings.openai import OpenAIEmbedding\nfrom gpt_index.indices.node_utils import get_nodes_from_document\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.query_runner import QueryRunner\nfrom gpt_index.indices.query.schema import QueryConfig, QueryMode\nfrom gpt_index.indices.registry import IndexRegistry\nfrom", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "2": {"text": "gpt_index.indices.registry import IndexRegistry\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.readers.schema.base import Document\nfrom gpt_index.response.schema import Response\nfrom gpt_index.schema import BaseDocument\nfrom gpt_index.token_counter.token_counter import llm_token_counter\n\nIS = TypeVar(\"IS\", bound=IndexStruct)\n\n\nDOCUMENTS_INPUT = Union[BaseDocument, \"BaseGPTIndex\"]\n\n\nclass BaseGPTIndex(Generic[IS]):\n    \"\"\"Base GPT Index.\n\n    Args:\n        documents (Optional[Sequence[BaseDocument]]): List of documents to\n            build the index from.\n        llm_predictor (LLMPredictor): Optional LLMPredictor object. If not provided,\n            will use the default LLMPredictor (text-davinci-003)\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "3": {"text": "(text-davinci-003)\n        prompt_helper (PromptHelper): Optional PromptHelper object. If not provided,\n            will use the default PromptHelper.\n        chunk_size_limit (Optional[int]): Optional chunk size limit. If not provided,\n            will use the default chunk size limit (4096 max input size).\n        include_extra_info (bool): Optional bool. If True, extra info (i.e. metadata)\n            of each Document will be prepended to its text to help with queries.\n            Default is True.\n\n    \"\"\"\n\n    index_struct_cls: Type[IS]\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[IS] = None,\n        llm_predictor:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "4": {"text": "       llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        docstore: Optional[DocumentStore] = None,\n        index_registry: Optional[IndexRegistry] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        chunk_size_limit: Optional[int] = None,\n        include_extra_info: bool = True,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        if index_struct is None and documents is None:\n            raise ValueError(\"One of documents or index_struct must be provided.\")\n        if index_struct is not None and documents is not None:\n            raise ValueError(\"Only one of documents or index_struct can be provided.\")\n\n        self._llm_predictor", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 4, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "5": {"text": "       self._llm_predictor = llm_predictor or LLMPredictor()\n        # NOTE: the embed_model isn't used in all indices\n        self._embed_model = embed_model or OpenAIEmbedding()\n        self._include_extra_info = include_extra_info\n\n        # TODO: move out of base if we need custom params per index\n        self._prompt_helper = prompt_helper or PromptHelper.from_llm_predictor(\n            self._llm_predictor, chunk_size_limit=chunk_size_limit\n        )\n\n        # build index struct in the init function\n        self._docstore = docstore or DocumentStore()\n        self._index_registry = index_registry or IndexRegistry()\n\n        if index_struct is not None:\n            if not", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 5, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "6": {"text": "None:\n            if not isinstance(index_struct, self.index_struct_cls):\n                raise ValueError(\n                    f\"index_struct must be of type {self.index_struct_cls}\"\n                )\n            self._index_struct = index_struct\n        else:\n            documents = cast(Sequence[DOCUMENTS_INPUT], documents)\n            documents = self._process_documents(\n                documents, self._docstore, self._index_registry\n            )\n            self._validate_documents(documents)\n            # TODO: introduce document store outside __init__ function\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 6, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "7": {"text": "introduce document store outside __init__ function\n            self._index_struct = self.build_index_from_documents(documents)\n        # update index registry and docstore with index_struct\n        self._update_index_registry_and_docstore()\n\n    @property\n    def prompt_helper(self) -> PromptHelper:\n        \"\"\"Get the prompt helper corresponding to the index.\"\"\"\n        return self._prompt_helper\n\n    @property\n    def docstore(self) -> DocumentStore:\n        \"\"\"Get the docstore corresponding to the index.\"\"\"\n        return self._docstore\n\n    @property\n    def index_registry(self) -> IndexRegistry:\n        \"\"\"Get the index registry corresponding to the index.\"\"\"\n        return self._index_registry\n\n    @property\n    def llm_predictor(self) -> LLMPredictor:\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 7, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "8": {"text": "-> LLMPredictor:\n        \"\"\"Get the llm predictor.\"\"\"\n        return self._llm_predictor\n\n    @property\n    def embed_model(self) -> BaseEmbedding:\n        \"\"\"Get the llm predictor.\"\"\"\n        return self._embed_model\n\n    def _update_index_registry_and_docstore(self) -> None:\n        \"\"\"Update index registry and docstore.\"\"\"\n        # update index registry with current struct\n        cur_type = self.index_struct_cls.get_type()\n        self._index_registry.type_to_struct[cur_type] = self.index_struct_cls\n        self._index_registry.type_to_query[cur_type] = self.get_query_map()\n\n        # update docstore with current struct\n        self._docstore.add_documents([self.index_struct])\n\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 8, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "9": {"text": "   def _process_documents(\n        self,\n        documents: Sequence[DOCUMENTS_INPUT],\n        docstore: DocumentStore,\n        index_registry: IndexRegistry,\n    ) -> List[BaseDocument]:\n        \"\"\"Process documents.\"\"\"\n        results: List[DOC_TYPE] = []\n        for doc in documents:\n            if isinstance(doc, BaseGPTIndex):\n                # if user passed in another index, we need to do the following:\n                # - update docstore with the docstore in the index\n                # - validate that the index is in the docstore\n                # - update the index registry\n\n               ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 9, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "10": {"text": "               index_registry.update(doc.index_registry)\n                docstore.update_docstore(doc.docstore)\n                # assert that the doc exists within the docstore\n                sub_index_struct = doc.index_struct_with_text\n                if not docstore.document_exists(sub_index_struct.get_doc_id()):\n                    raise ValueError(\n                        \"The index struct of the sub-index must exist in the docstore. \"\n                        f\"Invalid doc ID: {sub_index_struct.get_doc_id()}\"\n                    )\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 10, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "11": {"text": "           )\n                results.append(sub_index_struct)\n            elif isinstance(doc, (Document, IndexStruct)):\n                results.append(doc)\n                # update docstore\n                docstore.add_documents([doc])\n            else:\n                raise ValueError(f\"Invalid document type: {type(doc)}.\")\n        return cast(List[BaseDocument], results)\n\n    def _validate_documents(self, documents: Sequence[BaseDocument]) -> None:\n        \"\"\"Validate documents.\"\"\"\n        for doc in documents:\n            if not isinstance(doc, BaseDocument):\n               ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 11, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "12": {"text": "               raise ValueError(\"Documents must be of type BaseDocument.\")\n\n    @property\n    def index_struct(self) -> IS:\n        \"\"\"Get the index struct.\"\"\"\n        return self._index_struct\n\n    @property\n    def index_struct_with_text(self) -> IS:\n        \"\"\"Get the index struct with text.\n\n        If text not set, raise an error.\n        For use when composing indices with other indices.\n\n        \"\"\"\n        # make sure that we generate text for index struct\n        if self._index_struct.text is None:\n            # NOTE: set text to be empty string for now\n            raise ValueError(\n                \"Index must have text property set in order \"\n                \"to be composed with other", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 12, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "13": {"text": "         \"to be composed with other indices. \"\n                \"In order to set text, please run `index.set_text()`.\"\n            )\n        return self._index_struct\n\n    def set_text(self, text: str) -> None:\n        \"\"\"Set summary text for index struct.\n\n        This allows index_struct_with_text to be used to compose indices\n        with other indices.\n\n        \"\"\"\n        self._index_struct.text = text\n\n    def set_extra_info(self, extra_info: Dict[str, Any]) -> None:\n        \"\"\"Set extra info (metadata) for index struct.\n\n        If this index is used as a subindex for a parent index, the metadata\n        will be propagated to all nodes derived from this subindex, in the\n        parent index.\n\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 13, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "14": {"text": "the\n        parent index.\n\n        \"\"\"\n        self._index_struct.extra_info = extra_info\n\n    def set_doc_id(self, doc_id: str) -> None:\n        \"\"\"Set doc_id for index struct.\n\n        This is used to uniquely identify the index struct in the docstore.\n        If you wish to delete the index struct, you can use this doc_id.\n\n        \"\"\"\n        old_doc_id = self._index_struct.get_doc_id()\n        self._index_struct.doc_id = doc_id\n        # Note: we also need to delete old doc_id, and update docstore\n        self._docstore.delete_document(old_doc_id)\n        self._docstore.add_documents([self._index_struct])\n\n    def get_doc_id(self) -> str:\n        \"\"\"Get doc_id for", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 14, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "15": {"text": "str:\n        \"\"\"Get doc_id for index struct.\n\n        If doc_id not set, raise an error.\n\n        \"\"\"\n        if self._index_struct.doc_id is None:\n            raise ValueError(\"Index must have doc_id property set.\")\n        return self._index_struct.doc_id\n\n    def _get_nodes_from_document(\n        self,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n        start_idx: int = 0,\n    ) -> List[Node]:\n        return get_nodes_from_document(\n            document=document,\n            text_splitter=text_splitter,\n            start_idx=start_idx,\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 15, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "16": {"text": "           include_extra_info=self._include_extra_info,\n        )\n\n    @abstractmethod\n    def _build_index_from_documents(self, documents: Sequence[BaseDocument]) -> IS:\n        \"\"\"Build the index from documents.\"\"\"\n\n    @llm_token_counter(\"build_index_from_documents\")\n    def build_index_from_documents(self, documents: Sequence[BaseDocument]) -> IS:\n        \"\"\"Build the index from documents.\"\"\"\n        return self._build_index_from_documents(documents)\n\n    @abstractmethod\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n\n    @llm_token_counter(\"insert\")\n    def insert(self, document: DOCUMENTS_INPUT, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\n\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 16, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "17": {"text": "   \"\"\"Insert a document.\n\n        Args:\n            document (Union[BaseDocument, BaseGPTIndex]): document to insert\n\n        \"\"\"\n        processed_doc = self._process_documents(\n            [document], self._docstore, self._index_registry\n        )[0]\n        self._validate_documents([processed_doc])\n        self._insert(processed_doc, **insert_kwargs)\n\n    @abstractmethod\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n\n    def delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document from the index.\n\n        All nodes in the index related to the index will be deleted.\n\n        Args:\n        ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 17, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "18": {"text": "     Args:\n            doc_id (str): document id\n            full_delete (bool): whether to delete the document from the docstore.\n                By default this is True.\n\n        \"\"\"\n        full_delete = delete_kwargs.pop(\"full_delete\", True)\n        logging.debug(f\"> Deleting document: {doc_id}\")\n        if full_delete:\n            self._docstore.delete_document(doc_id)\n        self._delete(doc_id, **delete_kwargs)\n\n    def update(self, document: DOCUMENTS_INPUT, **update_kwargs: Any) -> None:\n        \"\"\"Update a document.\n\n        This is equivalent to deleting the document and then inserting it again.\n\n        Args:\n            document", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 18, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "19": {"text": " Args:\n            document (Union[BaseDocument, BaseGPTIndex]): document to update\n            insert_kwargs (Dict): kwargs to pass to insert\n            delete_kwargs (Dict): kwargs to pass to delete\n\n        \"\"\"\n        self.delete(document.get_doc_id(), **update_kwargs.pop(\"delete_kwargs\", {}))\n        self.insert(document, **update_kwargs.pop(\"insert_kwargs\", {}))\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Dict) -> None:\n        \"\"\"Preprocess query.\n\n        This allows subclasses to pass in additional query kwargs\n        to query, for instance arguments that are shared between the\n        index and the query class. By default, this does nothing.\n        This also allows subclasses to do validation.\n\n        \"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 19, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "20": {"text": "to do validation.\n\n        \"\"\"\n        pass\n\n    def query(\n        self,\n        query_str: str,\n        mode: str = QueryMode.DEFAULT,\n        **query_kwargs: Any,\n    ) -> Response:\n        \"\"\"Answer a query.\n\n        When `query` is called, we query the index with the given `mode` and\n        `query_kwargs`. The `mode` determines the type of query to run, and\n        `query_kwargs` are parameters that are specific to the query type.\n\n        For a comprehensive documentation of available `mode` and `query_kwargs` to\n        query a given index, please visit :ref:`Ref-Query`.\n\n\n        \"\"\"\n        mode_enum = QueryMode(mode)\n        if mode_enum ==", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 20, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "21": {"text": "       if mode_enum == QueryMode.RECURSIVE:\n            # TODO: deprecated, use ComposableGraph instead.\n            if \"query_configs\" not in query_kwargs:\n                raise ValueError(\"query_configs must be provided for recursive mode.\")\n            query_configs = query_kwargs[\"query_configs\"]\n            query_runner = QueryRunner(\n                self._llm_predictor,\n                self._prompt_helper,\n                self._embed_model,\n                self._docstore,\n                self._index_registry,\n               ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 21, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "22": {"text": "               query_configs=query_configs,\n                recursive=True,\n            )\n            return query_runner.query(query_str, self._index_struct)\n        else:\n            self._preprocess_query(mode_enum, query_kwargs)\n            # TODO: pass in query config directly\n            query_config = QueryConfig(\n                index_struct_type=self._index_struct.get_type(),\n                query_mode=mode_enum,\n                query_kwargs=query_kwargs,\n            )\n            query_runner = QueryRunner(\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 22, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "23": {"text": "  query_runner = QueryRunner(\n                self._llm_predictor,\n                self._prompt_helper,\n                self._embed_model,\n                self._docstore,\n                self._index_registry,\n                query_configs=[query_config],\n                recursive=False,\n            )\n            return query_runner.query(query_str, self._index_struct)\n\n    @classmethod\n    @abstractmethod\n    def get_query_map(cls) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n\n    @classmethod\n    def", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 23, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "24": {"text": "query map.\"\"\"\n\n    @classmethod\n    def load_from_string(cls, index_string: str, **kwargs: Any) -> \"BaseGPTIndex\":\n        \"\"\"Load index from string (in JSON-format).\n\n        This method loads the index from a JSON string. The index data\n        structure itself is preserved completely. If the index is defined over\n        subindices, those subindices will also be preserved (and subindices of\n        those subindices, etc.).\n\n        NOTE: load_from_string should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_string` and `load_from_string` on that instead.\n\n        Args:\n            index_string (str): The index string (in JSON-format).\n\n        Returns:\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 24, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "25": {"text": "  Returns:\n            BaseGPTIndex: The loaded index.\n\n        \"\"\"\n        result_dict = json.loads(index_string)\n        index_struct = cls.index_struct_cls.from_dict(result_dict[\"index_struct\"])\n        type_to_struct = {index_struct.get_type(): type(index_struct)}\n        docstore = DocumentStore.load_from_dict(\n            result_dict[\"docstore\"],\n            type_to_struct=type_to_struct,\n        )\n        return cls(index_struct=index_struct, docstore=docstore, **kwargs)\n\n    @classmethod\n    def load_from_disk(cls, save_path: str, **kwargs: Any) -> \"BaseGPTIndex\":\n        \"\"\"Load index from disk.\n\n        This method loads the index from a", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 25, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "26": {"text": "       This method loads the index from a JSON file stored on disk. The index data\n        structure itself is preserved completely. If the index is defined over\n        subindices, those subindices will also be preserved (and subindices of\n        those subindices, etc.).\n\n        NOTE: load_from_disk should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_disk` and `load_from_disk` on that instead.\n\n        Args:\n            save_path (str): The save_path of the file.\n\n        Returns:\n            BaseGPTIndex: The loaded index.\n\n        \"\"\"\n        with open(save_path, \"r\") as f:\n            file_contents = f.read()\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 26, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "27": {"text": "  file_contents = f.read()\n            return cls.load_from_string(file_contents, **kwargs)\n\n    def save_to_string(self, **save_kwargs: Any) -> str:\n        \"\"\"Save to string.\n\n        This method stores the index into a JSON string.\n\n        NOTE: save_to_string should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_string` and `load_from_string` on that instead.\n\n        Returns:\n            str: The JSON string of the index.\n\n        \"\"\"\n        if self.docstore.contains_index_struct(\n            exclude_ids=[self.index_struct.get_doc_id()]\n        ):\n            raise", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 27, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "28": {"text": "  ):\n            raise ValueError(\n                \"Cannot call `save_to_string` on index if index is composed on top of \"\n                \"other indices. Please define a `ComposableGraph` and use \"\n                \"`save_to_string` and `load_from_string` on that instead.\"\n            )\n        out_dict: Dict[str, dict] = {\n            \"index_struct\": self.index_struct.to_dict(),\n            \"docstore\": self.docstore.serialize_to_dict(),\n        }\n        return json.dumps(out_dict, **save_kwargs)\n\n    def save_to_disk(self, save_path: str, **save_kwargs: Any) -> None:\n        \"\"\"Save to", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 28, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "29": {"text": "Any) -> None:\n        \"\"\"Save to file.\n\n        This method stores the index into a JSON file stored on disk.\n\n        NOTE: save_to_disk should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_disk` and `load_from_disk` on that instead.\n\n        Args:\n            save_path (str): The save_path of the file.\n\n        \"\"\"\n        index_string = self.save_to_string(**save_kwargs)\n        with open(save_path, \"w\") as f:\n            f.write(index_string)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 29, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "30": {"text": "\"\"\"General node utils.\"\"\"\n\n\nimport logging\nfrom typing import List\n\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.indices.utils import truncate_text\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.schema import BaseDocument\n\n\ndef get_nodes_from_document(\n    document: BaseDocument,\n    text_splitter: TokenTextSplitter,\n    start_idx: int = 0,\n    include_extra_info: bool = True,\n) -> List[Node]:\n    \"\"\"Add document to index.\"\"\"\n    text_chunks_with_overlap = text_splitter.split_text_with_overlaps(\n        document.get_text(),\n        extra_info_str=document.extra_info_str if include_extra_info else None,\n    )\n    nodes = []\n    index_counter = 0\n    for i, text_split in", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/node_utils.py", "file_name": "node_utils.py"}, "index": 30, "child_indices": [], "ref_doc_id": "57e0df84f5302d7788c835809895fe5ae92d7f57", "node_info": null}, "31": {"text": "index_counter = 0\n    for i, text_split in enumerate(text_chunks_with_overlap):\n        text_chunk = text_split.text_chunk\n        logging.debug(f\"> Adding chunk: {truncate_text(text_chunk, 50)}\")\n        index_pos_info = {\n            # NOTE: start is inclusive, end is exclusive\n            \"start\": index_counter - text_split.num_char_overlap,\n            \"end\": index_counter - text_split.num_char_overlap + len(text_chunk),\n        }\n        index_counter += len(text_chunk) + 1\n        # if embedding specified in document, pass it to the Node\n        node = Node(\n            text=text_chunk,\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/node_utils.py", "file_name": "node_utils.py"}, "index": 31, "child_indices": [], "ref_doc_id": "57e0df84f5302d7788c835809895fe5ae92d7f57", "node_info": null}, "32": {"text": "           index=start_idx + i,\n            ref_doc_id=document.get_doc_id(),\n            embedding=document.embedding,\n            extra_info=document.extra_info if include_extra_info else None,\n            node_info=index_pos_info,\n        )\n        nodes.append(node)\n    return nodes\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/node_utils.py", "file_name": "node_utils.py"}, "index": 32, "child_indices": [], "ref_doc_id": "57e0df84f5302d7788c835809895fe5ae92d7f57", "node_info": null}, "33": {"text": "\"\"\"General prompt helper that can help deal with token limitations.\n\nThe helper can split text. It can also concatenate text from Node\nstructs but keeping token limitations in mind.\n\n\"\"\"\n\nfrom typing import Callable, List, Optional\n\nfrom gpt_index.constants import MAX_CHUNK_OVERLAP\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.utils import globals_helper\n\n\nclass PromptHelper:\n    \"\"\"Prompt helper.\n\n    This utility helps us fill in the prompt, split the text,\n    and fill in context information according to necessary token limitations.\n\n    Args:\n        max_input_size (int): Maximum input size for the LLM.\n        num_output (int): Number of outputs for the LLM.\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 33, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "34": {"text": "Number of outputs for the LLM.\n        max_chunk_overlap (int): Maximum chunk overlap for the LLM.\n        embedding_limit (Optional[int]): Maximum number of embeddings to use.\n        chunk_size_limit (Optional[int]): Maximum chunk size to use.\n        tokenizer (Optional[Callable[[str], List]]): Tokenizer to use.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        max_input_size: int,\n        num_output: int,\n        max_chunk_overlap: int,\n        embedding_limit: Optional[int] = None,\n        chunk_size_limit: Optional[int] = None,\n        tokenizer: Optional[Callable[[str], List]] = None,\n        separator: str = \" \",\n    ) -> None:\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 34, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "35": {"text": "= \" \",\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self.max_input_size = max_input_size\n        self.num_output = num_output\n        self.max_chunk_overlap = max_chunk_overlap\n        self.embedding_limit = embedding_limit\n        self.chunk_size_limit = chunk_size_limit\n        # TODO: make configurable\n        self._tokenizer = tokenizer or globals_helper.tokenizer\n        self._separator = separator\n        self.use_chunk_size_limit = chunk_size_limit is not None\n\n    @classmethod\n    def from_llm_predictor(\n        self,\n        llm_predictor: LLMPredictor,\n        max_chunk_overlap:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 35, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "36": {"text": "       max_chunk_overlap: Optional[int] = None,\n        embedding_limit: Optional[int] = None,\n        chunk_size_limit: Optional[int] = None,\n        tokenizer: Optional[Callable[[str], List]] = None,\n    ) -> \"PromptHelper\":\n        \"\"\"Create from llm predictor.\n\n        This will autofill values like max_input_size and num_output.\n\n        \"\"\"\n        llm_metadata = llm_predictor.get_llm_metadata()\n        max_chunk_overlap = max_chunk_overlap or min(\n            MAX_CHUNK_OVERLAP, llm_metadata.max_input_size // 10\n        )\n        return self(\n            llm_metadata.max_input_size,\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 36, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "37": {"text": "  llm_metadata.max_input_size,\n            llm_metadata.num_output,\n            max_chunk_overlap,\n            embedding_limit=embedding_limit,\n            chunk_size_limit=chunk_size_limit,\n            tokenizer=tokenizer,\n        )\n\n    def get_chunk_size_given_prompt(\n        self, prompt_text: str, num_chunks: int, padding: Optional[int] = 1\n    ) -> int:\n        \"\"\"Get chunk size making sure we can also fit the prompt in.\n\n        Chunk size is computed based on a function of the total input size,\n        the prompt length, the number of outputs, and the number of chunks.\n\n        If padding is specified, then we subtract that from the", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 37, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "38": {"text": "   If padding is specified, then we subtract that from the chunk size.\n        By default we assume there is a padding of 1 (for the newline between chunks).\n\n        Limit by embedding_limit and chunk_size_limit if specified.\n\n        \"\"\"\n        prompt_tokens = self._tokenizer(prompt_text)\n        num_prompt_tokens = len(prompt_tokens)\n\n        # NOTE: if embedding limit is specified, then chunk_size must not be larger than\n        # embedding_limit\n        result = (\n            self.max_input_size - num_prompt_tokens - self.num_output\n        ) // num_chunks\n        if padding is not None:\n            result -= padding\n\n        if self.embedding_limit is not", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 38, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "39": {"text": "       if self.embedding_limit is not None:\n            result = min(result, self.embedding_limit)\n        if self.chunk_size_limit is not None and self.use_chunk_size_limit:\n            result = min(result, self.chunk_size_limit)\n\n        return result\n\n    def _get_empty_prompt_txt(self, prompt: Prompt) -> str:\n        \"\"\"Get empty prompt text.\n\n        Substitute empty strings in parts of the prompt that have\n        not yet been filled out. Skip variables that have already\n        been partially formatted. This is used to compute the initial tokens.\n\n        \"\"\"\n        fmt_dict = {\n            v: \"\" for v in prompt.input_variables if v not in prompt.partial_dict\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 39, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "40": {"text": "if v not in prompt.partial_dict\n        }\n        empty_prompt_txt = prompt.format(**fmt_dict)\n        return empty_prompt_txt\n\n    def get_biggest_prompt(self, prompts: List[Prompt]) -> Prompt:\n        \"\"\"Get biggest prompt.\n\n        Oftentimes we need to fetch the biggest prompt, in order to\n        be the most conservative about chunking text. This\n        is a helper utility for that.\n\n        \"\"\"\n        empty_prompt_txts = [self._get_empty_prompt_txt(prompt) for prompt in prompts]\n        empty_prompt_txt_lens = [len(txt) for txt in empty_prompt_txts]\n        biggest_prompt = prompts[\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 40, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "41": {"text": "prompts[\n            empty_prompt_txt_lens.index(max(empty_prompt_txt_lens))\n        ]\n        return biggest_prompt\n\n    def get_text_splitter_given_prompt(\n        self, prompt: Prompt, num_chunks: int, padding: Optional[int] = 1\n    ) -> TokenTextSplitter:\n        \"\"\"Get text splitter given initial prompt.\n\n        Allows us to get the text splitter which will split up text according\n        to the desired chunk size.\n\n        \"\"\"\n        # generate empty_prompt_txt to compute initial tokens\n        empty_prompt_txt = self._get_empty_prompt_txt(prompt)\n        chunk_size = self.get_chunk_size_given_prompt(\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 41, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "42": {"text": "           empty_prompt_txt, num_chunks, padding=padding\n        )\n        text_splitter = TokenTextSplitter(\n            separator=self._separator,\n            chunk_size=chunk_size,\n            chunk_overlap=self.max_chunk_overlap // num_chunks,\n            tokenizer=self._tokenizer,\n        )\n        return text_splitter\n\n    def get_text_from_nodes(\n        self, node_list: List[Node], prompt: Optional[Prompt] = None\n    ) -> str:\n        \"\"\"Get text from nodes. Used by tree-structured indices.\"\"\"\n        num_nodes = len(node_list)\n        text_splitter = None\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 42, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "43": {"text": "      text_splitter = None\n        if prompt is not None:\n            # add padding given the newline character\n            text_splitter = self.get_text_splitter_given_prompt(\n                prompt,\n                num_nodes,\n                padding=1,\n            )\n        results = []\n        for node in node_list:\n            text = (\n                text_splitter.truncate_text(node.get_text())\n                if text_splitter is not None\n                else node.get_text()\n         ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 43, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "44": {"text": "node.get_text()\n            )\n            results.append(text)\n\n        return \"\\n\".join(results)\n\n    def get_numbered_text_from_nodes(\n        self, node_list: List[Node], prompt: Optional[Prompt] = None\n    ) -> str:\n        \"\"\"Get text from nodes in the format of a numbered list.\n\n        Used by tree-structured indices.\n\n        \"\"\"\n        num_nodes = len(node_list)\n        text_splitter = None\n        if prompt is not None:\n            # add padding given the number, and the newlines\n            text_splitter = self.get_text_splitter_given_prompt(\n                prompt,\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 44, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "45": {"text": "      prompt,\n                num_nodes,\n                padding=5,\n            )\n        results = []\n        number = 1\n        for node in node_list:\n            node_text = \" \".join(node.get_text().splitlines())\n            if text_splitter is not None:\n                node_text = text_splitter.truncate_text(node_text)\n            text = f\"({number}) {node_text}\"\n            results.append(text)\n            number += 1\n        return \"\\n\\n\".join(results)\n\n    def compact_text_chunks(self, prompt: Prompt,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 45, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "46": {"text": "compact_text_chunks(self, prompt: Prompt, text_chunks: List[str]) -> List[str]:\n        \"\"\"Compact text chunks.\n\n        This will combine text chunks into consolidated chunks\n        that more fully \"pack\" the prompt template given the max_input_size.\n\n        \"\"\"\n        combined_str = \"\\n\\n\".join([c.strip() for c in text_chunks if c.strip()])\n        # resplit based on self.max_chunk_overlap\n        text_splitter = self.get_text_splitter_given_prompt(prompt, 1, padding=1)\n        return text_splitter.split_text(combined_str)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 46, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "47": {"text": "\"\"\"Index registry.\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Type\n\nfrom gpt_index.data_structs.data_structs import IndexStruct\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\n\n# map from mode to query class\nQUERY_MAP_TYPE = Dict[str, Type[BaseGPTIndexQuery]]\n\n\n@dataclass\nclass IndexRegistry:\n    \"\"\"Index registry.\n\n    Stores mapping from index type to index_struct + queries.\n    NOTE: this cannot be easily serialized, so must be re-initialized\n    each time.\n    If the user defines custom IndexStruct or query classes,\n    they must be added to the registry manually.\n\n    \"\"\"\n\n    type_to_struct: Dict[str, Type[IndexStruct]] = field(default_factory=dict)\n    type_to_query: Dict[str, QUERY_MAP_TYPE] = field(default_factory=dict)\n\n    def update(self, other: \"IndexRegistry\") -> None:\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/registry.py", "file_name": "registry.py"}, "index": 47, "child_indices": [], "ref_doc_id": "4d36a4f799b01b46bb1527a5b95f7186cc3554b6", "node_info": null}, "48": {"text": "\"IndexRegistry\") -> None:\n        \"\"\"Update the registry with another registry.\"\"\"\n        self.type_to_struct.update(other.type_to_struct)\n        self.type_to_query.update(other.type_to_query)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/registry.py", "file_name": "registry.py"}, "index": 48, "child_indices": [], "ref_doc_id": "4d36a4f799b01b46bb1527a5b95f7186cc3554b6", "node_info": null}, "49": {"text": "\"\"\"Utilities for GPT indices.\"\"\"\nimport re\nfrom typing import Dict, List, Optional, Set\n\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.utils import globals_helper\n\n\ndef get_sorted_node_list(node_dict: Dict[int, Node]) -> List[Node]:\n    \"\"\"Get sorted node list. Used by tree-strutured indices.\"\"\"\n    sorted_indices = sorted(node_dict.keys())\n    return [node_dict[index] for index in sorted_indices]\n\n\ndef extract_numbers_given_response(response: str, n: int = 1) -> Optional[List[int]]:\n    \"\"\"Extract number given the GPT-generated response.\n\n    Used by tree-structured indices.\n\n    \"\"\"\n    numbers = re.findall(r\"\\d+\", response)\n    if len(numbers) == 0:\n        return None\n    else:\n        return numbers[:n]\n\n\ndef", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/utils.py", "file_name": "utils.py"}, "index": 49, "child_indices": [], "ref_doc_id": "4622a34aedaeb346c6f275d45c96bb8ae57d202b", "node_info": null}, "50": {"text": "      return numbers[:n]\n\n\ndef expand_tokens_with_subtokens(tokens: Set[str]) -> Set[str]:\n    \"\"\"Get subtokens from a list of tokens., filtering for stopwords.\"\"\"\n    results = set()\n    for token in tokens:\n        results.add(token)\n        sub_tokens = re.findall(r\"\\w+\", token)\n        if len(sub_tokens) > 1:\n            results.update({w for w in sub_tokens if w not in globals_helper.stopwords})\n\n    return results\n\n\ndef truncate_text(text: str, max_length: int) -> str:\n    \"\"\"Truncate text to a maximum length.\"\"\"\n    return text[: max_length - 3] + \"...\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/utils.py", "file_name": "utils.py"}, "index": 50, "child_indices": [], "ref_doc_id": "4622a34aedaeb346c6f275d45c96bb8ae57d202b", "node_info": null}, "51": {"text": "This code file is part of the GPT Index data structures. It contains the base class for GPT Index, which is used to build an index from a list of documents. It includes parameters such as documents, llm_predictor, embed_model, docstore, index_registry, prompt_helper, chunk_size_limit, and include_extra_info. It also contains methods to process documents, validate documents, and update the index registry and docstore.", "doc_id": null, "embedding": null, "extra_info": null, "index": 51, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "52": {"text": "This code file is the base.py file from the gpt_index/indices directory. It contains the base class for all GPT indices, and provides methods for building, inserting, deleting, and updating documents in the index. It also provides methods for querying the index, including a recursive query mode and a query map for each index type. The code also includes methods for setting the text, extra info, and doc_id for the index struct, as well as methods for validating documents and preprocessing queries.", "doc_id": null, "embedding": null, "extra_info": null, "index": 52, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "53": {"text": "\nThis code file contains two classes, BaseGPTIndex and PromptHelper. BaseGPTIndex provides methods to load and save an index from/to a JSON string or file, while PromptHelper provides methods to fill in a prompt, split text, and fill in context information according to necessary token limitations. Both classes have various parameters and methods to help with their respective tasks.", "doc_id": null, "embedding": null, "extra_info": null, "index": 53, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}, "54": {"text": "The prompt_helper.py file is a Python script that provides a class for creating a PromptHelper object from a llm predictor. It also provides methods for getting the chunk size given a prompt, getting the biggest prompt, getting the text splitter given a prompt, getting text from nodes, getting numbered text from nodes, and compacting text chunks. The registry.py file is a Python script that provides a class for creating an IndexRegistry object. It also provides a method for updating the registry with another registry.", "doc_id": null, "embedding": null, "extra_info": null, "index": 54, "child_indices": [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], "ref_doc_id": null, "node_info": null}, "55": {"text": "This code file contains utilities for GPT indices. It includes functions to get a sorted node list, extract numbers from a GPT-generated response, expand tokens with subtokens, and truncate text to a maximum length. It also includes a function to update the registry with another registry. All of these functions are used to help with the GPT indexing process.", "doc_id": null, "embedding": null, "extra_info": null, "index": 55, "child_indices": [48, 49, 50], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"51": {"text": "This code file is part of the GPT Index data structures. It contains the base class for GPT Index, which is used to build an index from a list of documents. It includes parameters such as documents, llm_predictor, embed_model, docstore, index_registry, prompt_helper, chunk_size_limit, and include_extra_info. It also contains methods to process documents, validate documents, and update the index registry and docstore.", "doc_id": null, "embedding": null, "extra_info": null, "index": 51, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "52": {"text": "This code file is the base.py file from the gpt_index/indices directory. It contains the base class for all GPT indices, and provides methods for building, inserting, deleting, and updating documents in the index. It also provides methods for querying the index, including a recursive query mode and a query map for each index type. The code also includes methods for setting the text, extra info, and doc_id for the index struct, as well as methods for validating documents and preprocessing queries.", "doc_id": null, "embedding": null, "extra_info": null, "index": 52, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "53": {"text": "\nThis code file contains two classes, BaseGPTIndex and PromptHelper. BaseGPTIndex provides methods to load and save an index from/to a JSON string or file, while PromptHelper provides methods to fill in a prompt, split text, and fill in context information according to necessary token limitations. Both classes have various parameters and methods to help with their respective tasks.", "doc_id": null, "embedding": null, "extra_info": null, "index": 53, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}, "54": {"text": "The prompt_helper.py file is a Python script that provides a class for creating a PromptHelper object from a llm predictor. It also provides methods for getting the chunk size given a prompt, getting the biggest prompt, getting the text splitter given a prompt, getting text from nodes, getting numbered text from nodes, and compacting text chunks. The registry.py file is a Python script that provides a class for creating an IndexRegistry object. It also provides a method for updating the registry with another registry.", "doc_id": null, "embedding": null, "extra_info": null, "index": 54, "child_indices": [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], "ref_doc_id": null, "node_info": null}, "55": {"text": "This code file contains utilities for GPT indices. It includes functions to get a sorted node list, extract numbers from a GPT-generated response, expand tokens with subtokens, and truncate text to a maximum length. It also includes a function to update the registry with another registry. All of these functions are used to help with the GPT indexing process.", "doc_id": null, "embedding": null, "extra_info": null, "index": 55, "child_indices": [48, 49, 50], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"e7bbbf6f654fcec74653151ee1feeea4b388c65b": {"text": "\"\"\"GPT Index data structures.\"\"\"\n\n# indices\nfrom gpt_index.indices.keyword_table.base import GPTKeywordTableIndex\nfrom gpt_index.indices.keyword_table.rake_base import GPTRAKEKeywordTableIndex\nfrom gpt_index.indices.keyword_table.simple_base import GPTSimpleKeywordTableIndex\nfrom gpt_index.indices.list.base import GPTListIndex\nfrom gpt_index.indices.tree.base import GPTTreeIndex\nfrom gpt_index.indices.vector_store.faiss import GPTFaissIndex\n\n__all__ = [\n    \"GPTKeywordTableIndex\",\n    \"GPTSimpleKeywordTableIndex\",\n    \"GPTRAKEKeywordTableIndex\",\n    \"GPTListIndex\",\n    \"GPTTreeIndex\",\n    \"GPTFaissIndex\",\n]\n", "doc_id": "e7bbbf6f654fcec74653151ee1feeea4b388c65b", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "2dec2853fc1bc65bc09aec5679d40f3cd702a74a": {"text": "\"\"\"Base index classes.\"\"\"\nimport json\nimport logging\nfrom abc import abstractmethod\nfrom typing import (\n    Any,\n    Dict,\n    Generic,\n    List,\n    Optional,\n    Sequence,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom gpt_index.data_structs.data_structs import IndexStruct, Node\nfrom gpt_index.docstore import DOC_TYPE, DocumentStore\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.embeddings.openai import OpenAIEmbedding\nfrom gpt_index.indices.node_utils import get_nodes_from_document\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.query_runner import QueryRunner\nfrom gpt_index.indices.query.schema import QueryConfig, QueryMode\nfrom gpt_index.indices.registry import IndexRegistry\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.readers.schema.base import Document\nfrom gpt_index.response.schema import Response\nfrom gpt_index.schema import BaseDocument\nfrom gpt_index.token_counter.token_counter import llm_token_counter\n\nIS = TypeVar(\"IS\", bound=IndexStruct)\n\n\nDOCUMENTS_INPUT = Union[BaseDocument, \"BaseGPTIndex\"]\n\n\nclass BaseGPTIndex(Generic[IS]):\n    \"\"\"Base GPT Index.\n\n    Args:\n        documents (Optional[Sequence[BaseDocument]]): List of documents to\n            build the index from.\n        llm_predictor (LLMPredictor): Optional LLMPredictor object. If not provided,\n            will use the default LLMPredictor (text-davinci-003)\n        prompt_helper (PromptHelper): Optional PromptHelper object. If not provided,\n            will use the default PromptHelper.\n        chunk_size_limit (Optional[int]): Optional chunk size limit. If not provided,\n            will use the default chunk size limit (4096 max input size).\n        include_extra_info (bool): Optional bool. If True, extra info (i.e. metadata)\n            of each Document will be prepended to its text to help with queries.\n            Default is True.\n\n    \"\"\"\n\n    index_struct_cls: Type[IS]\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[IS] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        docstore: Optional[DocumentStore] = None,\n        index_registry: Optional[IndexRegistry] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        chunk_size_limit: Optional[int] = None,\n        include_extra_info: bool = True,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        if index_struct is None and documents is None:\n            raise ValueError(\"One of documents or index_struct must be provided.\")\n        if index_struct is not None and documents is not None:\n            raise ValueError(\"Only one of documents or index_struct can be provided.\")\n\n        self._llm_predictor = llm_predictor or LLMPredictor()\n        # NOTE: the embed_model isn't used in all indices\n        self._embed_model = embed_model or OpenAIEmbedding()\n        self._include_extra_info = include_extra_info\n\n        # TODO: move out of base if we need custom params per index\n        self._prompt_helper = prompt_helper or PromptHelper.from_llm_predictor(\n            self._llm_predictor, chunk_size_limit=chunk_size_limit\n        )\n\n        # build index struct in the init function\n        self._docstore = docstore or DocumentStore()\n        self._index_registry = index_registry or IndexRegistry()\n\n        if index_struct is not None:\n            if not isinstance(index_struct, self.index_struct_cls):\n                raise ValueError(\n                    f\"index_struct must be of type {self.index_struct_cls}\"\n                )\n            self._index_struct = index_struct\n        else:\n            documents = cast(Sequence[DOCUMENTS_INPUT], documents)\n            documents = self._process_documents(\n                documents, self._docstore, self._index_registry\n            )\n            self._validate_documents(documents)\n            # TODO: introduce document store outside __init__ function\n            self._index_struct = self.build_index_from_documents(documents)\n        # update index registry and docstore with index_struct\n        self._update_index_registry_and_docstore()\n\n    @property\n    def prompt_helper(self) -> PromptHelper:\n        \"\"\"Get the prompt helper corresponding to the index.\"\"\"\n        return self._prompt_helper\n\n    @property\n    def docstore(self) -> DocumentStore:\n        \"\"\"Get the docstore corresponding to the index.\"\"\"\n        return self._docstore\n\n    @property\n    def index_registry(self) -> IndexRegistry:\n        \"\"\"Get the index registry corresponding to the index.\"\"\"\n        return self._index_registry\n\n    @property\n    def llm_predictor(self) -> LLMPredictor:\n        \"\"\"Get the llm predictor.\"\"\"\n        return self._llm_predictor\n\n    @property\n    def embed_model(self) -> BaseEmbedding:\n        \"\"\"Get the llm predictor.\"\"\"\n        return self._embed_model\n\n    def _update_index_registry_and_docstore(self) -> None:\n        \"\"\"Update index registry and docstore.\"\"\"\n        # update index registry with current struct\n        cur_type = self.index_struct_cls.get_type()\n        self._index_registry.type_to_struct[cur_type] = self.index_struct_cls\n        self._index_registry.type_to_query[cur_type] = self.get_query_map()\n\n        # update docstore with current struct\n        self._docstore.add_documents([self.index_struct])\n\n    def _process_documents(\n        self,\n        documents: Sequence[DOCUMENTS_INPUT],\n        docstore: DocumentStore,\n        index_registry: IndexRegistry,\n    ) -> List[BaseDocument]:\n        \"\"\"Process documents.\"\"\"\n        results: List[DOC_TYPE] = []\n        for doc in documents:\n            if isinstance(doc, BaseGPTIndex):\n                # if user passed in another index, we need to do the following:\n                # - update docstore with the docstore in the index\n                # - validate that the index is in the docstore\n                # - update the index registry\n\n                index_registry.update(doc.index_registry)\n                docstore.update_docstore(doc.docstore)\n                # assert that the doc exists within the docstore\n                sub_index_struct = doc.index_struct_with_text\n                if not docstore.document_exists(sub_index_struct.get_doc_id()):\n                    raise ValueError(\n                        \"The index struct of the sub-index must exist in the docstore. \"\n                        f\"Invalid doc ID: {sub_index_struct.get_doc_id()}\"\n                    )\n                results.append(sub_index_struct)\n            elif isinstance(doc, (Document, IndexStruct)):\n                results.append(doc)\n                # update docstore\n                docstore.add_documents([doc])\n            else:\n                raise ValueError(f\"Invalid document type: {type(doc)}.\")\n        return cast(List[BaseDocument], results)\n\n    def _validate_documents(self, documents: Sequence[BaseDocument]) -> None:\n        \"\"\"Validate documents.\"\"\"\n        for doc in documents:\n            if not isinstance(doc, BaseDocument):\n                raise ValueError(\"Documents must be of type BaseDocument.\")\n\n    @property\n    def index_struct(self) -> IS:\n        \"\"\"Get the index struct.\"\"\"\n        return self._index_struct\n\n    @property\n    def index_struct_with_text(self) -> IS:\n        \"\"\"Get the index struct with text.\n\n        If text not set, raise an error.\n        For use when composing indices with other indices.\n\n        \"\"\"\n        # make sure that we generate text for index struct\n        if self._index_struct.text is None:\n            # NOTE: set text to be empty string for now\n            raise ValueError(\n                \"Index must have text property set in order \"\n                \"to be composed with other indices. \"\n                \"In order to set text, please run `index.set_text()`.\"\n            )\n        return self._index_struct\n\n    def set_text(self, text: str) -> None:\n        \"\"\"Set summary text for index struct.\n\n        This allows index_struct_with_text to be used to compose indices\n        with other indices.\n\n        \"\"\"\n        self._index_struct.text = text\n\n    def set_extra_info(self, extra_info: Dict[str, Any]) -> None:\n        \"\"\"Set extra info (metadata) for index struct.\n\n        If this index is used as a subindex for a parent index, the metadata\n        will be propagated to all nodes derived from this subindex, in the\n        parent index.\n\n        \"\"\"\n        self._index_struct.extra_info = extra_info\n\n    def set_doc_id(self, doc_id: str) -> None:\n        \"\"\"Set doc_id for index struct.\n\n        This is used to uniquely identify the index struct in the docstore.\n        If you wish to delete the index struct, you can use this doc_id.\n\n        \"\"\"\n        old_doc_id = self._index_struct.get_doc_id()\n        self._index_struct.doc_id = doc_id\n        # Note: we also need to delete old doc_id, and update docstore\n        self._docstore.delete_document(old_doc_id)\n        self._docstore.add_documents([self._index_struct])\n\n    def get_doc_id(self) -> str:\n        \"\"\"Get doc_id for index struct.\n\n        If doc_id not set, raise an error.\n\n        \"\"\"\n        if self._index_struct.doc_id is None:\n            raise ValueError(\"Index must have doc_id property set.\")\n        return self._index_struct.doc_id\n\n    def _get_nodes_from_document(\n        self,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n        start_idx: int = 0,\n    ) -> List[Node]:\n        return get_nodes_from_document(\n            document=document,\n            text_splitter=text_splitter,\n            start_idx=start_idx,\n            include_extra_info=self._include_extra_info,\n        )\n\n    @abstractmethod\n    def _build_index_from_documents(self, documents: Sequence[BaseDocument]) -> IS:\n        \"\"\"Build the index from documents.\"\"\"\n\n    @llm_token_counter(\"build_index_from_documents\")\n    def build_index_from_documents(self, documents: Sequence[BaseDocument]) -> IS:\n        \"\"\"Build the index from documents.\"\"\"\n        return self._build_index_from_documents(documents)\n\n    @abstractmethod\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n\n    @llm_token_counter(\"insert\")\n    def insert(self, document: DOCUMENTS_INPUT, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\n\n        Args:\n            document (Union[BaseDocument, BaseGPTIndex]): document to insert\n\n        \"\"\"\n        processed_doc = self._process_documents(\n            [document], self._docstore, self._index_registry\n        )[0]\n        self._validate_documents([processed_doc])\n        self._insert(processed_doc, **insert_kwargs)\n\n    @abstractmethod\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n\n    def delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document from the index.\n\n        All nodes in the index related to the index will be deleted.\n\n        Args:\n            doc_id (str): document id\n            full_delete (bool): whether to delete the document from the docstore.\n                By default this is True.\n\n        \"\"\"\n        full_delete = delete_kwargs.pop(\"full_delete\", True)\n        logging.debug(f\"> Deleting document: {doc_id}\")\n        if full_delete:\n            self._docstore.delete_document(doc_id)\n        self._delete(doc_id, **delete_kwargs)\n\n    def update(self, document: DOCUMENTS_INPUT, **update_kwargs: Any) -> None:\n        \"\"\"Update a document.\n\n        This is equivalent to deleting the document and then inserting it again.\n\n        Args:\n            document (Union[BaseDocument, BaseGPTIndex]): document to update\n            insert_kwargs (Dict): kwargs to pass to insert\n            delete_kwargs (Dict): kwargs to pass to delete\n\n        \"\"\"\n        self.delete(document.get_doc_id(), **update_kwargs.pop(\"delete_kwargs\", {}))\n        self.insert(document, **update_kwargs.pop(\"insert_kwargs\", {}))\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Dict) -> None:\n        \"\"\"Preprocess query.\n\n        This allows subclasses to pass in additional query kwargs\n        to query, for instance arguments that are shared between the\n        index and the query class. By default, this does nothing.\n        This also allows subclasses to do validation.\n\n        \"\"\"\n        pass\n\n    def query(\n        self,\n        query_str: str,\n        mode: str = QueryMode.DEFAULT,\n        **query_kwargs: Any,\n    ) -> Response:\n        \"\"\"Answer a query.\n\n        When `query` is called, we query the index with the given `mode` and\n        `query_kwargs`. The `mode` determines the type of query to run, and\n        `query_kwargs` are parameters that are specific to the query type.\n\n        For a comprehensive documentation of available `mode` and `query_kwargs` to\n        query a given index, please visit :ref:`Ref-Query`.\n\n\n        \"\"\"\n        mode_enum = QueryMode(mode)\n        if mode_enum == QueryMode.RECURSIVE:\n            # TODO: deprecated, use ComposableGraph instead.\n            if \"query_configs\" not in query_kwargs:\n                raise ValueError(\"query_configs must be provided for recursive mode.\")\n            query_configs = query_kwargs[\"query_configs\"]\n            query_runner = QueryRunner(\n                self._llm_predictor,\n                self._prompt_helper,\n                self._embed_model,\n                self._docstore,\n                self._index_registry,\n                query_configs=query_configs,\n                recursive=True,\n            )\n            return query_runner.query(query_str, self._index_struct)\n        else:\n            self._preprocess_query(mode_enum, query_kwargs)\n            # TODO: pass in query config directly\n            query_config = QueryConfig(\n                index_struct_type=self._index_struct.get_type(),\n                query_mode=mode_enum,\n                query_kwargs=query_kwargs,\n            )\n            query_runner = QueryRunner(\n                self._llm_predictor,\n                self._prompt_helper,\n                self._embed_model,\n                self._docstore,\n                self._index_registry,\n                query_configs=[query_config],\n                recursive=False,\n            )\n            return query_runner.query(query_str, self._index_struct)\n\n    @classmethod\n    @abstractmethod\n    def get_query_map(cls) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n\n    @classmethod\n    def load_from_string(cls, index_string: str, **kwargs: Any) -> \"BaseGPTIndex\":\n        \"\"\"Load index from string (in JSON-format).\n\n        This method loads the index from a JSON string. The index data\n        structure itself is preserved completely. If the index is defined over\n        subindices, those subindices will also be preserved (and subindices of\n        those subindices, etc.).\n\n        NOTE: load_from_string should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_string` and `load_from_string` on that instead.\n\n        Args:\n            index_string (str): The index string (in JSON-format).\n\n        Returns:\n            BaseGPTIndex: The loaded index.\n\n        \"\"\"\n        result_dict = json.loads(index_string)\n        index_struct = cls.index_struct_cls.from_dict(result_dict[\"index_struct\"])\n        type_to_struct = {index_struct.get_type(): type(index_struct)}\n        docstore = DocumentStore.load_from_dict(\n            result_dict[\"docstore\"],\n            type_to_struct=type_to_struct,\n        )\n        return cls(index_struct=index_struct, docstore=docstore, **kwargs)\n\n    @classmethod\n    def load_from_disk(cls, save_path: str, **kwargs: Any) -> \"BaseGPTIndex\":\n        \"\"\"Load index from disk.\n\n        This method loads the index from a JSON file stored on disk. The index data\n        structure itself is preserved completely. If the index is defined over\n        subindices, those subindices will also be preserved (and subindices of\n        those subindices, etc.).\n\n        NOTE: load_from_disk should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_disk` and `load_from_disk` on that instead.\n\n        Args:\n            save_path (str): The save_path of the file.\n\n        Returns:\n            BaseGPTIndex: The loaded index.\n\n        \"\"\"\n        with open(save_path, \"r\") as f:\n            file_contents = f.read()\n            return cls.load_from_string(file_contents, **kwargs)\n\n    def save_to_string(self, **save_kwargs: Any) -> str:\n        \"\"\"Save to string.\n\n        This method stores the index into a JSON string.\n\n        NOTE: save_to_string should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_string` and `load_from_string` on that instead.\n\n        Returns:\n            str: The JSON string of the index.\n\n        \"\"\"\n        if self.docstore.contains_index_struct(\n            exclude_ids=[self.index_struct.get_doc_id()]\n        ):\n            raise ValueError(\n                \"Cannot call `save_to_string` on index if index is composed on top of \"\n                \"other indices. Please define a `ComposableGraph` and use \"\n                \"`save_to_string` and `load_from_string` on that instead.\"\n            )\n        out_dict: Dict[str, dict] = {\n            \"index_struct\": self.index_struct.to_dict(),\n            \"docstore\": self.docstore.serialize_to_dict(),\n        }\n        return json.dumps(out_dict, **save_kwargs)\n\n    def save_to_disk(self, save_path: str, **save_kwargs: Any) -> None:\n        \"\"\"Save to file.\n\n        This method stores the index into a JSON file stored on disk.\n\n        NOTE: save_to_disk should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_disk` and `load_from_disk` on that instead.\n\n        Args:\n            save_path (str): The save_path of the file.\n\n        \"\"\"\n        index_string = self.save_to_string(**save_kwargs)\n        with open(save_path, \"w\") as f:\n            f.write(index_string)\n", "doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "__type__": "Document"}, "57e0df84f5302d7788c835809895fe5ae92d7f57": {"text": "\"\"\"General node utils.\"\"\"\n\n\nimport logging\nfrom typing import List\n\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.indices.utils import truncate_text\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.schema import BaseDocument\n\n\ndef get_nodes_from_document(\n    document: BaseDocument,\n    text_splitter: TokenTextSplitter,\n    start_idx: int = 0,\n    include_extra_info: bool = True,\n) -> List[Node]:\n    \"\"\"Add document to index.\"\"\"\n    text_chunks_with_overlap = text_splitter.split_text_with_overlaps(\n        document.get_text(),\n        extra_info_str=document.extra_info_str if include_extra_info else None,\n    )\n    nodes = []\n    index_counter = 0\n    for i, text_split in enumerate(text_chunks_with_overlap):\n        text_chunk = text_split.text_chunk\n        logging.debug(f\"> Adding chunk: {truncate_text(text_chunk, 50)}\")\n        index_pos_info = {\n            # NOTE: start is inclusive, end is exclusive\n            \"start\": index_counter - text_split.num_char_overlap,\n            \"end\": index_counter - text_split.num_char_overlap + len(text_chunk),\n        }\n        index_counter += len(text_chunk) + 1\n        # if embedding specified in document, pass it to the Node\n        node = Node(\n            text=text_chunk,\n            index=start_idx + i,\n            ref_doc_id=document.get_doc_id(),\n            embedding=document.embedding,\n            extra_info=document.extra_info if include_extra_info else None,\n            node_info=index_pos_info,\n        )\n        nodes.append(node)\n    return nodes\n", "doc_id": "57e0df84f5302d7788c835809895fe5ae92d7f57", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/node_utils.py", "file_name": "node_utils.py"}, "__type__": "Document"}, "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01": {"text": "\"\"\"General prompt helper that can help deal with token limitations.\n\nThe helper can split text. It can also concatenate text from Node\nstructs but keeping token limitations in mind.\n\n\"\"\"\n\nfrom typing import Callable, List, Optional\n\nfrom gpt_index.constants import MAX_CHUNK_OVERLAP\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.utils import globals_helper\n\n\nclass PromptHelper:\n    \"\"\"Prompt helper.\n\n    This utility helps us fill in the prompt, split the text,\n    and fill in context information according to necessary token limitations.\n\n    Args:\n        max_input_size (int): Maximum input size for the LLM.\n        num_output (int): Number of outputs for the LLM.\n        max_chunk_overlap (int): Maximum chunk overlap for the LLM.\n        embedding_limit (Optional[int]): Maximum number of embeddings to use.\n        chunk_size_limit (Optional[int]): Maximum chunk size to use.\n        tokenizer (Optional[Callable[[str], List]]): Tokenizer to use.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        max_input_size: int,\n        num_output: int,\n        max_chunk_overlap: int,\n        embedding_limit: Optional[int] = None,\n        chunk_size_limit: Optional[int] = None,\n        tokenizer: Optional[Callable[[str], List]] = None,\n        separator: str = \" \",\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self.max_input_size = max_input_size\n        self.num_output = num_output\n        self.max_chunk_overlap = max_chunk_overlap\n        self.embedding_limit = embedding_limit\n        self.chunk_size_limit = chunk_size_limit\n        # TODO: make configurable\n        self._tokenizer = tokenizer or globals_helper.tokenizer\n        self._separator = separator\n        self.use_chunk_size_limit = chunk_size_limit is not None\n\n    @classmethod\n    def from_llm_predictor(\n        self,\n        llm_predictor: LLMPredictor,\n        max_chunk_overlap: Optional[int] = None,\n        embedding_limit: Optional[int] = None,\n        chunk_size_limit: Optional[int] = None,\n        tokenizer: Optional[Callable[[str], List]] = None,\n    ) -> \"PromptHelper\":\n        \"\"\"Create from llm predictor.\n\n        This will autofill values like max_input_size and num_output.\n\n        \"\"\"\n        llm_metadata = llm_predictor.get_llm_metadata()\n        max_chunk_overlap = max_chunk_overlap or min(\n            MAX_CHUNK_OVERLAP, llm_metadata.max_input_size // 10\n        )\n        return self(\n            llm_metadata.max_input_size,\n            llm_metadata.num_output,\n            max_chunk_overlap,\n            embedding_limit=embedding_limit,\n            chunk_size_limit=chunk_size_limit,\n            tokenizer=tokenizer,\n        )\n\n    def get_chunk_size_given_prompt(\n        self, prompt_text: str, num_chunks: int, padding: Optional[int] = 1\n    ) -> int:\n        \"\"\"Get chunk size making sure we can also fit the prompt in.\n\n        Chunk size is computed based on a function of the total input size,\n        the prompt length, the number of outputs, and the number of chunks.\n\n        If padding is specified, then we subtract that from the chunk size.\n        By default we assume there is a padding of 1 (for the newline between chunks).\n\n        Limit by embedding_limit and chunk_size_limit if specified.\n\n        \"\"\"\n        prompt_tokens = self._tokenizer(prompt_text)\n        num_prompt_tokens = len(prompt_tokens)\n\n        # NOTE: if embedding limit is specified, then chunk_size must not be larger than\n        # embedding_limit\n        result = (\n            self.max_input_size - num_prompt_tokens - self.num_output\n        ) // num_chunks\n        if padding is not None:\n            result -= padding\n\n        if self.embedding_limit is not None:\n            result = min(result, self.embedding_limit)\n        if self.chunk_size_limit is not None and self.use_chunk_size_limit:\n            result = min(result, self.chunk_size_limit)\n\n        return result\n\n    def _get_empty_prompt_txt(self, prompt: Prompt) -> str:\n        \"\"\"Get empty prompt text.\n\n        Substitute empty strings in parts of the prompt that have\n        not yet been filled out. Skip variables that have already\n        been partially formatted. This is used to compute the initial tokens.\n\n        \"\"\"\n        fmt_dict = {\n            v: \"\" for v in prompt.input_variables if v not in prompt.partial_dict\n        }\n        empty_prompt_txt = prompt.format(**fmt_dict)\n        return empty_prompt_txt\n\n    def get_biggest_prompt(self, prompts: List[Prompt]) -> Prompt:\n        \"\"\"Get biggest prompt.\n\n        Oftentimes we need to fetch the biggest prompt, in order to\n        be the most conservative about chunking text. This\n        is a helper utility for that.\n\n        \"\"\"\n        empty_prompt_txts = [self._get_empty_prompt_txt(prompt) for prompt in prompts]\n        empty_prompt_txt_lens = [len(txt) for txt in empty_prompt_txts]\n        biggest_prompt = prompts[\n            empty_prompt_txt_lens.index(max(empty_prompt_txt_lens))\n        ]\n        return biggest_prompt\n\n    def get_text_splitter_given_prompt(\n        self, prompt: Prompt, num_chunks: int, padding: Optional[int] = 1\n    ) -> TokenTextSplitter:\n        \"\"\"Get text splitter given initial prompt.\n\n        Allows us to get the text splitter which will split up text according\n        to the desired chunk size.\n\n        \"\"\"\n        # generate empty_prompt_txt to compute initial tokens\n        empty_prompt_txt = self._get_empty_prompt_txt(prompt)\n        chunk_size = self.get_chunk_size_given_prompt(\n            empty_prompt_txt, num_chunks, padding=padding\n        )\n        text_splitter = TokenTextSplitter(\n            separator=self._separator,\n            chunk_size=chunk_size,\n            chunk_overlap=self.max_chunk_overlap // num_chunks,\n            tokenizer=self._tokenizer,\n        )\n        return text_splitter\n\n    def get_text_from_nodes(\n        self, node_list: List[Node], prompt: Optional[Prompt] = None\n    ) -> str:\n        \"\"\"Get text from nodes. Used by tree-structured indices.\"\"\"\n        num_nodes = len(node_list)\n        text_splitter = None\n        if prompt is not None:\n            # add padding given the newline character\n            text_splitter = self.get_text_splitter_given_prompt(\n                prompt,\n                num_nodes,\n                padding=1,\n            )\n        results = []\n        for node in node_list:\n            text = (\n                text_splitter.truncate_text(node.get_text())\n                if text_splitter is not None\n                else node.get_text()\n            )\n            results.append(text)\n\n        return \"\\n\".join(results)\n\n    def get_numbered_text_from_nodes(\n        self, node_list: List[Node], prompt: Optional[Prompt] = None\n    ) -> str:\n        \"\"\"Get text from nodes in the format of a numbered list.\n\n        Used by tree-structured indices.\n\n        \"\"\"\n        num_nodes = len(node_list)\n        text_splitter = None\n        if prompt is not None:\n            # add padding given the number, and the newlines\n            text_splitter = self.get_text_splitter_given_prompt(\n                prompt,\n                num_nodes,\n                padding=5,\n            )\n        results = []\n        number = 1\n        for node in node_list:\n            node_text = \" \".join(node.get_text().splitlines())\n            if text_splitter is not None:\n                node_text = text_splitter.truncate_text(node_text)\n            text = f\"({number}) {node_text}\"\n            results.append(text)\n            number += 1\n        return \"\\n\\n\".join(results)\n\n    def compact_text_chunks(self, prompt: Prompt, text_chunks: List[str]) -> List[str]:\n        \"\"\"Compact text chunks.\n\n        This will combine text chunks into consolidated chunks\n        that more fully \"pack\" the prompt template given the max_input_size.\n\n        \"\"\"\n        combined_str = \"\\n\\n\".join([c.strip() for c in text_chunks if c.strip()])\n        # resplit based on self.max_chunk_overlap\n        text_splitter = self.get_text_splitter_given_prompt(prompt, 1, padding=1)\n        return text_splitter.split_text(combined_str)\n", "doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "__type__": "Document"}, "4d36a4f799b01b46bb1527a5b95f7186cc3554b6": {"text": "\"\"\"Index registry.\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Type\n\nfrom gpt_index.data_structs.data_structs import IndexStruct\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\n\n# map from mode to query class\nQUERY_MAP_TYPE = Dict[str, Type[BaseGPTIndexQuery]]\n\n\n@dataclass\nclass IndexRegistry:\n    \"\"\"Index registry.\n\n    Stores mapping from index type to index_struct + queries.\n    NOTE: this cannot be easily serialized, so must be re-initialized\n    each time.\n    If the user defines custom IndexStruct or query classes,\n    they must be added to the registry manually.\n\n    \"\"\"\n\n    type_to_struct: Dict[str, Type[IndexStruct]] = field(default_factory=dict)\n    type_to_query: Dict[str, QUERY_MAP_TYPE] = field(default_factory=dict)\n\n    def update(self, other: \"IndexRegistry\") -> None:\n        \"\"\"Update the registry with another registry.\"\"\"\n        self.type_to_struct.update(other.type_to_struct)\n        self.type_to_query.update(other.type_to_query)\n", "doc_id": "4d36a4f799b01b46bb1527a5b95f7186cc3554b6", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/registry.py", "file_name": "registry.py"}, "__type__": "Document"}, "4622a34aedaeb346c6f275d45c96bb8ae57d202b": {"text": "\"\"\"Utilities for GPT indices.\"\"\"\nimport re\nfrom typing import Dict, List, Optional, Set\n\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.utils import globals_helper\n\n\ndef get_sorted_node_list(node_dict: Dict[int, Node]) -> List[Node]:\n    \"\"\"Get sorted node list. Used by tree-strutured indices.\"\"\"\n    sorted_indices = sorted(node_dict.keys())\n    return [node_dict[index] for index in sorted_indices]\n\n\ndef extract_numbers_given_response(response: str, n: int = 1) -> Optional[List[int]]:\n    \"\"\"Extract number given the GPT-generated response.\n\n    Used by tree-structured indices.\n\n    \"\"\"\n    numbers = re.findall(r\"\\d+\", response)\n    if len(numbers) == 0:\n        return None\n    else:\n        return numbers[:n]\n\n\ndef expand_tokens_with_subtokens(tokens: Set[str]) -> Set[str]:\n    \"\"\"Get subtokens from a list of tokens., filtering for stopwords.\"\"\"\n    results = set()\n    for token in tokens:\n        results.add(token)\n        sub_tokens = re.findall(r\"\\w+\", token)\n        if len(sub_tokens) > 1:\n            results.update({w for w in sub_tokens if w not in globals_helper.stopwords})\n\n    return results\n\n\ndef truncate_text(text: str, max_length: int) -> str:\n    \"\"\"Truncate text to a maximum length.\"\"\"\n    return text[: max_length - 3] + \"...\"\n", "doc_id": "4622a34aedaeb346c6f275d45c96bb8ae57d202b", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/utils.py", "file_name": "utils.py"}, "__type__": "Document"}, "3a35f0ed-9595-47b0-9600-e63edd1d18e3": {"text": "\n__init__.py:\nThis document provides a summary of the data structures used in GPT Index, including the keyword table, list, tree, and vector store indices.\n\nbase.py:\nThis document provides a summary of the base index classes, including parameters such as documents, LLM predictor, embed model, docstore, index registry, prompt helper, chunk size limit, and include extra info. It also includes methods such as _update_index_registry_and_docstore, _process_documents, and _validate_documents.\n\nprompt_helper.py:\nThis document provides a summary of the PromptHelper class, which provides methods for creating a text splitter given a prompt, getting the biggest prompt from a list of prompts, and getting text from nodes in a tree-structured index. It also provides methods for compacting text chunks and getting text from nodes in the format of a numbered list.\n\nregistry.py:\nThis document provides a summary of the Registry class, which provides methods for registering and retrieving indices, models, and tokenizers. It also provides methods for getting the list of registered indices, models, and tokenizers, as well as methods for getting", "doc_id": "3a35f0ed-9595-47b0-9600-e63edd1d18e3", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"GPT Index data structures.\"\"\"\n\n# indices\nfrom gpt_index.indices.keyword_table.base import GPTKeywordTableIndex\nfrom gpt_index.indices.keyword_table.rake_base import GPTRAKEKeywordTableIndex\nfrom gpt_index.indices.keyword_table.simple_base import GPTSimpleKeywordTableIndex\nfrom gpt_index.indices.list.base import GPTListIndex\nfrom gpt_index.indices.tree.base import GPTTreeIndex\nfrom gpt_index.indices.vector_store.faiss import GPTFaissIndex\n\n__all__ = [\n    \"GPTKeywordTableIndex\",\n    \"GPTSimpleKeywordTableIndex\",\n    \"GPTRAKEKeywordTableIndex\",\n    \"GPTListIndex\",\n    \"GPTTreeIndex\",\n    \"GPTFaissIndex\",\n]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "e7bbbf6f654fcec74653151ee1feeea4b388c65b", "node_info": null}, "1": {"text": "\"\"\"Base index classes.\"\"\"\nimport json\nimport logging\nfrom abc import abstractmethod\nfrom typing import (\n    Any,\n    Dict,\n    Generic,\n    List,\n    Optional,\n    Sequence,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom gpt_index.data_structs.data_structs import IndexStruct, Node\nfrom gpt_index.docstore import DOC_TYPE, DocumentStore\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.embeddings.openai import OpenAIEmbedding\nfrom gpt_index.indices.node_utils import get_nodes_from_document\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.query_runner import QueryRunner\nfrom gpt_index.indices.query.schema import QueryConfig, QueryMode\nfrom gpt_index.indices.registry import IndexRegistry\nfrom", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "2": {"text": "gpt_index.indices.registry import IndexRegistry\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.readers.schema.base import Document\nfrom gpt_index.response.schema import Response\nfrom gpt_index.schema import BaseDocument\nfrom gpt_index.token_counter.token_counter import llm_token_counter\n\nIS = TypeVar(\"IS\", bound=IndexStruct)\n\n\nDOCUMENTS_INPUT = Union[BaseDocument, \"BaseGPTIndex\"]\n\n\nclass BaseGPTIndex(Generic[IS]):\n    \"\"\"Base GPT Index.\n\n    Args:\n        documents (Optional[Sequence[BaseDocument]]): List of documents to\n            build the index from.\n        llm_predictor (LLMPredictor): Optional LLMPredictor object. If not provided,\n            will use the default LLMPredictor (text-davinci-003)\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "3": {"text": "(text-davinci-003)\n        prompt_helper (PromptHelper): Optional PromptHelper object. If not provided,\n            will use the default PromptHelper.\n        chunk_size_limit (Optional[int]): Optional chunk size limit. If not provided,\n            will use the default chunk size limit (4096 max input size).\n        include_extra_info (bool): Optional bool. If True, extra info (i.e. metadata)\n            of each Document will be prepended to its text to help with queries.\n            Default is True.\n\n    \"\"\"\n\n    index_struct_cls: Type[IS]\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[IS] = None,\n        llm_predictor:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "4": {"text": "       llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        docstore: Optional[DocumentStore] = None,\n        index_registry: Optional[IndexRegistry] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        chunk_size_limit: Optional[int] = None,\n        include_extra_info: bool = True,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        if index_struct is None and documents is None:\n            raise ValueError(\"One of documents or index_struct must be provided.\")\n        if index_struct is not None and documents is not None:\n            raise ValueError(\"Only one of documents or index_struct can be provided.\")\n\n        self._llm_predictor", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 4, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "5": {"text": "       self._llm_predictor = llm_predictor or LLMPredictor()\n        # NOTE: the embed_model isn't used in all indices\n        self._embed_model = embed_model or OpenAIEmbedding()\n        self._include_extra_info = include_extra_info\n\n        # TODO: move out of base if we need custom params per index\n        self._prompt_helper = prompt_helper or PromptHelper.from_llm_predictor(\n            self._llm_predictor, chunk_size_limit=chunk_size_limit\n        )\n\n        # build index struct in the init function\n        self._docstore = docstore or DocumentStore()\n        self._index_registry = index_registry or IndexRegistry()\n\n        if index_struct is not None:\n            if not", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 5, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "6": {"text": "None:\n            if not isinstance(index_struct, self.index_struct_cls):\n                raise ValueError(\n                    f\"index_struct must be of type {self.index_struct_cls}\"\n                )\n            self._index_struct = index_struct\n        else:\n            documents = cast(Sequence[DOCUMENTS_INPUT], documents)\n            documents = self._process_documents(\n                documents, self._docstore, self._index_registry\n            )\n            self._validate_documents(documents)\n            # TODO: introduce document store outside __init__ function\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 6, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "7": {"text": "introduce document store outside __init__ function\n            self._index_struct = self.build_index_from_documents(documents)\n        # update index registry and docstore with index_struct\n        self._update_index_registry_and_docstore()\n\n    @property\n    def prompt_helper(self) -> PromptHelper:\n        \"\"\"Get the prompt helper corresponding to the index.\"\"\"\n        return self._prompt_helper\n\n    @property\n    def docstore(self) -> DocumentStore:\n        \"\"\"Get the docstore corresponding to the index.\"\"\"\n        return self._docstore\n\n    @property\n    def index_registry(self) -> IndexRegistry:\n        \"\"\"Get the index registry corresponding to the index.\"\"\"\n        return self._index_registry\n\n    @property\n    def llm_predictor(self) -> LLMPredictor:\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 7, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "8": {"text": "-> LLMPredictor:\n        \"\"\"Get the llm predictor.\"\"\"\n        return self._llm_predictor\n\n    @property\n    def embed_model(self) -> BaseEmbedding:\n        \"\"\"Get the llm predictor.\"\"\"\n        return self._embed_model\n\n    def _update_index_registry_and_docstore(self) -> None:\n        \"\"\"Update index registry and docstore.\"\"\"\n        # update index registry with current struct\n        cur_type = self.index_struct_cls.get_type()\n        self._index_registry.type_to_struct[cur_type] = self.index_struct_cls\n        self._index_registry.type_to_query[cur_type] = self.get_query_map()\n\n        # update docstore with current struct\n        self._docstore.add_documents([self.index_struct])\n\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 8, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "9": {"text": "   def _process_documents(\n        self,\n        documents: Sequence[DOCUMENTS_INPUT],\n        docstore: DocumentStore,\n        index_registry: IndexRegistry,\n    ) -> List[BaseDocument]:\n        \"\"\"Process documents.\"\"\"\n        results: List[DOC_TYPE] = []\n        for doc in documents:\n            if isinstance(doc, BaseGPTIndex):\n                # if user passed in another index, we need to do the following:\n                # - update docstore with the docstore in the index\n                # - validate that the index is in the docstore\n                # - update the index registry\n\n               ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 9, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "10": {"text": "               index_registry.update(doc.index_registry)\n                docstore.update_docstore(doc.docstore)\n                # assert that the doc exists within the docstore\n                sub_index_struct = doc.index_struct_with_text\n                if not docstore.document_exists(sub_index_struct.get_doc_id()):\n                    raise ValueError(\n                        \"The index struct of the sub-index must exist in the docstore. \"\n                        f\"Invalid doc ID: {sub_index_struct.get_doc_id()}\"\n                    )\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 10, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "11": {"text": "           )\n                results.append(sub_index_struct)\n            elif isinstance(doc, (Document, IndexStruct)):\n                results.append(doc)\n                # update docstore\n                docstore.add_documents([doc])\n            else:\n                raise ValueError(f\"Invalid document type: {type(doc)}.\")\n        return cast(List[BaseDocument], results)\n\n    def _validate_documents(self, documents: Sequence[BaseDocument]) -> None:\n        \"\"\"Validate documents.\"\"\"\n        for doc in documents:\n            if not isinstance(doc, BaseDocument):\n               ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 11, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "12": {"text": "               raise ValueError(\"Documents must be of type BaseDocument.\")\n\n    @property\n    def index_struct(self) -> IS:\n        \"\"\"Get the index struct.\"\"\"\n        return self._index_struct\n\n    @property\n    def index_struct_with_text(self) -> IS:\n        \"\"\"Get the index struct with text.\n\n        If text not set, raise an error.\n        For use when composing indices with other indices.\n\n        \"\"\"\n        # make sure that we generate text for index struct\n        if self._index_struct.text is None:\n            # NOTE: set text to be empty string for now\n            raise ValueError(\n                \"Index must have text property set in order \"\n                \"to be composed with other", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 12, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "13": {"text": "         \"to be composed with other indices. \"\n                \"In order to set text, please run `index.set_text()`.\"\n            )\n        return self._index_struct\n\n    def set_text(self, text: str) -> None:\n        \"\"\"Set summary text for index struct.\n\n        This allows index_struct_with_text to be used to compose indices\n        with other indices.\n\n        \"\"\"\n        self._index_struct.text = text\n\n    def set_extra_info(self, extra_info: Dict[str, Any]) -> None:\n        \"\"\"Set extra info (metadata) for index struct.\n\n        If this index is used as a subindex for a parent index, the metadata\n        will be propagated to all nodes derived from this subindex, in the\n        parent index.\n\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 13, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "14": {"text": "the\n        parent index.\n\n        \"\"\"\n        self._index_struct.extra_info = extra_info\n\n    def set_doc_id(self, doc_id: str) -> None:\n        \"\"\"Set doc_id for index struct.\n\n        This is used to uniquely identify the index struct in the docstore.\n        If you wish to delete the index struct, you can use this doc_id.\n\n        \"\"\"\n        old_doc_id = self._index_struct.get_doc_id()\n        self._index_struct.doc_id = doc_id\n        # Note: we also need to delete old doc_id, and update docstore\n        self._docstore.delete_document(old_doc_id)\n        self._docstore.add_documents([self._index_struct])\n\n    def get_doc_id(self) -> str:\n        \"\"\"Get doc_id for", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 14, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "15": {"text": "str:\n        \"\"\"Get doc_id for index struct.\n\n        If doc_id not set, raise an error.\n\n        \"\"\"\n        if self._index_struct.doc_id is None:\n            raise ValueError(\"Index must have doc_id property set.\")\n        return self._index_struct.doc_id\n\n    def _get_nodes_from_document(\n        self,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n        start_idx: int = 0,\n    ) -> List[Node]:\n        return get_nodes_from_document(\n            document=document,\n            text_splitter=text_splitter,\n            start_idx=start_idx,\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 15, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "16": {"text": "           include_extra_info=self._include_extra_info,\n        )\n\n    @abstractmethod\n    def _build_index_from_documents(self, documents: Sequence[BaseDocument]) -> IS:\n        \"\"\"Build the index from documents.\"\"\"\n\n    @llm_token_counter(\"build_index_from_documents\")\n    def build_index_from_documents(self, documents: Sequence[BaseDocument]) -> IS:\n        \"\"\"Build the index from documents.\"\"\"\n        return self._build_index_from_documents(documents)\n\n    @abstractmethod\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n\n    @llm_token_counter(\"insert\")\n    def insert(self, document: DOCUMENTS_INPUT, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\n\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 16, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "17": {"text": "   \"\"\"Insert a document.\n\n        Args:\n            document (Union[BaseDocument, BaseGPTIndex]): document to insert\n\n        \"\"\"\n        processed_doc = self._process_documents(\n            [document], self._docstore, self._index_registry\n        )[0]\n        self._validate_documents([processed_doc])\n        self._insert(processed_doc, **insert_kwargs)\n\n    @abstractmethod\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n\n    def delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document from the index.\n\n        All nodes in the index related to the index will be deleted.\n\n        Args:\n        ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 17, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "18": {"text": "     Args:\n            doc_id (str): document id\n            full_delete (bool): whether to delete the document from the docstore.\n                By default this is True.\n\n        \"\"\"\n        full_delete = delete_kwargs.pop(\"full_delete\", True)\n        logging.debug(f\"> Deleting document: {doc_id}\")\n        if full_delete:\n            self._docstore.delete_document(doc_id)\n        self._delete(doc_id, **delete_kwargs)\n\n    def update(self, document: DOCUMENTS_INPUT, **update_kwargs: Any) -> None:\n        \"\"\"Update a document.\n\n        This is equivalent to deleting the document and then inserting it again.\n\n        Args:\n            document", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 18, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "19": {"text": " Args:\n            document (Union[BaseDocument, BaseGPTIndex]): document to update\n            insert_kwargs (Dict): kwargs to pass to insert\n            delete_kwargs (Dict): kwargs to pass to delete\n\n        \"\"\"\n        self.delete(document.get_doc_id(), **update_kwargs.pop(\"delete_kwargs\", {}))\n        self.insert(document, **update_kwargs.pop(\"insert_kwargs\", {}))\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Dict) -> None:\n        \"\"\"Preprocess query.\n\n        This allows subclasses to pass in additional query kwargs\n        to query, for instance arguments that are shared between the\n        index and the query class. By default, this does nothing.\n        This also allows subclasses to do validation.\n\n        \"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 19, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "20": {"text": "to do validation.\n\n        \"\"\"\n        pass\n\n    def query(\n        self,\n        query_str: str,\n        mode: str = QueryMode.DEFAULT,\n        **query_kwargs: Any,\n    ) -> Response:\n        \"\"\"Answer a query.\n\n        When `query` is called, we query the index with the given `mode` and\n        `query_kwargs`. The `mode` determines the type of query to run, and\n        `query_kwargs` are parameters that are specific to the query type.\n\n        For a comprehensive documentation of available `mode` and `query_kwargs` to\n        query a given index, please visit :ref:`Ref-Query`.\n\n\n        \"\"\"\n        mode_enum = QueryMode(mode)\n        if mode_enum ==", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 20, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "21": {"text": "       if mode_enum == QueryMode.RECURSIVE:\n            # TODO: deprecated, use ComposableGraph instead.\n            if \"query_configs\" not in query_kwargs:\n                raise ValueError(\"query_configs must be provided for recursive mode.\")\n            query_configs = query_kwargs[\"query_configs\"]\n            query_runner = QueryRunner(\n                self._llm_predictor,\n                self._prompt_helper,\n                self._embed_model,\n                self._docstore,\n                self._index_registry,\n               ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 21, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "22": {"text": "               query_configs=query_configs,\n                recursive=True,\n            )\n            return query_runner.query(query_str, self._index_struct)\n        else:\n            self._preprocess_query(mode_enum, query_kwargs)\n            # TODO: pass in query config directly\n            query_config = QueryConfig(\n                index_struct_type=self._index_struct.get_type(),\n                query_mode=mode_enum,\n                query_kwargs=query_kwargs,\n            )\n            query_runner = QueryRunner(\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 22, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "23": {"text": "  query_runner = QueryRunner(\n                self._llm_predictor,\n                self._prompt_helper,\n                self._embed_model,\n                self._docstore,\n                self._index_registry,\n                query_configs=[query_config],\n                recursive=False,\n            )\n            return query_runner.query(query_str, self._index_struct)\n\n    @classmethod\n    @abstractmethod\n    def get_query_map(cls) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n\n    @classmethod\n    def", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 23, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "24": {"text": "query map.\"\"\"\n\n    @classmethod\n    def load_from_string(cls, index_string: str, **kwargs: Any) -> \"BaseGPTIndex\":\n        \"\"\"Load index from string (in JSON-format).\n\n        This method loads the index from a JSON string. The index data\n        structure itself is preserved completely. If the index is defined over\n        subindices, those subindices will also be preserved (and subindices of\n        those subindices, etc.).\n\n        NOTE: load_from_string should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_string` and `load_from_string` on that instead.\n\n        Args:\n            index_string (str): The index string (in JSON-format).\n\n        Returns:\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 24, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "25": {"text": "  Returns:\n            BaseGPTIndex: The loaded index.\n\n        \"\"\"\n        result_dict = json.loads(index_string)\n        index_struct = cls.index_struct_cls.from_dict(result_dict[\"index_struct\"])\n        type_to_struct = {index_struct.get_type(): type(index_struct)}\n        docstore = DocumentStore.load_from_dict(\n            result_dict[\"docstore\"],\n            type_to_struct=type_to_struct,\n        )\n        return cls(index_struct=index_struct, docstore=docstore, **kwargs)\n\n    @classmethod\n    def load_from_disk(cls, save_path: str, **kwargs: Any) -> \"BaseGPTIndex\":\n        \"\"\"Load index from disk.\n\n        This method loads the index from a", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 25, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "26": {"text": "       This method loads the index from a JSON file stored on disk. The index data\n        structure itself is preserved completely. If the index is defined over\n        subindices, those subindices will also be preserved (and subindices of\n        those subindices, etc.).\n\n        NOTE: load_from_disk should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_disk` and `load_from_disk` on that instead.\n\n        Args:\n            save_path (str): The save_path of the file.\n\n        Returns:\n            BaseGPTIndex: The loaded index.\n\n        \"\"\"\n        with open(save_path, \"r\") as f:\n            file_contents = f.read()\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 26, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "27": {"text": "  file_contents = f.read()\n            return cls.load_from_string(file_contents, **kwargs)\n\n    def save_to_string(self, **save_kwargs: Any) -> str:\n        \"\"\"Save to string.\n\n        This method stores the index into a JSON string.\n\n        NOTE: save_to_string should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_string` and `load_from_string` on that instead.\n\n        Returns:\n            str: The JSON string of the index.\n\n        \"\"\"\n        if self.docstore.contains_index_struct(\n            exclude_ids=[self.index_struct.get_doc_id()]\n        ):\n            raise", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 27, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "28": {"text": "  ):\n            raise ValueError(\n                \"Cannot call `save_to_string` on index if index is composed on top of \"\n                \"other indices. Please define a `ComposableGraph` and use \"\n                \"`save_to_string` and `load_from_string` on that instead.\"\n            )\n        out_dict: Dict[str, dict] = {\n            \"index_struct\": self.index_struct.to_dict(),\n            \"docstore\": self.docstore.serialize_to_dict(),\n        }\n        return json.dumps(out_dict, **save_kwargs)\n\n    def save_to_disk(self, save_path: str, **save_kwargs: Any) -> None:\n        \"\"\"Save to", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 28, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "29": {"text": "Any) -> None:\n        \"\"\"Save to file.\n\n        This method stores the index into a JSON file stored on disk.\n\n        NOTE: save_to_disk should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_disk` and `load_from_disk` on that instead.\n\n        Args:\n            save_path (str): The save_path of the file.\n\n        \"\"\"\n        index_string = self.save_to_string(**save_kwargs)\n        with open(save_path, \"w\") as f:\n            f.write(index_string)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 29, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "30": {"text": "\"\"\"General node utils.\"\"\"\n\n\nimport logging\nfrom typing import List\n\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.indices.utils import truncate_text\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.schema import BaseDocument\n\n\ndef get_nodes_from_document(\n    document: BaseDocument,\n    text_splitter: TokenTextSplitter,\n    start_idx: int = 0,\n    include_extra_info: bool = True,\n) -> List[Node]:\n    \"\"\"Add document to index.\"\"\"\n    text_chunks_with_overlap = text_splitter.split_text_with_overlaps(\n        document.get_text(),\n        extra_info_str=document.extra_info_str if include_extra_info else None,\n    )\n    nodes = []\n    index_counter = 0\n    for i, text_split in", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/node_utils.py", "file_name": "node_utils.py"}, "index": 30, "child_indices": [], "ref_doc_id": "57e0df84f5302d7788c835809895fe5ae92d7f57", "node_info": null}, "31": {"text": "index_counter = 0\n    for i, text_split in enumerate(text_chunks_with_overlap):\n        text_chunk = text_split.text_chunk\n        logging.debug(f\"> Adding chunk: {truncate_text(text_chunk, 50)}\")\n        index_pos_info = {\n            # NOTE: start is inclusive, end is exclusive\n            \"start\": index_counter - text_split.num_char_overlap,\n            \"end\": index_counter - text_split.num_char_overlap + len(text_chunk),\n        }\n        index_counter += len(text_chunk) + 1\n        # if embedding specified in document, pass it to the Node\n        node = Node(\n            text=text_chunk,\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/node_utils.py", "file_name": "node_utils.py"}, "index": 31, "child_indices": [], "ref_doc_id": "57e0df84f5302d7788c835809895fe5ae92d7f57", "node_info": null}, "32": {"text": "           index=start_idx + i,\n            ref_doc_id=document.get_doc_id(),\n            embedding=document.embedding,\n            extra_info=document.extra_info if include_extra_info else None,\n            node_info=index_pos_info,\n        )\n        nodes.append(node)\n    return nodes\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/node_utils.py", "file_name": "node_utils.py"}, "index": 32, "child_indices": [], "ref_doc_id": "57e0df84f5302d7788c835809895fe5ae92d7f57", "node_info": null}, "33": {"text": "\"\"\"General prompt helper that can help deal with token limitations.\n\nThe helper can split text. It can also concatenate text from Node\nstructs but keeping token limitations in mind.\n\n\"\"\"\n\nfrom typing import Callable, List, Optional\n\nfrom gpt_index.constants import MAX_CHUNK_OVERLAP\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.utils import globals_helper\n\n\nclass PromptHelper:\n    \"\"\"Prompt helper.\n\n    This utility helps us fill in the prompt, split the text,\n    and fill in context information according to necessary token limitations.\n\n    Args:\n        max_input_size (int): Maximum input size for the LLM.\n        num_output (int): Number of outputs for the LLM.\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 33, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "34": {"text": "Number of outputs for the LLM.\n        max_chunk_overlap (int): Maximum chunk overlap for the LLM.\n        embedding_limit (Optional[int]): Maximum number of embeddings to use.\n        chunk_size_limit (Optional[int]): Maximum chunk size to use.\n        tokenizer (Optional[Callable[[str], List]]): Tokenizer to use.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        max_input_size: int,\n        num_output: int,\n        max_chunk_overlap: int,\n        embedding_limit: Optional[int] = None,\n        chunk_size_limit: Optional[int] = None,\n        tokenizer: Optional[Callable[[str], List]] = None,\n        separator: str = \" \",\n    ) -> None:\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 34, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "35": {"text": "= \" \",\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self.max_input_size = max_input_size\n        self.num_output = num_output\n        self.max_chunk_overlap = max_chunk_overlap\n        self.embedding_limit = embedding_limit\n        self.chunk_size_limit = chunk_size_limit\n        # TODO: make configurable\n        self._tokenizer = tokenizer or globals_helper.tokenizer\n        self._separator = separator\n        self.use_chunk_size_limit = chunk_size_limit is not None\n\n    @classmethod\n    def from_llm_predictor(\n        self,\n        llm_predictor: LLMPredictor,\n        max_chunk_overlap:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 35, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "36": {"text": "       max_chunk_overlap: Optional[int] = None,\n        embedding_limit: Optional[int] = None,\n        chunk_size_limit: Optional[int] = None,\n        tokenizer: Optional[Callable[[str], List]] = None,\n    ) -> \"PromptHelper\":\n        \"\"\"Create from llm predictor.\n\n        This will autofill values like max_input_size and num_output.\n\n        \"\"\"\n        llm_metadata = llm_predictor.get_llm_metadata()\n        max_chunk_overlap = max_chunk_overlap or min(\n            MAX_CHUNK_OVERLAP, llm_metadata.max_input_size // 10\n        )\n        return self(\n            llm_metadata.max_input_size,\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 36, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "37": {"text": "  llm_metadata.max_input_size,\n            llm_metadata.num_output,\n            max_chunk_overlap,\n            embedding_limit=embedding_limit,\n            chunk_size_limit=chunk_size_limit,\n            tokenizer=tokenizer,\n        )\n\n    def get_chunk_size_given_prompt(\n        self, prompt_text: str, num_chunks: int, padding: Optional[int] = 1\n    ) -> int:\n        \"\"\"Get chunk size making sure we can also fit the prompt in.\n\n        Chunk size is computed based on a function of the total input size,\n        the prompt length, the number of outputs, and the number of chunks.\n\n        If padding is specified, then we subtract that from the", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 37, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "38": {"text": "   If padding is specified, then we subtract that from the chunk size.\n        By default we assume there is a padding of 1 (for the newline between chunks).\n\n        Limit by embedding_limit and chunk_size_limit if specified.\n\n        \"\"\"\n        prompt_tokens = self._tokenizer(prompt_text)\n        num_prompt_tokens = len(prompt_tokens)\n\n        # NOTE: if embedding limit is specified, then chunk_size must not be larger than\n        # embedding_limit\n        result = (\n            self.max_input_size - num_prompt_tokens - self.num_output\n        ) // num_chunks\n        if padding is not None:\n            result -= padding\n\n        if self.embedding_limit is not", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 38, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "39": {"text": "       if self.embedding_limit is not None:\n            result = min(result, self.embedding_limit)\n        if self.chunk_size_limit is not None and self.use_chunk_size_limit:\n            result = min(result, self.chunk_size_limit)\n\n        return result\n\n    def _get_empty_prompt_txt(self, prompt: Prompt) -> str:\n        \"\"\"Get empty prompt text.\n\n        Substitute empty strings in parts of the prompt that have\n        not yet been filled out. Skip variables that have already\n        been partially formatted. This is used to compute the initial tokens.\n\n        \"\"\"\n        fmt_dict = {\n            v: \"\" for v in prompt.input_variables if v not in prompt.partial_dict\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 39, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "40": {"text": "if v not in prompt.partial_dict\n        }\n        empty_prompt_txt = prompt.format(**fmt_dict)\n        return empty_prompt_txt\n\n    def get_biggest_prompt(self, prompts: List[Prompt]) -> Prompt:\n        \"\"\"Get biggest prompt.\n\n        Oftentimes we need to fetch the biggest prompt, in order to\n        be the most conservative about chunking text. This\n        is a helper utility for that.\n\n        \"\"\"\n        empty_prompt_txts = [self._get_empty_prompt_txt(prompt) for prompt in prompts]\n        empty_prompt_txt_lens = [len(txt) for txt in empty_prompt_txts]\n        biggest_prompt = prompts[\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 40, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "41": {"text": "prompts[\n            empty_prompt_txt_lens.index(max(empty_prompt_txt_lens))\n        ]\n        return biggest_prompt\n\n    def get_text_splitter_given_prompt(\n        self, prompt: Prompt, num_chunks: int, padding: Optional[int] = 1\n    ) -> TokenTextSplitter:\n        \"\"\"Get text splitter given initial prompt.\n\n        Allows us to get the text splitter which will split up text according\n        to the desired chunk size.\n\n        \"\"\"\n        # generate empty_prompt_txt to compute initial tokens\n        empty_prompt_txt = self._get_empty_prompt_txt(prompt)\n        chunk_size = self.get_chunk_size_given_prompt(\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 41, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "42": {"text": "           empty_prompt_txt, num_chunks, padding=padding\n        )\n        text_splitter = TokenTextSplitter(\n            separator=self._separator,\n            chunk_size=chunk_size,\n            chunk_overlap=self.max_chunk_overlap // num_chunks,\n            tokenizer=self._tokenizer,\n        )\n        return text_splitter\n\n    def get_text_from_nodes(\n        self, node_list: List[Node], prompt: Optional[Prompt] = None\n    ) -> str:\n        \"\"\"Get text from nodes. Used by tree-structured indices.\"\"\"\n        num_nodes = len(node_list)\n        text_splitter = None\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 42, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "43": {"text": "      text_splitter = None\n        if prompt is not None:\n            # add padding given the newline character\n            text_splitter = self.get_text_splitter_given_prompt(\n                prompt,\n                num_nodes,\n                padding=1,\n            )\n        results = []\n        for node in node_list:\n            text = (\n                text_splitter.truncate_text(node.get_text())\n                if text_splitter is not None\n                else node.get_text()\n         ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 43, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "44": {"text": "node.get_text()\n            )\n            results.append(text)\n\n        return \"\\n\".join(results)\n\n    def get_numbered_text_from_nodes(\n        self, node_list: List[Node], prompt: Optional[Prompt] = None\n    ) -> str:\n        \"\"\"Get text from nodes in the format of a numbered list.\n\n        Used by tree-structured indices.\n\n        \"\"\"\n        num_nodes = len(node_list)\n        text_splitter = None\n        if prompt is not None:\n            # add padding given the number, and the newlines\n            text_splitter = self.get_text_splitter_given_prompt(\n                prompt,\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 44, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "45": {"text": "      prompt,\n                num_nodes,\n                padding=5,\n            )\n        results = []\n        number = 1\n        for node in node_list:\n            node_text = \" \".join(node.get_text().splitlines())\n            if text_splitter is not None:\n                node_text = text_splitter.truncate_text(node_text)\n            text = f\"({number}) {node_text}\"\n            results.append(text)\n            number += 1\n        return \"\\n\\n\".join(results)\n\n    def compact_text_chunks(self, prompt: Prompt,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 45, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "46": {"text": "compact_text_chunks(self, prompt: Prompt, text_chunks: List[str]) -> List[str]:\n        \"\"\"Compact text chunks.\n\n        This will combine text chunks into consolidated chunks\n        that more fully \"pack\" the prompt template given the max_input_size.\n\n        \"\"\"\n        combined_str = \"\\n\\n\".join([c.strip() for c in text_chunks if c.strip()])\n        # resplit based on self.max_chunk_overlap\n        text_splitter = self.get_text_splitter_given_prompt(prompt, 1, padding=1)\n        return text_splitter.split_text(combined_str)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 46, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "47": {"text": "\"\"\"Index registry.\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Type\n\nfrom gpt_index.data_structs.data_structs import IndexStruct\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\n\n# map from mode to query class\nQUERY_MAP_TYPE = Dict[str, Type[BaseGPTIndexQuery]]\n\n\n@dataclass\nclass IndexRegistry:\n    \"\"\"Index registry.\n\n    Stores mapping from index type to index_struct + queries.\n    NOTE: this cannot be easily serialized, so must be re-initialized\n    each time.\n    If the user defines custom IndexStruct or query classes,\n    they must be added to the registry manually.\n\n    \"\"\"\n\n    type_to_struct: Dict[str, Type[IndexStruct]] = field(default_factory=dict)\n    type_to_query: Dict[str, QUERY_MAP_TYPE] = field(default_factory=dict)\n\n    def update(self, other: \"IndexRegistry\") -> None:\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/registry.py", "file_name": "registry.py"}, "index": 47, "child_indices": [], "ref_doc_id": "4d36a4f799b01b46bb1527a5b95f7186cc3554b6", "node_info": null}, "48": {"text": "\"IndexRegistry\") -> None:\n        \"\"\"Update the registry with another registry.\"\"\"\n        self.type_to_struct.update(other.type_to_struct)\n        self.type_to_query.update(other.type_to_query)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/registry.py", "file_name": "registry.py"}, "index": 48, "child_indices": [], "ref_doc_id": "4d36a4f799b01b46bb1527a5b95f7186cc3554b6", "node_info": null}, "49": {"text": "\"\"\"Utilities for GPT indices.\"\"\"\nimport re\nfrom typing import Dict, List, Optional, Set\n\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.utils import globals_helper\n\n\ndef get_sorted_node_list(node_dict: Dict[int, Node]) -> List[Node]:\n    \"\"\"Get sorted node list. Used by tree-strutured indices.\"\"\"\n    sorted_indices = sorted(node_dict.keys())\n    return [node_dict[index] for index in sorted_indices]\n\n\ndef extract_numbers_given_response(response: str, n: int = 1) -> Optional[List[int]]:\n    \"\"\"Extract number given the GPT-generated response.\n\n    Used by tree-structured indices.\n\n    \"\"\"\n    numbers = re.findall(r\"\\d+\", response)\n    if len(numbers) == 0:\n        return None\n    else:\n        return numbers[:n]\n\n\ndef", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/utils.py", "file_name": "utils.py"}, "index": 49, "child_indices": [], "ref_doc_id": "4622a34aedaeb346c6f275d45c96bb8ae57d202b", "node_info": null}, "50": {"text": "      return numbers[:n]\n\n\ndef expand_tokens_with_subtokens(tokens: Set[str]) -> Set[str]:\n    \"\"\"Get subtokens from a list of tokens., filtering for stopwords.\"\"\"\n    results = set()\n    for token in tokens:\n        results.add(token)\n        sub_tokens = re.findall(r\"\\w+\", token)\n        if len(sub_tokens) > 1:\n            results.update({w for w in sub_tokens if w not in globals_helper.stopwords})\n\n    return results\n\n\ndef truncate_text(text: str, max_length: int) -> str:\n    \"\"\"Truncate text to a maximum length.\"\"\"\n    return text[: max_length - 3] + \"...\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/utils.py", "file_name": "utils.py"}, "index": 50, "child_indices": [], "ref_doc_id": "4622a34aedaeb346c6f275d45c96bb8ae57d202b", "node_info": null}, "51": {"text": "This code file is part of the GPT Index data structures. It contains the base class for GPT Index, which is used to build an index from a list of documents. It includes parameters such as documents, llm_predictor, embed_model, docstore, index_registry, prompt_helper, chunk_size_limit, and include_extra_info. It also contains methods to process documents, validate documents, and update the index registry and docstore.", "doc_id": null, "embedding": null, "extra_info": null, "index": 51, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "52": {"text": "This code file is the base.py file from the gpt_index/indices directory. It contains the base class for all GPT indices, and provides methods for building, inserting, deleting, and updating documents in the index. It also provides methods for querying the index, including a recursive query mode and a query map for each index type. The code also includes methods for setting the text, extra info, and doc_id for the index struct, as well as methods for validating documents and preprocessing queries.", "doc_id": null, "embedding": null, "extra_info": null, "index": 52, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "53": {"text": "\nThis code file contains two classes, BaseGPTIndex and PromptHelper. BaseGPTIndex provides methods to load and save an index from/to a JSON string or file, while PromptHelper provides methods to fill in a prompt, split text, and fill in context information according to necessary token limitations. Both classes have various parameters and methods to help with their respective tasks.", "doc_id": null, "embedding": null, "extra_info": null, "index": 53, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}, "54": {"text": "The prompt_helper.py file is a Python script that provides a class for creating a PromptHelper object from a llm predictor. It also provides methods for getting the chunk size given a prompt, getting the biggest prompt, getting the text splitter given a prompt, getting text from nodes, getting numbered text from nodes, and compacting text chunks. The registry.py file is a Python script that provides a class for creating an IndexRegistry object. It also provides a method for updating the registry with another registry.", "doc_id": null, "embedding": null, "extra_info": null, "index": 54, "child_indices": [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], "ref_doc_id": null, "node_info": null}, "55": {"text": "This code file contains utilities for GPT indices. It includes functions to get a sorted node list, extract numbers from a GPT-generated response, expand tokens with subtokens, and truncate text to a maximum length. It also includes a function to update the registry with another registry. All of these functions are used to help with the GPT indexing process.", "doc_id": null, "embedding": null, "extra_info": null, "index": 55, "child_indices": [48, 49, 50], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"51": {"text": "This code file is part of the GPT Index data structures. It contains the base class for GPT Index, which is used to build an index from a list of documents. It includes parameters such as documents, llm_predictor, embed_model, docstore, index_registry, prompt_helper, chunk_size_limit, and include_extra_info. It also contains methods to process documents, validate documents, and update the index registry and docstore.", "doc_id": null, "embedding": null, "extra_info": null, "index": 51, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "52": {"text": "This code file is the base.py file from the gpt_index/indices directory. It contains the base class for all GPT indices, and provides methods for building, inserting, deleting, and updating documents in the index. It also provides methods for querying the index, including a recursive query mode and a query map for each index type. The code also includes methods for setting the text, extra info, and doc_id for the index struct, as well as methods for validating documents and preprocessing queries.", "doc_id": null, "embedding": null, "extra_info": null, "index": 52, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "53": {"text": "\nThis code file contains two classes, BaseGPTIndex and PromptHelper. BaseGPTIndex provides methods to load and save an index from/to a JSON string or file, while PromptHelper provides methods to fill in a prompt, split text, and fill in context information according to necessary token limitations. Both classes have various parameters and methods to help with their respective tasks.", "doc_id": null, "embedding": null, "extra_info": null, "index": 53, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}, "54": {"text": "The prompt_helper.py file is a Python script that provides a class for creating a PromptHelper object from a llm predictor. It also provides methods for getting the chunk size given a prompt, getting the biggest prompt, getting the text splitter given a prompt, getting text from nodes, getting numbered text from nodes, and compacting text chunks. The registry.py file is a Python script that provides a class for creating an IndexRegistry object. It also provides a method for updating the registry with another registry.", "doc_id": null, "embedding": null, "extra_info": null, "index": 54, "child_indices": [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], "ref_doc_id": null, "node_info": null}, "55": {"text": "This code file contains utilities for GPT indices. It includes functions to get a sorted node list, extract numbers from a GPT-generated response, expand tokens with subtokens, and truncate text to a maximum length. It also includes a function to update the registry with another registry. All of these functions are used to help with the GPT indexing process.", "doc_id": null, "embedding": null, "extra_info": null, "index": 55, "child_indices": [48, 49, 50], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}