{"index_struct": {"text": "\nThe documents provide implementations of a TextSplitter class and a SQLDatabase class to help with the functionality of the langchain library. The TextSplitter class provides a TokenTextSplitter implementation which splits text based on word tokens, and the SQLDatabase class provides a wrapper around the langchain SQLDatabase object with helper utilities for insertion and querying. LLMMetadata and LLMPredictor are classes used to wrap around an LLM chain from Langchain, and GPTIndexMemory is a class used to wrap around a GPT Index instance and make queries. MemoryWrapper and SQLDatabase are classes used to help with the functionality of the langchain library.", "doc_id": "54a9255d-77cd-4c7d-bd72-02cbdffa7176", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Init file for langchain helpers.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "ced105858f1e3558ef6ed5556d0e548659b84440", "node_info": null}, "1": {"text": "\"\"\"Wrapper functions around an LLM chain.\"\"\"\n\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Any, Optional, Tuple\n\nimport openai\nfrom langchain import Cohere, LLMChain, OpenAI\nfrom langchain.llms import AI21\nfrom langchain.llms.base import BaseLLM\n\nfrom gpt_index.constants import MAX_CHUNK_SIZE, NUM_OUTPUTS\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.utils import (\n    ErrorToRetry,\n    globals_helper,\n    retry_on_exceptions_with_backoff,\n)\n\n\n@dataclass\nclass LLMMetadata:\n    \"\"\"LLM metadata.\n\n    We extract this metadata to help with our prompts.\n\n    \"\"\"\n\n    max_input_size: int = MAX_CHUNK_SIZE\n    num_output: int = NUM_OUTPUTS\n\n\ndef _get_llm_metadata(llm: BaseLLM) -> LLMMetadata:\n    \"\"\"Get LLM metadata from", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 1, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "2": {"text": "-> LLMMetadata:\n    \"\"\"Get LLM metadata from llm.\"\"\"\n    if not isinstance(llm, BaseLLM):\n        raise ValueError(\"llm must be an instance of langchain.llms.base.LLM\")\n    if isinstance(llm, OpenAI):\n        return LLMMetadata(\n            max_input_size=llm.modelname_to_contextsize(llm.model_name),\n            num_output=llm.max_tokens,\n        )\n    elif isinstance(llm, Cohere):\n        # TODO: figure out max input size for cohere\n        return LLMMetadata(num_output=llm.max_tokens)\n    elif isinstance(llm, AI21):\n        # TODO: figure out max input size for AI21\n        return", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 2, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "3": {"text": "out max input size for AI21\n        return LLMMetadata(num_output=llm.maxTokens)\n    else:\n        return LLMMetadata()\n\n\nclass LLMPredictor:\n    \"\"\"LLM predictor class.\n\n    Wrapper around an LLMChain from Langchain.\n\n    Args:\n        llm (Optional[langchain.llms.base.LLM]): LLM from Langchain to use\n            for predictions. Defaults to OpenAI's text-davinci-003 model.\n            Please see `Langchain's LLM Page\n            <https://langchain.readthedocs.io/en/latest/modules/llms.html>`_\n            for more details.\n\n        retry_on_throttling (bool): Whether to retry on rate limit errors.\n            Defaults to true.\n\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 3, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "4": {"text": "        Defaults to true.\n\n    \"\"\"\n\n    def __init__(\n        self, llm: Optional[BaseLLM] = None, retry_on_throttling: bool = True\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._llm = llm or OpenAI(temperature=0, model_name=\"text-davinci-003\")\n        self.retry_on_throttling = retry_on_throttling\n        self._total_tokens_used = 0\n        self.flag = True\n        self._last_token_usage: Optional[int] = None\n\n    def get_llm_metadata(self) -> LLMMetadata:\n        \"\"\"Get LLM metadata.\"\"\"\n        # TODO: refactor mocks in unit tests, this is a stopgap solution\n        if hasattr(self,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 4, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "5": {"text": "solution\n        if hasattr(self, \"_llm\"):\n            return _get_llm_metadata(self._llm)\n        else:\n            return LLMMetadata()\n\n    def _predict(self, prompt: Prompt, **prompt_args: Any) -> str:\n        \"\"\"Inner predict function.\n\n        If retry_on_throttling is true, we will retry on rate limit errors.\n\n        \"\"\"\n        llm_chain = LLMChain(prompt=prompt.get_langchain_prompt(), llm=self._llm)\n\n        # Note: we don't pass formatted_prompt to llm_chain.predict because\n        # langchain does the same formatting under the hood\n        full_prompt_args = prompt.get_full_format_args(prompt_args)\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 5, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "6": {"text": "       if self.retry_on_throttling:\n            llm_prediction = retry_on_exceptions_with_backoff(\n                lambda: llm_chain.predict(**full_prompt_args),\n                [\n                    ErrorToRetry(openai.error.RateLimitError),\n                    ErrorToRetry(openai.error.ServiceUnavailableError),\n                    ErrorToRetry(openai.error.TryAgain),\n                    ErrorToRetry(\n                        openai.error.APIConnectionError, lambda e: e.should_retry\n        ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 6, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "7": {"text": "e.should_retry\n                    ),\n                ],\n            )\n        else:\n            llm_prediction = llm_chain.predict(**full_prompt_args)\n        return llm_prediction\n\n    def predict(self, prompt: Prompt, **prompt_args: Any) -> Tuple[str, str]:\n        \"\"\"Predict the answer to a query.\n\n        Args:\n            prompt (Prompt): Prompt to use for prediction.\n\n        Returns:\n            Tuple[str, str]: Tuple of the predicted answer and the formatted prompt.\n\n        \"\"\"\n        formatted_prompt = prompt.format(**prompt_args)\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 7, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "8": {"text": "prompt.format(**prompt_args)\n        llm_prediction = self._predict(prompt, **prompt_args)\n        logging.debug(llm_prediction)\n\n        # We assume that the value of formatted_prompt is exactly the thing\n        # eventually sent to OpenAI, or whatever LLM downstream\n        prompt_tokens_count = self._count_tokens(formatted_prompt)\n        prediction_tokens_count = self._count_tokens(llm_prediction)\n        self._total_tokens_used += prompt_tokens_count + prediction_tokens_count\n        return llm_prediction, formatted_prompt\n\n    @property\n    def total_tokens_used(self) -> int:\n        \"\"\"Get the total tokens used so far.\"\"\"\n        return", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 8, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "9": {"text": "used so far.\"\"\"\n        return self._total_tokens_used\n\n    def _count_tokens(self, text: str) -> int:\n        tokens = globals_helper.tokenizer(text)\n        return len(tokens)\n\n    @property\n    def last_token_usage(self) -> int:\n        \"\"\"Get the last token usage.\"\"\"\n        if self._last_token_usage is None:\n            return 0\n        return self._last_token_usage\n\n    @last_token_usage.setter\n    def last_token_usage(self, value: int) -> None:\n        \"\"\"Set the last token usage.\"\"\"\n        self._last_token_usage = value\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 9, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "10": {"text": "\"\"\"Langchain memory wrapper (for GPT Index).\"\"\"\n\nfrom typing import Any, Dict, List, Optional\n\nfrom langchain.chains.base import Memory\nfrom pydantic import Field\n\nfrom gpt_index.indices.base import BaseGPTIndex\nfrom gpt_index.readers.schema.base import Document\n\n\ndef get_prompt_input_key(inputs: Dict[str, Any], memory_variables: List[str]) -> str:\n    \"\"\"Get prompt input key.\n\n    Copied over from langchain.\n\n    \"\"\"\n    # \"stop\" is a special key that can be passed as input but is not used to\n    # format the prompt.\n    prompt_input_keys = list(set(inputs).difference(memory_variables + [\"stop\"]))\n    if len(prompt_input_keys) != 1:\n        raise ValueError(f\"One input key expected got {prompt_input_keys}\")\n    return prompt_input_keys[0]\n\n\nclass GPTIndexMemory(Memory):\n    \"\"\"Langchain memory", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/memory_wrapper.py", "file_name": "memory_wrapper.py"}, "index": 10, "child_indices": [], "ref_doc_id": "6af9d1a0133bbdf496857f225254cdf042edd0b3", "node_info": null}, "11": {"text": "GPTIndexMemory(Memory):\n    \"\"\"Langchain memory wrapper (for GPT Index).\n\n    Args:\n        human_prefix (str): Prefix for human input. Defaults to \"Human\".\n        ai_prefix (str): Prefix for AI output. Defaults to \"AI\".\n        memory_key (str): Key for memory. Defaults to \"history\".\n        index (BaseGPTIndex): GPT Index instance.\n        query_kwargs (Dict[str, Any]): Keyword arguments for GPT Index query.\n        input_key (Optional[str]): Input key. Defaults to None.\n        output_key (Optional[str]): Output key. Defaults to None.\n\n    \"\"\"\n\n    human_prefix: str = \"Human\"\n    ai_prefix: str = \"AI\"\n    memory_key: str = \"history\"\n    index: BaseGPTIndex\n    query_kwargs: Dict = Field(default_factory=dict)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/memory_wrapper.py", "file_name": "memory_wrapper.py"}, "index": 11, "child_indices": [], "ref_doc_id": "6af9d1a0133bbdf496857f225254cdf042edd0b3", "node_info": null}, "12": {"text": "Dict = Field(default_factory=dict)\n    output_key: Optional[str] = None\n    input_key: Optional[str] = None\n\n    @property\n    def memory_variables(self) -> List[str]:\n        \"\"\"Return memory variables.\"\"\"\n        return [self.memory_key]\n\n    def _get_prompt_input_key(self, inputs: Dict[str, Any]) -> str:\n        if self.input_key is None:\n            prompt_input_key = get_prompt_input_key(inputs, self.memory_variables)\n        else:\n            prompt_input_key = self.input_key\n        return prompt_input_key\n\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n        \"\"\"Return key-value pairs given the text input to the", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/memory_wrapper.py", "file_name": "memory_wrapper.py"}, "index": 12, "child_indices": [], "ref_doc_id": "6af9d1a0133bbdf496857f225254cdf042edd0b3", "node_info": null}, "13": {"text": "   \"\"\"Return key-value pairs given the text input to the chain.\"\"\"\n        prompt_input_key = self._get_prompt_input_key(inputs)\n        query_str = inputs[prompt_input_key]\n\n        # TODO: wrap in prompt\n        # TODO: add option to return the raw text\n        # NOTE: currently it's a hack\n        response = self.index.query(query_str, **self.query_kwargs)\n        return {self.memory_key: str(response)}\n\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\n        \"\"\"Save the context of this model run to memory.\"\"\"\n        prompt_input_key = self._get_prompt_input_key(inputs)\n        if self.output_key is None:\n          ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/memory_wrapper.py", "file_name": "memory_wrapper.py"}, "index": 13, "child_indices": [], "ref_doc_id": "6af9d1a0133bbdf496857f225254cdf042edd0b3", "node_info": null}, "14": {"text": "is None:\n            if len(outputs) != 1:\n                raise ValueError(f\"One output key expected, got {outputs.keys()}\")\n            output_key = list(outputs.keys())[0]\n        else:\n            output_key = self.output_key\n        human = f\"{self.human_prefix}: \" + inputs[prompt_input_key]\n        ai = f\"{self.ai_prefix}: \" + outputs[output_key]\n        doc_text = \"\\n\".join([human, ai])\n        doc = Document(text=doc_text)\n        self.index.insert(doc)\n\n    def clear(self) -> None:\n        \"\"\"Clear memory contents.\"\"\"\n        pass\n\n    def __repr__(self) ->", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/memory_wrapper.py", "file_name": "memory_wrapper.py"}, "index": 14, "child_indices": [], "ref_doc_id": "6af9d1a0133bbdf496857f225254cdf042edd0b3", "node_info": null}, "15": {"text": "  pass\n\n    def __repr__(self) -> str:\n        \"\"\"Return representation.\"\"\"\n        return \"GPTIndexMemory()\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/memory_wrapper.py", "file_name": "memory_wrapper.py"}, "index": 15, "child_indices": [], "ref_doc_id": "6af9d1a0133bbdf496857f225254cdf042edd0b3", "node_info": null}, "16": {"text": "\"\"\"SQL wrapper around SQLDatabase in langchain.\"\"\"\nfrom typing import Any, Dict, List, Tuple\n\nfrom langchain.sql_database import SQLDatabase as LangchainSQLDatabase\nfrom sqlalchemy import MetaData, create_engine, insert\nfrom sqlalchemy.engine import Engine\n\n\nclass SQLDatabase(LangchainSQLDatabase):\n    \"\"\"SQL Database.\n\n    Wrapper around SQLDatabase object from langchain. Offers\n    some helper utilities for insertion and querying.\n    See `langchain documentation <https://tinyurl.com/4we5ku8j>`_ for more details:\n\n    Args:\n        *args: Arguments to pass to langchain SQLDatabase.\n        **kwargs: Keyword arguments to pass to langchain SQLDatabase.\n\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.metadata_obj =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/sql_wrapper.py", "file_name": "sql_wrapper.py"}, "index": 16, "child_indices": [], "ref_doc_id": "e541c393c777483dca55d898b374d77f2dd7e023", "node_info": null}, "17": {"text": "       self.metadata_obj = MetaData(bind=self._engine)\n        self.metadata_obj.reflect()\n\n    @property\n    def engine(self) -> Engine:\n        \"\"\"Return SQL Alchemy engine.\"\"\"\n        return self._engine\n\n    @classmethod\n    def from_uri(cls, database_uri: str, **kwargs: Any) -> \"SQLDatabase\":\n        \"\"\"Construct a SQLAlchemy engine from URI.\"\"\"\n        return cls(create_engine(database_uri), **kwargs)\n\n    def get_table_columns(self, table_name: str) -> List[dict]:\n        \"\"\"Get table columns.\"\"\"\n        return self._inspector.get_columns(table_name)\n\n    def get_single_table_info(self, table_name: str) -> str:\n        \"\"\"Get table info for a single table.\"\"\"\n        # same logic as", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/sql_wrapper.py", "file_name": "sql_wrapper.py"}, "index": 17, "child_indices": [], "ref_doc_id": "e541c393c777483dca55d898b374d77f2dd7e023", "node_info": null}, "18": {"text": "table.\"\"\"\n        # same logic as table_info, but with specific table names\n        template = \"Table '{table_name}' has columns: {columns}.\"\n        columns = []\n        for column in self._inspector.get_columns(table_name):\n            columns.append(f\"{column['name']} ({str(column['type'])})\")\n        column_str = \", \".join(columns)\n        table_str = template.format(table_name=table_name, columns=column_str)\n        return table_str\n\n    def insert_into_table(self, table_name: str, data: dict) -> None:\n        \"\"\"Insert data into a table.\"\"\"\n        table = self.metadata_obj.tables[table_name]\n        stmt = insert(table).values(**data)\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/sql_wrapper.py", "file_name": "sql_wrapper.py"}, "index": 18, "child_indices": [], "ref_doc_id": "e541c393c777483dca55d898b374d77f2dd7e023", "node_info": null}, "19": {"text": "       self._engine.execute(stmt)\n\n    def run_sql(self, command: str) -> Tuple[str, Dict]:\n        \"\"\"Execute a SQL statement and return a string representing the results.\n\n        If the statement returns rows, a string of the results is returned.\n        If the statement returns no rows, an empty string is returned.\n        \"\"\"\n        with self._engine.connect() as connection:\n            cursor = connection.exec_driver_sql(command)\n            if cursor.returns_rows:\n                result = cursor.fetchall()\n                return str(result), {\"result\": result}\n        return \"\", {}\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/sql_wrapper.py", "file_name": "sql_wrapper.py"}, "index": 19, "child_indices": [], "ref_doc_id": "e541c393c777483dca55d898b374d77f2dd7e023", "node_info": null}, "20": {"text": "\"\"\"Text splitter implementations.\"\"\"\nfrom dataclasses import dataclass\nfrom typing import Callable, List, Optional\n\nfrom langchain.text_splitter import TextSplitter\n\nfrom gpt_index.utils import globals_helper\n\n\n@dataclass\nclass TextSplit:\n    \"\"\"Text split with overlap.\n\n    Attributes:\n        text_chunk: The text string.\n        num_char_overlap: The number of overlapping characters with the previous chunk.\n    \"\"\"\n\n    text_chunk: str\n    num_char_overlap: int\n\n\nclass TokenTextSplitter(TextSplitter):\n    \"\"\"Implementation of splitting text that looks at word tokens.\"\"\"\n\n    def __init__(\n        self,\n        separator: str = \" \",\n        chunk_size: int = 4000,\n        chunk_overlap: int = 200,\n        tokenizer: Optional[Callable] =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 20, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "21": {"text": "       tokenizer: Optional[Callable] = None,\n        backup_separators: Optional[List[str]] = [\"\\n\"],\n    ):\n        \"\"\"Initialize with parameters.\"\"\"\n        if chunk_overlap > chunk_size:\n            raise ValueError(\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\n                f\"({chunk_size}), should be smaller.\"\n            )\n        self._separator = separator\n        self._chunk_size = chunk_size\n        self._chunk_overlap = chunk_overlap\n        self.tokenizer = tokenizer or globals_helper.tokenizer\n        self._backup_separators =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 21, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "22": {"text": "       self._backup_separators = backup_separators\n\n    def _reduce_chunk_size(\n        self, start_idx: int, cur_idx: int, splits: List[str]\n    ) -> int:\n        \"\"\"Reduce the chunk size by reducing cur_idx.\n\n        Return the new cur_idx.\n\n        \"\"\"\n        current_doc_total = len(\n            self.tokenizer(self._separator.join(splits[start_idx:cur_idx]))\n        )\n        while current_doc_total > self._chunk_size:\n            percent_to_reduce = (\n                current_doc_total - self._chunk_size\n            ) / current_doc_total\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 22, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "23": {"text": "  ) / current_doc_total\n            num_to_reduce = int(percent_to_reduce * (cur_idx - start_idx)) + 1\n            cur_idx -= num_to_reduce\n            current_doc_total = len(\n                self.tokenizer(self._separator.join(splits[start_idx:cur_idx]))\n            )\n        return cur_idx\n\n    def _process_splits(self, splits: List[str], chunk_size: int) -> List[str]:\n        \"\"\"Process splits.\n\n        Specifically search for tokens that are too large for chunk size,\n        and see if we can separate those tokens more\n        (via backup separators if specified, or force chunking).\n\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 23, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "24": {"text": "if specified, or force chunking).\n\n        \"\"\"\n        new_splits = []\n        for split in splits:\n            num_cur_tokens = len(self.tokenizer(split))\n            if num_cur_tokens <= chunk_size:\n                new_splits.append(split)\n            else:\n                cur_splits = []\n                if self._backup_separators:\n                    for sep in self._backup_separators:\n                        if sep in split:\n                           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 24, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "25": {"text": "                cur_splits = split.split(sep)\n                            break\n                else:\n                    cur_splits = [split]\n\n                cur_splits2 = []\n                for cur_split in cur_splits:\n                    num_cur_tokens = len(self.tokenizer(cur_split))\n                    if num_cur_tokens <= chunk_size:\n                        cur_splits2.extend([cur_split])\n                ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 25, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "26": {"text": "                   else:\n                        cur_split_chunks = [\n                            cur_split[i : i + chunk_size]\n                            for i in range(0, len(cur_split), chunk_size)\n                        ]\n                        cur_splits2.extend(cur_split_chunks)\n\n                new_splits.extend(cur_splits2)\n        return new_splits\n\n    def split_text(self, text: str, extra_info_str: Optional[str] = None) ->", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 26, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "27": {"text": "str, extra_info_str: Optional[str] = None) -> List[str]:\n        \"\"\"Split incoming text and return chunks.\"\"\"\n        text_slits = self.split_text_with_overlaps(text, extra_info_str=extra_info_str)\n        return [text_split.text_chunk for text_split in text_slits]\n\n    def split_text_with_overlaps(\n        self, text: str, extra_info_str: Optional[str] = None\n    ) -> List[TextSplit]:\n        \"\"\"Split incoming text and return chunks with overlap size.\"\"\"\n        if text == \"\":\n            return []\n\n        # NOTE: Consider extra info str that will be added to the chunk at query time\n        #       This reduces the effective chunk size that we can have\n        if extra_info_str is not None:\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 27, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "28": {"text": "     if extra_info_str is not None:\n            # NOTE: extra 2 newline chars for formatting when prepending in query\n            num_extra_tokens = len(self.tokenizer(f\"{extra_info_str}\\n\\n\")) + 1\n            effective_chunk_size = self._chunk_size - num_extra_tokens\n\n            if effective_chunk_size <= 0:\n                raise ValueError(\n                    \"Effective chunk size is non positive after considering extra_info\"\n                )\n        else:\n            effective_chunk_size = self._chunk_size\n\n        # First we naively split the large input into a bunch of smaller ones.\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 28, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "29": {"text": "large input into a bunch of smaller ones.\n        splits = text.split(self._separator)\n        splits = self._process_splits(splits, effective_chunk_size)\n        # We now want to combine these smaller pieces into medium size\n        # chunks to send to the LLM.\n        docs = []\n\n        start_idx = 0\n        cur_idx = 0\n        cur_total = 0\n        prev_idx = 0  # store the previous end index\n        while cur_idx < len(splits):\n            cur_token = splits[cur_idx]\n            num_cur_tokens = max(len(self.tokenizer(cur_token)), 1)\n            if num_cur_tokens > effective_chunk_size:\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 29, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "30": {"text": "num_cur_tokens > effective_chunk_size:\n                raise ValueError(\n                    \"A single term is larger than the allowed chunk size.\\n\"\n                    f\"Term size: {num_cur_tokens}\\n\"\n                    f\"Chunk size: {self._chunk_size}\"\n                    f\"Effective chunk size: {effective_chunk_size}\"\n                )\n            # If adding token to current_doc would exceed the chunk size:\n            # 1. First verify with tokenizer that current_doc\n            # 1. Update the docs list\n            if", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 30, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "31": {"text": "the docs list\n            if cur_total + num_cur_tokens > effective_chunk_size:\n                # NOTE: since we use a proxy for counting tokens, we want to\n                # run tokenizer across all of current_doc first. If\n                # the chunk is too big, then we will reduce text in pieces\n                cur_idx = self._reduce_chunk_size(start_idx, cur_idx, splits)\n                overlap = 0\n                # after first round, check if last chunk ended after this chunk begins\n                if prev_idx > 0 and prev_idx > start_idx:\n                 ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 31, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "32": {"text": "                  overlap = sum([len(splits[i]) for i in range(start_idx, prev_idx)])\n                docs.append(\n                    TextSplit(self._separator.join(splits[start_idx:cur_idx]), overlap)\n                )\n                prev_idx = cur_idx\n                # 2. Shrink the current_doc (from the front) until it is gets smaller\n                # than the overlap size\n                # NOTE: because counting tokens individually is an imperfect\n                # proxy (but much faster proxy) for the total number of tokens", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 32, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "33": {"text": "proxy (but much faster proxy) for the total number of tokens consumed,\n                # we need to enforce that start_idx <= cur_idx, otherwise\n                # start_idx has a chance of going out of bounds.\n                while cur_total > self._chunk_overlap and start_idx < cur_idx:\n                    cur_num_tokens = max(len(self.tokenizer(splits[start_idx])), 1)\n                    cur_total -= cur_num_tokens\n                    start_idx += 1\n            # Build up the current_doc with term d, and update the total counter with\n            # the number of the number of", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 33, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "34": {"text": "         # the number of the number of tokens in d, wrt self.tokenizer\n\n            # we reassign cur_token and num_cur_tokens, because cur_idx\n            # may have changed\n            cur_token = splits[cur_idx]\n            num_cur_tokens = max(len(self.tokenizer(cur_token)), 1)\n\n            cur_total += num_cur_tokens\n            cur_idx += 1\n        overlap = 0\n        # after first round, check if last chunk ended after this chunk begins\n        if prev_idx > start_idx:\n            overlap = sum([len(splits[i]) for i in range(start_idx, prev_idx)]) + len(\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 34, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "35": {"text": "prev_idx)]) + len(\n                range(start_idx, prev_idx)\n            )\n        docs.append(TextSplit(self._separator.join(splits[start_idx:cur_idx]), overlap))\n        return docs\n\n    def truncate_text(self, text: str) -> str:\n        \"\"\"Truncate text in order to fit the underlying chunk size.\"\"\"\n        if text == \"\":\n            return \"\"\n        # First we naively split the large input into a bunch of smaller ones.\n        splits = text.split(self._separator)\n        splits = self._process_splits(splits, self._chunk_size)\n\n        start_idx = 0\n        cur_idx = 0\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 35, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "36": {"text": "  cur_idx = 0\n        cur_total = 0\n        while cur_idx < len(splits):\n            cur_token = splits[cur_idx]\n            num_cur_tokens = max(len(self.tokenizer(cur_token)), 1)\n            if cur_total + num_cur_tokens > self._chunk_size:\n                cur_idx = self._reduce_chunk_size(start_idx, cur_idx, splits)\n                break\n            cur_total += num_cur_tokens\n            cur_idx += 1\n        return self._separator.join(splits[start_idx:cur_idx])\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 36, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "37": {"text": "This code file contains two classes, LLMMetadata and LLMPredictor, which are used to wrap around an LLM chain from Langchain. LLMMetadata is used to extract metadata from the LLM, while LLMPredictor is used to make predictions using the LLM. It also contains a GPTIndexMemory class, which is used to wrap around a GPT Index instance and make queries.", "doc_id": null, "embedding": null, "extra_info": null, "index": 37, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "38": {"text": "\nThis code file contains two classes, MemoryWrapper and SQLDatabase, which are used to help with the functionality of the langchain library. MemoryWrapper provides methods for loading memory variables, saving context, and clearing memory contents. SQLDatabase provides methods for creating a SQLAlchemy engine from a URI, getting table columns, getting single table info, inserting into a table, running SQL statements, and processing splits. Additionally, TextSplit is a dataclass used to store text chunks and the number of overlapping characters with the previous chunk.", "doc_id": null, "embedding": null, "extra_info": null, "index": 38, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "39": {"text": "text_splitter.py is a Python file that contains functions to split incoming text into chunks of a specified size. It uses a tokenizer to count the number of tokens in each chunk and splits the text into smaller chunks if the number of tokens exceeds the specified chunk size. It also takes into account an extra info string that will be added to the chunk at query time, reducing the effective chunk size. The functions in this file also allow for overlapping chunks and truncating text to fit the underlying chunk size.", "doc_id": null, "embedding": null, "extra_info": null, "index": 39, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}, "40": {"text": "This code file, text_splitter.py, is part of the GPT-Index language chain helpers. It is used to split text into chunks of a specified size. It does this by looping through the text, tokenizing each word, and adding the number of tokens to a total. If the total exceeds the specified chunk size, the chunk size is reduced and the loop is broken. Finally, the text is joined back together and returned.", "doc_id": null, "embedding": null, "extra_info": null, "index": 40, "child_indices": [36], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"37": {"text": "This code file contains two classes, LLMMetadata and LLMPredictor, which are used to wrap around an LLM chain from Langchain. LLMMetadata is used to extract metadata from the LLM, while LLMPredictor is used to make predictions using the LLM. It also contains a GPTIndexMemory class, which is used to wrap around a GPT Index instance and make queries.", "doc_id": null, "embedding": null, "extra_info": null, "index": 37, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "38": {"text": "\nThis code file contains two classes, MemoryWrapper and SQLDatabase, which are used to help with the functionality of the langchain library. MemoryWrapper provides methods for loading memory variables, saving context, and clearing memory contents. SQLDatabase provides methods for creating a SQLAlchemy engine from a URI, getting table columns, getting single table info, inserting into a table, running SQL statements, and processing splits. Additionally, TextSplit is a dataclass used to store text chunks and the number of overlapping characters with the previous chunk.", "doc_id": null, "embedding": null, "extra_info": null, "index": 38, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "39": {"text": "text_splitter.py is a Python file that contains functions to split incoming text into chunks of a specified size. It uses a tokenizer to count the number of tokens in each chunk and splits the text into smaller chunks if the number of tokens exceeds the specified chunk size. It also takes into account an extra info string that will be added to the chunk at query time, reducing the effective chunk size. The functions in this file also allow for overlapping chunks and truncating text to fit the underlying chunk size.", "doc_id": null, "embedding": null, "extra_info": null, "index": 39, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}, "40": {"text": "This code file, text_splitter.py, is part of the GPT-Index language chain helpers. It is used to split text into chunks of a specified size. It does this by looping through the text, tokenizing each word, and adding the number of tokens to a total. If the total exceeds the specified chunk size, the chunk size is reduced and the loop is broken. Finally, the text is joined back together and returned.", "doc_id": null, "embedding": null, "extra_info": null, "index": 40, "child_indices": [36], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"ced105858f1e3558ef6ed5556d0e548659b84440": {"text": "\"\"\"Init file for langchain helpers.\"\"\"\n", "doc_id": "ced105858f1e3558ef6ed5556d0e548659b84440", "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "10540e3570e6348e14f5264d0904fd9dc4488e93": {"text": "\"\"\"Wrapper functions around an LLM chain.\"\"\"\n\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Any, Optional, Tuple\n\nimport openai\nfrom langchain import Cohere, LLMChain, OpenAI\nfrom langchain.llms import AI21\nfrom langchain.llms.base import BaseLLM\n\nfrom gpt_index.constants import MAX_CHUNK_SIZE, NUM_OUTPUTS\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.utils import (\n    ErrorToRetry,\n    globals_helper,\n    retry_on_exceptions_with_backoff,\n)\n\n\n@dataclass\nclass LLMMetadata:\n    \"\"\"LLM metadata.\n\n    We extract this metadata to help with our prompts.\n\n    \"\"\"\n\n    max_input_size: int = MAX_CHUNK_SIZE\n    num_output: int = NUM_OUTPUTS\n\n\ndef _get_llm_metadata(llm: BaseLLM) -> LLMMetadata:\n    \"\"\"Get LLM metadata from llm.\"\"\"\n    if not isinstance(llm, BaseLLM):\n        raise ValueError(\"llm must be an instance of langchain.llms.base.LLM\")\n    if isinstance(llm, OpenAI):\n        return LLMMetadata(\n            max_input_size=llm.modelname_to_contextsize(llm.model_name),\n            num_output=llm.max_tokens,\n        )\n    elif isinstance(llm, Cohere):\n        # TODO: figure out max input size for cohere\n        return LLMMetadata(num_output=llm.max_tokens)\n    elif isinstance(llm, AI21):\n        # TODO: figure out max input size for AI21\n        return LLMMetadata(num_output=llm.maxTokens)\n    else:\n        return LLMMetadata()\n\n\nclass LLMPredictor:\n    \"\"\"LLM predictor class.\n\n    Wrapper around an LLMChain from Langchain.\n\n    Args:\n        llm (Optional[langchain.llms.base.LLM]): LLM from Langchain to use\n            for predictions. Defaults to OpenAI's text-davinci-003 model.\n            Please see `Langchain's LLM Page\n            <https://langchain.readthedocs.io/en/latest/modules/llms.html>`_\n            for more details.\n\n        retry_on_throttling (bool): Whether to retry on rate limit errors.\n            Defaults to true.\n\n    \"\"\"\n\n    def __init__(\n        self, llm: Optional[BaseLLM] = None, retry_on_throttling: bool = True\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._llm = llm or OpenAI(temperature=0, model_name=\"text-davinci-003\")\n        self.retry_on_throttling = retry_on_throttling\n        self._total_tokens_used = 0\n        self.flag = True\n        self._last_token_usage: Optional[int] = None\n\n    def get_llm_metadata(self) -> LLMMetadata:\n        \"\"\"Get LLM metadata.\"\"\"\n        # TODO: refactor mocks in unit tests, this is a stopgap solution\n        if hasattr(self, \"_llm\"):\n            return _get_llm_metadata(self._llm)\n        else:\n            return LLMMetadata()\n\n    def _predict(self, prompt: Prompt, **prompt_args: Any) -> str:\n        \"\"\"Inner predict function.\n\n        If retry_on_throttling is true, we will retry on rate limit errors.\n\n        \"\"\"\n        llm_chain = LLMChain(prompt=prompt.get_langchain_prompt(), llm=self._llm)\n\n        # Note: we don't pass formatted_prompt to llm_chain.predict because\n        # langchain does the same formatting under the hood\n        full_prompt_args = prompt.get_full_format_args(prompt_args)\n        if self.retry_on_throttling:\n            llm_prediction = retry_on_exceptions_with_backoff(\n                lambda: llm_chain.predict(**full_prompt_args),\n                [\n                    ErrorToRetry(openai.error.RateLimitError),\n                    ErrorToRetry(openai.error.ServiceUnavailableError),\n                    ErrorToRetry(openai.error.TryAgain),\n                    ErrorToRetry(\n                        openai.error.APIConnectionError, lambda e: e.should_retry\n                    ),\n                ],\n            )\n        else:\n            llm_prediction = llm_chain.predict(**full_prompt_args)\n        return llm_prediction\n\n    def predict(self, prompt: Prompt, **prompt_args: Any) -> Tuple[str, str]:\n        \"\"\"Predict the answer to a query.\n\n        Args:\n            prompt (Prompt): Prompt to use for prediction.\n\n        Returns:\n            Tuple[str, str]: Tuple of the predicted answer and the formatted prompt.\n\n        \"\"\"\n        formatted_prompt = prompt.format(**prompt_args)\n        llm_prediction = self._predict(prompt, **prompt_args)\n        logging.debug(llm_prediction)\n\n        # We assume that the value of formatted_prompt is exactly the thing\n        # eventually sent to OpenAI, or whatever LLM downstream\n        prompt_tokens_count = self._count_tokens(formatted_prompt)\n        prediction_tokens_count = self._count_tokens(llm_prediction)\n        self._total_tokens_used += prompt_tokens_count + prediction_tokens_count\n        return llm_prediction, formatted_prompt\n\n    @property\n    def total_tokens_used(self) -> int:\n        \"\"\"Get the total tokens used so far.\"\"\"\n        return self._total_tokens_used\n\n    def _count_tokens(self, text: str) -> int:\n        tokens = globals_helper.tokenizer(text)\n        return len(tokens)\n\n    @property\n    def last_token_usage(self) -> int:\n        \"\"\"Get the last token usage.\"\"\"\n        if self._last_token_usage is None:\n            return 0\n        return self._last_token_usage\n\n    @last_token_usage.setter\n    def last_token_usage(self, value: int) -> None:\n        \"\"\"Set the last token usage.\"\"\"\n        self._last_token_usage = value\n", "doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "__type__": "Document"}, "6af9d1a0133bbdf496857f225254cdf042edd0b3": {"text": "\"\"\"Langchain memory wrapper (for GPT Index).\"\"\"\n\nfrom typing import Any, Dict, List, Optional\n\nfrom langchain.chains.base import Memory\nfrom pydantic import Field\n\nfrom gpt_index.indices.base import BaseGPTIndex\nfrom gpt_index.readers.schema.base import Document\n\n\ndef get_prompt_input_key(inputs: Dict[str, Any], memory_variables: List[str]) -> str:\n    \"\"\"Get prompt input key.\n\n    Copied over from langchain.\n\n    \"\"\"\n    # \"stop\" is a special key that can be passed as input but is not used to\n    # format the prompt.\n    prompt_input_keys = list(set(inputs).difference(memory_variables + [\"stop\"]))\n    if len(prompt_input_keys) != 1:\n        raise ValueError(f\"One input key expected got {prompt_input_keys}\")\n    return prompt_input_keys[0]\n\n\nclass GPTIndexMemory(Memory):\n    \"\"\"Langchain memory wrapper (for GPT Index).\n\n    Args:\n        human_prefix (str): Prefix for human input. Defaults to \"Human\".\n        ai_prefix (str): Prefix for AI output. Defaults to \"AI\".\n        memory_key (str): Key for memory. Defaults to \"history\".\n        index (BaseGPTIndex): GPT Index instance.\n        query_kwargs (Dict[str, Any]): Keyword arguments for GPT Index query.\n        input_key (Optional[str]): Input key. Defaults to None.\n        output_key (Optional[str]): Output key. Defaults to None.\n\n    \"\"\"\n\n    human_prefix: str = \"Human\"\n    ai_prefix: str = \"AI\"\n    memory_key: str = \"history\"\n    index: BaseGPTIndex\n    query_kwargs: Dict = Field(default_factory=dict)\n    output_key: Optional[str] = None\n    input_key: Optional[str] = None\n\n    @property\n    def memory_variables(self) -> List[str]:\n        \"\"\"Return memory variables.\"\"\"\n        return [self.memory_key]\n\n    def _get_prompt_input_key(self, inputs: Dict[str, Any]) -> str:\n        if self.input_key is None:\n            prompt_input_key = get_prompt_input_key(inputs, self.memory_variables)\n        else:\n            prompt_input_key = self.input_key\n        return prompt_input_key\n\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n        \"\"\"Return key-value pairs given the text input to the chain.\"\"\"\n        prompt_input_key = self._get_prompt_input_key(inputs)\n        query_str = inputs[prompt_input_key]\n\n        # TODO: wrap in prompt\n        # TODO: add option to return the raw text\n        # NOTE: currently it's a hack\n        response = self.index.query(query_str, **self.query_kwargs)\n        return {self.memory_key: str(response)}\n\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\n        \"\"\"Save the context of this model run to memory.\"\"\"\n        prompt_input_key = self._get_prompt_input_key(inputs)\n        if self.output_key is None:\n            if len(outputs) != 1:\n                raise ValueError(f\"One output key expected, got {outputs.keys()}\")\n            output_key = list(outputs.keys())[0]\n        else:\n            output_key = self.output_key\n        human = f\"{self.human_prefix}: \" + inputs[prompt_input_key]\n        ai = f\"{self.ai_prefix}: \" + outputs[output_key]\n        doc_text = \"\\n\".join([human, ai])\n        doc = Document(text=doc_text)\n        self.index.insert(doc)\n\n    def clear(self) -> None:\n        \"\"\"Clear memory contents.\"\"\"\n        pass\n\n    def __repr__(self) -> str:\n        \"\"\"Return representation.\"\"\"\n        return \"GPTIndexMemory()\"\n", "doc_id": "6af9d1a0133bbdf496857f225254cdf042edd0b3", "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/memory_wrapper.py", "file_name": "memory_wrapper.py"}, "__type__": "Document"}, "e541c393c777483dca55d898b374d77f2dd7e023": {"text": "\"\"\"SQL wrapper around SQLDatabase in langchain.\"\"\"\nfrom typing import Any, Dict, List, Tuple\n\nfrom langchain.sql_database import SQLDatabase as LangchainSQLDatabase\nfrom sqlalchemy import MetaData, create_engine, insert\nfrom sqlalchemy.engine import Engine\n\n\nclass SQLDatabase(LangchainSQLDatabase):\n    \"\"\"SQL Database.\n\n    Wrapper around SQLDatabase object from langchain. Offers\n    some helper utilities for insertion and querying.\n    See `langchain documentation <https://tinyurl.com/4we5ku8j>`_ for more details:\n\n    Args:\n        *args: Arguments to pass to langchain SQLDatabase.\n        **kwargs: Keyword arguments to pass to langchain SQLDatabase.\n\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.metadata_obj = MetaData(bind=self._engine)\n        self.metadata_obj.reflect()\n\n    @property\n    def engine(self) -> Engine:\n        \"\"\"Return SQL Alchemy engine.\"\"\"\n        return self._engine\n\n    @classmethod\n    def from_uri(cls, database_uri: str, **kwargs: Any) -> \"SQLDatabase\":\n        \"\"\"Construct a SQLAlchemy engine from URI.\"\"\"\n        return cls(create_engine(database_uri), **kwargs)\n\n    def get_table_columns(self, table_name: str) -> List[dict]:\n        \"\"\"Get table columns.\"\"\"\n        return self._inspector.get_columns(table_name)\n\n    def get_single_table_info(self, table_name: str) -> str:\n        \"\"\"Get table info for a single table.\"\"\"\n        # same logic as table_info, but with specific table names\n        template = \"Table '{table_name}' has columns: {columns}.\"\n        columns = []\n        for column in self._inspector.get_columns(table_name):\n            columns.append(f\"{column['name']} ({str(column['type'])})\")\n        column_str = \", \".join(columns)\n        table_str = template.format(table_name=table_name, columns=column_str)\n        return table_str\n\n    def insert_into_table(self, table_name: str, data: dict) -> None:\n        \"\"\"Insert data into a table.\"\"\"\n        table = self.metadata_obj.tables[table_name]\n        stmt = insert(table).values(**data)\n        self._engine.execute(stmt)\n\n    def run_sql(self, command: str) -> Tuple[str, Dict]:\n        \"\"\"Execute a SQL statement and return a string representing the results.\n\n        If the statement returns rows, a string of the results is returned.\n        If the statement returns no rows, an empty string is returned.\n        \"\"\"\n        with self._engine.connect() as connection:\n            cursor = connection.exec_driver_sql(command)\n            if cursor.returns_rows:\n                result = cursor.fetchall()\n                return str(result), {\"result\": result}\n        return \"\", {}\n", "doc_id": "e541c393c777483dca55d898b374d77f2dd7e023", "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/sql_wrapper.py", "file_name": "sql_wrapper.py"}, "__type__": "Document"}, "b329d2f933f606b0fe34b6e40cf993b5ebe75d02": {"text": "\"\"\"Text splitter implementations.\"\"\"\nfrom dataclasses import dataclass\nfrom typing import Callable, List, Optional\n\nfrom langchain.text_splitter import TextSplitter\n\nfrom gpt_index.utils import globals_helper\n\n\n@dataclass\nclass TextSplit:\n    \"\"\"Text split with overlap.\n\n    Attributes:\n        text_chunk: The text string.\n        num_char_overlap: The number of overlapping characters with the previous chunk.\n    \"\"\"\n\n    text_chunk: str\n    num_char_overlap: int\n\n\nclass TokenTextSplitter(TextSplitter):\n    \"\"\"Implementation of splitting text that looks at word tokens.\"\"\"\n\n    def __init__(\n        self,\n        separator: str = \" \",\n        chunk_size: int = 4000,\n        chunk_overlap: int = 200,\n        tokenizer: Optional[Callable] = None,\n        backup_separators: Optional[List[str]] = [\"\\n\"],\n    ):\n        \"\"\"Initialize with parameters.\"\"\"\n        if chunk_overlap > chunk_size:\n            raise ValueError(\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\n                f\"({chunk_size}), should be smaller.\"\n            )\n        self._separator = separator\n        self._chunk_size = chunk_size\n        self._chunk_overlap = chunk_overlap\n        self.tokenizer = tokenizer or globals_helper.tokenizer\n        self._backup_separators = backup_separators\n\n    def _reduce_chunk_size(\n        self, start_idx: int, cur_idx: int, splits: List[str]\n    ) -> int:\n        \"\"\"Reduce the chunk size by reducing cur_idx.\n\n        Return the new cur_idx.\n\n        \"\"\"\n        current_doc_total = len(\n            self.tokenizer(self._separator.join(splits[start_idx:cur_idx]))\n        )\n        while current_doc_total > self._chunk_size:\n            percent_to_reduce = (\n                current_doc_total - self._chunk_size\n            ) / current_doc_total\n            num_to_reduce = int(percent_to_reduce * (cur_idx - start_idx)) + 1\n            cur_idx -= num_to_reduce\n            current_doc_total = len(\n                self.tokenizer(self._separator.join(splits[start_idx:cur_idx]))\n            )\n        return cur_idx\n\n    def _process_splits(self, splits: List[str], chunk_size: int) -> List[str]:\n        \"\"\"Process splits.\n\n        Specifically search for tokens that are too large for chunk size,\n        and see if we can separate those tokens more\n        (via backup separators if specified, or force chunking).\n\n        \"\"\"\n        new_splits = []\n        for split in splits:\n            num_cur_tokens = len(self.tokenizer(split))\n            if num_cur_tokens <= chunk_size:\n                new_splits.append(split)\n            else:\n                cur_splits = []\n                if self._backup_separators:\n                    for sep in self._backup_separators:\n                        if sep in split:\n                            cur_splits = split.split(sep)\n                            break\n                else:\n                    cur_splits = [split]\n\n                cur_splits2 = []\n                for cur_split in cur_splits:\n                    num_cur_tokens = len(self.tokenizer(cur_split))\n                    if num_cur_tokens <= chunk_size:\n                        cur_splits2.extend([cur_split])\n                    else:\n                        cur_split_chunks = [\n                            cur_split[i : i + chunk_size]\n                            for i in range(0, len(cur_split), chunk_size)\n                        ]\n                        cur_splits2.extend(cur_split_chunks)\n\n                new_splits.extend(cur_splits2)\n        return new_splits\n\n    def split_text(self, text: str, extra_info_str: Optional[str] = None) -> List[str]:\n        \"\"\"Split incoming text and return chunks.\"\"\"\n        text_slits = self.split_text_with_overlaps(text, extra_info_str=extra_info_str)\n        return [text_split.text_chunk for text_split in text_slits]\n\n    def split_text_with_overlaps(\n        self, text: str, extra_info_str: Optional[str] = None\n    ) -> List[TextSplit]:\n        \"\"\"Split incoming text and return chunks with overlap size.\"\"\"\n        if text == \"\":\n            return []\n\n        # NOTE: Consider extra info str that will be added to the chunk at query time\n        #       This reduces the effective chunk size that we can have\n        if extra_info_str is not None:\n            # NOTE: extra 2 newline chars for formatting when prepending in query\n            num_extra_tokens = len(self.tokenizer(f\"{extra_info_str}\\n\\n\")) + 1\n            effective_chunk_size = self._chunk_size - num_extra_tokens\n\n            if effective_chunk_size <= 0:\n                raise ValueError(\n                    \"Effective chunk size is non positive after considering extra_info\"\n                )\n        else:\n            effective_chunk_size = self._chunk_size\n\n        # First we naively split the large input into a bunch of smaller ones.\n        splits = text.split(self._separator)\n        splits = self._process_splits(splits, effective_chunk_size)\n        # We now want to combine these smaller pieces into medium size\n        # chunks to send to the LLM.\n        docs = []\n\n        start_idx = 0\n        cur_idx = 0\n        cur_total = 0\n        prev_idx = 0  # store the previous end index\n        while cur_idx < len(splits):\n            cur_token = splits[cur_idx]\n            num_cur_tokens = max(len(self.tokenizer(cur_token)), 1)\n            if num_cur_tokens > effective_chunk_size:\n                raise ValueError(\n                    \"A single term is larger than the allowed chunk size.\\n\"\n                    f\"Term size: {num_cur_tokens}\\n\"\n                    f\"Chunk size: {self._chunk_size}\"\n                    f\"Effective chunk size: {effective_chunk_size}\"\n                )\n            # If adding token to current_doc would exceed the chunk size:\n            # 1. First verify with tokenizer that current_doc\n            # 1. Update the docs list\n            if cur_total + num_cur_tokens > effective_chunk_size:\n                # NOTE: since we use a proxy for counting tokens, we want to\n                # run tokenizer across all of current_doc first. If\n                # the chunk is too big, then we will reduce text in pieces\n                cur_idx = self._reduce_chunk_size(start_idx, cur_idx, splits)\n                overlap = 0\n                # after first round, check if last chunk ended after this chunk begins\n                if prev_idx > 0 and prev_idx > start_idx:\n                    overlap = sum([len(splits[i]) for i in range(start_idx, prev_idx)])\n                docs.append(\n                    TextSplit(self._separator.join(splits[start_idx:cur_idx]), overlap)\n                )\n                prev_idx = cur_idx\n                # 2. Shrink the current_doc (from the front) until it is gets smaller\n                # than the overlap size\n                # NOTE: because counting tokens individually is an imperfect\n                # proxy (but much faster proxy) for the total number of tokens consumed,\n                # we need to enforce that start_idx <= cur_idx, otherwise\n                # start_idx has a chance of going out of bounds.\n                while cur_total > self._chunk_overlap and start_idx < cur_idx:\n                    cur_num_tokens = max(len(self.tokenizer(splits[start_idx])), 1)\n                    cur_total -= cur_num_tokens\n                    start_idx += 1\n            # Build up the current_doc with term d, and update the total counter with\n            # the number of the number of tokens in d, wrt self.tokenizer\n\n            # we reassign cur_token and num_cur_tokens, because cur_idx\n            # may have changed\n            cur_token = splits[cur_idx]\n            num_cur_tokens = max(len(self.tokenizer(cur_token)), 1)\n\n            cur_total += num_cur_tokens\n            cur_idx += 1\n        overlap = 0\n        # after first round, check if last chunk ended after this chunk begins\n        if prev_idx > start_idx:\n            overlap = sum([len(splits[i]) for i in range(start_idx, prev_idx)]) + len(\n                range(start_idx, prev_idx)\n            )\n        docs.append(TextSplit(self._separator.join(splits[start_idx:cur_idx]), overlap))\n        return docs\n\n    def truncate_text(self, text: str) -> str:\n        \"\"\"Truncate text in order to fit the underlying chunk size.\"\"\"\n        if text == \"\":\n            return \"\"\n        # First we naively split the large input into a bunch of smaller ones.\n        splits = text.split(self._separator)\n        splits = self._process_splits(splits, self._chunk_size)\n\n        start_idx = 0\n        cur_idx = 0\n        cur_total = 0\n        while cur_idx < len(splits):\n            cur_token = splits[cur_idx]\n            num_cur_tokens = max(len(self.tokenizer(cur_token)), 1)\n            if cur_total + num_cur_tokens > self._chunk_size:\n                cur_idx = self._reduce_chunk_size(start_idx, cur_idx, splits)\n                break\n            cur_total += num_cur_tokens\n            cur_idx += 1\n        return self._separator.join(splits[start_idx:cur_idx])\n", "doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "__type__": "Document"}, "54a9255d-77cd-4c7d-bd72-02cbdffa7176": {"text": "\nThe documents provide implementations of a TextSplitter class and a SQLDatabase class to help with the functionality of the langchain library. The TextSplitter class provides a TokenTextSplitter implementation which splits text based on word tokens, and the SQLDatabase class provides a wrapper around the langchain SQLDatabase object with helper utilities for insertion and querying. LLMMetadata and LLMPredictor are classes used to wrap around an LLM chain from Langchain, and GPTIndexMemory is a class used to wrap around a GPT Index instance and make queries. MemoryWrapper and SQLDatabase are classes used to help with the functionality of the langchain library.", "doc_id": "54a9255d-77cd-4c7d-bd72-02cbdffa7176", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Init file for langchain helpers.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "ced105858f1e3558ef6ed5556d0e548659b84440", "node_info": null}, "1": {"text": "\"\"\"Wrapper functions around an LLM chain.\"\"\"\n\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Any, Optional, Tuple\n\nimport openai\nfrom langchain import Cohere, LLMChain, OpenAI\nfrom langchain.llms import AI21\nfrom langchain.llms.base import BaseLLM\n\nfrom gpt_index.constants import MAX_CHUNK_SIZE, NUM_OUTPUTS\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.utils import (\n    ErrorToRetry,\n    globals_helper,\n    retry_on_exceptions_with_backoff,\n)\n\n\n@dataclass\nclass LLMMetadata:\n    \"\"\"LLM metadata.\n\n    We extract this metadata to help with our prompts.\n\n    \"\"\"\n\n    max_input_size: int = MAX_CHUNK_SIZE\n    num_output: int = NUM_OUTPUTS\n\n\ndef _get_llm_metadata(llm: BaseLLM) -> LLMMetadata:\n    \"\"\"Get LLM metadata from", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 1, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "2": {"text": "-> LLMMetadata:\n    \"\"\"Get LLM metadata from llm.\"\"\"\n    if not isinstance(llm, BaseLLM):\n        raise ValueError(\"llm must be an instance of langchain.llms.base.LLM\")\n    if isinstance(llm, OpenAI):\n        return LLMMetadata(\n            max_input_size=llm.modelname_to_contextsize(llm.model_name),\n            num_output=llm.max_tokens,\n        )\n    elif isinstance(llm, Cohere):\n        # TODO: figure out max input size for cohere\n        return LLMMetadata(num_output=llm.max_tokens)\n    elif isinstance(llm, AI21):\n        # TODO: figure out max input size for AI21\n        return", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 2, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "3": {"text": "out max input size for AI21\n        return LLMMetadata(num_output=llm.maxTokens)\n    else:\n        return LLMMetadata()\n\n\nclass LLMPredictor:\n    \"\"\"LLM predictor class.\n\n    Wrapper around an LLMChain from Langchain.\n\n    Args:\n        llm (Optional[langchain.llms.base.LLM]): LLM from Langchain to use\n            for predictions. Defaults to OpenAI's text-davinci-003 model.\n            Please see `Langchain's LLM Page\n            <https://langchain.readthedocs.io/en/latest/modules/llms.html>`_\n            for more details.\n\n        retry_on_throttling (bool): Whether to retry on rate limit errors.\n            Defaults to true.\n\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 3, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "4": {"text": "        Defaults to true.\n\n    \"\"\"\n\n    def __init__(\n        self, llm: Optional[BaseLLM] = None, retry_on_throttling: bool = True\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._llm = llm or OpenAI(temperature=0, model_name=\"text-davinci-003\")\n        self.retry_on_throttling = retry_on_throttling\n        self._total_tokens_used = 0\n        self.flag = True\n        self._last_token_usage: Optional[int] = None\n\n    def get_llm_metadata(self) -> LLMMetadata:\n        \"\"\"Get LLM metadata.\"\"\"\n        # TODO: refactor mocks in unit tests, this is a stopgap solution\n        if hasattr(self,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 4, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "5": {"text": "solution\n        if hasattr(self, \"_llm\"):\n            return _get_llm_metadata(self._llm)\n        else:\n            return LLMMetadata()\n\n    def _predict(self, prompt: Prompt, **prompt_args: Any) -> str:\n        \"\"\"Inner predict function.\n\n        If retry_on_throttling is true, we will retry on rate limit errors.\n\n        \"\"\"\n        llm_chain = LLMChain(prompt=prompt.get_langchain_prompt(), llm=self._llm)\n\n        # Note: we don't pass formatted_prompt to llm_chain.predict because\n        # langchain does the same formatting under the hood\n        full_prompt_args = prompt.get_full_format_args(prompt_args)\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 5, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "6": {"text": "       if self.retry_on_throttling:\n            llm_prediction = retry_on_exceptions_with_backoff(\n                lambda: llm_chain.predict(**full_prompt_args),\n                [\n                    ErrorToRetry(openai.error.RateLimitError),\n                    ErrorToRetry(openai.error.ServiceUnavailableError),\n                    ErrorToRetry(openai.error.TryAgain),\n                    ErrorToRetry(\n                        openai.error.APIConnectionError, lambda e: e.should_retry\n        ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 6, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "7": {"text": "e.should_retry\n                    ),\n                ],\n            )\n        else:\n            llm_prediction = llm_chain.predict(**full_prompt_args)\n        return llm_prediction\n\n    def predict(self, prompt: Prompt, **prompt_args: Any) -> Tuple[str, str]:\n        \"\"\"Predict the answer to a query.\n\n        Args:\n            prompt (Prompt): Prompt to use for prediction.\n\n        Returns:\n            Tuple[str, str]: Tuple of the predicted answer and the formatted prompt.\n\n        \"\"\"\n        formatted_prompt = prompt.format(**prompt_args)\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 7, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "8": {"text": "prompt.format(**prompt_args)\n        llm_prediction = self._predict(prompt, **prompt_args)\n        logging.debug(llm_prediction)\n\n        # We assume that the value of formatted_prompt is exactly the thing\n        # eventually sent to OpenAI, or whatever LLM downstream\n        prompt_tokens_count = self._count_tokens(formatted_prompt)\n        prediction_tokens_count = self._count_tokens(llm_prediction)\n        self._total_tokens_used += prompt_tokens_count + prediction_tokens_count\n        return llm_prediction, formatted_prompt\n\n    @property\n    def total_tokens_used(self) -> int:\n        \"\"\"Get the total tokens used so far.\"\"\"\n        return", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 8, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "9": {"text": "used so far.\"\"\"\n        return self._total_tokens_used\n\n    def _count_tokens(self, text: str) -> int:\n        tokens = globals_helper.tokenizer(text)\n        return len(tokens)\n\n    @property\n    def last_token_usage(self) -> int:\n        \"\"\"Get the last token usage.\"\"\"\n        if self._last_token_usage is None:\n            return 0\n        return self._last_token_usage\n\n    @last_token_usage.setter\n    def last_token_usage(self, value: int) -> None:\n        \"\"\"Set the last token usage.\"\"\"\n        self._last_token_usage = value\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 9, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "10": {"text": "\"\"\"Langchain memory wrapper (for GPT Index).\"\"\"\n\nfrom typing import Any, Dict, List, Optional\n\nfrom langchain.chains.base import Memory\nfrom pydantic import Field\n\nfrom gpt_index.indices.base import BaseGPTIndex\nfrom gpt_index.readers.schema.base import Document\n\n\ndef get_prompt_input_key(inputs: Dict[str, Any], memory_variables: List[str]) -> str:\n    \"\"\"Get prompt input key.\n\n    Copied over from langchain.\n\n    \"\"\"\n    # \"stop\" is a special key that can be passed as input but is not used to\n    # format the prompt.\n    prompt_input_keys = list(set(inputs).difference(memory_variables + [\"stop\"]))\n    if len(prompt_input_keys) != 1:\n        raise ValueError(f\"One input key expected got {prompt_input_keys}\")\n    return prompt_input_keys[0]\n\n\nclass GPTIndexMemory(Memory):\n    \"\"\"Langchain memory", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/memory_wrapper.py", "file_name": "memory_wrapper.py"}, "index": 10, "child_indices": [], "ref_doc_id": "6af9d1a0133bbdf496857f225254cdf042edd0b3", "node_info": null}, "11": {"text": "GPTIndexMemory(Memory):\n    \"\"\"Langchain memory wrapper (for GPT Index).\n\n    Args:\n        human_prefix (str): Prefix for human input. Defaults to \"Human\".\n        ai_prefix (str): Prefix for AI output. Defaults to \"AI\".\n        memory_key (str): Key for memory. Defaults to \"history\".\n        index (BaseGPTIndex): GPT Index instance.\n        query_kwargs (Dict[str, Any]): Keyword arguments for GPT Index query.\n        input_key (Optional[str]): Input key. Defaults to None.\n        output_key (Optional[str]): Output key. Defaults to None.\n\n    \"\"\"\n\n    human_prefix: str = \"Human\"\n    ai_prefix: str = \"AI\"\n    memory_key: str = \"history\"\n    index: BaseGPTIndex\n    query_kwargs: Dict = Field(default_factory=dict)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/memory_wrapper.py", "file_name": "memory_wrapper.py"}, "index": 11, "child_indices": [], "ref_doc_id": "6af9d1a0133bbdf496857f225254cdf042edd0b3", "node_info": null}, "12": {"text": "Dict = Field(default_factory=dict)\n    output_key: Optional[str] = None\n    input_key: Optional[str] = None\n\n    @property\n    def memory_variables(self) -> List[str]:\n        \"\"\"Return memory variables.\"\"\"\n        return [self.memory_key]\n\n    def _get_prompt_input_key(self, inputs: Dict[str, Any]) -> str:\n        if self.input_key is None:\n            prompt_input_key = get_prompt_input_key(inputs, self.memory_variables)\n        else:\n            prompt_input_key = self.input_key\n        return prompt_input_key\n\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n        \"\"\"Return key-value pairs given the text input to the", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/memory_wrapper.py", "file_name": "memory_wrapper.py"}, "index": 12, "child_indices": [], "ref_doc_id": "6af9d1a0133bbdf496857f225254cdf042edd0b3", "node_info": null}, "13": {"text": "   \"\"\"Return key-value pairs given the text input to the chain.\"\"\"\n        prompt_input_key = self._get_prompt_input_key(inputs)\n        query_str = inputs[prompt_input_key]\n\n        # TODO: wrap in prompt\n        # TODO: add option to return the raw text\n        # NOTE: currently it's a hack\n        response = self.index.query(query_str, **self.query_kwargs)\n        return {self.memory_key: str(response)}\n\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\n        \"\"\"Save the context of this model run to memory.\"\"\"\n        prompt_input_key = self._get_prompt_input_key(inputs)\n        if self.output_key is None:\n          ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/memory_wrapper.py", "file_name": "memory_wrapper.py"}, "index": 13, "child_indices": [], "ref_doc_id": "6af9d1a0133bbdf496857f225254cdf042edd0b3", "node_info": null}, "14": {"text": "is None:\n            if len(outputs) != 1:\n                raise ValueError(f\"One output key expected, got {outputs.keys()}\")\n            output_key = list(outputs.keys())[0]\n        else:\n            output_key = self.output_key\n        human = f\"{self.human_prefix}: \" + inputs[prompt_input_key]\n        ai = f\"{self.ai_prefix}: \" + outputs[output_key]\n        doc_text = \"\\n\".join([human, ai])\n        doc = Document(text=doc_text)\n        self.index.insert(doc)\n\n    def clear(self) -> None:\n        \"\"\"Clear memory contents.\"\"\"\n        pass\n\n    def __repr__(self) ->", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/memory_wrapper.py", "file_name": "memory_wrapper.py"}, "index": 14, "child_indices": [], "ref_doc_id": "6af9d1a0133bbdf496857f225254cdf042edd0b3", "node_info": null}, "15": {"text": "  pass\n\n    def __repr__(self) -> str:\n        \"\"\"Return representation.\"\"\"\n        return \"GPTIndexMemory()\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/memory_wrapper.py", "file_name": "memory_wrapper.py"}, "index": 15, "child_indices": [], "ref_doc_id": "6af9d1a0133bbdf496857f225254cdf042edd0b3", "node_info": null}, "16": {"text": "\"\"\"SQL wrapper around SQLDatabase in langchain.\"\"\"\nfrom typing import Any, Dict, List, Tuple\n\nfrom langchain.sql_database import SQLDatabase as LangchainSQLDatabase\nfrom sqlalchemy import MetaData, create_engine, insert\nfrom sqlalchemy.engine import Engine\n\n\nclass SQLDatabase(LangchainSQLDatabase):\n    \"\"\"SQL Database.\n\n    Wrapper around SQLDatabase object from langchain. Offers\n    some helper utilities for insertion and querying.\n    See `langchain documentation <https://tinyurl.com/4we5ku8j>`_ for more details:\n\n    Args:\n        *args: Arguments to pass to langchain SQLDatabase.\n        **kwargs: Keyword arguments to pass to langchain SQLDatabase.\n\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.metadata_obj =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/sql_wrapper.py", "file_name": "sql_wrapper.py"}, "index": 16, "child_indices": [], "ref_doc_id": "e541c393c777483dca55d898b374d77f2dd7e023", "node_info": null}, "17": {"text": "       self.metadata_obj = MetaData(bind=self._engine)\n        self.metadata_obj.reflect()\n\n    @property\n    def engine(self) -> Engine:\n        \"\"\"Return SQL Alchemy engine.\"\"\"\n        return self._engine\n\n    @classmethod\n    def from_uri(cls, database_uri: str, **kwargs: Any) -> \"SQLDatabase\":\n        \"\"\"Construct a SQLAlchemy engine from URI.\"\"\"\n        return cls(create_engine(database_uri), **kwargs)\n\n    def get_table_columns(self, table_name: str) -> List[dict]:\n        \"\"\"Get table columns.\"\"\"\n        return self._inspector.get_columns(table_name)\n\n    def get_single_table_info(self, table_name: str) -> str:\n        \"\"\"Get table info for a single table.\"\"\"\n        # same logic as", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/sql_wrapper.py", "file_name": "sql_wrapper.py"}, "index": 17, "child_indices": [], "ref_doc_id": "e541c393c777483dca55d898b374d77f2dd7e023", "node_info": null}, "18": {"text": "table.\"\"\"\n        # same logic as table_info, but with specific table names\n        template = \"Table '{table_name}' has columns: {columns}.\"\n        columns = []\n        for column in self._inspector.get_columns(table_name):\n            columns.append(f\"{column['name']} ({str(column['type'])})\")\n        column_str = \", \".join(columns)\n        table_str = template.format(table_name=table_name, columns=column_str)\n        return table_str\n\n    def insert_into_table(self, table_name: str, data: dict) -> None:\n        \"\"\"Insert data into a table.\"\"\"\n        table = self.metadata_obj.tables[table_name]\n        stmt = insert(table).values(**data)\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/sql_wrapper.py", "file_name": "sql_wrapper.py"}, "index": 18, "child_indices": [], "ref_doc_id": "e541c393c777483dca55d898b374d77f2dd7e023", "node_info": null}, "19": {"text": "       self._engine.execute(stmt)\n\n    def run_sql(self, command: str) -> Tuple[str, Dict]:\n        \"\"\"Execute a SQL statement and return a string representing the results.\n\n        If the statement returns rows, a string of the results is returned.\n        If the statement returns no rows, an empty string is returned.\n        \"\"\"\n        with self._engine.connect() as connection:\n            cursor = connection.exec_driver_sql(command)\n            if cursor.returns_rows:\n                result = cursor.fetchall()\n                return str(result), {\"result\": result}\n        return \"\", {}\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/sql_wrapper.py", "file_name": "sql_wrapper.py"}, "index": 19, "child_indices": [], "ref_doc_id": "e541c393c777483dca55d898b374d77f2dd7e023", "node_info": null}, "20": {"text": "\"\"\"Text splitter implementations.\"\"\"\nfrom dataclasses import dataclass\nfrom typing import Callable, List, Optional\n\nfrom langchain.text_splitter import TextSplitter\n\nfrom gpt_index.utils import globals_helper\n\n\n@dataclass\nclass TextSplit:\n    \"\"\"Text split with overlap.\n\n    Attributes:\n        text_chunk: The text string.\n        num_char_overlap: The number of overlapping characters with the previous chunk.\n    \"\"\"\n\n    text_chunk: str\n    num_char_overlap: int\n\n\nclass TokenTextSplitter(TextSplitter):\n    \"\"\"Implementation of splitting text that looks at word tokens.\"\"\"\n\n    def __init__(\n        self,\n        separator: str = \" \",\n        chunk_size: int = 4000,\n        chunk_overlap: int = 200,\n        tokenizer: Optional[Callable] =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 20, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "21": {"text": "       tokenizer: Optional[Callable] = None,\n        backup_separators: Optional[List[str]] = [\"\\n\"],\n    ):\n        \"\"\"Initialize with parameters.\"\"\"\n        if chunk_overlap > chunk_size:\n            raise ValueError(\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\n                f\"({chunk_size}), should be smaller.\"\n            )\n        self._separator = separator\n        self._chunk_size = chunk_size\n        self._chunk_overlap = chunk_overlap\n        self.tokenizer = tokenizer or globals_helper.tokenizer\n        self._backup_separators =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 21, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "22": {"text": "       self._backup_separators = backup_separators\n\n    def _reduce_chunk_size(\n        self, start_idx: int, cur_idx: int, splits: List[str]\n    ) -> int:\n        \"\"\"Reduce the chunk size by reducing cur_idx.\n\n        Return the new cur_idx.\n\n        \"\"\"\n        current_doc_total = len(\n            self.tokenizer(self._separator.join(splits[start_idx:cur_idx]))\n        )\n        while current_doc_total > self._chunk_size:\n            percent_to_reduce = (\n                current_doc_total - self._chunk_size\n            ) / current_doc_total\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 22, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "23": {"text": "  ) / current_doc_total\n            num_to_reduce = int(percent_to_reduce * (cur_idx - start_idx)) + 1\n            cur_idx -= num_to_reduce\n            current_doc_total = len(\n                self.tokenizer(self._separator.join(splits[start_idx:cur_idx]))\n            )\n        return cur_idx\n\n    def _process_splits(self, splits: List[str], chunk_size: int) -> List[str]:\n        \"\"\"Process splits.\n\n        Specifically search for tokens that are too large for chunk size,\n        and see if we can separate those tokens more\n        (via backup separators if specified, or force chunking).\n\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 23, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "24": {"text": "if specified, or force chunking).\n\n        \"\"\"\n        new_splits = []\n        for split in splits:\n            num_cur_tokens = len(self.tokenizer(split))\n            if num_cur_tokens <= chunk_size:\n                new_splits.append(split)\n            else:\n                cur_splits = []\n                if self._backup_separators:\n                    for sep in self._backup_separators:\n                        if sep in split:\n                           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 24, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "25": {"text": "                cur_splits = split.split(sep)\n                            break\n                else:\n                    cur_splits = [split]\n\n                cur_splits2 = []\n                for cur_split in cur_splits:\n                    num_cur_tokens = len(self.tokenizer(cur_split))\n                    if num_cur_tokens <= chunk_size:\n                        cur_splits2.extend([cur_split])\n                ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 25, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "26": {"text": "                   else:\n                        cur_split_chunks = [\n                            cur_split[i : i + chunk_size]\n                            for i in range(0, len(cur_split), chunk_size)\n                        ]\n                        cur_splits2.extend(cur_split_chunks)\n\n                new_splits.extend(cur_splits2)\n        return new_splits\n\n    def split_text(self, text: str, extra_info_str: Optional[str] = None) ->", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 26, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "27": {"text": "str, extra_info_str: Optional[str] = None) -> List[str]:\n        \"\"\"Split incoming text and return chunks.\"\"\"\n        text_slits = self.split_text_with_overlaps(text, extra_info_str=extra_info_str)\n        return [text_split.text_chunk for text_split in text_slits]\n\n    def split_text_with_overlaps(\n        self, text: str, extra_info_str: Optional[str] = None\n    ) -> List[TextSplit]:\n        \"\"\"Split incoming text and return chunks with overlap size.\"\"\"\n        if text == \"\":\n            return []\n\n        # NOTE: Consider extra info str that will be added to the chunk at query time\n        #       This reduces the effective chunk size that we can have\n        if extra_info_str is not None:\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 27, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "28": {"text": "     if extra_info_str is not None:\n            # NOTE: extra 2 newline chars for formatting when prepending in query\n            num_extra_tokens = len(self.tokenizer(f\"{extra_info_str}\\n\\n\")) + 1\n            effective_chunk_size = self._chunk_size - num_extra_tokens\n\n            if effective_chunk_size <= 0:\n                raise ValueError(\n                    \"Effective chunk size is non positive after considering extra_info\"\n                )\n        else:\n            effective_chunk_size = self._chunk_size\n\n        # First we naively split the large input into a bunch of smaller ones.\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 28, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "29": {"text": "large input into a bunch of smaller ones.\n        splits = text.split(self._separator)\n        splits = self._process_splits(splits, effective_chunk_size)\n        # We now want to combine these smaller pieces into medium size\n        # chunks to send to the LLM.\n        docs = []\n\n        start_idx = 0\n        cur_idx = 0\n        cur_total = 0\n        prev_idx = 0  # store the previous end index\n        while cur_idx < len(splits):\n            cur_token = splits[cur_idx]\n            num_cur_tokens = max(len(self.tokenizer(cur_token)), 1)\n            if num_cur_tokens > effective_chunk_size:\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 29, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "30": {"text": "num_cur_tokens > effective_chunk_size:\n                raise ValueError(\n                    \"A single term is larger than the allowed chunk size.\\n\"\n                    f\"Term size: {num_cur_tokens}\\n\"\n                    f\"Chunk size: {self._chunk_size}\"\n                    f\"Effective chunk size: {effective_chunk_size}\"\n                )\n            # If adding token to current_doc would exceed the chunk size:\n            # 1. First verify with tokenizer that current_doc\n            # 1. Update the docs list\n            if", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 30, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "31": {"text": "the docs list\n            if cur_total + num_cur_tokens > effective_chunk_size:\n                # NOTE: since we use a proxy for counting tokens, we want to\n                # run tokenizer across all of current_doc first. If\n                # the chunk is too big, then we will reduce text in pieces\n                cur_idx = self._reduce_chunk_size(start_idx, cur_idx, splits)\n                overlap = 0\n                # after first round, check if last chunk ended after this chunk begins\n                if prev_idx > 0 and prev_idx > start_idx:\n                 ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 31, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "32": {"text": "                  overlap = sum([len(splits[i]) for i in range(start_idx, prev_idx)])\n                docs.append(\n                    TextSplit(self._separator.join(splits[start_idx:cur_idx]), overlap)\n                )\n                prev_idx = cur_idx\n                # 2. Shrink the current_doc (from the front) until it is gets smaller\n                # than the overlap size\n                # NOTE: because counting tokens individually is an imperfect\n                # proxy (but much faster proxy) for the total number of tokens", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 32, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "33": {"text": "proxy (but much faster proxy) for the total number of tokens consumed,\n                # we need to enforce that start_idx <= cur_idx, otherwise\n                # start_idx has a chance of going out of bounds.\n                while cur_total > self._chunk_overlap and start_idx < cur_idx:\n                    cur_num_tokens = max(len(self.tokenizer(splits[start_idx])), 1)\n                    cur_total -= cur_num_tokens\n                    start_idx += 1\n            # Build up the current_doc with term d, and update the total counter with\n            # the number of the number of", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 33, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "34": {"text": "         # the number of the number of tokens in d, wrt self.tokenizer\n\n            # we reassign cur_token and num_cur_tokens, because cur_idx\n            # may have changed\n            cur_token = splits[cur_idx]\n            num_cur_tokens = max(len(self.tokenizer(cur_token)), 1)\n\n            cur_total += num_cur_tokens\n            cur_idx += 1\n        overlap = 0\n        # after first round, check if last chunk ended after this chunk begins\n        if prev_idx > start_idx:\n            overlap = sum([len(splits[i]) for i in range(start_idx, prev_idx)]) + len(\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 34, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "35": {"text": "prev_idx)]) + len(\n                range(start_idx, prev_idx)\n            )\n        docs.append(TextSplit(self._separator.join(splits[start_idx:cur_idx]), overlap))\n        return docs\n\n    def truncate_text(self, text: str) -> str:\n        \"\"\"Truncate text in order to fit the underlying chunk size.\"\"\"\n        if text == \"\":\n            return \"\"\n        # First we naively split the large input into a bunch of smaller ones.\n        splits = text.split(self._separator)\n        splits = self._process_splits(splits, self._chunk_size)\n\n        start_idx = 0\n        cur_idx = 0\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 35, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "36": {"text": "  cur_idx = 0\n        cur_total = 0\n        while cur_idx < len(splits):\n            cur_token = splits[cur_idx]\n            num_cur_tokens = max(len(self.tokenizer(cur_token)), 1)\n            if cur_total + num_cur_tokens > self._chunk_size:\n                cur_idx = self._reduce_chunk_size(start_idx, cur_idx, splits)\n                break\n            cur_total += num_cur_tokens\n            cur_idx += 1\n        return self._separator.join(splits[start_idx:cur_idx])\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 36, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "37": {"text": "This code file contains two classes, LLMMetadata and LLMPredictor, which are used to wrap around an LLM chain from Langchain. LLMMetadata is used to extract metadata from the LLM, while LLMPredictor is used to make predictions using the LLM. It also contains a GPTIndexMemory class, which is used to wrap around a GPT Index instance and make queries.", "doc_id": null, "embedding": null, "extra_info": null, "index": 37, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "38": {"text": "\nThis code file contains two classes, MemoryWrapper and SQLDatabase, which are used to help with the functionality of the langchain library. MemoryWrapper provides methods for loading memory variables, saving context, and clearing memory contents. SQLDatabase provides methods for creating a SQLAlchemy engine from a URI, getting table columns, getting single table info, inserting into a table, running SQL statements, and processing splits. Additionally, TextSplit is a dataclass used to store text chunks and the number of overlapping characters with the previous chunk.", "doc_id": null, "embedding": null, "extra_info": null, "index": 38, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "39": {"text": "text_splitter.py is a Python file that contains functions to split incoming text into chunks of a specified size. It uses a tokenizer to count the number of tokens in each chunk and splits the text into smaller chunks if the number of tokens exceeds the specified chunk size. It also takes into account an extra info string that will be added to the chunk at query time, reducing the effective chunk size. The functions in this file also allow for overlapping chunks and truncating text to fit the underlying chunk size.", "doc_id": null, "embedding": null, "extra_info": null, "index": 39, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}, "40": {"text": "This code file, text_splitter.py, is part of the GPT-Index language chain helpers. It is used to split text into chunks of a specified size. It does this by looping through the text, tokenizing each word, and adding the number of tokens to a total. If the total exceeds the specified chunk size, the chunk size is reduced and the loop is broken. Finally, the text is joined back together and returned.", "doc_id": null, "embedding": null, "extra_info": null, "index": 40, "child_indices": [36], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"37": {"text": "This code file contains two classes, LLMMetadata and LLMPredictor, which are used to wrap around an LLM chain from Langchain. LLMMetadata is used to extract metadata from the LLM, while LLMPredictor is used to make predictions using the LLM. It also contains a GPTIndexMemory class, which is used to wrap around a GPT Index instance and make queries.", "doc_id": null, "embedding": null, "extra_info": null, "index": 37, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "38": {"text": "\nThis code file contains two classes, MemoryWrapper and SQLDatabase, which are used to help with the functionality of the langchain library. MemoryWrapper provides methods for loading memory variables, saving context, and clearing memory contents. SQLDatabase provides methods for creating a SQLAlchemy engine from a URI, getting table columns, getting single table info, inserting into a table, running SQL statements, and processing splits. Additionally, TextSplit is a dataclass used to store text chunks and the number of overlapping characters with the previous chunk.", "doc_id": null, "embedding": null, "extra_info": null, "index": 38, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "39": {"text": "text_splitter.py is a Python file that contains functions to split incoming text into chunks of a specified size. It uses a tokenizer to count the number of tokens in each chunk and splits the text into smaller chunks if the number of tokens exceeds the specified chunk size. It also takes into account an extra info string that will be added to the chunk at query time, reducing the effective chunk size. The functions in this file also allow for overlapping chunks and truncating text to fit the underlying chunk size.", "doc_id": null, "embedding": null, "extra_info": null, "index": 39, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}, "40": {"text": "This code file, text_splitter.py, is part of the GPT-Index language chain helpers. It is used to split text into chunks of a specified size. It does this by looping through the text, tokenizing each word, and adding the number of tokens to a total. If the total exceeds the specified chunk size, the chunk size is reduced and the loop is broken. Finally, the text is joined back together and returned.", "doc_id": null, "embedding": null, "extra_info": null, "index": 40, "child_indices": [36], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}