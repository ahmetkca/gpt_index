{"index_struct": {"text": "\nThe SimpleDirectoryReader class is a simple directory reader that can read files into separate documents or concatenate files into one document text. The BaseParser class is a base class for all parsers and provides methods such as init_parser and parser_config_set. The PDFParser, DocxParser, EpubParser, and ImageParser classes are all subclasses of the BaseParser class and contain methods for parsing the respective file types. The MarkdownParser class is used to extract text from markdown files, and the MboxParser class is used to extract messages from mailbox files. The TabularParser class contains two parsers for tabular data files, and the VideoAudio class extracts text from the transcript of video/audio files.", "doc_id": "8e23fb86-74d7-4bf2-8558-33a11d9d9440", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Init params.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "c637335013c599b07de054fba07b47ecb86ad3e8", "node_info": null}, "1": {"text": "\"\"\"Simple reader that reads files of different formats from a directory.\"\"\"\nimport logging\nfrom pathlib import Path\nfrom typing import Callable, Dict, List, Optional, Union\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.file.base_parser import BaseParser\nfrom gpt_index.readers.file.docs_parser import DocxParser, PDFParser\nfrom gpt_index.readers.file.epub_parser import EpubParser\nfrom gpt_index.readers.file.image_parser import ImageParser\nfrom gpt_index.readers.file.markdown_parser import MarkdownParser\nfrom gpt_index.readers.file.mbox_parser import MboxParser\nfrom gpt_index.readers.file.slides_parser import PptxParser\nfrom gpt_index.readers.file.tabular_parser import PandasCSVParser\nfrom gpt_index.readers.file.video_audio import VideoAudioParser\nfrom gpt_index.readers.schema.base import Document\n\nDEFAULT_FILE_EXTRACTOR: Dict[str, BaseParser] = {\n    \".pdf\": PDFParser(),\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base.py", "file_name": "base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "d0027a089853c5ff8efcaa11483f67fc0e61b666", "node_info": null}, "2": {"text": "= {\n    \".pdf\": PDFParser(),\n    \".docx\": DocxParser(),\n    \".pptx\": PptxParser(),\n    \".jpg\": ImageParser(),\n    \".png\": ImageParser(),\n    \".jpeg\": ImageParser(),\n    \".mp3\": VideoAudioParser(),\n    \".mp4\": VideoAudioParser(),\n    \".csv\": PandasCSVParser(),\n    \".epub\": EpubParser(),\n    \".md\": MarkdownParser(),\n    \".mbox\": MboxParser(),\n}\n\n\nclass SimpleDirectoryReader(BaseReader):\n    \"\"\"Simple directory reader.\n\n    Can read files into separate documents, or concatenates\n    files into one document text.\n\n    Args:\n        input_dir (str): Path to the directory.\n        input_files (List): List of file paths to read (Optional; overrides input_dir)\n        exclude_hidden (bool): Whether to exclude hidden files (dotfiles).\n        errors (str): how encoding and", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base.py", "file_name": "base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "d0027a089853c5ff8efcaa11483f67fc0e61b666", "node_info": null}, "3": {"text": "       errors (str): how encoding and decoding errors are to be handled,\n              see https://docs.python.org/3/library/functions.html#open\n        recursive (bool): Whether to recursively search in subdirectories.\n            False by default.\n        required_exts (Optional[List[str]]): List of required extensions.\n            Default is None.\n        file_extractor (Optional[Dict[str, BaseParser]]): A mapping of file\n            extension to a BaseParser class that specifies how to convert that file\n            to text. See DEFAULT_FILE_EXTRACTOR.\n        num_files_limit (Optional[int]): Maximum number of files to read.\n            Default is None.\n        file_metadata (Optional[Callable[str, Dict]]): A", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base.py", "file_name": "base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "d0027a089853c5ff8efcaa11483f67fc0e61b666", "node_info": null}, "4": {"text": "file_metadata (Optional[Callable[str, Dict]]): A function that takes\n            in a filename and returns a Dict of metadata for the Document.\n            Default is None.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dir: Optional[str] = None,\n        input_files: Optional[List] = None,\n        exclude_hidden: bool = True,\n        errors: str = \"ignore\",\n        recursive: bool = False,\n        required_exts: Optional[List[str]] = None,\n        file_extractor: Optional[Dict[str, BaseParser]] = None,\n        num_files_limit: Optional[int] = None,\n        file_metadata: Optional[Callable[[str], Dict]] = None,\n    ) -> None:\n        \"\"\"Initialize with", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base.py", "file_name": "base.py"}, "index": 4, "child_indices": [], "ref_doc_id": "d0027a089853c5ff8efcaa11483f67fc0e61b666", "node_info": null}, "5": {"text": ") -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        super().__init__()\n\n        if not input_dir and not input_files:\n            raise ValueError(\"Must provide either `input_dir` or `input_files`.\")\n\n        self.errors = errors\n\n        self.recursive = recursive\n        self.exclude_hidden = exclude_hidden\n        self.required_exts = required_exts\n        self.num_files_limit = num_files_limit\n\n        if input_files:\n            self.input_files = []\n            for path in input_files:\n                input_file = Path(path)\n                self.input_files.append(input_file)\n        elif", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base.py", "file_name": "base.py"}, "index": 5, "child_indices": [], "ref_doc_id": "d0027a089853c5ff8efcaa11483f67fc0e61b666", "node_info": null}, "6": {"text": "       elif input_dir:\n            self.input_dir = Path(input_dir)\n            self.input_files = self._add_files(self.input_dir)\n\n        self.file_extractor = file_extractor or DEFAULT_FILE_EXTRACTOR\n        self.file_metadata = file_metadata\n\n    def _add_files(self, input_dir: Path) -> List[Path]:\n        \"\"\"Add files.\"\"\"\n        input_files = sorted(input_dir.iterdir())\n        new_input_files = []\n        dirs_to_explore = []\n        for input_file in input_files:\n            if input_file.is_dir():\n                if self.recursive:\n                   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base.py", "file_name": "base.py"}, "index": 6, "child_indices": [], "ref_doc_id": "d0027a089853c5ff8efcaa11483f67fc0e61b666", "node_info": null}, "7": {"text": "                dirs_to_explore.append(input_file)\n            elif self.exclude_hidden and input_file.name.startswith(\".\"):\n                continue\n            elif (\n                self.required_exts is not None\n                and input_file.suffix not in self.required_exts\n            ):\n                continue\n            else:\n                new_input_files.append(input_file)\n\n        for dir_to_explore in dirs_to_explore:\n            sub_input_files = self._add_files(dir_to_explore)\n        ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base.py", "file_name": "base.py"}, "index": 7, "child_indices": [], "ref_doc_id": "d0027a089853c5ff8efcaa11483f67fc0e61b666", "node_info": null}, "8": {"text": "           new_input_files.extend(sub_input_files)\n\n        if self.num_files_limit is not None and self.num_files_limit > 0:\n            new_input_files = new_input_files[0 : self.num_files_limit]\n\n        # print total number of files added\n        logging.debug(\n            f\"> [SimpleDirectoryReader] Total files added: {len(new_input_files)}\"\n        )\n\n        return new_input_files\n\n    def load_data(self, concatenate: bool = False) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            concatenate (bool): whether to concatenate all files into one document.\n                If set to True, file metadata is ignored.\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base.py", "file_name": "base.py"}, "index": 8, "child_indices": [], "ref_doc_id": "d0027a089853c5ff8efcaa11483f67fc0e61b666", "node_info": null}, "9": {"text": " If set to True, file metadata is ignored.\n                False by default.\n\n        Returns:\n            List[Document]: A list of documents.\n\n        \"\"\"\n        data: Union[str, List[str]] = \"\"\n        data_list: List[str] = []\n        metadata_list = []\n        for input_file in self.input_files:\n            if input_file.suffix in self.file_extractor:\n                parser = self.file_extractor[input_file.suffix]\n                if not parser.parser_config_set:\n                    parser.init_parser()\n                data = parser.parse_file(input_file,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base.py", "file_name": "base.py"}, "index": 9, "child_indices": [], "ref_doc_id": "d0027a089853c5ff8efcaa11483f67fc0e61b666", "node_info": null}, "10": {"text": "    data = parser.parse_file(input_file, errors=self.errors)\n            else:\n                # do standard read\n                with open(input_file, \"r\", errors=self.errors) as f:\n                    data = f.read()\n            if isinstance(data, List):\n                data_list.extend(data)\n            else:\n                data_list.append(str(data))\n            if self.file_metadata is not None:\n                metadata_list.append(self.file_metadata(str(input_file)))\n\n        if concatenate:\n            return", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base.py", "file_name": "base.py"}, "index": 10, "child_indices": [], "ref_doc_id": "d0027a089853c5ff8efcaa11483f67fc0e61b666", "node_info": null}, "11": {"text": "           return [Document(\"\\n\".join(data_list))]\n        elif self.file_metadata is not None:\n            return [Document(d, extra_info=m) for d, m in zip(data_list, metadata_list)]\n        else:\n            return [Document(d) for d in data_list]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base.py", "file_name": "base.py"}, "index": 11, "child_indices": [], "ref_doc_id": "d0027a089853c5ff8efcaa11483f67fc0e61b666", "node_info": null}, "12": {"text": "\"\"\"Base parser and config class.\"\"\"\n\nfrom abc import abstractmethod\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Union\n\n\nclass BaseParser:\n    \"\"\"Base class for all parsers.\"\"\"\n\n    def __init__(self, parser_config: Optional[Dict] = None):\n        \"\"\"Init params.\"\"\"\n        self._parser_config = parser_config\n\n    def init_parser(self) -> None:\n        \"\"\"Init parser and store it.\"\"\"\n        parser_config = self._init_parser()\n        self._parser_config = parser_config\n\n    @property\n    def parser_config_set(self) -> bool:\n        \"\"\"Check if parser config is set.\"\"\"\n        return self._parser_config is not None\n\n    @property\n    def parser_config(self) -> Dict:\n        \"\"\"Check if parser config is set.\"\"\"\n        if self._parser_config is", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base_parser.py", "file_name": "base_parser.py"}, "index": 12, "child_indices": [], "ref_doc_id": "753a56f9797432f053fb96a72bdb782a2b20bd05", "node_info": null}, "13": {"text": "       if self._parser_config is None:\n            raise ValueError(\"Parser config not set.\")\n        return self._parser_config\n\n    @abstractmethod\n    def _init_parser(self) -> Dict:\n        \"\"\"Initialize the parser with the config.\"\"\"\n\n    @abstractmethod\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> Union[str, List[str]]:\n        \"\"\"Parse file.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base_parser.py", "file_name": "base_parser.py"}, "index": 13, "child_indices": [], "ref_doc_id": "753a56f9797432f053fb96a72bdb782a2b20bd05", "node_info": null}, "14": {"text": "\"\"\"Docs parser.\n\nContains parsers for docx, pdf files.\n\n\"\"\"\nfrom pathlib import Path\nfrom typing import Dict\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass PDFParser(BaseParser):\n    \"\"\"PDF parser.\"\"\"\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        return {}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> str:\n        \"\"\"Parse file.\"\"\"\n        try:\n            import PyPDF2\n        except ImportError:\n            raise ValueError(\"PyPDF2 is required to read PDF files.\")\n        text_list = []\n        with open(file, \"rb\") as fp:\n            # Create a PDF object\n            pdf =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/docs_parser.py", "file_name": "docs_parser.py"}, "index": 14, "child_indices": [], "ref_doc_id": "ac3dce9de6352d92aa69fdf67294da228f5085c3", "node_info": null}, "15": {"text": "PDF object\n            pdf = PyPDF2.PdfReader(fp)\n\n            # Get the number of pages in the PDF document\n            num_pages = len(pdf.pages)\n\n            # Iterate over every page\n            for page in range(num_pages):\n                # Extract the text from the page\n                page_text = pdf.pages[page].extract_text()\n                text_list.append(page_text)\n        text = \"\\n\".join(text_list)\n\n        return text\n\n\nclass DocxParser(BaseParser):\n    \"\"\"Docx parser.\"\"\"\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        return {}\n\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/docs_parser.py", "file_name": "docs_parser.py"}, "index": 15, "child_indices": [], "ref_doc_id": "ac3dce9de6352d92aa69fdf67294da228f5085c3", "node_info": null}, "16": {"text": "       return {}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> str:\n        \"\"\"Parse file.\"\"\"\n        try:\n            import docx2txt\n        except ImportError:\n            raise ValueError(\"docx2txt is required to read Microsoft Word files.\")\n\n        text = docx2txt.process(file)\n\n        return text\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/docs_parser.py", "file_name": "docs_parser.py"}, "index": 16, "child_indices": [], "ref_doc_id": "ac3dce9de6352d92aa69fdf67294da228f5085c3", "node_info": null}, "17": {"text": "\"\"\"Epub parser.\n\nContains parsers for epub files.\n\"\"\"\n\nfrom pathlib import Path\nfrom typing import Dict\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass EpubParser(BaseParser):\n    \"\"\"Epub Parser.\"\"\"\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        return {}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> str:\n        \"\"\"Parse file.\"\"\"\n        try:\n            import ebooklib\n            from ebooklib import epub\n        except ImportError:\n            raise ValueError(\"`EbookLib` is required to read Epub files.\")\n        try:\n            import html2text\n        except ImportError:\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/epub_parser.py", "file_name": "epub_parser.py"}, "index": 17, "child_indices": [], "ref_doc_id": "d04f28177f58cd45629b425083bd5affd6fba442", "node_info": null}, "18": {"text": "    except ImportError:\n            raise ValueError(\"`html2text` is required to parse Epub files.\")\n\n        text_list = []\n        book = epub.read_epub(file, options={\"ignore_ncx\": True})\n\n        # Iterate through all chapters.\n        for item in book.get_items():\n            # Chapters are typically located in epub documents items.\n            if item.get_type() == ebooklib.ITEM_DOCUMENT:\n                text_list.append(\n                    html2text.html2text(item.get_content().decode(\"utf-8\"))\n                )\n\n        text = \"\\n\".join(text_list)\n        return text\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/epub_parser.py", "file_name": "epub_parser.py"}, "index": 18, "child_indices": [], "ref_doc_id": "d04f28177f58cd45629b425083bd5affd6fba442", "node_info": null}, "19": {"text": "\"\"\"Image parser.\n\nContains parsers for image files.\n\n\"\"\"\n\nimport re\nfrom pathlib import Path\nfrom typing import Dict\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass ImageParser(BaseParser):\n    \"\"\"Image parser.\n\n    Extract text from images using DONUT.\n\n    \"\"\"\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        try:\n            import torch  # noqa: F401\n        except ImportError:\n            raise ValueError(\"install pytorch to use the model\")\n        try:\n            from transformers import DonutProcessor, VisionEncoderDecoderModel\n        except ImportError:\n            raise ValueError(\"transformers is required for using DONUT model.\")\n        try:\n          ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/image_parser.py", "file_name": "image_parser.py"}, "index": 19, "child_indices": [], "ref_doc_id": "618f6054d906179da2ba7ae65568378f60173c97", "node_info": null}, "20": {"text": "   try:\n            import sentencepiece  # noqa: F401\n        except ImportError:\n            raise ValueError(\"sentencepiece is required for using DONUT model.\")\n        try:\n            from PIL import Image  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"PIL is required to read image files.\" \"Please run `pip install Pillow`\"\n            )\n\n        processor = DonutProcessor.from_pretrained(\n            \"naver-clova-ix/donut-base-finetuned-cord-v2\"\n        )\n        model = VisionEncoderDecoderModel.from_pretrained(\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/image_parser.py", "file_name": "image_parser.py"}, "index": 20, "child_indices": [], "ref_doc_id": "618f6054d906179da2ba7ae65568378f60173c97", "node_info": null}, "21": {"text": "           \"naver-clova-ix/donut-base-finetuned-cord-v2\"\n        )\n        return {\"processor\": processor, \"model\": model}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> str:\n        \"\"\"Parse file.\"\"\"\n        import torch\n        from PIL import Image\n\n        model = self.parser_config[\"model\"]\n        processor = self.parser_config[\"processor\"]\n\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        model.to(device)\n        # load document image\n        image = Image.open(file)\n        if image.mode != \"RGB\":\n            image = image.convert(\"RGB\")\n\n        # prepare", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/image_parser.py", "file_name": "image_parser.py"}, "index": 21, "child_indices": [], "ref_doc_id": "618f6054d906179da2ba7ae65568378f60173c97", "node_info": null}, "22": {"text": "       # prepare decoder inputs\n        task_prompt = \"<s_cord-v2>\"\n        decoder_input_ids = processor.tokenizer(\n            task_prompt, add_special_tokens=False, return_tensors=\"pt\"\n        ).input_ids\n\n        pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n\n        outputs = model.generate(\n            pixel_values.to(device),\n            decoder_input_ids=decoder_input_ids.to(device),\n            max_length=model.decoder.config.max_position_embeddings,\n            early_stopping=True,\n            pad_token_id=processor.tokenizer.pad_token_id,\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/image_parser.py", "file_name": "image_parser.py"}, "index": 22, "child_indices": [], "ref_doc_id": "618f6054d906179da2ba7ae65568378f60173c97", "node_info": null}, "23": {"text": "           eos_token_id=processor.tokenizer.eos_token_id,\n            use_cache=True,\n            num_beams=1,\n            bad_words_ids=[[processor.tokenizer.unk_token_id]],\n            return_dict_in_generate=True,\n        )\n\n        sequence = processor.batch_decode(outputs.sequences)[0]\n        sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(\n            processor.tokenizer.pad_token, \"\"\n        )\n        # remove first task start token\n        sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()\n\n        return sequence\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/image_parser.py", "file_name": "image_parser.py"}, "index": 23, "child_indices": [], "ref_doc_id": "618f6054d906179da2ba7ae65568378f60173c97", "node_info": null}, "24": {"text": "\"\"\"Markdown parser.\n\nContains parser for md files.\n\n\"\"\"\nimport re\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Tuple, Union, cast\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass MarkdownParser(BaseParser):\n    \"\"\"Markdown parser.\n\n    Extract text from markdown files.\n    Returns dictionary with keys as headers and values as the text between headers.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *args: Any,\n        remove_hyperlinks: bool = True,\n        remove_images: bool = True,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self._remove_hyperlinks = remove_hyperlinks\n        self._remove_images = remove_images\n\n    def", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/markdown_parser.py", "file_name": "markdown_parser.py"}, "index": 24, "child_indices": [], "ref_doc_id": "2785c7a340d0f872e5864e0c02e6ff6f150de538", "node_info": null}, "25": {"text": "  self._remove_images = remove_images\n\n    def markdown_to_tups(self, markdown_text: str) -> List[Tuple[Optional[str], str]]:\n        \"\"\"Convert a markdown file to a dictionary.\n\n        The keys are the headers and the values are the text under each header.\n\n        \"\"\"\n        markdown_tups: List[Tuple[Optional[str], str]] = []\n        lines = markdown_text.split(\"\\n\")\n\n        current_header = None\n        current_text = \"\"\n\n        for line in lines:\n            header_match = re.match(r\"^#+\\s\", line)\n            if header_match:\n                if current_header is not None:\n                    if current_text == \"\"", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/markdown_parser.py", "file_name": "markdown_parser.py"}, "index": 25, "child_indices": [], "ref_doc_id": "2785c7a340d0f872e5864e0c02e6ff6f150de538", "node_info": null}, "26": {"text": "          if current_text == \"\" or None:\n                        continue\n                    markdown_tups.append((current_header, current_text))\n\n                current_header = line\n                current_text = \"\"\n            else:\n                current_text += line + \"\\n\"\n        markdown_tups.append((current_header, current_text))\n\n        if current_header is not None:\n            # pass linting, assert keys are defined\n            markdown_tups = [\n                (re.sub(r\"#\", \"\", cast(str, key)).strip(),", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/markdown_parser.py", "file_name": "markdown_parser.py"}, "index": 26, "child_indices": [], "ref_doc_id": "2785c7a340d0f872e5864e0c02e6ff6f150de538", "node_info": null}, "27": {"text": "\"\", cast(str, key)).strip(), re.sub(r\"<.*?>\", \"\", value))\n                for key, value in markdown_tups\n            ]\n        else:\n            markdown_tups = [\n                (key, re.sub(\"\\n\", \"\", value)) for key, value in markdown_tups\n            ]\n\n        return markdown_tups\n\n    def remove_images(self, content: str) -> str:\n        \"\"\"Get a dictionary of a markdown file from its path.\"\"\"\n        pattern = r\"!{1}\\[\\[(.*)\\]\\]\"\n        content = re.sub(pattern, \"\", content)\n        return content\n\n    def remove_hyperlinks(self, content: str) -> str:\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/markdown_parser.py", "file_name": "markdown_parser.py"}, "index": 27, "child_indices": [], "ref_doc_id": "2785c7a340d0f872e5864e0c02e6ff6f150de538", "node_info": null}, "28": {"text": "content: str) -> str:\n        \"\"\"Get a dictionary of a markdown file from its path.\"\"\"\n        pattern = r\"\\[(.*?)\\]\\((.*?)\\)\"\n        content = re.sub(pattern, r\"\\1\", content)\n        return content\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Initialize the parser with the config.\"\"\"\n        return {}\n\n    def parse_tups(\n        self, filepath: Path, errors: str = \"ignore\"\n    ) -> List[Tuple[Optional[str], str]]:\n        \"\"\"Parse file into tuples.\"\"\"\n        with open(filepath, \"r\") as f:\n            content = f.read()\n        if self._remove_hyperlinks:\n            content = self.remove_hyperlinks(content)\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/markdown_parser.py", "file_name": "markdown_parser.py"}, "index": 28, "child_indices": [], "ref_doc_id": "2785c7a340d0f872e5864e0c02e6ff6f150de538", "node_info": null}, "29": {"text": " content = self.remove_hyperlinks(content)\n        if self._remove_images:\n            content = self.remove_images(content)\n        markdown_tups = self.markdown_to_tups(content)\n        return markdown_tups\n\n    def parse_file(\n        self, filepath: Path, errors: str = \"ignore\"\n    ) -> Union[str, List[str]]:\n        \"\"\"Parse file into string.\"\"\"\n        tups = self.parse_tups(filepath, errors=errors)\n        results = []\n        # TODO: don't include headers right now\n        for header, value in tups:\n            if header is None:\n                results.append(value)\n            else:\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/markdown_parser.py", "file_name": "markdown_parser.py"}, "index": 29, "child_indices": [], "ref_doc_id": "2785c7a340d0f872e5864e0c02e6ff6f150de538", "node_info": null}, "30": {"text": "        else:\n                results.append(f\"\\n\\n{header}\\n{value}\")\n        return results\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/markdown_parser.py", "file_name": "markdown_parser.py"}, "index": 30, "child_indices": [], "ref_doc_id": "2785c7a340d0f872e5864e0c02e6ff6f150de538", "node_info": null}, "31": {"text": "\"\"\"Mbox parser.\n\nContains simple parser for mbox files.\n\n\"\"\"\nfrom pathlib import Path\nfrom typing import Any, Dict, List\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass MboxParser(BaseParser):\n    \"\"\"Mbox parser.\n\n    Extract messages from mailbox files.\n    Returns string including date, subject, sender, receiver and\n    content for each message.\n\n    \"\"\"\n\n    DEFAULT_MESSAGE_FORMAT: str = (\n        \"Date: {_date}\\n\"\n        \"From: {_from}\\n\"\n        \"To: {_to}\\n\"\n        \"Subject: {_subject}\\n\"\n        \"Content: {_content}\"\n    )\n\n    def __init__(\n        self,\n        *args: Any,\n        max_count: int = 0,\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/mbox_parser.py", "file_name": "mbox_parser.py"}, "index": 31, "child_indices": [], "ref_doc_id": "29f2fc552f2d6f0ec85e8fb06255667555ac9e94", "node_info": null}, "32": {"text": "max_count: int = 0,\n        message_format: str = DEFAULT_MESSAGE_FORMAT,\n        **kwargs: Any\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.max_count = max_count\n        self.message_format = message_format\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Initialize parser.\"\"\"\n        try:\n            from bs4 import BeautifulSoup  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`beautifulsoup4` package not found,\"\n                \"please run `pip install beautifulsoup4`\"\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/mbox_parser.py", "file_name": "mbox_parser.py"}, "index": 32, "child_indices": [], "ref_doc_id": "29f2fc552f2d6f0ec85e8fb06255667555ac9e94", "node_info": null}, "33": {"text": "install beautifulsoup4`\"\n            )\n        return {}\n\n    def parse_file(self, filepath: Path, errors: str = \"ignore\") -> List[str]:\n        \"\"\"Parse file into string.\"\"\"\n        # Import required libraries\n        import mailbox\n        from email.parser import BytesParser\n        from email.policy import default\n\n        from bs4 import BeautifulSoup\n\n        i = 0\n        results: List[str] = []\n        # Load file using mailbox\n        bytes_parser = BytesParser(policy=default).parse\n        mbox = mailbox.mbox(filepath, factory=bytes_parser)  # type: ignore\n\n        # Iterate through all messages\n        for _, _msg in enumerate(mbox):\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/mbox_parser.py", "file_name": "mbox_parser.py"}, "index": 33, "child_indices": [], "ref_doc_id": "29f2fc552f2d6f0ec85e8fb06255667555ac9e94", "node_info": null}, "34": {"text": "in enumerate(mbox):\n            msg: mailbox.mboxMessage = _msg\n            # Parse multipart messages\n            if msg.is_multipart():\n                for part in msg.walk():\n                    ctype = part.get_content_type()\n                    cdispo = str(part.get(\"Content-Disposition\"))\n                    if ctype == \"text/plain\" and \"attachment\" not in cdispo:\n                        content = part.get_payload(decode=True)  # decode\n                        break\n            #", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/mbox_parser.py", "file_name": "mbox_parser.py"}, "index": 34, "child_indices": [], "ref_doc_id": "29f2fc552f2d6f0ec85e8fb06255667555ac9e94", "node_info": null}, "35": {"text": "  break\n            # Get plain message payload for non-multipart messages\n            else:\n                content = msg.get_payload(decode=True)\n\n            # Parse message HTML content and remove unneeded whitespace\n            soup = BeautifulSoup(content)\n            stripped_content = \" \".join(soup.get_text().split())\n            # Format message to include date, sender, receiver and subject\n            msg_string = self.message_format.format(\n                _date=msg[\"date\"],\n                _from=msg[\"from\"],\n                _to=msg[\"to\"],\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/mbox_parser.py", "file_name": "mbox_parser.py"}, "index": 35, "child_indices": [], "ref_doc_id": "29f2fc552f2d6f0ec85e8fb06255667555ac9e94", "node_info": null}, "36": {"text": "               _subject=msg[\"subject\"],\n                _content=stripped_content,\n            )\n            # Add message string to results\n            results.append(msg_string)\n            # Increment counter and return if max count is met\n            i += 1\n            if self.max_count > 0 and i >= self.max_count:\n                break\n        return results\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/mbox_parser.py", "file_name": "mbox_parser.py"}, "index": 36, "child_indices": [], "ref_doc_id": "29f2fc552f2d6f0ec85e8fb06255667555ac9e94", "node_info": null}, "37": {"text": "\"\"\"Slides parser.\n\nContains parsers for .pptx files.\n\n\"\"\"\n\nimport os\nfrom pathlib import Path\nfrom typing import Dict\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass PptxParser(BaseParser):\n    \"\"\"Powerpoint parser.\n\n    Extract text, caption images, and specify slides.\n\n    \"\"\"\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        try:\n            from pptx import Presentation  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"The package `python-pptx` is required to read Powerpoint files.\"\n            )\n        try:\n            import torch  # noqa: F401\n        except", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/slides_parser.py", "file_name": "slides_parser.py"}, "index": 37, "child_indices": [], "ref_doc_id": "bf5faad5fc2e97987c1786f59411af5b69411313", "node_info": null}, "38": {"text": " # noqa: F401\n        except ImportError:\n            raise ValueError(\"The package `pytorch` is required to caption images.\")\n        try:\n            from transformers import (\n                AutoTokenizer,\n                VisionEncoderDecoderModel,\n                ViTFeatureExtractor,\n            )\n        except ImportError:\n            raise ValueError(\n                \"The package `transformers` is required to caption images.\"\n            )\n        try:\n            from PIL import Image  # noqa: F401\n        except ImportError:\n         ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/slides_parser.py", "file_name": "slides_parser.py"}, "index": 38, "child_indices": [], "ref_doc_id": "bf5faad5fc2e97987c1786f59411af5b69411313", "node_info": null}, "39": {"text": "  except ImportError:\n            raise ValueError(\n                \"PIL is required to read image files.\" \"Please run `pip install Pillow`\"\n            )\n\n        model = VisionEncoderDecoderModel.from_pretrained(\n            \"nlpconnect/vit-gpt2-image-captioning\"\n        )\n        feature_extractor = ViTFeatureExtractor.from_pretrained(\n            \"nlpconnect/vit-gpt2-image-captioning\"\n        )\n        tokenizer = AutoTokenizer.from_pretrained(\n            \"nlpconnect/vit-gpt2-image-captioning\"\n        )\n\n        return {\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/slides_parser.py", "file_name": "slides_parser.py"}, "index": 39, "child_indices": [], "ref_doc_id": "bf5faad5fc2e97987c1786f59411af5b69411313", "node_info": null}, "40": {"text": "  return {\n            \"feature_extractor\": feature_extractor,\n            \"model\": model,\n            \"tokenizer\": tokenizer,\n        }\n\n    def caption_image(self, tmp_image_file: str) -> str:\n        \"\"\"Generate text caption of image.\"\"\"\n        import torch\n        from PIL import Image\n\n        model = self.parser_config[\"model\"]\n        feature_extractor = self.parser_config[\"feature_extractor\"]\n        tokenizer = self.parser_config[\"tokenizer\"]\n\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        model.to(device)\n\n        max_length = 16\n        num_beams = 4\n        gen_kwargs =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/slides_parser.py", "file_name": "slides_parser.py"}, "index": 40, "child_indices": [], "ref_doc_id": "bf5faad5fc2e97987c1786f59411af5b69411313", "node_info": null}, "41": {"text": "= 4\n        gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n\n        i_image = Image.open(tmp_image_file)\n        if i_image.mode != \"RGB\":\n            i_image = i_image.convert(mode=\"RGB\")\n\n        pixel_values = feature_extractor(\n            images=[i_image], return_tensors=\"pt\"\n        ).pixel_values\n        pixel_values = pixel_values.to(device)\n\n        output_ids = model.generate(pixel_values, **gen_kwargs)\n\n        preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n        return preds[0].strip()\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> str:\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/slides_parser.py", "file_name": "slides_parser.py"}, "index": 41, "child_indices": [], "ref_doc_id": "bf5faad5fc2e97987c1786f59411af5b69411313", "node_info": null}, "42": {"text": "Path, errors: str = \"ignore\") -> str:\n        \"\"\"Parse file.\"\"\"\n        from pptx import Presentation\n\n        presentation = Presentation(file)\n        result = \"\"\n        for i, slide in enumerate(presentation.slides):\n            result += f\"\\n\\nSlide #{i}: \\n\"\n            for shape in slide.shapes:\n                if hasattr(shape, \"image\"):\n                    image = shape.image\n                    # get image \"file\" contents\n                    image_bytes = image.blob\n                    # temporarily save the image to feed into model\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/slides_parser.py", "file_name": "slides_parser.py"}, "index": 42, "child_indices": [], "ref_doc_id": "bf5faad5fc2e97987c1786f59411af5b69411313", "node_info": null}, "43": {"text": "  # temporarily save the image to feed into model\n                    image_filename = f\"tmp_image.{image.ext}\"\n                    with open(image_filename, \"wb\") as f:\n                        f.write(image_bytes)\n                    result += f\"\\n Image: {self.caption_image(image_filename)}\\n\\n\"\n\n                    os.remove(image_filename)\n                if hasattr(shape, \"text\"):\n                    result += f\"{shape.text}\\n\"\n\n        return result\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/slides_parser.py", "file_name": "slides_parser.py"}, "index": 43, "child_indices": [], "ref_doc_id": "bf5faad5fc2e97987c1786f59411af5b69411313", "node_info": null}, "44": {"text": "\"\"\"Tabular parser.\n\nContains parsers for tabular data files.\n\n\"\"\"\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Union\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass CSVParser(BaseParser):\n    \"\"\"CSV parser.\n\n    Args:\n        concat_rows (bool): whether to concatenate all rows into one document.\n            If set to False, a Document will be created for each row.\n            True by default.\n\n    \"\"\"\n\n    def __init__(self, *args: Any, concat_rows: bool = True, **kwargs: Any) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self._concat_rows = concat_rows\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/tabular_parser.py", "file_name": "tabular_parser.py"}, "index": 44, "child_indices": [], "ref_doc_id": "d730452fdec35104b8b57cd3b6f80a982fb35957", "node_info": null}, "45": {"text": "   \"\"\"Init parser.\"\"\"\n        return {}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> Union[str, List[str]]:\n        \"\"\"Parse file.\n\n        Returns:\n            Union[str, List[str]]: a string or a List of strings.\n\n        \"\"\"\n        try:\n            import csv\n        except ImportError:\n            raise ValueError(\"csv module is required to read CSV files.\")\n        text_list = []\n        with open(file, \"r\") as fp:\n            csv_reader = csv.reader(fp)\n            for row in csv_reader:\n                text_list.append(\", \".join(row))\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/tabular_parser.py", "file_name": "tabular_parser.py"}, "index": 45, "child_indices": [], "ref_doc_id": "d730452fdec35104b8b57cd3b6f80a982fb35957", "node_info": null}, "46": {"text": "\".join(row))\n        if self._concat_rows:\n            return \"\\n\".join(text_list)\n        else:\n            return text_list\n\n\nclass PandasCSVParser(BaseParser):\n    r\"\"\"Pandas-based CSV parser.\n\n    Parses CSVs using the separator detection from Pandas `read_csv`function.\n    If special parameters are required, use the `pandas_config` dict.\n\n    Args:\n        concat_rows (bool): whether to concatenate all rows into one document.\n            If set to False, a Document will be created for each row.\n            True by default.\n\n        col_joiner (str): Separator to use for joining cols per row.\n            Set to \", \" by default.\n\n        row_joiner (str):", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/tabular_parser.py", "file_name": "tabular_parser.py"}, "index": 46, "child_indices": [], "ref_doc_id": "d730452fdec35104b8b57cd3b6f80a982fb35957", "node_info": null}, "47": {"text": "       row_joiner (str): Separator to use for joining each row.\n            Only used when `concat_rows=True`.\n            Set to \"\\n\" by default.\n\n        pandas_config (dict): Options for the `pandas.read_csv` function call.\n            Refer to https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n            for more information.\n            Set to empty dict by default, this means pandas will try to figure\n            out the separators, table head, etc. on its own.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *args: Any,\n        concat_rows: bool = True,\n        col_joiner: str = \",", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/tabular_parser.py", "file_name": "tabular_parser.py"}, "index": 47, "child_indices": [], "ref_doc_id": "d730452fdec35104b8b57cd3b6f80a982fb35957", "node_info": null}, "48": {"text": "       col_joiner: str = \", \",\n        row_joiner: str = \"\\n\",\n        pandas_config: dict = {},\n        **kwargs: Any\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self._concat_rows = concat_rows\n        self._col_joiner = col_joiner\n        self._row_joiner = row_joiner\n        self._pandas_config = pandas_config\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        return {}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> Union[str, List[str]]:\n        \"\"\"Parse file.\"\"\"\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/tabular_parser.py", "file_name": "tabular_parser.py"}, "index": 48, "child_indices": [], "ref_doc_id": "d730452fdec35104b8b57cd3b6f80a982fb35957", "node_info": null}, "49": {"text": "  \"\"\"Parse file.\"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ValueError(\"pandas module is required to read CSV files.\")\n\n        df = pd.read_csv(file, **self._pandas_config)\n\n        text_list = df.apply(\n            lambda row: (self._col_joiner).join(row.astype(str).tolist()), axis=1\n        ).tolist()\n\n        if self._concat_rows:\n            return (self._row_joiner).join(text_list)\n        else:\n            return text_list\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/tabular_parser.py", "file_name": "tabular_parser.py"}, "index": 49, "child_indices": [], "ref_doc_id": "d730452fdec35104b8b57cd3b6f80a982fb35957", "node_info": null}, "50": {"text": "\"\"\"Video audio parser.\n\nContains parsers for mp3, mp4 files.\n\n\"\"\"\nfrom pathlib import Path\nfrom typing import Any, Dict, cast\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass VideoAudioParser(BaseParser):\n    \"\"\"Video audio parser.\n\n    Extract text from transcript of video/audio files.\n\n    \"\"\"\n\n    def __init__(self, *args: Any, model_version: str = \"base\", **kwargs: Any) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self._model_version = model_version\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        try:\n            import whisper\n        except ImportError:\n            raise ValueError(\n               ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/video_audio.py", "file_name": "video_audio.py"}, "index": 50, "child_indices": [], "ref_doc_id": "875d5164e96b8249b7c1672175e199266f13d110", "node_info": null}, "51": {"text": "               \"Please install OpenAI whisper model \"\n                \"'pip install git+https://github.com/openai/whisper.git' \"\n                \"to use the model\"\n            )\n\n        model = whisper.load_model(self._model_version)\n\n        return {\"model\": model}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> str:\n        \"\"\"Parse file.\"\"\"\n        import whisper\n\n        if file.name.endswith(\"mp4\"):\n            try:\n                from pydub import AudioSegment  # noqa: F401\n            except ImportError:\n             ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/video_audio.py", "file_name": "video_audio.py"}, "index": 51, "child_indices": [], "ref_doc_id": "875d5164e96b8249b7c1672175e199266f13d110", "node_info": null}, "52": {"text": "               raise ValueError(\"Please install pydub 'pip install pydub' \")\n            # open file\n            video = AudioSegment.from_file(file, format=\"mp4\")\n\n            # Extract audio from video\n            audio = video.split_to_mono()[0]\n\n            file_str = str(file)[:-4] + \".mp3\"\n            # export file\n            audio.export(file_str, format=\"mp3\")\n\n        model = cast(whisper.Whisper, self.parser_config[\"model\"])\n        result = model.transcribe(str(file))\n\n        transcript = result[\"text\"]\n\n        return transcript\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/video_audio.py", "file_name": "video_audio.py"}, "index": 52, "child_indices": [], "ref_doc_id": "875d5164e96b8249b7c1672175e199266f13d110", "node_info": null}, "53": {"text": "The SimpleDirectoryReader class is a BaseReader subclass that reads files of different formats from a directory. It can read files into separate documents, or concatenate them into one document text. It takes in parameters such as input_dir, input_files, exclude_hidden, errors, recursive, required_exts, file_extractor, num_files_limit, and file_metadata. It also has a load_data() method that takes in a concatenate parameter and returns a list of documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 53, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "54": {"text": "This code file contains several classes that are used to parse different types of files, such as PDF, Docx, Epub, and Image files. The BaseParser class is the base class for all parsers and contains methods for initializing the parser and parsing the file. The PDFParser, DocxParser, EpubParser, and ImageParser classes are all subclasses of the BaseParser class and contain methods for parsing the respective file types. Each class contains an _init_parser() method for initializing the parser and a parse_file() method for parsing the file. The ImageParser class also contains methods for loading the DONUT model and extracting text from images.", "doc_id": null, "embedding": null, "extra_info": null, "index": 54, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "55": {"text": "MarkdownParser and MboxParser are two classes that are part of the gpt_index/readers/file/ directory. MarkdownParser is used to extract text from markdown files and returns a dictionary with keys as headers and values as the text between headers. MboxParser is used to extract messages from mailbox files and returns a string including date, subject, sender, receiver and content for each message. Both classes have methods to parse files into strings and tuples, as well as methods to remove hyperlinks and images.", "doc_id": null, "embedding": null, "extra_info": null, "index": 55, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}, "56": {"text": "This code file contains two parsers for tabular data files, a CSVParser and a PandasCSVParser. The CSVParser uses the csv module to read the file and can concatenate all rows into one document or create a document for each row. The PandasCSVParser uses the pandas.read_csv function to read the file and can also concatenate all rows into one document or create a document for each row. It also has additional parameters for specifying the separator, table head, etc. The SlidesParser uses the python-pptx, pytorch, transformers, and PIL packages to extract text, caption images, and specify slides from .pptx files. The MboxParser uses the email package to parse mbox files and extract the subject, content, and message string from each message.", "doc_id": null, "embedding": null, "extra_info": null, "index": 56, "child_indices": [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], "ref_doc_id": null, "node_info": null}, "57": {"text": "The tabular_parser.py and video_audio.py files are part of the gpt_index/readers/file directory. The tabular_parser.py file contains a class that parses a file using the pandas module and returns either a string or a list of strings. The video_audio.py file contains a class that extracts text from the transcript of video/audio files using the OpenAI whisper model.", "doc_id": null, "embedding": null, "extra_info": null, "index": 57, "child_indices": [48, 49, 50, 51, 52], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"53": {"text": "The SimpleDirectoryReader class is a BaseReader subclass that reads files of different formats from a directory. It can read files into separate documents, or concatenate them into one document text. It takes in parameters such as input_dir, input_files, exclude_hidden, errors, recursive, required_exts, file_extractor, num_files_limit, and file_metadata. It also has a load_data() method that takes in a concatenate parameter and returns a list of documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 53, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "54": {"text": "This code file contains several classes that are used to parse different types of files, such as PDF, Docx, Epub, and Image files. The BaseParser class is the base class for all parsers and contains methods for initializing the parser and parsing the file. The PDFParser, DocxParser, EpubParser, and ImageParser classes are all subclasses of the BaseParser class and contain methods for parsing the respective file types. Each class contains an _init_parser() method for initializing the parser and a parse_file() method for parsing the file. The ImageParser class also contains methods for loading the DONUT model and extracting text from images.", "doc_id": null, "embedding": null, "extra_info": null, "index": 54, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "55": {"text": "MarkdownParser and MboxParser are two classes that are part of the gpt_index/readers/file/ directory. MarkdownParser is used to extract text from markdown files and returns a dictionary with keys as headers and values as the text between headers. MboxParser is used to extract messages from mailbox files and returns a string including date, subject, sender, receiver and content for each message. Both classes have methods to parse files into strings and tuples, as well as methods to remove hyperlinks and images.", "doc_id": null, "embedding": null, "extra_info": null, "index": 55, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}, "56": {"text": "This code file contains two parsers for tabular data files, a CSVParser and a PandasCSVParser. The CSVParser uses the csv module to read the file and can concatenate all rows into one document or create a document for each row. The PandasCSVParser uses the pandas.read_csv function to read the file and can also concatenate all rows into one document or create a document for each row. It also has additional parameters for specifying the separator, table head, etc. The SlidesParser uses the python-pptx, pytorch, transformers, and PIL packages to extract text, caption images, and specify slides from .pptx files. The MboxParser uses the email package to parse mbox files and extract the subject, content, and message string from each message.", "doc_id": null, "embedding": null, "extra_info": null, "index": 56, "child_indices": [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], "ref_doc_id": null, "node_info": null}, "57": {"text": "The tabular_parser.py and video_audio.py files are part of the gpt_index/readers/file directory. The tabular_parser.py file contains a class that parses a file using the pandas module and returns either a string or a list of strings. The video_audio.py file contains a class that extracts text from the transcript of video/audio files using the OpenAI whisper model.", "doc_id": null, "embedding": null, "extra_info": null, "index": 57, "child_indices": [48, 49, 50, 51, 52], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"c637335013c599b07de054fba07b47ecb86ad3e8": {"text": "\"\"\"Init params.\"\"\"\n", "doc_id": "c637335013c599b07de054fba07b47ecb86ad3e8", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "d0027a089853c5ff8efcaa11483f67fc0e61b666": {"text": "\"\"\"Simple reader that reads files of different formats from a directory.\"\"\"\nimport logging\nfrom pathlib import Path\nfrom typing import Callable, Dict, List, Optional, Union\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.file.base_parser import BaseParser\nfrom gpt_index.readers.file.docs_parser import DocxParser, PDFParser\nfrom gpt_index.readers.file.epub_parser import EpubParser\nfrom gpt_index.readers.file.image_parser import ImageParser\nfrom gpt_index.readers.file.markdown_parser import MarkdownParser\nfrom gpt_index.readers.file.mbox_parser import MboxParser\nfrom gpt_index.readers.file.slides_parser import PptxParser\nfrom gpt_index.readers.file.tabular_parser import PandasCSVParser\nfrom gpt_index.readers.file.video_audio import VideoAudioParser\nfrom gpt_index.readers.schema.base import Document\n\nDEFAULT_FILE_EXTRACTOR: Dict[str, BaseParser] = {\n    \".pdf\": PDFParser(),\n    \".docx\": DocxParser(),\n    \".pptx\": PptxParser(),\n    \".jpg\": ImageParser(),\n    \".png\": ImageParser(),\n    \".jpeg\": ImageParser(),\n    \".mp3\": VideoAudioParser(),\n    \".mp4\": VideoAudioParser(),\n    \".csv\": PandasCSVParser(),\n    \".epub\": EpubParser(),\n    \".md\": MarkdownParser(),\n    \".mbox\": MboxParser(),\n}\n\n\nclass SimpleDirectoryReader(BaseReader):\n    \"\"\"Simple directory reader.\n\n    Can read files into separate documents, or concatenates\n    files into one document text.\n\n    Args:\n        input_dir (str): Path to the directory.\n        input_files (List): List of file paths to read (Optional; overrides input_dir)\n        exclude_hidden (bool): Whether to exclude hidden files (dotfiles).\n        errors (str): how encoding and decoding errors are to be handled,\n              see https://docs.python.org/3/library/functions.html#open\n        recursive (bool): Whether to recursively search in subdirectories.\n            False by default.\n        required_exts (Optional[List[str]]): List of required extensions.\n            Default is None.\n        file_extractor (Optional[Dict[str, BaseParser]]): A mapping of file\n            extension to a BaseParser class that specifies how to convert that file\n            to text. See DEFAULT_FILE_EXTRACTOR.\n        num_files_limit (Optional[int]): Maximum number of files to read.\n            Default is None.\n        file_metadata (Optional[Callable[str, Dict]]): A function that takes\n            in a filename and returns a Dict of metadata for the Document.\n            Default is None.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dir: Optional[str] = None,\n        input_files: Optional[List] = None,\n        exclude_hidden: bool = True,\n        errors: str = \"ignore\",\n        recursive: bool = False,\n        required_exts: Optional[List[str]] = None,\n        file_extractor: Optional[Dict[str, BaseParser]] = None,\n        num_files_limit: Optional[int] = None,\n        file_metadata: Optional[Callable[[str], Dict]] = None,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        super().__init__()\n\n        if not input_dir and not input_files:\n            raise ValueError(\"Must provide either `input_dir` or `input_files`.\")\n\n        self.errors = errors\n\n        self.recursive = recursive\n        self.exclude_hidden = exclude_hidden\n        self.required_exts = required_exts\n        self.num_files_limit = num_files_limit\n\n        if input_files:\n            self.input_files = []\n            for path in input_files:\n                input_file = Path(path)\n                self.input_files.append(input_file)\n        elif input_dir:\n            self.input_dir = Path(input_dir)\n            self.input_files = self._add_files(self.input_dir)\n\n        self.file_extractor = file_extractor or DEFAULT_FILE_EXTRACTOR\n        self.file_metadata = file_metadata\n\n    def _add_files(self, input_dir: Path) -> List[Path]:\n        \"\"\"Add files.\"\"\"\n        input_files = sorted(input_dir.iterdir())\n        new_input_files = []\n        dirs_to_explore = []\n        for input_file in input_files:\n            if input_file.is_dir():\n                if self.recursive:\n                    dirs_to_explore.append(input_file)\n            elif self.exclude_hidden and input_file.name.startswith(\".\"):\n                continue\n            elif (\n                self.required_exts is not None\n                and input_file.suffix not in self.required_exts\n            ):\n                continue\n            else:\n                new_input_files.append(input_file)\n\n        for dir_to_explore in dirs_to_explore:\n            sub_input_files = self._add_files(dir_to_explore)\n            new_input_files.extend(sub_input_files)\n\n        if self.num_files_limit is not None and self.num_files_limit > 0:\n            new_input_files = new_input_files[0 : self.num_files_limit]\n\n        # print total number of files added\n        logging.debug(\n            f\"> [SimpleDirectoryReader] Total files added: {len(new_input_files)}\"\n        )\n\n        return new_input_files\n\n    def load_data(self, concatenate: bool = False) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            concatenate (bool): whether to concatenate all files into one document.\n                If set to True, file metadata is ignored.\n                False by default.\n\n        Returns:\n            List[Document]: A list of documents.\n\n        \"\"\"\n        data: Union[str, List[str]] = \"\"\n        data_list: List[str] = []\n        metadata_list = []\n        for input_file in self.input_files:\n            if input_file.suffix in self.file_extractor:\n                parser = self.file_extractor[input_file.suffix]\n                if not parser.parser_config_set:\n                    parser.init_parser()\n                data = parser.parse_file(input_file, errors=self.errors)\n            else:\n                # do standard read\n                with open(input_file, \"r\", errors=self.errors) as f:\n                    data = f.read()\n            if isinstance(data, List):\n                data_list.extend(data)\n            else:\n                data_list.append(str(data))\n            if self.file_metadata is not None:\n                metadata_list.append(self.file_metadata(str(input_file)))\n\n        if concatenate:\n            return [Document(\"\\n\".join(data_list))]\n        elif self.file_metadata is not None:\n            return [Document(d, extra_info=m) for d, m in zip(data_list, metadata_list)]\n        else:\n            return [Document(d) for d in data_list]\n", "doc_id": "d0027a089853c5ff8efcaa11483f67fc0e61b666", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base.py", "file_name": "base.py"}, "__type__": "Document"}, "753a56f9797432f053fb96a72bdb782a2b20bd05": {"text": "\"\"\"Base parser and config class.\"\"\"\n\nfrom abc import abstractmethod\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Union\n\n\nclass BaseParser:\n    \"\"\"Base class for all parsers.\"\"\"\n\n    def __init__(self, parser_config: Optional[Dict] = None):\n        \"\"\"Init params.\"\"\"\n        self._parser_config = parser_config\n\n    def init_parser(self) -> None:\n        \"\"\"Init parser and store it.\"\"\"\n        parser_config = self._init_parser()\n        self._parser_config = parser_config\n\n    @property\n    def parser_config_set(self) -> bool:\n        \"\"\"Check if parser config is set.\"\"\"\n        return self._parser_config is not None\n\n    @property\n    def parser_config(self) -> Dict:\n        \"\"\"Check if parser config is set.\"\"\"\n        if self._parser_config is None:\n            raise ValueError(\"Parser config not set.\")\n        return self._parser_config\n\n    @abstractmethod\n    def _init_parser(self) -> Dict:\n        \"\"\"Initialize the parser with the config.\"\"\"\n\n    @abstractmethod\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> Union[str, List[str]]:\n        \"\"\"Parse file.\"\"\"\n", "doc_id": "753a56f9797432f053fb96a72bdb782a2b20bd05", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base_parser.py", "file_name": "base_parser.py"}, "__type__": "Document"}, "ac3dce9de6352d92aa69fdf67294da228f5085c3": {"text": "\"\"\"Docs parser.\n\nContains parsers for docx, pdf files.\n\n\"\"\"\nfrom pathlib import Path\nfrom typing import Dict\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass PDFParser(BaseParser):\n    \"\"\"PDF parser.\"\"\"\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        return {}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> str:\n        \"\"\"Parse file.\"\"\"\n        try:\n            import PyPDF2\n        except ImportError:\n            raise ValueError(\"PyPDF2 is required to read PDF files.\")\n        text_list = []\n        with open(file, \"rb\") as fp:\n            # Create a PDF object\n            pdf = PyPDF2.PdfReader(fp)\n\n            # Get the number of pages in the PDF document\n            num_pages = len(pdf.pages)\n\n            # Iterate over every page\n            for page in range(num_pages):\n                # Extract the text from the page\n                page_text = pdf.pages[page].extract_text()\n                text_list.append(page_text)\n        text = \"\\n\".join(text_list)\n\n        return text\n\n\nclass DocxParser(BaseParser):\n    \"\"\"Docx parser.\"\"\"\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        return {}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> str:\n        \"\"\"Parse file.\"\"\"\n        try:\n            import docx2txt\n        except ImportError:\n            raise ValueError(\"docx2txt is required to read Microsoft Word files.\")\n\n        text = docx2txt.process(file)\n\n        return text\n", "doc_id": "ac3dce9de6352d92aa69fdf67294da228f5085c3", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/docs_parser.py", "file_name": "docs_parser.py"}, "__type__": "Document"}, "d04f28177f58cd45629b425083bd5affd6fba442": {"text": "\"\"\"Epub parser.\n\nContains parsers for epub files.\n\"\"\"\n\nfrom pathlib import Path\nfrom typing import Dict\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass EpubParser(BaseParser):\n    \"\"\"Epub Parser.\"\"\"\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        return {}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> str:\n        \"\"\"Parse file.\"\"\"\n        try:\n            import ebooklib\n            from ebooklib import epub\n        except ImportError:\n            raise ValueError(\"`EbookLib` is required to read Epub files.\")\n        try:\n            import html2text\n        except ImportError:\n            raise ValueError(\"`html2text` is required to parse Epub files.\")\n\n        text_list = []\n        book = epub.read_epub(file, options={\"ignore_ncx\": True})\n\n        # Iterate through all chapters.\n        for item in book.get_items():\n            # Chapters are typically located in epub documents items.\n            if item.get_type() == ebooklib.ITEM_DOCUMENT:\n                text_list.append(\n                    html2text.html2text(item.get_content().decode(\"utf-8\"))\n                )\n\n        text = \"\\n\".join(text_list)\n        return text\n", "doc_id": "d04f28177f58cd45629b425083bd5affd6fba442", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/epub_parser.py", "file_name": "epub_parser.py"}, "__type__": "Document"}, "618f6054d906179da2ba7ae65568378f60173c97": {"text": "\"\"\"Image parser.\n\nContains parsers for image files.\n\n\"\"\"\n\nimport re\nfrom pathlib import Path\nfrom typing import Dict\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass ImageParser(BaseParser):\n    \"\"\"Image parser.\n\n    Extract text from images using DONUT.\n\n    \"\"\"\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        try:\n            import torch  # noqa: F401\n        except ImportError:\n            raise ValueError(\"install pytorch to use the model\")\n        try:\n            from transformers import DonutProcessor, VisionEncoderDecoderModel\n        except ImportError:\n            raise ValueError(\"transformers is required for using DONUT model.\")\n        try:\n            import sentencepiece  # noqa: F401\n        except ImportError:\n            raise ValueError(\"sentencepiece is required for using DONUT model.\")\n        try:\n            from PIL import Image  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"PIL is required to read image files.\" \"Please run `pip install Pillow`\"\n            )\n\n        processor = DonutProcessor.from_pretrained(\n            \"naver-clova-ix/donut-base-finetuned-cord-v2\"\n        )\n        model = VisionEncoderDecoderModel.from_pretrained(\n            \"naver-clova-ix/donut-base-finetuned-cord-v2\"\n        )\n        return {\"processor\": processor, \"model\": model}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> str:\n        \"\"\"Parse file.\"\"\"\n        import torch\n        from PIL import Image\n\n        model = self.parser_config[\"model\"]\n        processor = self.parser_config[\"processor\"]\n\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        model.to(device)\n        # load document image\n        image = Image.open(file)\n        if image.mode != \"RGB\":\n            image = image.convert(\"RGB\")\n\n        # prepare decoder inputs\n        task_prompt = \"<s_cord-v2>\"\n        decoder_input_ids = processor.tokenizer(\n            task_prompt, add_special_tokens=False, return_tensors=\"pt\"\n        ).input_ids\n\n        pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n\n        outputs = model.generate(\n            pixel_values.to(device),\n            decoder_input_ids=decoder_input_ids.to(device),\n            max_length=model.decoder.config.max_position_embeddings,\n            early_stopping=True,\n            pad_token_id=processor.tokenizer.pad_token_id,\n            eos_token_id=processor.tokenizer.eos_token_id,\n            use_cache=True,\n            num_beams=1,\n            bad_words_ids=[[processor.tokenizer.unk_token_id]],\n            return_dict_in_generate=True,\n        )\n\n        sequence = processor.batch_decode(outputs.sequences)[0]\n        sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(\n            processor.tokenizer.pad_token, \"\"\n        )\n        # remove first task start token\n        sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()\n\n        return sequence\n", "doc_id": "618f6054d906179da2ba7ae65568378f60173c97", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/image_parser.py", "file_name": "image_parser.py"}, "__type__": "Document"}, "2785c7a340d0f872e5864e0c02e6ff6f150de538": {"text": "\"\"\"Markdown parser.\n\nContains parser for md files.\n\n\"\"\"\nimport re\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Tuple, Union, cast\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass MarkdownParser(BaseParser):\n    \"\"\"Markdown parser.\n\n    Extract text from markdown files.\n    Returns dictionary with keys as headers and values as the text between headers.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *args: Any,\n        remove_hyperlinks: bool = True,\n        remove_images: bool = True,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self._remove_hyperlinks = remove_hyperlinks\n        self._remove_images = remove_images\n\n    def markdown_to_tups(self, markdown_text: str) -> List[Tuple[Optional[str], str]]:\n        \"\"\"Convert a markdown file to a dictionary.\n\n        The keys are the headers and the values are the text under each header.\n\n        \"\"\"\n        markdown_tups: List[Tuple[Optional[str], str]] = []\n        lines = markdown_text.split(\"\\n\")\n\n        current_header = None\n        current_text = \"\"\n\n        for line in lines:\n            header_match = re.match(r\"^#+\\s\", line)\n            if header_match:\n                if current_header is not None:\n                    if current_text == \"\" or None:\n                        continue\n                    markdown_tups.append((current_header, current_text))\n\n                current_header = line\n                current_text = \"\"\n            else:\n                current_text += line + \"\\n\"\n        markdown_tups.append((current_header, current_text))\n\n        if current_header is not None:\n            # pass linting, assert keys are defined\n            markdown_tups = [\n                (re.sub(r\"#\", \"\", cast(str, key)).strip(), re.sub(r\"<.*?>\", \"\", value))\n                for key, value in markdown_tups\n            ]\n        else:\n            markdown_tups = [\n                (key, re.sub(\"\\n\", \"\", value)) for key, value in markdown_tups\n            ]\n\n        return markdown_tups\n\n    def remove_images(self, content: str) -> str:\n        \"\"\"Get a dictionary of a markdown file from its path.\"\"\"\n        pattern = r\"!{1}\\[\\[(.*)\\]\\]\"\n        content = re.sub(pattern, \"\", content)\n        return content\n\n    def remove_hyperlinks(self, content: str) -> str:\n        \"\"\"Get a dictionary of a markdown file from its path.\"\"\"\n        pattern = r\"\\[(.*?)\\]\\((.*?)\\)\"\n        content = re.sub(pattern, r\"\\1\", content)\n        return content\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Initialize the parser with the config.\"\"\"\n        return {}\n\n    def parse_tups(\n        self, filepath: Path, errors: str = \"ignore\"\n    ) -> List[Tuple[Optional[str], str]]:\n        \"\"\"Parse file into tuples.\"\"\"\n        with open(filepath, \"r\") as f:\n            content = f.read()\n        if self._remove_hyperlinks:\n            content = self.remove_hyperlinks(content)\n        if self._remove_images:\n            content = self.remove_images(content)\n        markdown_tups = self.markdown_to_tups(content)\n        return markdown_tups\n\n    def parse_file(\n        self, filepath: Path, errors: str = \"ignore\"\n    ) -> Union[str, List[str]]:\n        \"\"\"Parse file into string.\"\"\"\n        tups = self.parse_tups(filepath, errors=errors)\n        results = []\n        # TODO: don't include headers right now\n        for header, value in tups:\n            if header is None:\n                results.append(value)\n            else:\n                results.append(f\"\\n\\n{header}\\n{value}\")\n        return results\n", "doc_id": "2785c7a340d0f872e5864e0c02e6ff6f150de538", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/markdown_parser.py", "file_name": "markdown_parser.py"}, "__type__": "Document"}, "29f2fc552f2d6f0ec85e8fb06255667555ac9e94": {"text": "\"\"\"Mbox parser.\n\nContains simple parser for mbox files.\n\n\"\"\"\nfrom pathlib import Path\nfrom typing import Any, Dict, List\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass MboxParser(BaseParser):\n    \"\"\"Mbox parser.\n\n    Extract messages from mailbox files.\n    Returns string including date, subject, sender, receiver and\n    content for each message.\n\n    \"\"\"\n\n    DEFAULT_MESSAGE_FORMAT: str = (\n        \"Date: {_date}\\n\"\n        \"From: {_from}\\n\"\n        \"To: {_to}\\n\"\n        \"Subject: {_subject}\\n\"\n        \"Content: {_content}\"\n    )\n\n    def __init__(\n        self,\n        *args: Any,\n        max_count: int = 0,\n        message_format: str = DEFAULT_MESSAGE_FORMAT,\n        **kwargs: Any\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.max_count = max_count\n        self.message_format = message_format\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Initialize parser.\"\"\"\n        try:\n            from bs4 import BeautifulSoup  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`beautifulsoup4` package not found,\"\n                \"please run `pip install beautifulsoup4`\"\n            )\n        return {}\n\n    def parse_file(self, filepath: Path, errors: str = \"ignore\") -> List[str]:\n        \"\"\"Parse file into string.\"\"\"\n        # Import required libraries\n        import mailbox\n        from email.parser import BytesParser\n        from email.policy import default\n\n        from bs4 import BeautifulSoup\n\n        i = 0\n        results: List[str] = []\n        # Load file using mailbox\n        bytes_parser = BytesParser(policy=default).parse\n        mbox = mailbox.mbox(filepath, factory=bytes_parser)  # type: ignore\n\n        # Iterate through all messages\n        for _, _msg in enumerate(mbox):\n            msg: mailbox.mboxMessage = _msg\n            # Parse multipart messages\n            if msg.is_multipart():\n                for part in msg.walk():\n                    ctype = part.get_content_type()\n                    cdispo = str(part.get(\"Content-Disposition\"))\n                    if ctype == \"text/plain\" and \"attachment\" not in cdispo:\n                        content = part.get_payload(decode=True)  # decode\n                        break\n            # Get plain message payload for non-multipart messages\n            else:\n                content = msg.get_payload(decode=True)\n\n            # Parse message HTML content and remove unneeded whitespace\n            soup = BeautifulSoup(content)\n            stripped_content = \" \".join(soup.get_text().split())\n            # Format message to include date, sender, receiver and subject\n            msg_string = self.message_format.format(\n                _date=msg[\"date\"],\n                _from=msg[\"from\"],\n                _to=msg[\"to\"],\n                _subject=msg[\"subject\"],\n                _content=stripped_content,\n            )\n            # Add message string to results\n            results.append(msg_string)\n            # Increment counter and return if max count is met\n            i += 1\n            if self.max_count > 0 and i >= self.max_count:\n                break\n        return results\n", "doc_id": "29f2fc552f2d6f0ec85e8fb06255667555ac9e94", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/mbox_parser.py", "file_name": "mbox_parser.py"}, "__type__": "Document"}, "bf5faad5fc2e97987c1786f59411af5b69411313": {"text": "\"\"\"Slides parser.\n\nContains parsers for .pptx files.\n\n\"\"\"\n\nimport os\nfrom pathlib import Path\nfrom typing import Dict\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass PptxParser(BaseParser):\n    \"\"\"Powerpoint parser.\n\n    Extract text, caption images, and specify slides.\n\n    \"\"\"\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        try:\n            from pptx import Presentation  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"The package `python-pptx` is required to read Powerpoint files.\"\n            )\n        try:\n            import torch  # noqa: F401\n        except ImportError:\n            raise ValueError(\"The package `pytorch` is required to caption images.\")\n        try:\n            from transformers import (\n                AutoTokenizer,\n                VisionEncoderDecoderModel,\n                ViTFeatureExtractor,\n            )\n        except ImportError:\n            raise ValueError(\n                \"The package `transformers` is required to caption images.\"\n            )\n        try:\n            from PIL import Image  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"PIL is required to read image files.\" \"Please run `pip install Pillow`\"\n            )\n\n        model = VisionEncoderDecoderModel.from_pretrained(\n            \"nlpconnect/vit-gpt2-image-captioning\"\n        )\n        feature_extractor = ViTFeatureExtractor.from_pretrained(\n            \"nlpconnect/vit-gpt2-image-captioning\"\n        )\n        tokenizer = AutoTokenizer.from_pretrained(\n            \"nlpconnect/vit-gpt2-image-captioning\"\n        )\n\n        return {\n            \"feature_extractor\": feature_extractor,\n            \"model\": model,\n            \"tokenizer\": tokenizer,\n        }\n\n    def caption_image(self, tmp_image_file: str) -> str:\n        \"\"\"Generate text caption of image.\"\"\"\n        import torch\n        from PIL import Image\n\n        model = self.parser_config[\"model\"]\n        feature_extractor = self.parser_config[\"feature_extractor\"]\n        tokenizer = self.parser_config[\"tokenizer\"]\n\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        model.to(device)\n\n        max_length = 16\n        num_beams = 4\n        gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n\n        i_image = Image.open(tmp_image_file)\n        if i_image.mode != \"RGB\":\n            i_image = i_image.convert(mode=\"RGB\")\n\n        pixel_values = feature_extractor(\n            images=[i_image], return_tensors=\"pt\"\n        ).pixel_values\n        pixel_values = pixel_values.to(device)\n\n        output_ids = model.generate(pixel_values, **gen_kwargs)\n\n        preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n        return preds[0].strip()\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> str:\n        \"\"\"Parse file.\"\"\"\n        from pptx import Presentation\n\n        presentation = Presentation(file)\n        result = \"\"\n        for i, slide in enumerate(presentation.slides):\n            result += f\"\\n\\nSlide #{i}: \\n\"\n            for shape in slide.shapes:\n                if hasattr(shape, \"image\"):\n                    image = shape.image\n                    # get image \"file\" contents\n                    image_bytes = image.blob\n                    # temporarily save the image to feed into model\n                    image_filename = f\"tmp_image.{image.ext}\"\n                    with open(image_filename, \"wb\") as f:\n                        f.write(image_bytes)\n                    result += f\"\\n Image: {self.caption_image(image_filename)}\\n\\n\"\n\n                    os.remove(image_filename)\n                if hasattr(shape, \"text\"):\n                    result += f\"{shape.text}\\n\"\n\n        return result\n", "doc_id": "bf5faad5fc2e97987c1786f59411af5b69411313", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/slides_parser.py", "file_name": "slides_parser.py"}, "__type__": "Document"}, "d730452fdec35104b8b57cd3b6f80a982fb35957": {"text": "\"\"\"Tabular parser.\n\nContains parsers for tabular data files.\n\n\"\"\"\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Union\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass CSVParser(BaseParser):\n    \"\"\"CSV parser.\n\n    Args:\n        concat_rows (bool): whether to concatenate all rows into one document.\n            If set to False, a Document will be created for each row.\n            True by default.\n\n    \"\"\"\n\n    def __init__(self, *args: Any, concat_rows: bool = True, **kwargs: Any) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self._concat_rows = concat_rows\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        return {}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> Union[str, List[str]]:\n        \"\"\"Parse file.\n\n        Returns:\n            Union[str, List[str]]: a string or a List of strings.\n\n        \"\"\"\n        try:\n            import csv\n        except ImportError:\n            raise ValueError(\"csv module is required to read CSV files.\")\n        text_list = []\n        with open(file, \"r\") as fp:\n            csv_reader = csv.reader(fp)\n            for row in csv_reader:\n                text_list.append(\", \".join(row))\n        if self._concat_rows:\n            return \"\\n\".join(text_list)\n        else:\n            return text_list\n\n\nclass PandasCSVParser(BaseParser):\n    r\"\"\"Pandas-based CSV parser.\n\n    Parses CSVs using the separator detection from Pandas `read_csv`function.\n    If special parameters are required, use the `pandas_config` dict.\n\n    Args:\n        concat_rows (bool): whether to concatenate all rows into one document.\n            If set to False, a Document will be created for each row.\n            True by default.\n\n        col_joiner (str): Separator to use for joining cols per row.\n            Set to \", \" by default.\n\n        row_joiner (str): Separator to use for joining each row.\n            Only used when `concat_rows=True`.\n            Set to \"\\n\" by default.\n\n        pandas_config (dict): Options for the `pandas.read_csv` function call.\n            Refer to https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n            for more information.\n            Set to empty dict by default, this means pandas will try to figure\n            out the separators, table head, etc. on its own.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *args: Any,\n        concat_rows: bool = True,\n        col_joiner: str = \", \",\n        row_joiner: str = \"\\n\",\n        pandas_config: dict = {},\n        **kwargs: Any\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self._concat_rows = concat_rows\n        self._col_joiner = col_joiner\n        self._row_joiner = row_joiner\n        self._pandas_config = pandas_config\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        return {}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> Union[str, List[str]]:\n        \"\"\"Parse file.\"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ValueError(\"pandas module is required to read CSV files.\")\n\n        df = pd.read_csv(file, **self._pandas_config)\n\n        text_list = df.apply(\n            lambda row: (self._col_joiner).join(row.astype(str).tolist()), axis=1\n        ).tolist()\n\n        if self._concat_rows:\n            return (self._row_joiner).join(text_list)\n        else:\n            return text_list\n", "doc_id": "d730452fdec35104b8b57cd3b6f80a982fb35957", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/tabular_parser.py", "file_name": "tabular_parser.py"}, "__type__": "Document"}, "875d5164e96b8249b7c1672175e199266f13d110": {"text": "\"\"\"Video audio parser.\n\nContains parsers for mp3, mp4 files.\n\n\"\"\"\nfrom pathlib import Path\nfrom typing import Any, Dict, cast\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass VideoAudioParser(BaseParser):\n    \"\"\"Video audio parser.\n\n    Extract text from transcript of video/audio files.\n\n    \"\"\"\n\n    def __init__(self, *args: Any, model_version: str = \"base\", **kwargs: Any) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self._model_version = model_version\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        try:\n            import whisper\n        except ImportError:\n            raise ValueError(\n                \"Please install OpenAI whisper model \"\n                \"'pip install git+https://github.com/openai/whisper.git' \"\n                \"to use the model\"\n            )\n\n        model = whisper.load_model(self._model_version)\n\n        return {\"model\": model}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> str:\n        \"\"\"Parse file.\"\"\"\n        import whisper\n\n        if file.name.endswith(\"mp4\"):\n            try:\n                from pydub import AudioSegment  # noqa: F401\n            except ImportError:\n                raise ValueError(\"Please install pydub 'pip install pydub' \")\n            # open file\n            video = AudioSegment.from_file(file, format=\"mp4\")\n\n            # Extract audio from video\n            audio = video.split_to_mono()[0]\n\n            file_str = str(file)[:-4] + \".mp3\"\n            # export file\n            audio.export(file_str, format=\"mp3\")\n\n        model = cast(whisper.Whisper, self.parser_config[\"model\"])\n        result = model.transcribe(str(file))\n\n        transcript = result[\"text\"]\n\n        return transcript\n", "doc_id": "875d5164e96b8249b7c1672175e199266f13d110", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/video_audio.py", "file_name": "video_audio.py"}, "__type__": "Document"}, "8e23fb86-74d7-4bf2-8558-33a11d9d9440": {"text": "\nThe SimpleDirectoryReader class is a simple directory reader that can read files into separate documents or concatenate files into one document text. The BaseParser class is a base class for all parsers and provides methods such as init_parser and parser_config_set. The PDFParser, DocxParser, EpubParser, and ImageParser classes are all subclasses of the BaseParser class and contain methods for parsing the respective file types. The MarkdownParser class is used to extract text from markdown files, and the MboxParser class is used to extract messages from mailbox files. The TabularParser class contains two parsers for tabular data files, and the VideoAudio class extracts text from the transcript of video/audio files.", "doc_id": "8e23fb86-74d7-4bf2-8558-33a11d9d9440", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Init params.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "c637335013c599b07de054fba07b47ecb86ad3e8", "node_info": null}, "1": {"text": "\"\"\"Simple reader that reads files of different formats from a directory.\"\"\"\nimport logging\nfrom pathlib import Path\nfrom typing import Callable, Dict, List, Optional, Union\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.file.base_parser import BaseParser\nfrom gpt_index.readers.file.docs_parser import DocxParser, PDFParser\nfrom gpt_index.readers.file.epub_parser import EpubParser\nfrom gpt_index.readers.file.image_parser import ImageParser\nfrom gpt_index.readers.file.markdown_parser import MarkdownParser\nfrom gpt_index.readers.file.mbox_parser import MboxParser\nfrom gpt_index.readers.file.slides_parser import PptxParser\nfrom gpt_index.readers.file.tabular_parser import PandasCSVParser\nfrom gpt_index.readers.file.video_audio import VideoAudioParser\nfrom gpt_index.readers.schema.base import Document\n\nDEFAULT_FILE_EXTRACTOR: Dict[str, BaseParser] = {\n    \".pdf\": PDFParser(),\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base.py", "file_name": "base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "d0027a089853c5ff8efcaa11483f67fc0e61b666", "node_info": null}, "2": {"text": "= {\n    \".pdf\": PDFParser(),\n    \".docx\": DocxParser(),\n    \".pptx\": PptxParser(),\n    \".jpg\": ImageParser(),\n    \".png\": ImageParser(),\n    \".jpeg\": ImageParser(),\n    \".mp3\": VideoAudioParser(),\n    \".mp4\": VideoAudioParser(),\n    \".csv\": PandasCSVParser(),\n    \".epub\": EpubParser(),\n    \".md\": MarkdownParser(),\n    \".mbox\": MboxParser(),\n}\n\n\nclass SimpleDirectoryReader(BaseReader):\n    \"\"\"Simple directory reader.\n\n    Can read files into separate documents, or concatenates\n    files into one document text.\n\n    Args:\n        input_dir (str): Path to the directory.\n        input_files (List): List of file paths to read (Optional; overrides input_dir)\n        exclude_hidden (bool): Whether to exclude hidden files (dotfiles).\n        errors (str): how encoding and", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base.py", "file_name": "base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "d0027a089853c5ff8efcaa11483f67fc0e61b666", "node_info": null}, "3": {"text": "       errors (str): how encoding and decoding errors are to be handled,\n              see https://docs.python.org/3/library/functions.html#open\n        recursive (bool): Whether to recursively search in subdirectories.\n            False by default.\n        required_exts (Optional[List[str]]): List of required extensions.\n            Default is None.\n        file_extractor (Optional[Dict[str, BaseParser]]): A mapping of file\n            extension to a BaseParser class that specifies how to convert that file\n            to text. See DEFAULT_FILE_EXTRACTOR.\n        num_files_limit (Optional[int]): Maximum number of files to read.\n            Default is None.\n        file_metadata (Optional[Callable[str, Dict]]): A", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base.py", "file_name": "base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "d0027a089853c5ff8efcaa11483f67fc0e61b666", "node_info": null}, "4": {"text": "file_metadata (Optional[Callable[str, Dict]]): A function that takes\n            in a filename and returns a Dict of metadata for the Document.\n            Default is None.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dir: Optional[str] = None,\n        input_files: Optional[List] = None,\n        exclude_hidden: bool = True,\n        errors: str = \"ignore\",\n        recursive: bool = False,\n        required_exts: Optional[List[str]] = None,\n        file_extractor: Optional[Dict[str, BaseParser]] = None,\n        num_files_limit: Optional[int] = None,\n        file_metadata: Optional[Callable[[str], Dict]] = None,\n    ) -> None:\n        \"\"\"Initialize with", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base.py", "file_name": "base.py"}, "index": 4, "child_indices": [], "ref_doc_id": "d0027a089853c5ff8efcaa11483f67fc0e61b666", "node_info": null}, "5": {"text": ") -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        super().__init__()\n\n        if not input_dir and not input_files:\n            raise ValueError(\"Must provide either `input_dir` or `input_files`.\")\n\n        self.errors = errors\n\n        self.recursive = recursive\n        self.exclude_hidden = exclude_hidden\n        self.required_exts = required_exts\n        self.num_files_limit = num_files_limit\n\n        if input_files:\n            self.input_files = []\n            for path in input_files:\n                input_file = Path(path)\n                self.input_files.append(input_file)\n        elif", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base.py", "file_name": "base.py"}, "index": 5, "child_indices": [], "ref_doc_id": "d0027a089853c5ff8efcaa11483f67fc0e61b666", "node_info": null}, "6": {"text": "       elif input_dir:\n            self.input_dir = Path(input_dir)\n            self.input_files = self._add_files(self.input_dir)\n\n        self.file_extractor = file_extractor or DEFAULT_FILE_EXTRACTOR\n        self.file_metadata = file_metadata\n\n    def _add_files(self, input_dir: Path) -> List[Path]:\n        \"\"\"Add files.\"\"\"\n        input_files = sorted(input_dir.iterdir())\n        new_input_files = []\n        dirs_to_explore = []\n        for input_file in input_files:\n            if input_file.is_dir():\n                if self.recursive:\n                   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base.py", "file_name": "base.py"}, "index": 6, "child_indices": [], "ref_doc_id": "d0027a089853c5ff8efcaa11483f67fc0e61b666", "node_info": null}, "7": {"text": "                dirs_to_explore.append(input_file)\n            elif self.exclude_hidden and input_file.name.startswith(\".\"):\n                continue\n            elif (\n                self.required_exts is not None\n                and input_file.suffix not in self.required_exts\n            ):\n                continue\n            else:\n                new_input_files.append(input_file)\n\n        for dir_to_explore in dirs_to_explore:\n            sub_input_files = self._add_files(dir_to_explore)\n        ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base.py", "file_name": "base.py"}, "index": 7, "child_indices": [], "ref_doc_id": "d0027a089853c5ff8efcaa11483f67fc0e61b666", "node_info": null}, "8": {"text": "           new_input_files.extend(sub_input_files)\n\n        if self.num_files_limit is not None and self.num_files_limit > 0:\n            new_input_files = new_input_files[0 : self.num_files_limit]\n\n        # print total number of files added\n        logging.debug(\n            f\"> [SimpleDirectoryReader] Total files added: {len(new_input_files)}\"\n        )\n\n        return new_input_files\n\n    def load_data(self, concatenate: bool = False) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            concatenate (bool): whether to concatenate all files into one document.\n                If set to True, file metadata is ignored.\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base.py", "file_name": "base.py"}, "index": 8, "child_indices": [], "ref_doc_id": "d0027a089853c5ff8efcaa11483f67fc0e61b666", "node_info": null}, "9": {"text": " If set to True, file metadata is ignored.\n                False by default.\n\n        Returns:\n            List[Document]: A list of documents.\n\n        \"\"\"\n        data: Union[str, List[str]] = \"\"\n        data_list: List[str] = []\n        metadata_list = []\n        for input_file in self.input_files:\n            if input_file.suffix in self.file_extractor:\n                parser = self.file_extractor[input_file.suffix]\n                if not parser.parser_config_set:\n                    parser.init_parser()\n                data = parser.parse_file(input_file,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base.py", "file_name": "base.py"}, "index": 9, "child_indices": [], "ref_doc_id": "d0027a089853c5ff8efcaa11483f67fc0e61b666", "node_info": null}, "10": {"text": "    data = parser.parse_file(input_file, errors=self.errors)\n            else:\n                # do standard read\n                with open(input_file, \"r\", errors=self.errors) as f:\n                    data = f.read()\n            if isinstance(data, List):\n                data_list.extend(data)\n            else:\n                data_list.append(str(data))\n            if self.file_metadata is not None:\n                metadata_list.append(self.file_metadata(str(input_file)))\n\n        if concatenate:\n            return", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base.py", "file_name": "base.py"}, "index": 10, "child_indices": [], "ref_doc_id": "d0027a089853c5ff8efcaa11483f67fc0e61b666", "node_info": null}, "11": {"text": "           return [Document(\"\\n\".join(data_list))]\n        elif self.file_metadata is not None:\n            return [Document(d, extra_info=m) for d, m in zip(data_list, metadata_list)]\n        else:\n            return [Document(d) for d in data_list]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base.py", "file_name": "base.py"}, "index": 11, "child_indices": [], "ref_doc_id": "d0027a089853c5ff8efcaa11483f67fc0e61b666", "node_info": null}, "12": {"text": "\"\"\"Base parser and config class.\"\"\"\n\nfrom abc import abstractmethod\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Union\n\n\nclass BaseParser:\n    \"\"\"Base class for all parsers.\"\"\"\n\n    def __init__(self, parser_config: Optional[Dict] = None):\n        \"\"\"Init params.\"\"\"\n        self._parser_config = parser_config\n\n    def init_parser(self) -> None:\n        \"\"\"Init parser and store it.\"\"\"\n        parser_config = self._init_parser()\n        self._parser_config = parser_config\n\n    @property\n    def parser_config_set(self) -> bool:\n        \"\"\"Check if parser config is set.\"\"\"\n        return self._parser_config is not None\n\n    @property\n    def parser_config(self) -> Dict:\n        \"\"\"Check if parser config is set.\"\"\"\n        if self._parser_config is", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base_parser.py", "file_name": "base_parser.py"}, "index": 12, "child_indices": [], "ref_doc_id": "753a56f9797432f053fb96a72bdb782a2b20bd05", "node_info": null}, "13": {"text": "       if self._parser_config is None:\n            raise ValueError(\"Parser config not set.\")\n        return self._parser_config\n\n    @abstractmethod\n    def _init_parser(self) -> Dict:\n        \"\"\"Initialize the parser with the config.\"\"\"\n\n    @abstractmethod\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> Union[str, List[str]]:\n        \"\"\"Parse file.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base_parser.py", "file_name": "base_parser.py"}, "index": 13, "child_indices": [], "ref_doc_id": "753a56f9797432f053fb96a72bdb782a2b20bd05", "node_info": null}, "14": {"text": "\"\"\"Docs parser.\n\nContains parsers for docx, pdf files.\n\n\"\"\"\nfrom pathlib import Path\nfrom typing import Dict\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass PDFParser(BaseParser):\n    \"\"\"PDF parser.\"\"\"\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        return {}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> str:\n        \"\"\"Parse file.\"\"\"\n        try:\n            import PyPDF2\n        except ImportError:\n            raise ValueError(\"PyPDF2 is required to read PDF files.\")\n        text_list = []\n        with open(file, \"rb\") as fp:\n            # Create a PDF object\n            pdf =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/docs_parser.py", "file_name": "docs_parser.py"}, "index": 14, "child_indices": [], "ref_doc_id": "ac3dce9de6352d92aa69fdf67294da228f5085c3", "node_info": null}, "15": {"text": "PDF object\n            pdf = PyPDF2.PdfReader(fp)\n\n            # Get the number of pages in the PDF document\n            num_pages = len(pdf.pages)\n\n            # Iterate over every page\n            for page in range(num_pages):\n                # Extract the text from the page\n                page_text = pdf.pages[page].extract_text()\n                text_list.append(page_text)\n        text = \"\\n\".join(text_list)\n\n        return text\n\n\nclass DocxParser(BaseParser):\n    \"\"\"Docx parser.\"\"\"\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        return {}\n\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/docs_parser.py", "file_name": "docs_parser.py"}, "index": 15, "child_indices": [], "ref_doc_id": "ac3dce9de6352d92aa69fdf67294da228f5085c3", "node_info": null}, "16": {"text": "       return {}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> str:\n        \"\"\"Parse file.\"\"\"\n        try:\n            import docx2txt\n        except ImportError:\n            raise ValueError(\"docx2txt is required to read Microsoft Word files.\")\n\n        text = docx2txt.process(file)\n\n        return text\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/docs_parser.py", "file_name": "docs_parser.py"}, "index": 16, "child_indices": [], "ref_doc_id": "ac3dce9de6352d92aa69fdf67294da228f5085c3", "node_info": null}, "17": {"text": "\"\"\"Epub parser.\n\nContains parsers for epub files.\n\"\"\"\n\nfrom pathlib import Path\nfrom typing import Dict\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass EpubParser(BaseParser):\n    \"\"\"Epub Parser.\"\"\"\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        return {}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> str:\n        \"\"\"Parse file.\"\"\"\n        try:\n            import ebooklib\n            from ebooklib import epub\n        except ImportError:\n            raise ValueError(\"`EbookLib` is required to read Epub files.\")\n        try:\n            import html2text\n        except ImportError:\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/epub_parser.py", "file_name": "epub_parser.py"}, "index": 17, "child_indices": [], "ref_doc_id": "d04f28177f58cd45629b425083bd5affd6fba442", "node_info": null}, "18": {"text": "    except ImportError:\n            raise ValueError(\"`html2text` is required to parse Epub files.\")\n\n        text_list = []\n        book = epub.read_epub(file, options={\"ignore_ncx\": True})\n\n        # Iterate through all chapters.\n        for item in book.get_items():\n            # Chapters are typically located in epub documents items.\n            if item.get_type() == ebooklib.ITEM_DOCUMENT:\n                text_list.append(\n                    html2text.html2text(item.get_content().decode(\"utf-8\"))\n                )\n\n        text = \"\\n\".join(text_list)\n        return text\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/epub_parser.py", "file_name": "epub_parser.py"}, "index": 18, "child_indices": [], "ref_doc_id": "d04f28177f58cd45629b425083bd5affd6fba442", "node_info": null}, "19": {"text": "\"\"\"Image parser.\n\nContains parsers for image files.\n\n\"\"\"\n\nimport re\nfrom pathlib import Path\nfrom typing import Dict\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass ImageParser(BaseParser):\n    \"\"\"Image parser.\n\n    Extract text from images using DONUT.\n\n    \"\"\"\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        try:\n            import torch  # noqa: F401\n        except ImportError:\n            raise ValueError(\"install pytorch to use the model\")\n        try:\n            from transformers import DonutProcessor, VisionEncoderDecoderModel\n        except ImportError:\n            raise ValueError(\"transformers is required for using DONUT model.\")\n        try:\n          ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/image_parser.py", "file_name": "image_parser.py"}, "index": 19, "child_indices": [], "ref_doc_id": "618f6054d906179da2ba7ae65568378f60173c97", "node_info": null}, "20": {"text": "   try:\n            import sentencepiece  # noqa: F401\n        except ImportError:\n            raise ValueError(\"sentencepiece is required for using DONUT model.\")\n        try:\n            from PIL import Image  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"PIL is required to read image files.\" \"Please run `pip install Pillow`\"\n            )\n\n        processor = DonutProcessor.from_pretrained(\n            \"naver-clova-ix/donut-base-finetuned-cord-v2\"\n        )\n        model = VisionEncoderDecoderModel.from_pretrained(\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/image_parser.py", "file_name": "image_parser.py"}, "index": 20, "child_indices": [], "ref_doc_id": "618f6054d906179da2ba7ae65568378f60173c97", "node_info": null}, "21": {"text": "           \"naver-clova-ix/donut-base-finetuned-cord-v2\"\n        )\n        return {\"processor\": processor, \"model\": model}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> str:\n        \"\"\"Parse file.\"\"\"\n        import torch\n        from PIL import Image\n\n        model = self.parser_config[\"model\"]\n        processor = self.parser_config[\"processor\"]\n\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        model.to(device)\n        # load document image\n        image = Image.open(file)\n        if image.mode != \"RGB\":\n            image = image.convert(\"RGB\")\n\n        # prepare", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/image_parser.py", "file_name": "image_parser.py"}, "index": 21, "child_indices": [], "ref_doc_id": "618f6054d906179da2ba7ae65568378f60173c97", "node_info": null}, "22": {"text": "       # prepare decoder inputs\n        task_prompt = \"<s_cord-v2>\"\n        decoder_input_ids = processor.tokenizer(\n            task_prompt, add_special_tokens=False, return_tensors=\"pt\"\n        ).input_ids\n\n        pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n\n        outputs = model.generate(\n            pixel_values.to(device),\n            decoder_input_ids=decoder_input_ids.to(device),\n            max_length=model.decoder.config.max_position_embeddings,\n            early_stopping=True,\n            pad_token_id=processor.tokenizer.pad_token_id,\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/image_parser.py", "file_name": "image_parser.py"}, "index": 22, "child_indices": [], "ref_doc_id": "618f6054d906179da2ba7ae65568378f60173c97", "node_info": null}, "23": {"text": "           eos_token_id=processor.tokenizer.eos_token_id,\n            use_cache=True,\n            num_beams=1,\n            bad_words_ids=[[processor.tokenizer.unk_token_id]],\n            return_dict_in_generate=True,\n        )\n\n        sequence = processor.batch_decode(outputs.sequences)[0]\n        sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(\n            processor.tokenizer.pad_token, \"\"\n        )\n        # remove first task start token\n        sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()\n\n        return sequence\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/image_parser.py", "file_name": "image_parser.py"}, "index": 23, "child_indices": [], "ref_doc_id": "618f6054d906179da2ba7ae65568378f60173c97", "node_info": null}, "24": {"text": "\"\"\"Markdown parser.\n\nContains parser for md files.\n\n\"\"\"\nimport re\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Tuple, Union, cast\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass MarkdownParser(BaseParser):\n    \"\"\"Markdown parser.\n\n    Extract text from markdown files.\n    Returns dictionary with keys as headers and values as the text between headers.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *args: Any,\n        remove_hyperlinks: bool = True,\n        remove_images: bool = True,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self._remove_hyperlinks = remove_hyperlinks\n        self._remove_images = remove_images\n\n    def", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/markdown_parser.py", "file_name": "markdown_parser.py"}, "index": 24, "child_indices": [], "ref_doc_id": "2785c7a340d0f872e5864e0c02e6ff6f150de538", "node_info": null}, "25": {"text": "  self._remove_images = remove_images\n\n    def markdown_to_tups(self, markdown_text: str) -> List[Tuple[Optional[str], str]]:\n        \"\"\"Convert a markdown file to a dictionary.\n\n        The keys are the headers and the values are the text under each header.\n\n        \"\"\"\n        markdown_tups: List[Tuple[Optional[str], str]] = []\n        lines = markdown_text.split(\"\\n\")\n\n        current_header = None\n        current_text = \"\"\n\n        for line in lines:\n            header_match = re.match(r\"^#+\\s\", line)\n            if header_match:\n                if current_header is not None:\n                    if current_text == \"\"", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/markdown_parser.py", "file_name": "markdown_parser.py"}, "index": 25, "child_indices": [], "ref_doc_id": "2785c7a340d0f872e5864e0c02e6ff6f150de538", "node_info": null}, "26": {"text": "          if current_text == \"\" or None:\n                        continue\n                    markdown_tups.append((current_header, current_text))\n\n                current_header = line\n                current_text = \"\"\n            else:\n                current_text += line + \"\\n\"\n        markdown_tups.append((current_header, current_text))\n\n        if current_header is not None:\n            # pass linting, assert keys are defined\n            markdown_tups = [\n                (re.sub(r\"#\", \"\", cast(str, key)).strip(),", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/markdown_parser.py", "file_name": "markdown_parser.py"}, "index": 26, "child_indices": [], "ref_doc_id": "2785c7a340d0f872e5864e0c02e6ff6f150de538", "node_info": null}, "27": {"text": "\"\", cast(str, key)).strip(), re.sub(r\"<.*?>\", \"\", value))\n                for key, value in markdown_tups\n            ]\n        else:\n            markdown_tups = [\n                (key, re.sub(\"\\n\", \"\", value)) for key, value in markdown_tups\n            ]\n\n        return markdown_tups\n\n    def remove_images(self, content: str) -> str:\n        \"\"\"Get a dictionary of a markdown file from its path.\"\"\"\n        pattern = r\"!{1}\\[\\[(.*)\\]\\]\"\n        content = re.sub(pattern, \"\", content)\n        return content\n\n    def remove_hyperlinks(self, content: str) -> str:\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/markdown_parser.py", "file_name": "markdown_parser.py"}, "index": 27, "child_indices": [], "ref_doc_id": "2785c7a340d0f872e5864e0c02e6ff6f150de538", "node_info": null}, "28": {"text": "content: str) -> str:\n        \"\"\"Get a dictionary of a markdown file from its path.\"\"\"\n        pattern = r\"\\[(.*?)\\]\\((.*?)\\)\"\n        content = re.sub(pattern, r\"\\1\", content)\n        return content\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Initialize the parser with the config.\"\"\"\n        return {}\n\n    def parse_tups(\n        self, filepath: Path, errors: str = \"ignore\"\n    ) -> List[Tuple[Optional[str], str]]:\n        \"\"\"Parse file into tuples.\"\"\"\n        with open(filepath, \"r\") as f:\n            content = f.read()\n        if self._remove_hyperlinks:\n            content = self.remove_hyperlinks(content)\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/markdown_parser.py", "file_name": "markdown_parser.py"}, "index": 28, "child_indices": [], "ref_doc_id": "2785c7a340d0f872e5864e0c02e6ff6f150de538", "node_info": null}, "29": {"text": " content = self.remove_hyperlinks(content)\n        if self._remove_images:\n            content = self.remove_images(content)\n        markdown_tups = self.markdown_to_tups(content)\n        return markdown_tups\n\n    def parse_file(\n        self, filepath: Path, errors: str = \"ignore\"\n    ) -> Union[str, List[str]]:\n        \"\"\"Parse file into string.\"\"\"\n        tups = self.parse_tups(filepath, errors=errors)\n        results = []\n        # TODO: don't include headers right now\n        for header, value in tups:\n            if header is None:\n                results.append(value)\n            else:\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/markdown_parser.py", "file_name": "markdown_parser.py"}, "index": 29, "child_indices": [], "ref_doc_id": "2785c7a340d0f872e5864e0c02e6ff6f150de538", "node_info": null}, "30": {"text": "        else:\n                results.append(f\"\\n\\n{header}\\n{value}\")\n        return results\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/markdown_parser.py", "file_name": "markdown_parser.py"}, "index": 30, "child_indices": [], "ref_doc_id": "2785c7a340d0f872e5864e0c02e6ff6f150de538", "node_info": null}, "31": {"text": "\"\"\"Mbox parser.\n\nContains simple parser for mbox files.\n\n\"\"\"\nfrom pathlib import Path\nfrom typing import Any, Dict, List\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass MboxParser(BaseParser):\n    \"\"\"Mbox parser.\n\n    Extract messages from mailbox files.\n    Returns string including date, subject, sender, receiver and\n    content for each message.\n\n    \"\"\"\n\n    DEFAULT_MESSAGE_FORMAT: str = (\n        \"Date: {_date}\\n\"\n        \"From: {_from}\\n\"\n        \"To: {_to}\\n\"\n        \"Subject: {_subject}\\n\"\n        \"Content: {_content}\"\n    )\n\n    def __init__(\n        self,\n        *args: Any,\n        max_count: int = 0,\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/mbox_parser.py", "file_name": "mbox_parser.py"}, "index": 31, "child_indices": [], "ref_doc_id": "29f2fc552f2d6f0ec85e8fb06255667555ac9e94", "node_info": null}, "32": {"text": "max_count: int = 0,\n        message_format: str = DEFAULT_MESSAGE_FORMAT,\n        **kwargs: Any\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.max_count = max_count\n        self.message_format = message_format\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Initialize parser.\"\"\"\n        try:\n            from bs4 import BeautifulSoup  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`beautifulsoup4` package not found,\"\n                \"please run `pip install beautifulsoup4`\"\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/mbox_parser.py", "file_name": "mbox_parser.py"}, "index": 32, "child_indices": [], "ref_doc_id": "29f2fc552f2d6f0ec85e8fb06255667555ac9e94", "node_info": null}, "33": {"text": "install beautifulsoup4`\"\n            )\n        return {}\n\n    def parse_file(self, filepath: Path, errors: str = \"ignore\") -> List[str]:\n        \"\"\"Parse file into string.\"\"\"\n        # Import required libraries\n        import mailbox\n        from email.parser import BytesParser\n        from email.policy import default\n\n        from bs4 import BeautifulSoup\n\n        i = 0\n        results: List[str] = []\n        # Load file using mailbox\n        bytes_parser = BytesParser(policy=default).parse\n        mbox = mailbox.mbox(filepath, factory=bytes_parser)  # type: ignore\n\n        # Iterate through all messages\n        for _, _msg in enumerate(mbox):\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/mbox_parser.py", "file_name": "mbox_parser.py"}, "index": 33, "child_indices": [], "ref_doc_id": "29f2fc552f2d6f0ec85e8fb06255667555ac9e94", "node_info": null}, "34": {"text": "in enumerate(mbox):\n            msg: mailbox.mboxMessage = _msg\n            # Parse multipart messages\n            if msg.is_multipart():\n                for part in msg.walk():\n                    ctype = part.get_content_type()\n                    cdispo = str(part.get(\"Content-Disposition\"))\n                    if ctype == \"text/plain\" and \"attachment\" not in cdispo:\n                        content = part.get_payload(decode=True)  # decode\n                        break\n            #", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/mbox_parser.py", "file_name": "mbox_parser.py"}, "index": 34, "child_indices": [], "ref_doc_id": "29f2fc552f2d6f0ec85e8fb06255667555ac9e94", "node_info": null}, "35": {"text": "  break\n            # Get plain message payload for non-multipart messages\n            else:\n                content = msg.get_payload(decode=True)\n\n            # Parse message HTML content and remove unneeded whitespace\n            soup = BeautifulSoup(content)\n            stripped_content = \" \".join(soup.get_text().split())\n            # Format message to include date, sender, receiver and subject\n            msg_string = self.message_format.format(\n                _date=msg[\"date\"],\n                _from=msg[\"from\"],\n                _to=msg[\"to\"],\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/mbox_parser.py", "file_name": "mbox_parser.py"}, "index": 35, "child_indices": [], "ref_doc_id": "29f2fc552f2d6f0ec85e8fb06255667555ac9e94", "node_info": null}, "36": {"text": "               _subject=msg[\"subject\"],\n                _content=stripped_content,\n            )\n            # Add message string to results\n            results.append(msg_string)\n            # Increment counter and return if max count is met\n            i += 1\n            if self.max_count > 0 and i >= self.max_count:\n                break\n        return results\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/mbox_parser.py", "file_name": "mbox_parser.py"}, "index": 36, "child_indices": [], "ref_doc_id": "29f2fc552f2d6f0ec85e8fb06255667555ac9e94", "node_info": null}, "37": {"text": "\"\"\"Slides parser.\n\nContains parsers for .pptx files.\n\n\"\"\"\n\nimport os\nfrom pathlib import Path\nfrom typing import Dict\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass PptxParser(BaseParser):\n    \"\"\"Powerpoint parser.\n\n    Extract text, caption images, and specify slides.\n\n    \"\"\"\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        try:\n            from pptx import Presentation  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"The package `python-pptx` is required to read Powerpoint files.\"\n            )\n        try:\n            import torch  # noqa: F401\n        except", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/slides_parser.py", "file_name": "slides_parser.py"}, "index": 37, "child_indices": [], "ref_doc_id": "bf5faad5fc2e97987c1786f59411af5b69411313", "node_info": null}, "38": {"text": " # noqa: F401\n        except ImportError:\n            raise ValueError(\"The package `pytorch` is required to caption images.\")\n        try:\n            from transformers import (\n                AutoTokenizer,\n                VisionEncoderDecoderModel,\n                ViTFeatureExtractor,\n            )\n        except ImportError:\n            raise ValueError(\n                \"The package `transformers` is required to caption images.\"\n            )\n        try:\n            from PIL import Image  # noqa: F401\n        except ImportError:\n         ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/slides_parser.py", "file_name": "slides_parser.py"}, "index": 38, "child_indices": [], "ref_doc_id": "bf5faad5fc2e97987c1786f59411af5b69411313", "node_info": null}, "39": {"text": "  except ImportError:\n            raise ValueError(\n                \"PIL is required to read image files.\" \"Please run `pip install Pillow`\"\n            )\n\n        model = VisionEncoderDecoderModel.from_pretrained(\n            \"nlpconnect/vit-gpt2-image-captioning\"\n        )\n        feature_extractor = ViTFeatureExtractor.from_pretrained(\n            \"nlpconnect/vit-gpt2-image-captioning\"\n        )\n        tokenizer = AutoTokenizer.from_pretrained(\n            \"nlpconnect/vit-gpt2-image-captioning\"\n        )\n\n        return {\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/slides_parser.py", "file_name": "slides_parser.py"}, "index": 39, "child_indices": [], "ref_doc_id": "bf5faad5fc2e97987c1786f59411af5b69411313", "node_info": null}, "40": {"text": "  return {\n            \"feature_extractor\": feature_extractor,\n            \"model\": model,\n            \"tokenizer\": tokenizer,\n        }\n\n    def caption_image(self, tmp_image_file: str) -> str:\n        \"\"\"Generate text caption of image.\"\"\"\n        import torch\n        from PIL import Image\n\n        model = self.parser_config[\"model\"]\n        feature_extractor = self.parser_config[\"feature_extractor\"]\n        tokenizer = self.parser_config[\"tokenizer\"]\n\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        model.to(device)\n\n        max_length = 16\n        num_beams = 4\n        gen_kwargs =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/slides_parser.py", "file_name": "slides_parser.py"}, "index": 40, "child_indices": [], "ref_doc_id": "bf5faad5fc2e97987c1786f59411af5b69411313", "node_info": null}, "41": {"text": "= 4\n        gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n\n        i_image = Image.open(tmp_image_file)\n        if i_image.mode != \"RGB\":\n            i_image = i_image.convert(mode=\"RGB\")\n\n        pixel_values = feature_extractor(\n            images=[i_image], return_tensors=\"pt\"\n        ).pixel_values\n        pixel_values = pixel_values.to(device)\n\n        output_ids = model.generate(pixel_values, **gen_kwargs)\n\n        preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n        return preds[0].strip()\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> str:\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/slides_parser.py", "file_name": "slides_parser.py"}, "index": 41, "child_indices": [], "ref_doc_id": "bf5faad5fc2e97987c1786f59411af5b69411313", "node_info": null}, "42": {"text": "Path, errors: str = \"ignore\") -> str:\n        \"\"\"Parse file.\"\"\"\n        from pptx import Presentation\n\n        presentation = Presentation(file)\n        result = \"\"\n        for i, slide in enumerate(presentation.slides):\n            result += f\"\\n\\nSlide #{i}: \\n\"\n            for shape in slide.shapes:\n                if hasattr(shape, \"image\"):\n                    image = shape.image\n                    # get image \"file\" contents\n                    image_bytes = image.blob\n                    # temporarily save the image to feed into model\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/slides_parser.py", "file_name": "slides_parser.py"}, "index": 42, "child_indices": [], "ref_doc_id": "bf5faad5fc2e97987c1786f59411af5b69411313", "node_info": null}, "43": {"text": "  # temporarily save the image to feed into model\n                    image_filename = f\"tmp_image.{image.ext}\"\n                    with open(image_filename, \"wb\") as f:\n                        f.write(image_bytes)\n                    result += f\"\\n Image: {self.caption_image(image_filename)}\\n\\n\"\n\n                    os.remove(image_filename)\n                if hasattr(shape, \"text\"):\n                    result += f\"{shape.text}\\n\"\n\n        return result\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/slides_parser.py", "file_name": "slides_parser.py"}, "index": 43, "child_indices": [], "ref_doc_id": "bf5faad5fc2e97987c1786f59411af5b69411313", "node_info": null}, "44": {"text": "\"\"\"Tabular parser.\n\nContains parsers for tabular data files.\n\n\"\"\"\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Union\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass CSVParser(BaseParser):\n    \"\"\"CSV parser.\n\n    Args:\n        concat_rows (bool): whether to concatenate all rows into one document.\n            If set to False, a Document will be created for each row.\n            True by default.\n\n    \"\"\"\n\n    def __init__(self, *args: Any, concat_rows: bool = True, **kwargs: Any) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self._concat_rows = concat_rows\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/tabular_parser.py", "file_name": "tabular_parser.py"}, "index": 44, "child_indices": [], "ref_doc_id": "d730452fdec35104b8b57cd3b6f80a982fb35957", "node_info": null}, "45": {"text": "   \"\"\"Init parser.\"\"\"\n        return {}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> Union[str, List[str]]:\n        \"\"\"Parse file.\n\n        Returns:\n            Union[str, List[str]]: a string or a List of strings.\n\n        \"\"\"\n        try:\n            import csv\n        except ImportError:\n            raise ValueError(\"csv module is required to read CSV files.\")\n        text_list = []\n        with open(file, \"r\") as fp:\n            csv_reader = csv.reader(fp)\n            for row in csv_reader:\n                text_list.append(\", \".join(row))\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/tabular_parser.py", "file_name": "tabular_parser.py"}, "index": 45, "child_indices": [], "ref_doc_id": "d730452fdec35104b8b57cd3b6f80a982fb35957", "node_info": null}, "46": {"text": "\".join(row))\n        if self._concat_rows:\n            return \"\\n\".join(text_list)\n        else:\n            return text_list\n\n\nclass PandasCSVParser(BaseParser):\n    r\"\"\"Pandas-based CSV parser.\n\n    Parses CSVs using the separator detection from Pandas `read_csv`function.\n    If special parameters are required, use the `pandas_config` dict.\n\n    Args:\n        concat_rows (bool): whether to concatenate all rows into one document.\n            If set to False, a Document will be created for each row.\n            True by default.\n\n        col_joiner (str): Separator to use for joining cols per row.\n            Set to \", \" by default.\n\n        row_joiner (str):", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/tabular_parser.py", "file_name": "tabular_parser.py"}, "index": 46, "child_indices": [], "ref_doc_id": "d730452fdec35104b8b57cd3b6f80a982fb35957", "node_info": null}, "47": {"text": "       row_joiner (str): Separator to use for joining each row.\n            Only used when `concat_rows=True`.\n            Set to \"\\n\" by default.\n\n        pandas_config (dict): Options for the `pandas.read_csv` function call.\n            Refer to https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n            for more information.\n            Set to empty dict by default, this means pandas will try to figure\n            out the separators, table head, etc. on its own.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *args: Any,\n        concat_rows: bool = True,\n        col_joiner: str = \",", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/tabular_parser.py", "file_name": "tabular_parser.py"}, "index": 47, "child_indices": [], "ref_doc_id": "d730452fdec35104b8b57cd3b6f80a982fb35957", "node_info": null}, "48": {"text": "       col_joiner: str = \", \",\n        row_joiner: str = \"\\n\",\n        pandas_config: dict = {},\n        **kwargs: Any\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self._concat_rows = concat_rows\n        self._col_joiner = col_joiner\n        self._row_joiner = row_joiner\n        self._pandas_config = pandas_config\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        return {}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> Union[str, List[str]]:\n        \"\"\"Parse file.\"\"\"\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/tabular_parser.py", "file_name": "tabular_parser.py"}, "index": 48, "child_indices": [], "ref_doc_id": "d730452fdec35104b8b57cd3b6f80a982fb35957", "node_info": null}, "49": {"text": "  \"\"\"Parse file.\"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ValueError(\"pandas module is required to read CSV files.\")\n\n        df = pd.read_csv(file, **self._pandas_config)\n\n        text_list = df.apply(\n            lambda row: (self._col_joiner).join(row.astype(str).tolist()), axis=1\n        ).tolist()\n\n        if self._concat_rows:\n            return (self._row_joiner).join(text_list)\n        else:\n            return text_list\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/tabular_parser.py", "file_name": "tabular_parser.py"}, "index": 49, "child_indices": [], "ref_doc_id": "d730452fdec35104b8b57cd3b6f80a982fb35957", "node_info": null}, "50": {"text": "\"\"\"Video audio parser.\n\nContains parsers for mp3, mp4 files.\n\n\"\"\"\nfrom pathlib import Path\nfrom typing import Any, Dict, cast\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass VideoAudioParser(BaseParser):\n    \"\"\"Video audio parser.\n\n    Extract text from transcript of video/audio files.\n\n    \"\"\"\n\n    def __init__(self, *args: Any, model_version: str = \"base\", **kwargs: Any) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self._model_version = model_version\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        try:\n            import whisper\n        except ImportError:\n            raise ValueError(\n               ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/video_audio.py", "file_name": "video_audio.py"}, "index": 50, "child_indices": [], "ref_doc_id": "875d5164e96b8249b7c1672175e199266f13d110", "node_info": null}, "51": {"text": "               \"Please install OpenAI whisper model \"\n                \"'pip install git+https://github.com/openai/whisper.git' \"\n                \"to use the model\"\n            )\n\n        model = whisper.load_model(self._model_version)\n\n        return {\"model\": model}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> str:\n        \"\"\"Parse file.\"\"\"\n        import whisper\n\n        if file.name.endswith(\"mp4\"):\n            try:\n                from pydub import AudioSegment  # noqa: F401\n            except ImportError:\n             ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/video_audio.py", "file_name": "video_audio.py"}, "index": 51, "child_indices": [], "ref_doc_id": "875d5164e96b8249b7c1672175e199266f13d110", "node_info": null}, "52": {"text": "               raise ValueError(\"Please install pydub 'pip install pydub' \")\n            # open file\n            video = AudioSegment.from_file(file, format=\"mp4\")\n\n            # Extract audio from video\n            audio = video.split_to_mono()[0]\n\n            file_str = str(file)[:-4] + \".mp3\"\n            # export file\n            audio.export(file_str, format=\"mp3\")\n\n        model = cast(whisper.Whisper, self.parser_config[\"model\"])\n        result = model.transcribe(str(file))\n\n        transcript = result[\"text\"]\n\n        return transcript\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/video_audio.py", "file_name": "video_audio.py"}, "index": 52, "child_indices": [], "ref_doc_id": "875d5164e96b8249b7c1672175e199266f13d110", "node_info": null}, "53": {"text": "The SimpleDirectoryReader class is a BaseReader subclass that reads files of different formats from a directory. It can read files into separate documents, or concatenate them into one document text. It takes in parameters such as input_dir, input_files, exclude_hidden, errors, recursive, required_exts, file_extractor, num_files_limit, and file_metadata. It also has a load_data() method that takes in a concatenate parameter and returns a list of documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 53, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "54": {"text": "This code file contains several classes that are used to parse different types of files, such as PDF, Docx, Epub, and Image files. The BaseParser class is the base class for all parsers and contains methods for initializing the parser and parsing the file. The PDFParser, DocxParser, EpubParser, and ImageParser classes are all subclasses of the BaseParser class and contain methods for parsing the respective file types. Each class contains an _init_parser() method for initializing the parser and a parse_file() method for parsing the file. The ImageParser class also contains methods for loading the DONUT model and extracting text from images.", "doc_id": null, "embedding": null, "extra_info": null, "index": 54, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "55": {"text": "MarkdownParser and MboxParser are two classes that are part of the gpt_index/readers/file/ directory. MarkdownParser is used to extract text from markdown files and returns a dictionary with keys as headers and values as the text between headers. MboxParser is used to extract messages from mailbox files and returns a string including date, subject, sender, receiver and content for each message. Both classes have methods to parse files into strings and tuples, as well as methods to remove hyperlinks and images.", "doc_id": null, "embedding": null, "extra_info": null, "index": 55, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}, "56": {"text": "This code file contains two parsers for tabular data files, a CSVParser and a PandasCSVParser. The CSVParser uses the csv module to read the file and can concatenate all rows into one document or create a document for each row. The PandasCSVParser uses the pandas.read_csv function to read the file and can also concatenate all rows into one document or create a document for each row. It also has additional parameters for specifying the separator, table head, etc. The SlidesParser uses the python-pptx, pytorch, transformers, and PIL packages to extract text, caption images, and specify slides from .pptx files. The MboxParser uses the email package to parse mbox files and extract the subject, content, and message string from each message.", "doc_id": null, "embedding": null, "extra_info": null, "index": 56, "child_indices": [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], "ref_doc_id": null, "node_info": null}, "57": {"text": "The tabular_parser.py and video_audio.py files are part of the gpt_index/readers/file directory. The tabular_parser.py file contains a class that parses a file using the pandas module and returns either a string or a list of strings. The video_audio.py file contains a class that extracts text from the transcript of video/audio files using the OpenAI whisper model.", "doc_id": null, "embedding": null, "extra_info": null, "index": 57, "child_indices": [48, 49, 50, 51, 52], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"53": {"text": "The SimpleDirectoryReader class is a BaseReader subclass that reads files of different formats from a directory. It can read files into separate documents, or concatenate them into one document text. It takes in parameters such as input_dir, input_files, exclude_hidden, errors, recursive, required_exts, file_extractor, num_files_limit, and file_metadata. It also has a load_data() method that takes in a concatenate parameter and returns a list of documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 53, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "54": {"text": "This code file contains several classes that are used to parse different types of files, such as PDF, Docx, Epub, and Image files. The BaseParser class is the base class for all parsers and contains methods for initializing the parser and parsing the file. The PDFParser, DocxParser, EpubParser, and ImageParser classes are all subclasses of the BaseParser class and contain methods for parsing the respective file types. Each class contains an _init_parser() method for initializing the parser and a parse_file() method for parsing the file. The ImageParser class also contains methods for loading the DONUT model and extracting text from images.", "doc_id": null, "embedding": null, "extra_info": null, "index": 54, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "55": {"text": "MarkdownParser and MboxParser are two classes that are part of the gpt_index/readers/file/ directory. MarkdownParser is used to extract text from markdown files and returns a dictionary with keys as headers and values as the text between headers. MboxParser is used to extract messages from mailbox files and returns a string including date, subject, sender, receiver and content for each message. Both classes have methods to parse files into strings and tuples, as well as methods to remove hyperlinks and images.", "doc_id": null, "embedding": null, "extra_info": null, "index": 55, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}, "56": {"text": "This code file contains two parsers for tabular data files, a CSVParser and a PandasCSVParser. The CSVParser uses the csv module to read the file and can concatenate all rows into one document or create a document for each row. The PandasCSVParser uses the pandas.read_csv function to read the file and can also concatenate all rows into one document or create a document for each row. It also has additional parameters for specifying the separator, table head, etc. The SlidesParser uses the python-pptx, pytorch, transformers, and PIL packages to extract text, caption images, and specify slides from .pptx files. The MboxParser uses the email package to parse mbox files and extract the subject, content, and message string from each message.", "doc_id": null, "embedding": null, "extra_info": null, "index": 56, "child_indices": [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], "ref_doc_id": null, "node_info": null}, "57": {"text": "The tabular_parser.py and video_audio.py files are part of the gpt_index/readers/file directory. The tabular_parser.py file contains a class that parses a file using the pandas module and returns either a string or a list of strings. The video_audio.py file contains a class that extracts text from the transcript of video/audio files using the OpenAI whisper model.", "doc_id": null, "embedding": null, "extra_info": null, "index": 57, "child_indices": [48, 49, 50, 51, 52], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}