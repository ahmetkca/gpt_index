{"index_struct": {"text": "\nThe summaries of these documents are that they contain code for accessing the Github API endpoints, dataclasses for the responses from the Github API's getTree, getBlob, getCommit, and getBranch endpoints, and utility functions for the Github readers. The github_repository_reader.py module contains functions for reading files from a Github repository, such as loading data from a commit or branch. The utils.py module contains utility functions for the Github readers, such as a BufferedAsyncIterator class for buffering the results of an async operation, and a BufferedGitBlobDataIterator class for lazily retrieving the contents of files in a Github repository.", "doc_id": "b508aabb-bd02-44ba-859a-96e0194587c7", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "node_info": null}, "1": {"text": "\"\"\"\nGithub API client for the GPT-Index library.\n\nThis module contains the Github API client for the GPT-Index library.\nIt is used by the Github readers to retrieve the data from Github.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional\n\nfrom dataclasses_json import DataClassJsonMixin\n\n\n@dataclass\nclass GitTreeResponseModel(DataClassJsonMixin):\n    \"\"\"\n    Dataclass for the response from the Github API's getTree endpoint.\n\n    Attributes:\n        - sha (str): SHA1 checksum ID of the tree.\n        - url (str): URL for the tree.\n        - tree (List[GitTreeObject]): List of objects in the tree.\n        - truncated (bool): Whether the tree is truncated.\n\n    Examples:\n        >>> tree = client.get_tree(\"owner\", \"repo\", \"branch\")\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 1, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "2": {"text": "\"repo\", \"branch\")\n        >>> tree.sha\n    \"\"\"\n\n    @dataclass\n    class GitTreeObject(DataClassJsonMixin):\n        \"\"\"\n        Dataclass for the objects in the tree.\n\n        Attributes:\n            - path (str): Path to the object.\n            - mode (str): Mode of the object.\n            - type (str): Type of the object.\n            - sha (str): SHA1 checksum ID of the object.\n            - url (str): URL for the object.\n            - size (Optional[int]): Size of the object (only for blobs).\n        \"\"\"\n\n        path: str\n        mode: str\n        type: str\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 2, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "3": {"text": "str\n        type: str\n        sha: str\n        url: str\n        size: Optional[int] = None\n\n    sha: str\n    url: str\n    tree: List[GitTreeObject]\n    truncated: bool\n\n\n@dataclass\nclass GitBlobResponseModel(DataClassJsonMixin):\n    \"\"\"\n    Dataclass for the response from the Github API's getBlob endpoint.\n\n    Attributes:\n        - content (str): Content of the blob.\n        - encoding (str): Encoding of the blob.\n        - url (str): URL for the blob.\n        - sha (str): SHA1 checksum ID of the blob.\n        - size (int): Size of the blob.\n        - node_id (str): Node ID of the blob.\n    \"\"\"\n\n    content:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 3, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "4": {"text": "of the blob.\n    \"\"\"\n\n    content: str\n    encoding: str\n    url: str\n    sha: str\n    size: int\n    node_id: str\n\n\n@dataclass\nclass GitCommitResponseModel(DataClassJsonMixin):\n    \"\"\"\n    Dataclass for the response from the Github API's getCommit endpoint.\n\n    Attributes:\n        - tree (Tree): Tree object for the commit.\n    \"\"\"\n\n    @dataclass\n    class Commit(DataClassJsonMixin):\n        \"\"\"Dataclass for the commit object in the commit. (commit.commit).\"\"\"\n\n        @dataclass\n        class Tree(DataClassJsonMixin):\n            \"\"\"\n            Dataclass for the tree object in the commit.\n\n            Attributes:\n            ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 4, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "5": {"text": " Attributes:\n                - sha (str): SHA for the commit\n            \"\"\"\n\n            sha: str\n\n        tree: Tree\n\n    commit: Commit\n\n\n@dataclass\nclass GitBranchResponseModel(DataClassJsonMixin):\n    \"\"\"\n    Dataclass for the response from the Github API's getBranch endpoint.\n\n    Attributes:\n        - commit (Commit): Commit object for the branch.\n    \"\"\"\n\n    @dataclass\n    class Commit(DataClassJsonMixin):\n        \"\"\"Dataclass for the commit object in the branch. (commit.commit).\"\"\"\n\n        @dataclass\n        class Commit(DataClassJsonMixin):\n            \"\"\"Dataclass for the commit object in the commit. (commit.commit.tree).\"\"\"\n\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 5, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "6": {"text": "commit. (commit.commit.tree).\"\"\"\n\n            @dataclass\n            class Tree(DataClassJsonMixin):\n                \"\"\"\n                Dataclass for the tree object in the commit.\n\n                Usage: commit.commit.tree.sha\n                \"\"\"\n\n                sha: str\n\n            tree: Tree\n\n        commit: Commit\n\n    commit: Commit\n\n\nclass GithubClient:\n    \"\"\"\n    An asynchronous client for interacting with the Github API.\n\n    This client is used for making API requests to Github.\n    It provides methods for accessing the Github API endpoints.\n    The client requires a Github token for authentication,\n    which can be", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 6, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "7": {"text": "requires a Github token for authentication,\n    which can be passed as an argument or set as an environment variable.\n    If no Github token is provided, the client will raise a ValueError.\n\n    Examples:\n        >>> client = GithubClient(\"my_github_token\")\n        >>> branch_info = client.get_branch(\"owner\", \"repo\", \"branch\")\n    \"\"\"\n\n    DEFAULT_BASE_URL = \"https://api.github.com\"\n    DEFAULT_API_VERSION = \"2022-11-28\"\n\n    def __init__(\n        self,\n        github_token: Optional[str] = None,\n        base_url: str = DEFAULT_BASE_URL,\n        api_version: str = DEFAULT_API_VERSION,\n        verbose: bool = False,\n    ) -> None:\n        \"\"\"\n        Initialize the", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 7, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "8": {"text": "    \"\"\"\n        Initialize the GithubClient.\n\n        Args:\n            - github_token (str): Github token for authentication.\n                If not provided, the client will try to get it from\n                the GITHUB_TOKEN environment variable.\n            - base_url (str): Base URL for the Github API\n                (defaults to \"https://api.github.com\").\n            - api_version (str): Github API version (defaults to \"2022-11-28\").\n\n        Raises:\n            ValueError: If no Github token is provided.\n        \"\"\"\n        if github_token is None:\n            github_token =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 8, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "9": {"text": "           github_token = os.getenv(\"GITHUB_TOKEN\")\n            if github_token is None:\n                raise ValueError(\n                    \"Please provide a Github token. \"\n                    + \"You can do so by passing it as an argument to the GithubReader,\"\n                    + \"or by setting the GITHUB_TOKEN environment variable.\"\n                )\n\n        self._base_url = base_url\n        self._api_version = api_version\n        self._verbose = verbose\n\n        self._endpoints = {\n            \"getTree\":", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 9, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "10": {"text": "           \"getTree\": \"/repos/{owner}/{repo}/git/trees/{tree_sha}\",\n            \"getBranch\": \"/repos/{owner}/{repo}/branches/{branch}\",\n            \"getBlob\": \"/repos/{owner}/{repo}/git/blobs/{file_sha}\",\n            \"getCommit\": \"/repos/{owner}/{repo}/commits/{commit_sha}\",\n        }\n\n        self._headers = {\n            \"Accept\": \"application/vnd.github+json\",\n            \"Authorization\": f\"Bearer {github_token}\",\n            \"X-GitHub-Api-Version\": f\"{self._api_version}\",\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 10, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "11": {"text": "       }\n\n    def get_all_endpoints(self) -> Dict[str, str]:\n        \"\"\"Get all available endpoints.\"\"\"\n        return {**self._endpoints}\n\n    async def request(\n        self,\n        endpoint: str,\n        method: str,\n        headers: Dict[str, Any] = {},\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"\n        Make an API request to the Github API.\n\n        This method is used for making API requests to the Github API.\n        It is used internally by the other methods in the client.\n\n        Args:\n            - `endpoint (str)`: Name of the endpoint to make the request to.\n            -", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 11, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "12": {"text": "request to.\n            - `method (str)`: HTTP method to use for the request.\n            - `headers (dict)`: HTTP headers to include in the request.\n            - `**kwargs`: Keyword arguments to pass to the endpoint URL.\n\n        Returns:\n            - `response (httpx.Response)`: Response from the API request.\n\n        Raises:\n            - ImportError: If the `httpx` library is not installed.\n            - httpx.HTTPError: If the API request fails.\n\n        Examples:\n            >>> response = client.request(\"getTree\", \"GET\",\n                                owner=\"owner\", repo=\"repo\",\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 12, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "13": {"text": "owner=\"owner\", repo=\"repo\",\n                                tree_sha=\"tree_sha\")\n        \"\"\"\n        try:\n            import httpx\n        except ImportError:\n            raise ImportError(\n                \"Please install httpx to use the GithubRepositoryReader. \"\n                \"You can do so by running `pip install httpx`.\"\n            )\n\n        _headers = {**self._headers, **headers}\n\n        _client: httpx.AsyncClient\n        async with httpx.AsyncClient(\n            headers=_headers, base_url=self._base_url\n        ) as", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 13, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "14": {"text": "       ) as _client:\n            try:\n                response = await _client.request(\n                    method, url=self._endpoints[endpoint].format(**kwargs)\n                )\n            except httpx.HTTPError as excp:\n                print(f\"HTTP Exception for {excp.request.url} - {excp}\")\n                raise excp\n            return response\n\n    async def get_branch(\n        self, owner: str, repo: str, branch: str\n    ) -> GitBranchResponseModel:\n        \"\"\"\n        Get information about a branch. (Github API", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 14, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "15": {"text": "     Get information about a branch. (Github API endpoint: getBranch).\n\n        Args:\n            - `owner (str)`: Owner of the repository.\n            - `repo (str)`: Name of the repository.\n            - `branch (str)`: Name of the branch.\n\n        Returns:\n            - `branch_info (GitBranchResponseModel)`: Information about the branch.\n\n        Examples:\n            >>> branch_info = client.get_branch(\"owner\", \"repo\", \"branch\")\n        \"\"\"\n        return GitBranchResponseModel.from_json(\n            (\n                await self.request(\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 15, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "16": {"text": "  await self.request(\n                    \"getBranch\", \"GET\", owner=owner, repo=repo, branch=branch\n                )\n            ).text\n        )\n\n    async def get_tree(\n        self, owner: str, repo: str, tree_sha: str\n    ) -> GitTreeResponseModel:\n        \"\"\"\n        Get information about a tree. (Github API endpoint: getTree).\n\n        Args:\n            - `owner (str)`: Owner of the repository.\n            - `repo (str)`: Name of the repository.\n            - `tree_sha (str)`: SHA of the tree.\n\n        Returns:\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 16, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "17": {"text": "the tree.\n\n        Returns:\n            - `tree_info (GitTreeResponseModel)`: Information about the tree.\n\n        Examples:\n            >>> tree_info = client.get_tree(\"owner\", \"repo\", \"tree_sha\")\n        \"\"\"\n        return GitTreeResponseModel.from_json(\n            (\n                await self.request(\n                    \"getTree\", \"GET\", owner=owner, repo=repo, tree_sha=tree_sha\n                )\n            ).text\n        )\n\n    async def get_blob(\n        self, owner: str, repo: str, file_sha: str\n    )", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 17, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "18": {"text": "repo: str, file_sha: str\n    ) -> GitBlobResponseModel:\n        \"\"\"\n        Get information about a blob. (Github API endpoint: getBlob).\n\n        Args:\n            - `owner (str)`: Owner of the repository.\n            - `repo (str)`: Name of the repository.\n            - `file_sha (str)`: SHA of the file.\n\n        Returns:\n            - `blob_info (GitBlobResponseModel)`: Information about the blob.\n\n        Examples:\n            >>> blob_info = client.get_blob(\"owner\", \"repo\", \"file_sha\")\n        \"\"\"\n        return GitBlobResponseModel.from_json(\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 18, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "19": {"text": "           (\n                await self.request(\n                    \"getBlob\", \"GET\", owner=owner, repo=repo, file_sha=file_sha\n                )\n            ).text\n        )\n\n    async def get_commit(\n        self, owner: str, repo: str, commit_sha: str\n    ) -> GitCommitResponseModel:\n        \"\"\"\n        Get information about a commit. (Github API endpoint: getCommit).\n\n        Args:\n            - `owner (str)`: Owner of the repository.\n            - `repo (str)`: Name of the repository.\n         ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 19, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "20": {"text": "of the repository.\n            - `commit_sha (str)`: SHA of the commit.\n\n        Returns:\n            - `commit_info (GitCommitResponseModel)`: Information about the commit.\n\n        Examples:\n            >>> commit_info = client.get_commit(\"owner\", \"repo\", \"commit_sha\")\n        \"\"\"\n        return GitCommitResponseModel.from_json(\n            (\n                await self.request(\n                    \"getCommit\", \"GET\", owner=owner, repo=repo, commit_sha=commit_sha\n                )\n            ).text\n        )\n\n\nif __name__ ==", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 20, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "21": {"text": "       )\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    async def main() -> None:\n        \"\"\"Test the GithubClient.\"\"\"\n        client = GithubClient()\n        response = await client.get_tree(\n            owner=\"ahmetkca\", repo=\"CommitAI\", tree_sha=\"with-body\"\n        )\n\n        for obj in response.tree:\n            if obj.type == \"blob\":\n                print(obj.path)\n                print(obj.sha)\n                blob_response = await client.get_blob(\n                    owner=\"ahmetkca\", repo=\"CommitAI\",", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 21, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "22": {"text": "  owner=\"ahmetkca\", repo=\"CommitAI\", file_sha=obj.sha\n                )\n                print(blob_response.content)\n\n    asyncio.run(main())\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 22, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "23": {"text": "\"\"\"\nGithub repository reader.\n\nRetrieves the contents of a Github repository and returns a list of documents.\nThe documents are either the contents of the files in the repository or\nthe text extracted from the files using the parser.\n\"\"\"\n\nimport asyncio\nimport base64\nimport binascii\nimport logging\nimport os\nimport pathlib\nimport tempfile\nfrom typing import Any, Callable, List, Optional, Tuple\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.file.base import DEFAULT_FILE_EXTRACTOR\nfrom gpt_index.readers.github_readers.github_api_client import (\n    GitBranchResponseModel,\n    GitCommitResponseModel,\n    GithubClient,\n    GitTreeResponseModel,\n)\nfrom gpt_index.readers.github_readers.utils import (\n    BufferedGitBlobDataIterator,\n    get_file_extension,\n    print_if_verbose,\n)\nfrom", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 23, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "24": {"text": "   print_if_verbose,\n)\nfrom gpt_index.readers.schema.base import Document\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass GithubRepositoryReader(BaseReader):\n    \"\"\"\n    Github repository reader.\n\n    Retrieves the contents of a Github repository and returns a list of documents.\n    The documents are either the contents of the files in the repository or the text\n    extracted from the files using the parser.\n\n    Examples:\n        >>> reader = GithubRepositoryReader(\"owner\", \"repo\")\n        >>> branch_documents = reader.load_data(branch=\"branch\")\n        >>> commit_documents = reader.load_data(commit_sha=\"commit_sha\")\n\n    \"\"\"\n\n    def __init__(\n        self,\n        owner: str,\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 24, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "25": {"text": "       owner: str,\n        repo: str,\n        use_parser: bool = True,\n        verbose: bool = False,\n        github_token: Optional[str] = None,\n        concurrent_requests: int = 5,\n        ignore_file_extensions: Optional[List[str]] = None,\n        ignore_directories: Optional[List[str]] = None,\n    ):\n        \"\"\"\n        Initialize params.\n\n        Args:\n            - owner (str): Owner of the repository.\n            - repo (str): Name of the repository.\n            - use_parser (bool): Whether to use the parser to extract\n                the text from the", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 25, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "26": {"text": "            the text from the files.\n            - verbose (bool): Whether to print verbose messages.\n            - github_token (str): Github token. If not provided,\n                it will be read from the GITHUB_TOKEN environment variable.\n            - concurrent_requests (int): Number of concurrent requests to\n                make to the Github API.\n            - ignore_file_extensions (List[str]): List of file extensions to ignore.\n                i.e. ['.png', '.jpg']\n            - ignore_directories (List[str]): List of directories to ignore.\n                i.e. ['node_modules', 'dist']\n\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 26, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "27": {"text": "   i.e. ['node_modules', 'dist']\n\n        Raises:\n            - `ValueError`: If the github_token is not provided and\n                the GITHUB_TOKEN environment variable is not set.\n        \"\"\"\n        super().__init__()\n        if github_token is None:\n            github_token = os.getenv(\"GITHUB_TOKEN\")\n            if github_token is None:\n                raise ValueError(\n                    \"Please provide a Github token. \"\n                    \"You can do so by passing it as an argument or\"\n                    +", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 27, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "28": {"text": "               + \"by setting the GITHUB_TOKEN environment variable.\"\n                )\n\n        self._owner = owner\n        self._repo = repo\n        self._use_parser = use_parser\n        self._verbose = verbose\n        self._concurrent_requests = concurrent_requests\n        self._ignore_file_extensions = ignore_file_extensions\n        self._ignore_directories = ignore_directories\n\n        # Set up the event loop\n        try:\n            self._loop = asyncio.get_running_loop()\n        except RuntimeError:\n            # If there is no running loop, create a new one\n            self._loop =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 28, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "29": {"text": "           self._loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(self._loop)\n\n        self._client = GithubClient(github_token)\n\n    def _load_data_from_commit(self, commit_sha: str) -> List[Document]:\n        \"\"\"\n        Load data from a commit.\n\n        Loads github repository data from a specific commit sha.\n\n        :param `commit`: commit sha\n\n        :return: list of documents\n        \"\"\"\n        commit_response: GitCommitResponseModel = self._loop.run_until_complete(\n            self._client.get_commit(self._owner, self._repo, commit_sha)\n        )\n\n        tree_sha =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 29, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "30": {"text": "   )\n\n        tree_sha = commit_response.commit.tree.sha\n        blobs_and_paths = self._loop.run_until_complete(self._recurse_tree(tree_sha))\n\n        print_if_verbose(self._verbose, f\"got {len(blobs_and_paths)} blobs\")\n\n        return self._loop.run_until_complete(\n            self._generate_documents(blobs_and_paths=blobs_and_paths)\n        )\n\n    def _load_data_from_branch(self, branch: str) -> List[Document]:\n        \"\"\"\n        Load data from a branch.\n\n        Loads github repository data from a specific branch.\n\n        :param `branch`: branch name\n\n        :return: list of documents\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 30, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "31": {"text": "     :return: list of documents\n        \"\"\"\n        branch_data: GitBranchResponseModel = self._loop.run_until_complete(\n            self._client.get_branch(self._owner, self._repo, branch)\n        )\n\n        tree_sha = branch_data.commit.commit.tree.sha\n        blobs_and_paths = self._loop.run_until_complete(self._recurse_tree(tree_sha))\n\n        print_if_verbose(self._verbose, f\"got {len(blobs_and_paths)} blobs\")\n\n        return self._loop.run_until_complete(\n            self._generate_documents(blobs_and_paths=blobs_and_paths)\n        )\n\n    def load_data(\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 31, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "32": {"text": "   def load_data(\n        self,\n        commit_sha: Optional[str] = None,\n        branch: Optional[str] = None,\n    ) -> List[Document]:\n        \"\"\"\n        Load data from a commit or a branch.\n\n        Loads github repository data from a specific commit sha or a branch.\n\n        :param `commit`: commit sha\n        :param `branch`: branch name\n\n        :return: list of documents\n        \"\"\"\n        if commit_sha is not None and branch is not None:\n            raise ValueError(\"You can only specify one of commit or branch.\")\n\n        if commit_sha is None and branch is None:\n            raise ValueError(\"You must specify one of commit or", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 32, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "33": {"text": "    raise ValueError(\"You must specify one of commit or branch.\")\n\n        if commit_sha is not None:\n            return self._load_data_from_commit(commit_sha)\n\n        if branch is not None:\n            return self._load_data_from_branch(branch)\n\n        raise ValueError(\"You must specify one of commit or branch.\")\n\n    async def _recurse_tree(\n        self, tree_sha: str, current_path: str = \"\", current_depth: int = 0\n    ) -> Any:\n        \"\"\"\n        Recursively get all blob tree objects in a tree.\n\n        And construct their full path relative to the root of the repository.\n        (see GitTreeResponseModel.GitTreeObject in\n            github_api_client.py", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 33, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "34": {"text": "         github_api_client.py for more information)\n\n        :param `tree_sha`: sha of the tree to recurse\n        :param `current_path`: current path of the tree\n        :param `current_depth`: current depth of the tree\n        :return: list of tuples of\n            (tree object, file's full path realtive to the root of the repo)\n        \"\"\"\n        blobs_and_full_paths: List[Tuple[GitTreeResponseModel.GitTreeObject, str]] = []\n        print_if_verbose(\n            self._verbose, \"\\t\" * current_depth + f\"current path: {current_path}\"\n        )\n\n        tree_data: GitTreeResponseModel = await self._client.get_tree(\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 34, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "35": {"text": "= await self._client.get_tree(\n            self._owner, self._repo, tree_sha\n        )\n        print_if_verbose(\n            self._verbose, \"\\t\" * current_depth + f\"processing tree {tree_sha}\"\n        )\n        for tree_obj in tree_data.tree:\n            file_path = os.path.join(current_path, tree_obj.path)\n            if tree_obj.type == \"tree\":\n                print_if_verbose(\n                    self._verbose,\n                    \"\\t\" * current_depth + f\"recursing into {tree_obj.path}\",\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 35, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "36": {"text": "into {tree_obj.path}\",\n                )\n                if self._ignore_directories is not None:\n                    if file_path in self._ignore_directories:\n                        print_if_verbose(\n                            self._verbose,\n                            \"\\t\" * current_depth\n                            + f\"ignoring tree {tree_obj.path} due to directory\",\n                        )\n                ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 36, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "37": {"text": "                       continue\n\n                blobs_and_full_paths.extend(\n                    await self._recurse_tree(tree_obj.sha, file_path, current_depth + 1)\n                )\n            elif tree_obj.type == \"blob\":\n                print_if_verbose(\n                    self._verbose, \"\\t\" * current_depth + f\"found blob {tree_obj.path}\"\n                )\n                if self._ignore_file_extensions is not None:\n                 ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 37, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "38": {"text": "                  if get_file_extension(file_path) in self._ignore_file_extensions:\n                        print_if_verbose(\n                            self._verbose,\n                            \"\\t\" * current_depth\n                            + f\"ignoring blob {tree_obj.path} due to file extension\",\n                        )\n                        continue\n                blobs_and_full_paths.append((tree_obj,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 38, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "39": {"text": "blobs_and_full_paths.append((tree_obj, file_path))\n        return blobs_and_full_paths\n\n    async def _generate_documents(\n        self, blobs_and_paths: List[Tuple[GitTreeResponseModel.GitTreeObject, str]]\n    ) -> List[Document]:\n        \"\"\"\n        Generate documents from a list of blobs and their full paths.\n\n        :param `blobs_and_paths`: list of tuples of\n            (tree object, file's full path in the repo realtive to the root of the repo)\n        :return: list of documents\n        \"\"\"\n        buffered_iterator = BufferedGitBlobDataIterator(\n            blobs_and_paths=blobs_and_paths,\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 39, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "40": {"text": "           github_client=self._client,\n            owner=self._owner,\n            repo=self._repo,\n            loop=self._loop,\n            buffer_size=self._concurrent_requests,  # TODO: make this configurable\n            verbose=self._verbose,\n        )\n\n        documents = []\n        async for blob_data, full_path in buffered_iterator:\n            print_if_verbose(self._verbose, f\"generating document for {full_path}\")\n            assert (\n                blob_data.encoding == \"base64\"\n            ), f\"blob encoding", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 40, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "41": {"text": "         ), f\"blob encoding {blob_data.encoding} not supported\"\n            decoded_bytes = None\n            try:\n                decoded_bytes = base64.b64decode(blob_data.content)\n                del blob_data.content\n            except binascii.Error:\n                print_if_verbose(\n                    self._verbose, f\"could not decode {full_path} as base64\"\n                )\n                continue\n\n            if self._use_parser:\n                document", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 41, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "42": {"text": "               document = self._parse_supported_file(\n                    file_path=full_path,\n                    file_content=decoded_bytes,\n                    tree_sha=blob_data.sha,\n                    tree_path=full_path,\n                )\n                if document is not None:\n                    documents.append(document)\n                else:\n                    continue\n\n            try:\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 42, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "43": {"text": "  try:\n                if decoded_bytes is None:\n                    raise ValueError(\"decoded_bytes is None\")\n                decoded_text = decoded_bytes.decode(\"utf-8\")\n            except UnicodeDecodeError:\n                print_if_verbose(\n                    self._verbose, f\"could not decode {full_path} as utf-8\"\n                )\n                continue\n            print_if_verbose(\n                self._verbose,\n                f\"got", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 43, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "44": {"text": "             f\"got {len(decoded_text)} characters\"\n                + f\"- adding to documents - {full_path}\",\n            )\n            document = Document(\n                text=decoded_text,\n                doc_id=blob_data.sha,\n                extra_info={\n                    \"file_path\": full_path,\n                    \"file_name\": full_path.split(\"/\")[-1],\n                },\n            )\n            documents.append(document)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 44, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "45": {"text": "        documents.append(document)\n        return documents\n\n    def _parse_supported_file(\n        self, file_path: str, file_content: bytes, tree_sha: str, tree_path: str\n    ) -> Optional[Document]:\n        \"\"\"\n        Parse a file if it is supported by a parser.\n\n        :param `file_path`: path of the file in the repo\n        :param `file_content`: content of the file\n        :return: Document if the file is supported by a parser, None otherwise\n        \"\"\"\n        file_extension = get_file_extension(file_path)\n        if (parser := DEFAULT_FILE_EXTRACTOR.get(file_extension)) is not None:\n            parser.init_parser()\n        ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 45, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "46": {"text": " parser.init_parser()\n            print_if_verbose(\n                self._verbose,\n                f\"parsing {file_path}\"\n                + f\"as {file_extension} with \"\n                + f\"{parser.__class__.__name__}\",\n            )\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                with tempfile.NamedTemporaryFile(\n                    dir=tmpdirname,\n                    suffix=f\".{file_extension}\",\n                 ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 46, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "47": {"text": "                  mode=\"w+b\",\n                    delete=False,\n                ) as tmpfile:\n                    print_if_verbose(\n                        self._verbose,\n                        \"created a temporary file\"\n                        + f\"{tmpfile.name} for parsing {file_path}\",\n                    )\n                    tmpfile.write(file_content)\n                   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 47, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "48": {"text": "                tmpfile.flush()\n                    tmpfile.close()\n                    try:\n                        parsed_file = parser.parse_file(pathlib.Path(tmpfile.name))\n                        parsed_file = \"\\n\\n\".join(parsed_file)\n                    except Exception as e:\n                        print_if_verbose(\n                            self._verbose, f\"error while parsing {file_path}\"\n            ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 48, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "49": {"text": "                       )\n                        logger.error(\n                            \"Error while parsing \"\n                            + f\"{file_path} with \"\n                            + f\"{parser.__class__.__name__}:\\n{e}\"\n                        )\n                        parsed_file = None\n                    finally:\n                   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 49, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "50": {"text": "                    os.remove(tmpfile.name)\n                    if parsed_file is None:\n                        return None\n                    return Document(\n                        text=parsed_file,\n                        doc_id=tree_sha,\n                        extra_info={\n                            \"file_path\": file_path,\n                            \"file_name\":", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 50, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "51": {"text": "           \"file_name\": tree_path,\n                        },\n                    )\n        return None\n\n\nif __name__ == \"__main__\":\n    import time\n\n    def timeit(func: Callable) -> Callable:\n        \"\"\"Time a function.\"\"\"\n\n        def wrapper(*args: Any, **kwargs: Any) -> None:\n            \"\"\"Callcuate time taken to run a function.\"\"\"\n            start = time.time()\n            func(*args, **kwargs)\n            end = time.time()\n            print(f\"Time taken: {end - start} seconds for {func.__name__}\")\n\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 51, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "52": {"text": "for {func.__name__}\")\n\n        return wrapper\n\n    reader1 = GithubRepositoryReader(\n        github_token=os.environ[\"GITHUB_TOKEN\"],\n        owner=\"jerryjliu\",\n        repo=\"gpt_index\",\n        use_parser=False,\n        verbose=True,\n        ignore_directories=[\"examples\"],\n    )\n\n    @timeit\n    def load_data_from_commit() -> None:\n        \"\"\"Load data from a commit.\"\"\"\n        documents = reader1.load_data(\n            commit_sha=\"22e198b3b166b5facd2843d6a62ac0db07894a13\"\n        )\n        for document in documents:\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 52, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "53": {"text": "in documents:\n            print(document.extra_info)\n\n    @timeit\n    def load_data_from_branch() -> None:\n        \"\"\"Load data from a branch.\"\"\"\n        documents = reader1.load_data(branch=\"main\")\n        for document in documents:\n            print(document.extra_info)\n\n    input(\"Press enter to load github repository from branch name...\")\n\n    load_data_from_branch()\n\n    input(\"Press enter to load github repository from commit sha...\")\n\n    load_data_from_commit()\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 53, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "54": {"text": "\"\"\"\nGithub readers utils.\n\nThis module contains utility functions for the Github readers.\n\"\"\"\nimport asyncio\nimport os\nimport time\nfrom abc import ABC, abstractmethod\nfrom typing import List, Tuple\n\nfrom gpt_index.readers.github_readers.github_api_client import (\n    GitBlobResponseModel,\n    GithubClient,\n    GitTreeResponseModel,\n)\n\n\ndef print_if_verbose(verbose: bool, message: str) -> None:\n    \"\"\"Log message if verbose is True.\"\"\"\n    if verbose:\n        print(message)\n\n\ndef get_file_extension(filename: str) -> str:\n    \"\"\"Get file extension.\"\"\"\n    return f\".{os.path.splitext(filename)[1][1:].lower()}\"\n\n\nclass BufferedAsyncIterator(ABC):\n    \"\"\"\n    Base class for buffered async iterators.\n\n    This class is to be used as a base class for async iterators\n    that need to buffer the results of an async", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/utils.py", "file_name": "utils.py"}, "index": 54, "child_indices": [], "ref_doc_id": "98449e52eda3d6ee33e51429362e457d29f7893e", "node_info": null}, "55": {"text": "iterators\n    that need to buffer the results of an async operation.\n    The async operation is defined in the _fill_buffer method.\n    The _fill_buffer method is called when the buffer is empty.\n    \"\"\"\n\n    def __init__(self, buffer_size: int):\n        \"\"\"\n        Initialize params.\n\n        Args:\n            - `buffer_size (int)`: Size of the buffer.\n                It is also the number of items that will\n                be retrieved from the async operation at once.\n                see _fill_buffer. Defaults to 2. Setting it to 1\n                will result in the same behavior as a synchronous iterator.\n        \"\"\"\n        self._buffer_size = buffer_size\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/utils.py", "file_name": "utils.py"}, "index": 55, "child_indices": [], "ref_doc_id": "98449e52eda3d6ee33e51429362e457d29f7893e", "node_info": null}, "56": {"text": "= buffer_size\n        self._buffer: List[Tuple[GitBlobResponseModel, str]] = []\n        self._index = 0\n\n    @abstractmethod\n    async def _fill_buffer(self) -> None:\n        raise NotImplementedError\n\n    def __aiter__(self) -> \"BufferedAsyncIterator\":\n        \"\"\"Return the iterator object.\"\"\"\n        return self\n\n    async def __anext__(self) -> Tuple[GitBlobResponseModel, str]:\n        \"\"\"\n        Get next item.\n\n        Returns:\n            - `item (Tuple[GitBlobResponseModel, str])`: Next item.\n\n        Raises:\n            - `StopAsyncIteration`: If there are no more items.\n        \"\"\"\n        if not self._buffer:\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/utils.py", "file_name": "utils.py"}, "index": 56, "child_indices": [], "ref_doc_id": "98449e52eda3d6ee33e51429362e457d29f7893e", "node_info": null}, "57": {"text": "       if not self._buffer:\n            await self._fill_buffer()\n\n        if not self._buffer:\n            raise StopAsyncIteration\n\n        item = self._buffer.pop(0)\n        self._index += 1\n        return item\n\n\nclass BufferedGitBlobDataIterator(BufferedAsyncIterator):\n    \"\"\"\n    Buffered async iterator for Git blobs.\n\n    This class is an async iterator that buffers the results of the get_blob operation.\n    It is used to retrieve the contents of the files in a Github repository.\n    getBlob endpoint supports up to 100 megabytes of content for blobs.\n    This concrete implementation of BufferedAsyncIterator allows you to lazily retrieve\n    the contents of the files in a Github repository.\n    Otherwise you would have to retrieve all the contents of\n    the files in the repository at once, which", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/utils.py", "file_name": "utils.py"}, "index": 57, "child_indices": [], "ref_doc_id": "98449e52eda3d6ee33e51429362e457d29f7893e", "node_info": null}, "58": {"text": "of\n    the files in the repository at once, which would\n    be problematic if the repository is large.\n    \"\"\"\n\n    def __init__(\n        self,\n        blobs_and_paths: List[Tuple[GitTreeResponseModel.GitTreeObject, str]],\n        github_client: GithubClient,\n        owner: str,\n        repo: str,\n        loop: asyncio.AbstractEventLoop,\n        buffer_size: int,\n        verbose: bool = False,\n    ):\n        \"\"\"\n        Initialize params.\n\n        Args:\n            - blobs_and_paths (List[Tuple[GitTreeResponseModel.GitTreeObject, str]]):\n                List of tuples containing the blob and the path of", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/utils.py", "file_name": "utils.py"}, "index": 58, "child_indices": [], "ref_doc_id": "98449e52eda3d6ee33e51429362e457d29f7893e", "node_info": null}, "59": {"text": "    List of tuples containing the blob and the path of the file.\n            - github_client (GithubClient): Github client.\n            - owner (str): Owner of the repository.\n            - repo (str): Name of the repository.\n            - loop (asyncio.AbstractEventLoop): Event loop.\n            - buffer_size (int): Size of the buffer.\n        \"\"\"\n        super().__init__(buffer_size)\n        self._blobs_and_paths = blobs_and_paths\n        self._github_client = github_client\n        self._owner = owner\n        self._repo = repo\n        self._verbose = verbose\n        if loop is None:\n            loop =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/utils.py", "file_name": "utils.py"}, "index": 59, "child_indices": [], "ref_doc_id": "98449e52eda3d6ee33e51429362e457d29f7893e", "node_info": null}, "60": {"text": "None:\n            loop = asyncio.get_event_loop()\n            if loop is None:\n                raise ValueError(\"No event loop found\")\n\n    async def _fill_buffer(self) -> None:\n        \"\"\"\n        Fill the buffer with the results of the get_blob operation.\n\n        The get_blob operation is called for each blob in the blobs_and_paths list.\n        The blobs are retrieved in batches of size buffer_size.\n        \"\"\"\n        del self._buffer[:]\n        self._buffer = []\n        start = self._index\n        end = min(start + self._buffer_size, len(self._blobs_and_paths))\n\n        if start >= end:\n            return\n\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/utils.py", "file_name": "utils.py"}, "index": 60, "child_indices": [], "ref_doc_id": "98449e52eda3d6ee33e51429362e457d29f7893e", "node_info": null}, "61": {"text": "          return\n\n        if self._verbose:\n            start_t = time.time()\n        results: List[GitBlobResponseModel] = await asyncio.gather(\n            *[\n                self._github_client.get_blob(self._owner, self._repo, blob.sha)\n                for blob, _ in self._blobs_and_paths[\n                    start:end\n                ]  # TODO: use batch_size instead of buffer_size for concurrent requests\n            ]\n        )\n        if self._verbose:\n            end_t = time.time()\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/utils.py", "file_name": "utils.py"}, "index": 61, "child_indices": [], "ref_doc_id": "98449e52eda3d6ee33e51429362e457d29f7893e", "node_info": null}, "62": {"text": "  end_t = time.time()\n            blob_names_and_sizes = [\n                (blob.path, blob.size) for blob, _ in self._blobs_and_paths[start:end]\n            ]\n            print(\n                \"Time to get blobs (\"\n                + f\"{blob_names_and_sizes}\"\n                + f\"): {end_t - start_t:.2f} seconds\"\n            )\n\n        self._buffer = [\n            (result, path)\n            for result, (_, path) in zip(results, self._blobs_and_paths[start:end])\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/utils.py", "file_name": "utils.py"}, "index": 62, "child_indices": [], "ref_doc_id": "98449e52eda3d6ee33e51429362e457d29f7893e", "node_info": null}, "63": {"text": "       ]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/utils.py", "file_name": "utils.py"}, "index": 63, "child_indices": [], "ref_doc_id": "98449e52eda3d6ee33e51429362e457d29f7893e", "node_info": null}, "64": {"text": "This code file contains the Github API client for the GPT-Index library. It provides methods for accessing the Github API endpoints, such as getTree, getBranch, getBlob, and getCommit. It requires a Github token for authentication, which can be passed as an argument or set as an environment variable. The code also contains dataclasses for the responses from the Github API endpoints, such as GitTreeResponseModel, GitBlobResponseModel, GitCommitResponseModel, and GitBranchResponseModel.", "doc_id": null, "embedding": null, "extra_info": null, "index": 64, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "65": {"text": "This code file is a Github repository reader. It retrieves the contents of a Github repository and returns a list of documents. The documents are either the contents of the files in the repository or the text extracted from the files using the parser. It uses the Github API Client to make requests to the Github API and get information about branches, trees, blobs, and commits. It also has functions to get the file extension and print if verbose. It also has a main function to test the Github Client.", "doc_id": null, "embedding": null, "extra_info": null, "index": 65, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "66": {"text": "The GithubRepositoryReader class is a Python script that retrieves the contents of a Github repository and returns a list of documents. It can be used to load data from a specific commit sha or a branch. It uses the Github API to recursively get all blob tree objects in a tree and construct their full path relative to the root of the repository. It also has the option to use a parser to extract the text from the files. It has several parameters that can be set, such as the Github token, the number of concurrent requests, and the list of file extensions and directories to ignore.", "doc_id": null, "embedding": null, "extra_info": null, "index": 66, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}, "67": {"text": "This code file is a python script that reads a Github repository and generates documents from the files in the repository. It uses a BufferedGitBlobDataIterator to iterate through the files in the repository, and it can parse supported files using a parser. It also has the ability to ignore certain directories and file extensions. The generated documents contain the file's content, its full path, and its SHA.", "doc_id": null, "embedding": null, "extra_info": null, "index": 67, "child_indices": [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], "ref_doc_id": null, "node_info": null}, "68": {"text": "This code file contains two modules, github_repository_reader.py and utils.py. The github_repository_reader.py module contains functions for reading files from a Github repository, such as loading data from a commit or branch. The utils.py module contains utility functions for the Github readers, such as a BufferedAsyncIterator class for buffering the results of an async operation, and a BufferedGitBlobDataIterator class for lazily retrieving the contents of files in a Github repository.", "doc_id": null, "embedding": null, "extra_info": null, "index": 68, "child_indices": [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59], "ref_doc_id": null, "node_info": null}, "69": {"text": "utils.py is a file in the gpt_index/readers/github_readers directory that contains functions related to retrieving blobs from a GitHub repository. The main function, _fill_buffer(), retrieves blobs in batches of size buffer_size from the blobs_and_paths list and stores them in the buffer. If verbose is set to true, the time it takes to get the blobs is printed. The results are stored in a list of GitBlobResponseModel objects and the paths are stored in a list of tuples.", "doc_id": null, "embedding": null, "extra_info": null, "index": 69, "child_indices": [60, 61, 62, 63], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"64": {"text": "This code file contains the Github API client for the GPT-Index library. It provides methods for accessing the Github API endpoints, such as getTree, getBranch, getBlob, and getCommit. It requires a Github token for authentication, which can be passed as an argument or set as an environment variable. The code also contains dataclasses for the responses from the Github API endpoints, such as GitTreeResponseModel, GitBlobResponseModel, GitCommitResponseModel, and GitBranchResponseModel.", "doc_id": null, "embedding": null, "extra_info": null, "index": 64, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "65": {"text": "This code file is a Github repository reader. It retrieves the contents of a Github repository and returns a list of documents. The documents are either the contents of the files in the repository or the text extracted from the files using the parser. It uses the Github API Client to make requests to the Github API and get information about branches, trees, blobs, and commits. It also has functions to get the file extension and print if verbose. It also has a main function to test the Github Client.", "doc_id": null, "embedding": null, "extra_info": null, "index": 65, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "66": {"text": "The GithubRepositoryReader class is a Python script that retrieves the contents of a Github repository and returns a list of documents. It can be used to load data from a specific commit sha or a branch. It uses the Github API to recursively get all blob tree objects in a tree and construct their full path relative to the root of the repository. It also has the option to use a parser to extract the text from the files. It has several parameters that can be set, such as the Github token, the number of concurrent requests, and the list of file extensions and directories to ignore.", "doc_id": null, "embedding": null, "extra_info": null, "index": 66, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}, "67": {"text": "This code file is a python script that reads a Github repository and generates documents from the files in the repository. It uses a BufferedGitBlobDataIterator to iterate through the files in the repository, and it can parse supported files using a parser. It also has the ability to ignore certain directories and file extensions. The generated documents contain the file's content, its full path, and its SHA.", "doc_id": null, "embedding": null, "extra_info": null, "index": 67, "child_indices": [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], "ref_doc_id": null, "node_info": null}, "68": {"text": "This code file contains two modules, github_repository_reader.py and utils.py. The github_repository_reader.py module contains functions for reading files from a Github repository, such as loading data from a commit or branch. The utils.py module contains utility functions for the Github readers, such as a BufferedAsyncIterator class for buffering the results of an async operation, and a BufferedGitBlobDataIterator class for lazily retrieving the contents of files in a Github repository.", "doc_id": null, "embedding": null, "extra_info": null, "index": 68, "child_indices": [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59], "ref_doc_id": null, "node_info": null}, "69": {"text": "utils.py is a file in the gpt_index/readers/github_readers directory that contains functions related to retrieving blobs from a GitHub repository. The main function, _fill_buffer(), retrieves blobs in batches of size buffer_size from the blobs_and_paths list and stores them in the buffer. If verbose is set to true, the time it takes to get the blobs is printed. The results are stored in a list of GitBlobResponseModel objects and the paths are stored in a list of tuples.", "doc_id": null, "embedding": null, "extra_info": null, "index": 69, "child_indices": [60, 61, 62, 63], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"1d4640565ae2765d9ca96a509dc9809217f62f2f": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "cde89efb045b4304cc4982ff8aa795ac8c522a23": {"text": "\"\"\"\nGithub API client for the GPT-Index library.\n\nThis module contains the Github API client for the GPT-Index library.\nIt is used by the Github readers to retrieve the data from Github.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional\n\nfrom dataclasses_json import DataClassJsonMixin\n\n\n@dataclass\nclass GitTreeResponseModel(DataClassJsonMixin):\n    \"\"\"\n    Dataclass for the response from the Github API's getTree endpoint.\n\n    Attributes:\n        - sha (str): SHA1 checksum ID of the tree.\n        - url (str): URL for the tree.\n        - tree (List[GitTreeObject]): List of objects in the tree.\n        - truncated (bool): Whether the tree is truncated.\n\n    Examples:\n        >>> tree = client.get_tree(\"owner\", \"repo\", \"branch\")\n        >>> tree.sha\n    \"\"\"\n\n    @dataclass\n    class GitTreeObject(DataClassJsonMixin):\n        \"\"\"\n        Dataclass for the objects in the tree.\n\n        Attributes:\n            - path (str): Path to the object.\n            - mode (str): Mode of the object.\n            - type (str): Type of the object.\n            - sha (str): SHA1 checksum ID of the object.\n            - url (str): URL for the object.\n            - size (Optional[int]): Size of the object (only for blobs).\n        \"\"\"\n\n        path: str\n        mode: str\n        type: str\n        sha: str\n        url: str\n        size: Optional[int] = None\n\n    sha: str\n    url: str\n    tree: List[GitTreeObject]\n    truncated: bool\n\n\n@dataclass\nclass GitBlobResponseModel(DataClassJsonMixin):\n    \"\"\"\n    Dataclass for the response from the Github API's getBlob endpoint.\n\n    Attributes:\n        - content (str): Content of the blob.\n        - encoding (str): Encoding of the blob.\n        - url (str): URL for the blob.\n        - sha (str): SHA1 checksum ID of the blob.\n        - size (int): Size of the blob.\n        - node_id (str): Node ID of the blob.\n    \"\"\"\n\n    content: str\n    encoding: str\n    url: str\n    sha: str\n    size: int\n    node_id: str\n\n\n@dataclass\nclass GitCommitResponseModel(DataClassJsonMixin):\n    \"\"\"\n    Dataclass for the response from the Github API's getCommit endpoint.\n\n    Attributes:\n        - tree (Tree): Tree object for the commit.\n    \"\"\"\n\n    @dataclass\n    class Commit(DataClassJsonMixin):\n        \"\"\"Dataclass for the commit object in the commit. (commit.commit).\"\"\"\n\n        @dataclass\n        class Tree(DataClassJsonMixin):\n            \"\"\"\n            Dataclass for the tree object in the commit.\n\n            Attributes:\n                - sha (str): SHA for the commit\n            \"\"\"\n\n            sha: str\n\n        tree: Tree\n\n    commit: Commit\n\n\n@dataclass\nclass GitBranchResponseModel(DataClassJsonMixin):\n    \"\"\"\n    Dataclass for the response from the Github API's getBranch endpoint.\n\n    Attributes:\n        - commit (Commit): Commit object for the branch.\n    \"\"\"\n\n    @dataclass\n    class Commit(DataClassJsonMixin):\n        \"\"\"Dataclass for the commit object in the branch. (commit.commit).\"\"\"\n\n        @dataclass\n        class Commit(DataClassJsonMixin):\n            \"\"\"Dataclass for the commit object in the commit. (commit.commit.tree).\"\"\"\n\n            @dataclass\n            class Tree(DataClassJsonMixin):\n                \"\"\"\n                Dataclass for the tree object in the commit.\n\n                Usage: commit.commit.tree.sha\n                \"\"\"\n\n                sha: str\n\n            tree: Tree\n\n        commit: Commit\n\n    commit: Commit\n\n\nclass GithubClient:\n    \"\"\"\n    An asynchronous client for interacting with the Github API.\n\n    This client is used for making API requests to Github.\n    It provides methods for accessing the Github API endpoints.\n    The client requires a Github token for authentication,\n    which can be passed as an argument or set as an environment variable.\n    If no Github token is provided, the client will raise a ValueError.\n\n    Examples:\n        >>> client = GithubClient(\"my_github_token\")\n        >>> branch_info = client.get_branch(\"owner\", \"repo\", \"branch\")\n    \"\"\"\n\n    DEFAULT_BASE_URL = \"https://api.github.com\"\n    DEFAULT_API_VERSION = \"2022-11-28\"\n\n    def __init__(\n        self,\n        github_token: Optional[str] = None,\n        base_url: str = DEFAULT_BASE_URL,\n        api_version: str = DEFAULT_API_VERSION,\n        verbose: bool = False,\n    ) -> None:\n        \"\"\"\n        Initialize the GithubClient.\n\n        Args:\n            - github_token (str): Github token for authentication.\n                If not provided, the client will try to get it from\n                the GITHUB_TOKEN environment variable.\n            - base_url (str): Base URL for the Github API\n                (defaults to \"https://api.github.com\").\n            - api_version (str): Github API version (defaults to \"2022-11-28\").\n\n        Raises:\n            ValueError: If no Github token is provided.\n        \"\"\"\n        if github_token is None:\n            github_token = os.getenv(\"GITHUB_TOKEN\")\n            if github_token is None:\n                raise ValueError(\n                    \"Please provide a Github token. \"\n                    + \"You can do so by passing it as an argument to the GithubReader,\"\n                    + \"or by setting the GITHUB_TOKEN environment variable.\"\n                )\n\n        self._base_url = base_url\n        self._api_version = api_version\n        self._verbose = verbose\n\n        self._endpoints = {\n            \"getTree\": \"/repos/{owner}/{repo}/git/trees/{tree_sha}\",\n            \"getBranch\": \"/repos/{owner}/{repo}/branches/{branch}\",\n            \"getBlob\": \"/repos/{owner}/{repo}/git/blobs/{file_sha}\",\n            \"getCommit\": \"/repos/{owner}/{repo}/commits/{commit_sha}\",\n        }\n\n        self._headers = {\n            \"Accept\": \"application/vnd.github+json\",\n            \"Authorization\": f\"Bearer {github_token}\",\n            \"X-GitHub-Api-Version\": f\"{self._api_version}\",\n        }\n\n    def get_all_endpoints(self) -> Dict[str, str]:\n        \"\"\"Get all available endpoints.\"\"\"\n        return {**self._endpoints}\n\n    async def request(\n        self,\n        endpoint: str,\n        method: str,\n        headers: Dict[str, Any] = {},\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"\n        Make an API request to the Github API.\n\n        This method is used for making API requests to the Github API.\n        It is used internally by the other methods in the client.\n\n        Args:\n            - `endpoint (str)`: Name of the endpoint to make the request to.\n            - `method (str)`: HTTP method to use for the request.\n            - `headers (dict)`: HTTP headers to include in the request.\n            - `**kwargs`: Keyword arguments to pass to the endpoint URL.\n\n        Returns:\n            - `response (httpx.Response)`: Response from the API request.\n\n        Raises:\n            - ImportError: If the `httpx` library is not installed.\n            - httpx.HTTPError: If the API request fails.\n\n        Examples:\n            >>> response = client.request(\"getTree\", \"GET\",\n                                owner=\"owner\", repo=\"repo\",\n                                tree_sha=\"tree_sha\")\n        \"\"\"\n        try:\n            import httpx\n        except ImportError:\n            raise ImportError(\n                \"Please install httpx to use the GithubRepositoryReader. \"\n                \"You can do so by running `pip install httpx`.\"\n            )\n\n        _headers = {**self._headers, **headers}\n\n        _client: httpx.AsyncClient\n        async with httpx.AsyncClient(\n            headers=_headers, base_url=self._base_url\n        ) as _client:\n            try:\n                response = await _client.request(\n                    method, url=self._endpoints[endpoint].format(**kwargs)\n                )\n            except httpx.HTTPError as excp:\n                print(f\"HTTP Exception for {excp.request.url} - {excp}\")\n                raise excp\n            return response\n\n    async def get_branch(\n        self, owner: str, repo: str, branch: str\n    ) -> GitBranchResponseModel:\n        \"\"\"\n        Get information about a branch. (Github API endpoint: getBranch).\n\n        Args:\n            - `owner (str)`: Owner of the repository.\n            - `repo (str)`: Name of the repository.\n            - `branch (str)`: Name of the branch.\n\n        Returns:\n            - `branch_info (GitBranchResponseModel)`: Information about the branch.\n\n        Examples:\n            >>> branch_info = client.get_branch(\"owner\", \"repo\", \"branch\")\n        \"\"\"\n        return GitBranchResponseModel.from_json(\n            (\n                await self.request(\n                    \"getBranch\", \"GET\", owner=owner, repo=repo, branch=branch\n                )\n            ).text\n        )\n\n    async def get_tree(\n        self, owner: str, repo: str, tree_sha: str\n    ) -> GitTreeResponseModel:\n        \"\"\"\n        Get information about a tree. (Github API endpoint: getTree).\n\n        Args:\n            - `owner (str)`: Owner of the repository.\n            - `repo (str)`: Name of the repository.\n            - `tree_sha (str)`: SHA of the tree.\n\n        Returns:\n            - `tree_info (GitTreeResponseModel)`: Information about the tree.\n\n        Examples:\n            >>> tree_info = client.get_tree(\"owner\", \"repo\", \"tree_sha\")\n        \"\"\"\n        return GitTreeResponseModel.from_json(\n            (\n                await self.request(\n                    \"getTree\", \"GET\", owner=owner, repo=repo, tree_sha=tree_sha\n                )\n            ).text\n        )\n\n    async def get_blob(\n        self, owner: str, repo: str, file_sha: str\n    ) -> GitBlobResponseModel:\n        \"\"\"\n        Get information about a blob. (Github API endpoint: getBlob).\n\n        Args:\n            - `owner (str)`: Owner of the repository.\n            - `repo (str)`: Name of the repository.\n            - `file_sha (str)`: SHA of the file.\n\n        Returns:\n            - `blob_info (GitBlobResponseModel)`: Information about the blob.\n\n        Examples:\n            >>> blob_info = client.get_blob(\"owner\", \"repo\", \"file_sha\")\n        \"\"\"\n        return GitBlobResponseModel.from_json(\n            (\n                await self.request(\n                    \"getBlob\", \"GET\", owner=owner, repo=repo, file_sha=file_sha\n                )\n            ).text\n        )\n\n    async def get_commit(\n        self, owner: str, repo: str, commit_sha: str\n    ) -> GitCommitResponseModel:\n        \"\"\"\n        Get information about a commit. (Github API endpoint: getCommit).\n\n        Args:\n            - `owner (str)`: Owner of the repository.\n            - `repo (str)`: Name of the repository.\n            - `commit_sha (str)`: SHA of the commit.\n\n        Returns:\n            - `commit_info (GitCommitResponseModel)`: Information about the commit.\n\n        Examples:\n            >>> commit_info = client.get_commit(\"owner\", \"repo\", \"commit_sha\")\n        \"\"\"\n        return GitCommitResponseModel.from_json(\n            (\n                await self.request(\n                    \"getCommit\", \"GET\", owner=owner, repo=repo, commit_sha=commit_sha\n                )\n            ).text\n        )\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    async def main() -> None:\n        \"\"\"Test the GithubClient.\"\"\"\n        client = GithubClient()\n        response = await client.get_tree(\n            owner=\"ahmetkca\", repo=\"CommitAI\", tree_sha=\"with-body\"\n        )\n\n        for obj in response.tree:\n            if obj.type == \"blob\":\n                print(obj.path)\n                print(obj.sha)\n                blob_response = await client.get_blob(\n                    owner=\"ahmetkca\", repo=\"CommitAI\", file_sha=obj.sha\n                )\n                print(blob_response.content)\n\n    asyncio.run(main())\n", "doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "__type__": "Document"}, "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a": {"text": "\"\"\"\nGithub repository reader.\n\nRetrieves the contents of a Github repository and returns a list of documents.\nThe documents are either the contents of the files in the repository or\nthe text extracted from the files using the parser.\n\"\"\"\n\nimport asyncio\nimport base64\nimport binascii\nimport logging\nimport os\nimport pathlib\nimport tempfile\nfrom typing import Any, Callable, List, Optional, Tuple\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.file.base import DEFAULT_FILE_EXTRACTOR\nfrom gpt_index.readers.github_readers.github_api_client import (\n    GitBranchResponseModel,\n    GitCommitResponseModel,\n    GithubClient,\n    GitTreeResponseModel,\n)\nfrom gpt_index.readers.github_readers.utils import (\n    BufferedGitBlobDataIterator,\n    get_file_extension,\n    print_if_verbose,\n)\nfrom gpt_index.readers.schema.base import Document\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass GithubRepositoryReader(BaseReader):\n    \"\"\"\n    Github repository reader.\n\n    Retrieves the contents of a Github repository and returns a list of documents.\n    The documents are either the contents of the files in the repository or the text\n    extracted from the files using the parser.\n\n    Examples:\n        >>> reader = GithubRepositoryReader(\"owner\", \"repo\")\n        >>> branch_documents = reader.load_data(branch=\"branch\")\n        >>> commit_documents = reader.load_data(commit_sha=\"commit_sha\")\n\n    \"\"\"\n\n    def __init__(\n        self,\n        owner: str,\n        repo: str,\n        use_parser: bool = True,\n        verbose: bool = False,\n        github_token: Optional[str] = None,\n        concurrent_requests: int = 5,\n        ignore_file_extensions: Optional[List[str]] = None,\n        ignore_directories: Optional[List[str]] = None,\n    ):\n        \"\"\"\n        Initialize params.\n\n        Args:\n            - owner (str): Owner of the repository.\n            - repo (str): Name of the repository.\n            - use_parser (bool): Whether to use the parser to extract\n                the text from the files.\n            - verbose (bool): Whether to print verbose messages.\n            - github_token (str): Github token. If not provided,\n                it will be read from the GITHUB_TOKEN environment variable.\n            - concurrent_requests (int): Number of concurrent requests to\n                make to the Github API.\n            - ignore_file_extensions (List[str]): List of file extensions to ignore.\n                i.e. ['.png', '.jpg']\n            - ignore_directories (List[str]): List of directories to ignore.\n                i.e. ['node_modules', 'dist']\n\n        Raises:\n            - `ValueError`: If the github_token is not provided and\n                the GITHUB_TOKEN environment variable is not set.\n        \"\"\"\n        super().__init__()\n        if github_token is None:\n            github_token = os.getenv(\"GITHUB_TOKEN\")\n            if github_token is None:\n                raise ValueError(\n                    \"Please provide a Github token. \"\n                    \"You can do so by passing it as an argument or\"\n                    + \"by setting the GITHUB_TOKEN environment variable.\"\n                )\n\n        self._owner = owner\n        self._repo = repo\n        self._use_parser = use_parser\n        self._verbose = verbose\n        self._concurrent_requests = concurrent_requests\n        self._ignore_file_extensions = ignore_file_extensions\n        self._ignore_directories = ignore_directories\n\n        # Set up the event loop\n        try:\n            self._loop = asyncio.get_running_loop()\n        except RuntimeError:\n            # If there is no running loop, create a new one\n            self._loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(self._loop)\n\n        self._client = GithubClient(github_token)\n\n    def _load_data_from_commit(self, commit_sha: str) -> List[Document]:\n        \"\"\"\n        Load data from a commit.\n\n        Loads github repository data from a specific commit sha.\n\n        :param `commit`: commit sha\n\n        :return: list of documents\n        \"\"\"\n        commit_response: GitCommitResponseModel = self._loop.run_until_complete(\n            self._client.get_commit(self._owner, self._repo, commit_sha)\n        )\n\n        tree_sha = commit_response.commit.tree.sha\n        blobs_and_paths = self._loop.run_until_complete(self._recurse_tree(tree_sha))\n\n        print_if_verbose(self._verbose, f\"got {len(blobs_and_paths)} blobs\")\n\n        return self._loop.run_until_complete(\n            self._generate_documents(blobs_and_paths=blobs_and_paths)\n        )\n\n    def _load_data_from_branch(self, branch: str) -> List[Document]:\n        \"\"\"\n        Load data from a branch.\n\n        Loads github repository data from a specific branch.\n\n        :param `branch`: branch name\n\n        :return: list of documents\n        \"\"\"\n        branch_data: GitBranchResponseModel = self._loop.run_until_complete(\n            self._client.get_branch(self._owner, self._repo, branch)\n        )\n\n        tree_sha = branch_data.commit.commit.tree.sha\n        blobs_and_paths = self._loop.run_until_complete(self._recurse_tree(tree_sha))\n\n        print_if_verbose(self._verbose, f\"got {len(blobs_and_paths)} blobs\")\n\n        return self._loop.run_until_complete(\n            self._generate_documents(blobs_and_paths=blobs_and_paths)\n        )\n\n    def load_data(\n        self,\n        commit_sha: Optional[str] = None,\n        branch: Optional[str] = None,\n    ) -> List[Document]:\n        \"\"\"\n        Load data from a commit or a branch.\n\n        Loads github repository data from a specific commit sha or a branch.\n\n        :param `commit`: commit sha\n        :param `branch`: branch name\n\n        :return: list of documents\n        \"\"\"\n        if commit_sha is not None and branch is not None:\n            raise ValueError(\"You can only specify one of commit or branch.\")\n\n        if commit_sha is None and branch is None:\n            raise ValueError(\"You must specify one of commit or branch.\")\n\n        if commit_sha is not None:\n            return self._load_data_from_commit(commit_sha)\n\n        if branch is not None:\n            return self._load_data_from_branch(branch)\n\n        raise ValueError(\"You must specify one of commit or branch.\")\n\n    async def _recurse_tree(\n        self, tree_sha: str, current_path: str = \"\", current_depth: int = 0\n    ) -> Any:\n        \"\"\"\n        Recursively get all blob tree objects in a tree.\n\n        And construct their full path relative to the root of the repository.\n        (see GitTreeResponseModel.GitTreeObject in\n            github_api_client.py for more information)\n\n        :param `tree_sha`: sha of the tree to recurse\n        :param `current_path`: current path of the tree\n        :param `current_depth`: current depth of the tree\n        :return: list of tuples of\n            (tree object, file's full path realtive to the root of the repo)\n        \"\"\"\n        blobs_and_full_paths: List[Tuple[GitTreeResponseModel.GitTreeObject, str]] = []\n        print_if_verbose(\n            self._verbose, \"\\t\" * current_depth + f\"current path: {current_path}\"\n        )\n\n        tree_data: GitTreeResponseModel = await self._client.get_tree(\n            self._owner, self._repo, tree_sha\n        )\n        print_if_verbose(\n            self._verbose, \"\\t\" * current_depth + f\"processing tree {tree_sha}\"\n        )\n        for tree_obj in tree_data.tree:\n            file_path = os.path.join(current_path, tree_obj.path)\n            if tree_obj.type == \"tree\":\n                print_if_verbose(\n                    self._verbose,\n                    \"\\t\" * current_depth + f\"recursing into {tree_obj.path}\",\n                )\n                if self._ignore_directories is not None:\n                    if file_path in self._ignore_directories:\n                        print_if_verbose(\n                            self._verbose,\n                            \"\\t\" * current_depth\n                            + f\"ignoring tree {tree_obj.path} due to directory\",\n                        )\n                        continue\n\n                blobs_and_full_paths.extend(\n                    await self._recurse_tree(tree_obj.sha, file_path, current_depth + 1)\n                )\n            elif tree_obj.type == \"blob\":\n                print_if_verbose(\n                    self._verbose, \"\\t\" * current_depth + f\"found blob {tree_obj.path}\"\n                )\n                if self._ignore_file_extensions is not None:\n                    if get_file_extension(file_path) in self._ignore_file_extensions:\n                        print_if_verbose(\n                            self._verbose,\n                            \"\\t\" * current_depth\n                            + f\"ignoring blob {tree_obj.path} due to file extension\",\n                        )\n                        continue\n                blobs_and_full_paths.append((tree_obj, file_path))\n        return blobs_and_full_paths\n\n    async def _generate_documents(\n        self, blobs_and_paths: List[Tuple[GitTreeResponseModel.GitTreeObject, str]]\n    ) -> List[Document]:\n        \"\"\"\n        Generate documents from a list of blobs and their full paths.\n\n        :param `blobs_and_paths`: list of tuples of\n            (tree object, file's full path in the repo realtive to the root of the repo)\n        :return: list of documents\n        \"\"\"\n        buffered_iterator = BufferedGitBlobDataIterator(\n            blobs_and_paths=blobs_and_paths,\n            github_client=self._client,\n            owner=self._owner,\n            repo=self._repo,\n            loop=self._loop,\n            buffer_size=self._concurrent_requests,  # TODO: make this configurable\n            verbose=self._verbose,\n        )\n\n        documents = []\n        async for blob_data, full_path in buffered_iterator:\n            print_if_verbose(self._verbose, f\"generating document for {full_path}\")\n            assert (\n                blob_data.encoding == \"base64\"\n            ), f\"blob encoding {blob_data.encoding} not supported\"\n            decoded_bytes = None\n            try:\n                decoded_bytes = base64.b64decode(blob_data.content)\n                del blob_data.content\n            except binascii.Error:\n                print_if_verbose(\n                    self._verbose, f\"could not decode {full_path} as base64\"\n                )\n                continue\n\n            if self._use_parser:\n                document = self._parse_supported_file(\n                    file_path=full_path,\n                    file_content=decoded_bytes,\n                    tree_sha=blob_data.sha,\n                    tree_path=full_path,\n                )\n                if document is not None:\n                    documents.append(document)\n                else:\n                    continue\n\n            try:\n                if decoded_bytes is None:\n                    raise ValueError(\"decoded_bytes is None\")\n                decoded_text = decoded_bytes.decode(\"utf-8\")\n            except UnicodeDecodeError:\n                print_if_verbose(\n                    self._verbose, f\"could not decode {full_path} as utf-8\"\n                )\n                continue\n            print_if_verbose(\n                self._verbose,\n                f\"got {len(decoded_text)} characters\"\n                + f\"- adding to documents - {full_path}\",\n            )\n            document = Document(\n                text=decoded_text,\n                doc_id=blob_data.sha,\n                extra_info={\n                    \"file_path\": full_path,\n                    \"file_name\": full_path.split(\"/\")[-1],\n                },\n            )\n            documents.append(document)\n        return documents\n\n    def _parse_supported_file(\n        self, file_path: str, file_content: bytes, tree_sha: str, tree_path: str\n    ) -> Optional[Document]:\n        \"\"\"\n        Parse a file if it is supported by a parser.\n\n        :param `file_path`: path of the file in the repo\n        :param `file_content`: content of the file\n        :return: Document if the file is supported by a parser, None otherwise\n        \"\"\"\n        file_extension = get_file_extension(file_path)\n        if (parser := DEFAULT_FILE_EXTRACTOR.get(file_extension)) is not None:\n            parser.init_parser()\n            print_if_verbose(\n                self._verbose,\n                f\"parsing {file_path}\"\n                + f\"as {file_extension} with \"\n                + f\"{parser.__class__.__name__}\",\n            )\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                with tempfile.NamedTemporaryFile(\n                    dir=tmpdirname,\n                    suffix=f\".{file_extension}\",\n                    mode=\"w+b\",\n                    delete=False,\n                ) as tmpfile:\n                    print_if_verbose(\n                        self._verbose,\n                        \"created a temporary file\"\n                        + f\"{tmpfile.name} for parsing {file_path}\",\n                    )\n                    tmpfile.write(file_content)\n                    tmpfile.flush()\n                    tmpfile.close()\n                    try:\n                        parsed_file = parser.parse_file(pathlib.Path(tmpfile.name))\n                        parsed_file = \"\\n\\n\".join(parsed_file)\n                    except Exception as e:\n                        print_if_verbose(\n                            self._verbose, f\"error while parsing {file_path}\"\n                        )\n                        logger.error(\n                            \"Error while parsing \"\n                            + f\"{file_path} with \"\n                            + f\"{parser.__class__.__name__}:\\n{e}\"\n                        )\n                        parsed_file = None\n                    finally:\n                        os.remove(tmpfile.name)\n                    if parsed_file is None:\n                        return None\n                    return Document(\n                        text=parsed_file,\n                        doc_id=tree_sha,\n                        extra_info={\n                            \"file_path\": file_path,\n                            \"file_name\": tree_path,\n                        },\n                    )\n        return None\n\n\nif __name__ == \"__main__\":\n    import time\n\n    def timeit(func: Callable) -> Callable:\n        \"\"\"Time a function.\"\"\"\n\n        def wrapper(*args: Any, **kwargs: Any) -> None:\n            \"\"\"Callcuate time taken to run a function.\"\"\"\n            start = time.time()\n            func(*args, **kwargs)\n            end = time.time()\n            print(f\"Time taken: {end - start} seconds for {func.__name__}\")\n\n        return wrapper\n\n    reader1 = GithubRepositoryReader(\n        github_token=os.environ[\"GITHUB_TOKEN\"],\n        owner=\"jerryjliu\",\n        repo=\"gpt_index\",\n        use_parser=False,\n        verbose=True,\n        ignore_directories=[\"examples\"],\n    )\n\n    @timeit\n    def load_data_from_commit() -> None:\n        \"\"\"Load data from a commit.\"\"\"\n        documents = reader1.load_data(\n            commit_sha=\"22e198b3b166b5facd2843d6a62ac0db07894a13\"\n        )\n        for document in documents:\n            print(document.extra_info)\n\n    @timeit\n    def load_data_from_branch() -> None:\n        \"\"\"Load data from a branch.\"\"\"\n        documents = reader1.load_data(branch=\"main\")\n        for document in documents:\n            print(document.extra_info)\n\n    input(\"Press enter to load github repository from branch name...\")\n\n    load_data_from_branch()\n\n    input(\"Press enter to load github repository from commit sha...\")\n\n    load_data_from_commit()\n", "doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "__type__": "Document"}, "98449e52eda3d6ee33e51429362e457d29f7893e": {"text": "\"\"\"\nGithub readers utils.\n\nThis module contains utility functions for the Github readers.\n\"\"\"\nimport asyncio\nimport os\nimport time\nfrom abc import ABC, abstractmethod\nfrom typing import List, Tuple\n\nfrom gpt_index.readers.github_readers.github_api_client import (\n    GitBlobResponseModel,\n    GithubClient,\n    GitTreeResponseModel,\n)\n\n\ndef print_if_verbose(verbose: bool, message: str) -> None:\n    \"\"\"Log message if verbose is True.\"\"\"\n    if verbose:\n        print(message)\n\n\ndef get_file_extension(filename: str) -> str:\n    \"\"\"Get file extension.\"\"\"\n    return f\".{os.path.splitext(filename)[1][1:].lower()}\"\n\n\nclass BufferedAsyncIterator(ABC):\n    \"\"\"\n    Base class for buffered async iterators.\n\n    This class is to be used as a base class for async iterators\n    that need to buffer the results of an async operation.\n    The async operation is defined in the _fill_buffer method.\n    The _fill_buffer method is called when the buffer is empty.\n    \"\"\"\n\n    def __init__(self, buffer_size: int):\n        \"\"\"\n        Initialize params.\n\n        Args:\n            - `buffer_size (int)`: Size of the buffer.\n                It is also the number of items that will\n                be retrieved from the async operation at once.\n                see _fill_buffer. Defaults to 2. Setting it to 1\n                will result in the same behavior as a synchronous iterator.\n        \"\"\"\n        self._buffer_size = buffer_size\n        self._buffer: List[Tuple[GitBlobResponseModel, str]] = []\n        self._index = 0\n\n    @abstractmethod\n    async def _fill_buffer(self) -> None:\n        raise NotImplementedError\n\n    def __aiter__(self) -> \"BufferedAsyncIterator\":\n        \"\"\"Return the iterator object.\"\"\"\n        return self\n\n    async def __anext__(self) -> Tuple[GitBlobResponseModel, str]:\n        \"\"\"\n        Get next item.\n\n        Returns:\n            - `item (Tuple[GitBlobResponseModel, str])`: Next item.\n\n        Raises:\n            - `StopAsyncIteration`: If there are no more items.\n        \"\"\"\n        if not self._buffer:\n            await self._fill_buffer()\n\n        if not self._buffer:\n            raise StopAsyncIteration\n\n        item = self._buffer.pop(0)\n        self._index += 1\n        return item\n\n\nclass BufferedGitBlobDataIterator(BufferedAsyncIterator):\n    \"\"\"\n    Buffered async iterator for Git blobs.\n\n    This class is an async iterator that buffers the results of the get_blob operation.\n    It is used to retrieve the contents of the files in a Github repository.\n    getBlob endpoint supports up to 100 megabytes of content for blobs.\n    This concrete implementation of BufferedAsyncIterator allows you to lazily retrieve\n    the contents of the files in a Github repository.\n    Otherwise you would have to retrieve all the contents of\n    the files in the repository at once, which would\n    be problematic if the repository is large.\n    \"\"\"\n\n    def __init__(\n        self,\n        blobs_and_paths: List[Tuple[GitTreeResponseModel.GitTreeObject, str]],\n        github_client: GithubClient,\n        owner: str,\n        repo: str,\n        loop: asyncio.AbstractEventLoop,\n        buffer_size: int,\n        verbose: bool = False,\n    ):\n        \"\"\"\n        Initialize params.\n\n        Args:\n            - blobs_and_paths (List[Tuple[GitTreeResponseModel.GitTreeObject, str]]):\n                List of tuples containing the blob and the path of the file.\n            - github_client (GithubClient): Github client.\n            - owner (str): Owner of the repository.\n            - repo (str): Name of the repository.\n            - loop (asyncio.AbstractEventLoop): Event loop.\n            - buffer_size (int): Size of the buffer.\n        \"\"\"\n        super().__init__(buffer_size)\n        self._blobs_and_paths = blobs_and_paths\n        self._github_client = github_client\n        self._owner = owner\n        self._repo = repo\n        self._verbose = verbose\n        if loop is None:\n            loop = asyncio.get_event_loop()\n            if loop is None:\n                raise ValueError(\"No event loop found\")\n\n    async def _fill_buffer(self) -> None:\n        \"\"\"\n        Fill the buffer with the results of the get_blob operation.\n\n        The get_blob operation is called for each blob in the blobs_and_paths list.\n        The blobs are retrieved in batches of size buffer_size.\n        \"\"\"\n        del self._buffer[:]\n        self._buffer = []\n        start = self._index\n        end = min(start + self._buffer_size, len(self._blobs_and_paths))\n\n        if start >= end:\n            return\n\n        if self._verbose:\n            start_t = time.time()\n        results: List[GitBlobResponseModel] = await asyncio.gather(\n            *[\n                self._github_client.get_blob(self._owner, self._repo, blob.sha)\n                for blob, _ in self._blobs_and_paths[\n                    start:end\n                ]  # TODO: use batch_size instead of buffer_size for concurrent requests\n            ]\n        )\n        if self._verbose:\n            end_t = time.time()\n            blob_names_and_sizes = [\n                (blob.path, blob.size) for blob, _ in self._blobs_and_paths[start:end]\n            ]\n            print(\n                \"Time to get blobs (\"\n                + f\"{blob_names_and_sizes}\"\n                + f\"): {end_t - start_t:.2f} seconds\"\n            )\n\n        self._buffer = [\n            (result, path)\n            for result, (_, path) in zip(results, self._blobs_and_paths[start:end])\n        ]\n", "doc_id": "98449e52eda3d6ee33e51429362e457d29f7893e", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/utils.py", "file_name": "utils.py"}, "__type__": "Document"}, "b508aabb-bd02-44ba-859a-96e0194587c7": {"text": "\nThe summaries of these documents are that they contain code for accessing the Github API endpoints, dataclasses for the responses from the Github API's getTree, getBlob, getCommit, and getBranch endpoints, and utility functions for the Github readers. The github_repository_reader.py module contains functions for reading files from a Github repository, such as loading data from a commit or branch. The utils.py module contains utility functions for the Github readers, such as a BufferedAsyncIterator class for buffering the results of an async operation, and a BufferedGitBlobDataIterator class for lazily retrieving the contents of files in a Github repository.", "doc_id": "b508aabb-bd02-44ba-859a-96e0194587c7", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "node_info": null}, "1": {"text": "\"\"\"\nGithub API client for the GPT-Index library.\n\nThis module contains the Github API client for the GPT-Index library.\nIt is used by the Github readers to retrieve the data from Github.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional\n\nfrom dataclasses_json import DataClassJsonMixin\n\n\n@dataclass\nclass GitTreeResponseModel(DataClassJsonMixin):\n    \"\"\"\n    Dataclass for the response from the Github API's getTree endpoint.\n\n    Attributes:\n        - sha (str): SHA1 checksum ID of the tree.\n        - url (str): URL for the tree.\n        - tree (List[GitTreeObject]): List of objects in the tree.\n        - truncated (bool): Whether the tree is truncated.\n\n    Examples:\n        >>> tree = client.get_tree(\"owner\", \"repo\", \"branch\")\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 1, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "2": {"text": "\"repo\", \"branch\")\n        >>> tree.sha\n    \"\"\"\n\n    @dataclass\n    class GitTreeObject(DataClassJsonMixin):\n        \"\"\"\n        Dataclass for the objects in the tree.\n\n        Attributes:\n            - path (str): Path to the object.\n            - mode (str): Mode of the object.\n            - type (str): Type of the object.\n            - sha (str): SHA1 checksum ID of the object.\n            - url (str): URL for the object.\n            - size (Optional[int]): Size of the object (only for blobs).\n        \"\"\"\n\n        path: str\n        mode: str\n        type: str\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 2, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "3": {"text": "str\n        type: str\n        sha: str\n        url: str\n        size: Optional[int] = None\n\n    sha: str\n    url: str\n    tree: List[GitTreeObject]\n    truncated: bool\n\n\n@dataclass\nclass GitBlobResponseModel(DataClassJsonMixin):\n    \"\"\"\n    Dataclass for the response from the Github API's getBlob endpoint.\n\n    Attributes:\n        - content (str): Content of the blob.\n        - encoding (str): Encoding of the blob.\n        - url (str): URL for the blob.\n        - sha (str): SHA1 checksum ID of the blob.\n        - size (int): Size of the blob.\n        - node_id (str): Node ID of the blob.\n    \"\"\"\n\n    content:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 3, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "4": {"text": "of the blob.\n    \"\"\"\n\n    content: str\n    encoding: str\n    url: str\n    sha: str\n    size: int\n    node_id: str\n\n\n@dataclass\nclass GitCommitResponseModel(DataClassJsonMixin):\n    \"\"\"\n    Dataclass for the response from the Github API's getCommit endpoint.\n\n    Attributes:\n        - tree (Tree): Tree object for the commit.\n    \"\"\"\n\n    @dataclass\n    class Commit(DataClassJsonMixin):\n        \"\"\"Dataclass for the commit object in the commit. (commit.commit).\"\"\"\n\n        @dataclass\n        class Tree(DataClassJsonMixin):\n            \"\"\"\n            Dataclass for the tree object in the commit.\n\n            Attributes:\n            ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 4, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "5": {"text": " Attributes:\n                - sha (str): SHA for the commit\n            \"\"\"\n\n            sha: str\n\n        tree: Tree\n\n    commit: Commit\n\n\n@dataclass\nclass GitBranchResponseModel(DataClassJsonMixin):\n    \"\"\"\n    Dataclass for the response from the Github API's getBranch endpoint.\n\n    Attributes:\n        - commit (Commit): Commit object for the branch.\n    \"\"\"\n\n    @dataclass\n    class Commit(DataClassJsonMixin):\n        \"\"\"Dataclass for the commit object in the branch. (commit.commit).\"\"\"\n\n        @dataclass\n        class Commit(DataClassJsonMixin):\n            \"\"\"Dataclass for the commit object in the commit. (commit.commit.tree).\"\"\"\n\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 5, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "6": {"text": "commit. (commit.commit.tree).\"\"\"\n\n            @dataclass\n            class Tree(DataClassJsonMixin):\n                \"\"\"\n                Dataclass for the tree object in the commit.\n\n                Usage: commit.commit.tree.sha\n                \"\"\"\n\n                sha: str\n\n            tree: Tree\n\n        commit: Commit\n\n    commit: Commit\n\n\nclass GithubClient:\n    \"\"\"\n    An asynchronous client for interacting with the Github API.\n\n    This client is used for making API requests to Github.\n    It provides methods for accessing the Github API endpoints.\n    The client requires a Github token for authentication,\n    which can be", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 6, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "7": {"text": "requires a Github token for authentication,\n    which can be passed as an argument or set as an environment variable.\n    If no Github token is provided, the client will raise a ValueError.\n\n    Examples:\n        >>> client = GithubClient(\"my_github_token\")\n        >>> branch_info = client.get_branch(\"owner\", \"repo\", \"branch\")\n    \"\"\"\n\n    DEFAULT_BASE_URL = \"https://api.github.com\"\n    DEFAULT_API_VERSION = \"2022-11-28\"\n\n    def __init__(\n        self,\n        github_token: Optional[str] = None,\n        base_url: str = DEFAULT_BASE_URL,\n        api_version: str = DEFAULT_API_VERSION,\n        verbose: bool = False,\n    ) -> None:\n        \"\"\"\n        Initialize the", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 7, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "8": {"text": "    \"\"\"\n        Initialize the GithubClient.\n\n        Args:\n            - github_token (str): Github token for authentication.\n                If not provided, the client will try to get it from\n                the GITHUB_TOKEN environment variable.\n            - base_url (str): Base URL for the Github API\n                (defaults to \"https://api.github.com\").\n            - api_version (str): Github API version (defaults to \"2022-11-28\").\n\n        Raises:\n            ValueError: If no Github token is provided.\n        \"\"\"\n        if github_token is None:\n            github_token =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 8, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "9": {"text": "           github_token = os.getenv(\"GITHUB_TOKEN\")\n            if github_token is None:\n                raise ValueError(\n                    \"Please provide a Github token. \"\n                    + \"You can do so by passing it as an argument to the GithubReader,\"\n                    + \"or by setting the GITHUB_TOKEN environment variable.\"\n                )\n\n        self._base_url = base_url\n        self._api_version = api_version\n        self._verbose = verbose\n\n        self._endpoints = {\n            \"getTree\":", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 9, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "10": {"text": "           \"getTree\": \"/repos/{owner}/{repo}/git/trees/{tree_sha}\",\n            \"getBranch\": \"/repos/{owner}/{repo}/branches/{branch}\",\n            \"getBlob\": \"/repos/{owner}/{repo}/git/blobs/{file_sha}\",\n            \"getCommit\": \"/repos/{owner}/{repo}/commits/{commit_sha}\",\n        }\n\n        self._headers = {\n            \"Accept\": \"application/vnd.github+json\",\n            \"Authorization\": f\"Bearer {github_token}\",\n            \"X-GitHub-Api-Version\": f\"{self._api_version}\",\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 10, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "11": {"text": "       }\n\n    def get_all_endpoints(self) -> Dict[str, str]:\n        \"\"\"Get all available endpoints.\"\"\"\n        return {**self._endpoints}\n\n    async def request(\n        self,\n        endpoint: str,\n        method: str,\n        headers: Dict[str, Any] = {},\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"\n        Make an API request to the Github API.\n\n        This method is used for making API requests to the Github API.\n        It is used internally by the other methods in the client.\n\n        Args:\n            - `endpoint (str)`: Name of the endpoint to make the request to.\n            -", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 11, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "12": {"text": "request to.\n            - `method (str)`: HTTP method to use for the request.\n            - `headers (dict)`: HTTP headers to include in the request.\n            - `**kwargs`: Keyword arguments to pass to the endpoint URL.\n\n        Returns:\n            - `response (httpx.Response)`: Response from the API request.\n\n        Raises:\n            - ImportError: If the `httpx` library is not installed.\n            - httpx.HTTPError: If the API request fails.\n\n        Examples:\n            >>> response = client.request(\"getTree\", \"GET\",\n                                owner=\"owner\", repo=\"repo\",\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 12, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "13": {"text": "owner=\"owner\", repo=\"repo\",\n                                tree_sha=\"tree_sha\")\n        \"\"\"\n        try:\n            import httpx\n        except ImportError:\n            raise ImportError(\n                \"Please install httpx to use the GithubRepositoryReader. \"\n                \"You can do so by running `pip install httpx`.\"\n            )\n\n        _headers = {**self._headers, **headers}\n\n        _client: httpx.AsyncClient\n        async with httpx.AsyncClient(\n            headers=_headers, base_url=self._base_url\n        ) as", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 13, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "14": {"text": "       ) as _client:\n            try:\n                response = await _client.request(\n                    method, url=self._endpoints[endpoint].format(**kwargs)\n                )\n            except httpx.HTTPError as excp:\n                print(f\"HTTP Exception for {excp.request.url} - {excp}\")\n                raise excp\n            return response\n\n    async def get_branch(\n        self, owner: str, repo: str, branch: str\n    ) -> GitBranchResponseModel:\n        \"\"\"\n        Get information about a branch. (Github API", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 14, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "15": {"text": "     Get information about a branch. (Github API endpoint: getBranch).\n\n        Args:\n            - `owner (str)`: Owner of the repository.\n            - `repo (str)`: Name of the repository.\n            - `branch (str)`: Name of the branch.\n\n        Returns:\n            - `branch_info (GitBranchResponseModel)`: Information about the branch.\n\n        Examples:\n            >>> branch_info = client.get_branch(\"owner\", \"repo\", \"branch\")\n        \"\"\"\n        return GitBranchResponseModel.from_json(\n            (\n                await self.request(\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 15, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "16": {"text": "  await self.request(\n                    \"getBranch\", \"GET\", owner=owner, repo=repo, branch=branch\n                )\n            ).text\n        )\n\n    async def get_tree(\n        self, owner: str, repo: str, tree_sha: str\n    ) -> GitTreeResponseModel:\n        \"\"\"\n        Get information about a tree. (Github API endpoint: getTree).\n\n        Args:\n            - `owner (str)`: Owner of the repository.\n            - `repo (str)`: Name of the repository.\n            - `tree_sha (str)`: SHA of the tree.\n\n        Returns:\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 16, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "17": {"text": "the tree.\n\n        Returns:\n            - `tree_info (GitTreeResponseModel)`: Information about the tree.\n\n        Examples:\n            >>> tree_info = client.get_tree(\"owner\", \"repo\", \"tree_sha\")\n        \"\"\"\n        return GitTreeResponseModel.from_json(\n            (\n                await self.request(\n                    \"getTree\", \"GET\", owner=owner, repo=repo, tree_sha=tree_sha\n                )\n            ).text\n        )\n\n    async def get_blob(\n        self, owner: str, repo: str, file_sha: str\n    )", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 17, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "18": {"text": "repo: str, file_sha: str\n    ) -> GitBlobResponseModel:\n        \"\"\"\n        Get information about a blob. (Github API endpoint: getBlob).\n\n        Args:\n            - `owner (str)`: Owner of the repository.\n            - `repo (str)`: Name of the repository.\n            - `file_sha (str)`: SHA of the file.\n\n        Returns:\n            - `blob_info (GitBlobResponseModel)`: Information about the blob.\n\n        Examples:\n            >>> blob_info = client.get_blob(\"owner\", \"repo\", \"file_sha\")\n        \"\"\"\n        return GitBlobResponseModel.from_json(\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 18, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "19": {"text": "           (\n                await self.request(\n                    \"getBlob\", \"GET\", owner=owner, repo=repo, file_sha=file_sha\n                )\n            ).text\n        )\n\n    async def get_commit(\n        self, owner: str, repo: str, commit_sha: str\n    ) -> GitCommitResponseModel:\n        \"\"\"\n        Get information about a commit. (Github API endpoint: getCommit).\n\n        Args:\n            - `owner (str)`: Owner of the repository.\n            - `repo (str)`: Name of the repository.\n         ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 19, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "20": {"text": "of the repository.\n            - `commit_sha (str)`: SHA of the commit.\n\n        Returns:\n            - `commit_info (GitCommitResponseModel)`: Information about the commit.\n\n        Examples:\n            >>> commit_info = client.get_commit(\"owner\", \"repo\", \"commit_sha\")\n        \"\"\"\n        return GitCommitResponseModel.from_json(\n            (\n                await self.request(\n                    \"getCommit\", \"GET\", owner=owner, repo=repo, commit_sha=commit_sha\n                )\n            ).text\n        )\n\n\nif __name__ ==", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 20, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "21": {"text": "       )\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    async def main() -> None:\n        \"\"\"Test the GithubClient.\"\"\"\n        client = GithubClient()\n        response = await client.get_tree(\n            owner=\"ahmetkca\", repo=\"CommitAI\", tree_sha=\"with-body\"\n        )\n\n        for obj in response.tree:\n            if obj.type == \"blob\":\n                print(obj.path)\n                print(obj.sha)\n                blob_response = await client.get_blob(\n                    owner=\"ahmetkca\", repo=\"CommitAI\",", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 21, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "22": {"text": "  owner=\"ahmetkca\", repo=\"CommitAI\", file_sha=obj.sha\n                )\n                print(blob_response.content)\n\n    asyncio.run(main())\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "index": 22, "child_indices": [], "ref_doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "node_info": null}, "23": {"text": "\"\"\"\nGithub repository reader.\n\nRetrieves the contents of a Github repository and returns a list of documents.\nThe documents are either the contents of the files in the repository or\nthe text extracted from the files using the parser.\n\"\"\"\n\nimport asyncio\nimport base64\nimport binascii\nimport logging\nimport os\nimport pathlib\nimport tempfile\nfrom typing import Any, Callable, List, Optional, Tuple\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.file.base import DEFAULT_FILE_EXTRACTOR\nfrom gpt_index.readers.github_readers.github_api_client import (\n    GitBranchResponseModel,\n    GitCommitResponseModel,\n    GithubClient,\n    GitTreeResponseModel,\n)\nfrom gpt_index.readers.github_readers.utils import (\n    BufferedGitBlobDataIterator,\n    get_file_extension,\n    print_if_verbose,\n)\nfrom", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 23, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "24": {"text": "   print_if_verbose,\n)\nfrom gpt_index.readers.schema.base import Document\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass GithubRepositoryReader(BaseReader):\n    \"\"\"\n    Github repository reader.\n\n    Retrieves the contents of a Github repository and returns a list of documents.\n    The documents are either the contents of the files in the repository or the text\n    extracted from the files using the parser.\n\n    Examples:\n        >>> reader = GithubRepositoryReader(\"owner\", \"repo\")\n        >>> branch_documents = reader.load_data(branch=\"branch\")\n        >>> commit_documents = reader.load_data(commit_sha=\"commit_sha\")\n\n    \"\"\"\n\n    def __init__(\n        self,\n        owner: str,\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 24, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "25": {"text": "       owner: str,\n        repo: str,\n        use_parser: bool = True,\n        verbose: bool = False,\n        github_token: Optional[str] = None,\n        concurrent_requests: int = 5,\n        ignore_file_extensions: Optional[List[str]] = None,\n        ignore_directories: Optional[List[str]] = None,\n    ):\n        \"\"\"\n        Initialize params.\n\n        Args:\n            - owner (str): Owner of the repository.\n            - repo (str): Name of the repository.\n            - use_parser (bool): Whether to use the parser to extract\n                the text from the", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 25, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "26": {"text": "            the text from the files.\n            - verbose (bool): Whether to print verbose messages.\n            - github_token (str): Github token. If not provided,\n                it will be read from the GITHUB_TOKEN environment variable.\n            - concurrent_requests (int): Number of concurrent requests to\n                make to the Github API.\n            - ignore_file_extensions (List[str]): List of file extensions to ignore.\n                i.e. ['.png', '.jpg']\n            - ignore_directories (List[str]): List of directories to ignore.\n                i.e. ['node_modules', 'dist']\n\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 26, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "27": {"text": "   i.e. ['node_modules', 'dist']\n\n        Raises:\n            - `ValueError`: If the github_token is not provided and\n                the GITHUB_TOKEN environment variable is not set.\n        \"\"\"\n        super().__init__()\n        if github_token is None:\n            github_token = os.getenv(\"GITHUB_TOKEN\")\n            if github_token is None:\n                raise ValueError(\n                    \"Please provide a Github token. \"\n                    \"You can do so by passing it as an argument or\"\n                    +", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 27, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "28": {"text": "               + \"by setting the GITHUB_TOKEN environment variable.\"\n                )\n\n        self._owner = owner\n        self._repo = repo\n        self._use_parser = use_parser\n        self._verbose = verbose\n        self._concurrent_requests = concurrent_requests\n        self._ignore_file_extensions = ignore_file_extensions\n        self._ignore_directories = ignore_directories\n\n        # Set up the event loop\n        try:\n            self._loop = asyncio.get_running_loop()\n        except RuntimeError:\n            # If there is no running loop, create a new one\n            self._loop =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 28, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "29": {"text": "           self._loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(self._loop)\n\n        self._client = GithubClient(github_token)\n\n    def _load_data_from_commit(self, commit_sha: str) -> List[Document]:\n        \"\"\"\n        Load data from a commit.\n\n        Loads github repository data from a specific commit sha.\n\n        :param `commit`: commit sha\n\n        :return: list of documents\n        \"\"\"\n        commit_response: GitCommitResponseModel = self._loop.run_until_complete(\n            self._client.get_commit(self._owner, self._repo, commit_sha)\n        )\n\n        tree_sha =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 29, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "30": {"text": "   )\n\n        tree_sha = commit_response.commit.tree.sha\n        blobs_and_paths = self._loop.run_until_complete(self._recurse_tree(tree_sha))\n\n        print_if_verbose(self._verbose, f\"got {len(blobs_and_paths)} blobs\")\n\n        return self._loop.run_until_complete(\n            self._generate_documents(blobs_and_paths=blobs_and_paths)\n        )\n\n    def _load_data_from_branch(self, branch: str) -> List[Document]:\n        \"\"\"\n        Load data from a branch.\n\n        Loads github repository data from a specific branch.\n\n        :param `branch`: branch name\n\n        :return: list of documents\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 30, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "31": {"text": "     :return: list of documents\n        \"\"\"\n        branch_data: GitBranchResponseModel = self._loop.run_until_complete(\n            self._client.get_branch(self._owner, self._repo, branch)\n        )\n\n        tree_sha = branch_data.commit.commit.tree.sha\n        blobs_and_paths = self._loop.run_until_complete(self._recurse_tree(tree_sha))\n\n        print_if_verbose(self._verbose, f\"got {len(blobs_and_paths)} blobs\")\n\n        return self._loop.run_until_complete(\n            self._generate_documents(blobs_and_paths=blobs_and_paths)\n        )\n\n    def load_data(\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 31, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "32": {"text": "   def load_data(\n        self,\n        commit_sha: Optional[str] = None,\n        branch: Optional[str] = None,\n    ) -> List[Document]:\n        \"\"\"\n        Load data from a commit or a branch.\n\n        Loads github repository data from a specific commit sha or a branch.\n\n        :param `commit`: commit sha\n        :param `branch`: branch name\n\n        :return: list of documents\n        \"\"\"\n        if commit_sha is not None and branch is not None:\n            raise ValueError(\"You can only specify one of commit or branch.\")\n\n        if commit_sha is None and branch is None:\n            raise ValueError(\"You must specify one of commit or", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 32, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "33": {"text": "    raise ValueError(\"You must specify one of commit or branch.\")\n\n        if commit_sha is not None:\n            return self._load_data_from_commit(commit_sha)\n\n        if branch is not None:\n            return self._load_data_from_branch(branch)\n\n        raise ValueError(\"You must specify one of commit or branch.\")\n\n    async def _recurse_tree(\n        self, tree_sha: str, current_path: str = \"\", current_depth: int = 0\n    ) -> Any:\n        \"\"\"\n        Recursively get all blob tree objects in a tree.\n\n        And construct their full path relative to the root of the repository.\n        (see GitTreeResponseModel.GitTreeObject in\n            github_api_client.py", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 33, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "34": {"text": "         github_api_client.py for more information)\n\n        :param `tree_sha`: sha of the tree to recurse\n        :param `current_path`: current path of the tree\n        :param `current_depth`: current depth of the tree\n        :return: list of tuples of\n            (tree object, file's full path realtive to the root of the repo)\n        \"\"\"\n        blobs_and_full_paths: List[Tuple[GitTreeResponseModel.GitTreeObject, str]] = []\n        print_if_verbose(\n            self._verbose, \"\\t\" * current_depth + f\"current path: {current_path}\"\n        )\n\n        tree_data: GitTreeResponseModel = await self._client.get_tree(\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 34, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "35": {"text": "= await self._client.get_tree(\n            self._owner, self._repo, tree_sha\n        )\n        print_if_verbose(\n            self._verbose, \"\\t\" * current_depth + f\"processing tree {tree_sha}\"\n        )\n        for tree_obj in tree_data.tree:\n            file_path = os.path.join(current_path, tree_obj.path)\n            if tree_obj.type == \"tree\":\n                print_if_verbose(\n                    self._verbose,\n                    \"\\t\" * current_depth + f\"recursing into {tree_obj.path}\",\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 35, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "36": {"text": "into {tree_obj.path}\",\n                )\n                if self._ignore_directories is not None:\n                    if file_path in self._ignore_directories:\n                        print_if_verbose(\n                            self._verbose,\n                            \"\\t\" * current_depth\n                            + f\"ignoring tree {tree_obj.path} due to directory\",\n                        )\n                ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 36, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "37": {"text": "                       continue\n\n                blobs_and_full_paths.extend(\n                    await self._recurse_tree(tree_obj.sha, file_path, current_depth + 1)\n                )\n            elif tree_obj.type == \"blob\":\n                print_if_verbose(\n                    self._verbose, \"\\t\" * current_depth + f\"found blob {tree_obj.path}\"\n                )\n                if self._ignore_file_extensions is not None:\n                 ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 37, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "38": {"text": "                  if get_file_extension(file_path) in self._ignore_file_extensions:\n                        print_if_verbose(\n                            self._verbose,\n                            \"\\t\" * current_depth\n                            + f\"ignoring blob {tree_obj.path} due to file extension\",\n                        )\n                        continue\n                blobs_and_full_paths.append((tree_obj,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 38, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "39": {"text": "blobs_and_full_paths.append((tree_obj, file_path))\n        return blobs_and_full_paths\n\n    async def _generate_documents(\n        self, blobs_and_paths: List[Tuple[GitTreeResponseModel.GitTreeObject, str]]\n    ) -> List[Document]:\n        \"\"\"\n        Generate documents from a list of blobs and their full paths.\n\n        :param `blobs_and_paths`: list of tuples of\n            (tree object, file's full path in the repo realtive to the root of the repo)\n        :return: list of documents\n        \"\"\"\n        buffered_iterator = BufferedGitBlobDataIterator(\n            blobs_and_paths=blobs_and_paths,\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 39, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "40": {"text": "           github_client=self._client,\n            owner=self._owner,\n            repo=self._repo,\n            loop=self._loop,\n            buffer_size=self._concurrent_requests,  # TODO: make this configurable\n            verbose=self._verbose,\n        )\n\n        documents = []\n        async for blob_data, full_path in buffered_iterator:\n            print_if_verbose(self._verbose, f\"generating document for {full_path}\")\n            assert (\n                blob_data.encoding == \"base64\"\n            ), f\"blob encoding", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 40, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "41": {"text": "         ), f\"blob encoding {blob_data.encoding} not supported\"\n            decoded_bytes = None\n            try:\n                decoded_bytes = base64.b64decode(blob_data.content)\n                del blob_data.content\n            except binascii.Error:\n                print_if_verbose(\n                    self._verbose, f\"could not decode {full_path} as base64\"\n                )\n                continue\n\n            if self._use_parser:\n                document", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 41, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "42": {"text": "               document = self._parse_supported_file(\n                    file_path=full_path,\n                    file_content=decoded_bytes,\n                    tree_sha=blob_data.sha,\n                    tree_path=full_path,\n                )\n                if document is not None:\n                    documents.append(document)\n                else:\n                    continue\n\n            try:\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 42, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "43": {"text": "  try:\n                if decoded_bytes is None:\n                    raise ValueError(\"decoded_bytes is None\")\n                decoded_text = decoded_bytes.decode(\"utf-8\")\n            except UnicodeDecodeError:\n                print_if_verbose(\n                    self._verbose, f\"could not decode {full_path} as utf-8\"\n                )\n                continue\n            print_if_verbose(\n                self._verbose,\n                f\"got", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 43, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "44": {"text": "             f\"got {len(decoded_text)} characters\"\n                + f\"- adding to documents - {full_path}\",\n            )\n            document = Document(\n                text=decoded_text,\n                doc_id=blob_data.sha,\n                extra_info={\n                    \"file_path\": full_path,\n                    \"file_name\": full_path.split(\"/\")[-1],\n                },\n            )\n            documents.append(document)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 44, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "45": {"text": "        documents.append(document)\n        return documents\n\n    def _parse_supported_file(\n        self, file_path: str, file_content: bytes, tree_sha: str, tree_path: str\n    ) -> Optional[Document]:\n        \"\"\"\n        Parse a file if it is supported by a parser.\n\n        :param `file_path`: path of the file in the repo\n        :param `file_content`: content of the file\n        :return: Document if the file is supported by a parser, None otherwise\n        \"\"\"\n        file_extension = get_file_extension(file_path)\n        if (parser := DEFAULT_FILE_EXTRACTOR.get(file_extension)) is not None:\n            parser.init_parser()\n        ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 45, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "46": {"text": " parser.init_parser()\n            print_if_verbose(\n                self._verbose,\n                f\"parsing {file_path}\"\n                + f\"as {file_extension} with \"\n                + f\"{parser.__class__.__name__}\",\n            )\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                with tempfile.NamedTemporaryFile(\n                    dir=tmpdirname,\n                    suffix=f\".{file_extension}\",\n                 ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 46, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "47": {"text": "                  mode=\"w+b\",\n                    delete=False,\n                ) as tmpfile:\n                    print_if_verbose(\n                        self._verbose,\n                        \"created a temporary file\"\n                        + f\"{tmpfile.name} for parsing {file_path}\",\n                    )\n                    tmpfile.write(file_content)\n                   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 47, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "48": {"text": "                tmpfile.flush()\n                    tmpfile.close()\n                    try:\n                        parsed_file = parser.parse_file(pathlib.Path(tmpfile.name))\n                        parsed_file = \"\\n\\n\".join(parsed_file)\n                    except Exception as e:\n                        print_if_verbose(\n                            self._verbose, f\"error while parsing {file_path}\"\n            ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 48, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "49": {"text": "                       )\n                        logger.error(\n                            \"Error while parsing \"\n                            + f\"{file_path} with \"\n                            + f\"{parser.__class__.__name__}:\\n{e}\"\n                        )\n                        parsed_file = None\n                    finally:\n                   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 49, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "50": {"text": "                    os.remove(tmpfile.name)\n                    if parsed_file is None:\n                        return None\n                    return Document(\n                        text=parsed_file,\n                        doc_id=tree_sha,\n                        extra_info={\n                            \"file_path\": file_path,\n                            \"file_name\":", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 50, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "51": {"text": "           \"file_name\": tree_path,\n                        },\n                    )\n        return None\n\n\nif __name__ == \"__main__\":\n    import time\n\n    def timeit(func: Callable) -> Callable:\n        \"\"\"Time a function.\"\"\"\n\n        def wrapper(*args: Any, **kwargs: Any) -> None:\n            \"\"\"Callcuate time taken to run a function.\"\"\"\n            start = time.time()\n            func(*args, **kwargs)\n            end = time.time()\n            print(f\"Time taken: {end - start} seconds for {func.__name__}\")\n\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 51, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "52": {"text": "for {func.__name__}\")\n\n        return wrapper\n\n    reader1 = GithubRepositoryReader(\n        github_token=os.environ[\"GITHUB_TOKEN\"],\n        owner=\"jerryjliu\",\n        repo=\"gpt_index\",\n        use_parser=False,\n        verbose=True,\n        ignore_directories=[\"examples\"],\n    )\n\n    @timeit\n    def load_data_from_commit() -> None:\n        \"\"\"Load data from a commit.\"\"\"\n        documents = reader1.load_data(\n            commit_sha=\"22e198b3b166b5facd2843d6a62ac0db07894a13\"\n        )\n        for document in documents:\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 52, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "53": {"text": "in documents:\n            print(document.extra_info)\n\n    @timeit\n    def load_data_from_branch() -> None:\n        \"\"\"Load data from a branch.\"\"\"\n        documents = reader1.load_data(branch=\"main\")\n        for document in documents:\n            print(document.extra_info)\n\n    input(\"Press enter to load github repository from branch name...\")\n\n    load_data_from_branch()\n\n    input(\"Press enter to load github repository from commit sha...\")\n\n    load_data_from_commit()\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "index": 53, "child_indices": [], "ref_doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "node_info": null}, "54": {"text": "\"\"\"\nGithub readers utils.\n\nThis module contains utility functions for the Github readers.\n\"\"\"\nimport asyncio\nimport os\nimport time\nfrom abc import ABC, abstractmethod\nfrom typing import List, Tuple\n\nfrom gpt_index.readers.github_readers.github_api_client import (\n    GitBlobResponseModel,\n    GithubClient,\n    GitTreeResponseModel,\n)\n\n\ndef print_if_verbose(verbose: bool, message: str) -> None:\n    \"\"\"Log message if verbose is True.\"\"\"\n    if verbose:\n        print(message)\n\n\ndef get_file_extension(filename: str) -> str:\n    \"\"\"Get file extension.\"\"\"\n    return f\".{os.path.splitext(filename)[1][1:].lower()}\"\n\n\nclass BufferedAsyncIterator(ABC):\n    \"\"\"\n    Base class for buffered async iterators.\n\n    This class is to be used as a base class for async iterators\n    that need to buffer the results of an async", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/utils.py", "file_name": "utils.py"}, "index": 54, "child_indices": [], "ref_doc_id": "98449e52eda3d6ee33e51429362e457d29f7893e", "node_info": null}, "55": {"text": "iterators\n    that need to buffer the results of an async operation.\n    The async operation is defined in the _fill_buffer method.\n    The _fill_buffer method is called when the buffer is empty.\n    \"\"\"\n\n    def __init__(self, buffer_size: int):\n        \"\"\"\n        Initialize params.\n\n        Args:\n            - `buffer_size (int)`: Size of the buffer.\n                It is also the number of items that will\n                be retrieved from the async operation at once.\n                see _fill_buffer. Defaults to 2. Setting it to 1\n                will result in the same behavior as a synchronous iterator.\n        \"\"\"\n        self._buffer_size = buffer_size\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/utils.py", "file_name": "utils.py"}, "index": 55, "child_indices": [], "ref_doc_id": "98449e52eda3d6ee33e51429362e457d29f7893e", "node_info": null}, "56": {"text": "= buffer_size\n        self._buffer: List[Tuple[GitBlobResponseModel, str]] = []\n        self._index = 0\n\n    @abstractmethod\n    async def _fill_buffer(self) -> None:\n        raise NotImplementedError\n\n    def __aiter__(self) -> \"BufferedAsyncIterator\":\n        \"\"\"Return the iterator object.\"\"\"\n        return self\n\n    async def __anext__(self) -> Tuple[GitBlobResponseModel, str]:\n        \"\"\"\n        Get next item.\n\n        Returns:\n            - `item (Tuple[GitBlobResponseModel, str])`: Next item.\n\n        Raises:\n            - `StopAsyncIteration`: If there are no more items.\n        \"\"\"\n        if not self._buffer:\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/utils.py", "file_name": "utils.py"}, "index": 56, "child_indices": [], "ref_doc_id": "98449e52eda3d6ee33e51429362e457d29f7893e", "node_info": null}, "57": {"text": "       if not self._buffer:\n            await self._fill_buffer()\n\n        if not self._buffer:\n            raise StopAsyncIteration\n\n        item = self._buffer.pop(0)\n        self._index += 1\n        return item\n\n\nclass BufferedGitBlobDataIterator(BufferedAsyncIterator):\n    \"\"\"\n    Buffered async iterator for Git blobs.\n\n    This class is an async iterator that buffers the results of the get_blob operation.\n    It is used to retrieve the contents of the files in a Github repository.\n    getBlob endpoint supports up to 100 megabytes of content for blobs.\n    This concrete implementation of BufferedAsyncIterator allows you to lazily retrieve\n    the contents of the files in a Github repository.\n    Otherwise you would have to retrieve all the contents of\n    the files in the repository at once, which", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/utils.py", "file_name": "utils.py"}, "index": 57, "child_indices": [], "ref_doc_id": "98449e52eda3d6ee33e51429362e457d29f7893e", "node_info": null}, "58": {"text": "of\n    the files in the repository at once, which would\n    be problematic if the repository is large.\n    \"\"\"\n\n    def __init__(\n        self,\n        blobs_and_paths: List[Tuple[GitTreeResponseModel.GitTreeObject, str]],\n        github_client: GithubClient,\n        owner: str,\n        repo: str,\n        loop: asyncio.AbstractEventLoop,\n        buffer_size: int,\n        verbose: bool = False,\n    ):\n        \"\"\"\n        Initialize params.\n\n        Args:\n            - blobs_and_paths (List[Tuple[GitTreeResponseModel.GitTreeObject, str]]):\n                List of tuples containing the blob and the path of", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/utils.py", "file_name": "utils.py"}, "index": 58, "child_indices": [], "ref_doc_id": "98449e52eda3d6ee33e51429362e457d29f7893e", "node_info": null}, "59": {"text": "    List of tuples containing the blob and the path of the file.\n            - github_client (GithubClient): Github client.\n            - owner (str): Owner of the repository.\n            - repo (str): Name of the repository.\n            - loop (asyncio.AbstractEventLoop): Event loop.\n            - buffer_size (int): Size of the buffer.\n        \"\"\"\n        super().__init__(buffer_size)\n        self._blobs_and_paths = blobs_and_paths\n        self._github_client = github_client\n        self._owner = owner\n        self._repo = repo\n        self._verbose = verbose\n        if loop is None:\n            loop =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/utils.py", "file_name": "utils.py"}, "index": 59, "child_indices": [], "ref_doc_id": "98449e52eda3d6ee33e51429362e457d29f7893e", "node_info": null}, "60": {"text": "None:\n            loop = asyncio.get_event_loop()\n            if loop is None:\n                raise ValueError(\"No event loop found\")\n\n    async def _fill_buffer(self) -> None:\n        \"\"\"\n        Fill the buffer with the results of the get_blob operation.\n\n        The get_blob operation is called for each blob in the blobs_and_paths list.\n        The blobs are retrieved in batches of size buffer_size.\n        \"\"\"\n        del self._buffer[:]\n        self._buffer = []\n        start = self._index\n        end = min(start + self._buffer_size, len(self._blobs_and_paths))\n\n        if start >= end:\n            return\n\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/utils.py", "file_name": "utils.py"}, "index": 60, "child_indices": [], "ref_doc_id": "98449e52eda3d6ee33e51429362e457d29f7893e", "node_info": null}, "61": {"text": "          return\n\n        if self._verbose:\n            start_t = time.time()\n        results: List[GitBlobResponseModel] = await asyncio.gather(\n            *[\n                self._github_client.get_blob(self._owner, self._repo, blob.sha)\n                for blob, _ in self._blobs_and_paths[\n                    start:end\n                ]  # TODO: use batch_size instead of buffer_size for concurrent requests\n            ]\n        )\n        if self._verbose:\n            end_t = time.time()\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/utils.py", "file_name": "utils.py"}, "index": 61, "child_indices": [], "ref_doc_id": "98449e52eda3d6ee33e51429362e457d29f7893e", "node_info": null}, "62": {"text": "  end_t = time.time()\n            blob_names_and_sizes = [\n                (blob.path, blob.size) for blob, _ in self._blobs_and_paths[start:end]\n            ]\n            print(\n                \"Time to get blobs (\"\n                + f\"{blob_names_and_sizes}\"\n                + f\"): {end_t - start_t:.2f} seconds\"\n            )\n\n        self._buffer = [\n            (result, path)\n            for result, (_, path) in zip(results, self._blobs_and_paths[start:end])\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/utils.py", "file_name": "utils.py"}, "index": 62, "child_indices": [], "ref_doc_id": "98449e52eda3d6ee33e51429362e457d29f7893e", "node_info": null}, "63": {"text": "       ]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/utils.py", "file_name": "utils.py"}, "index": 63, "child_indices": [], "ref_doc_id": "98449e52eda3d6ee33e51429362e457d29f7893e", "node_info": null}, "64": {"text": "This code file contains the Github API client for the GPT-Index library. It provides methods for accessing the Github API endpoints, such as getTree, getBranch, getBlob, and getCommit. It requires a Github token for authentication, which can be passed as an argument or set as an environment variable. The code also contains dataclasses for the responses from the Github API endpoints, such as GitTreeResponseModel, GitBlobResponseModel, GitCommitResponseModel, and GitBranchResponseModel.", "doc_id": null, "embedding": null, "extra_info": null, "index": 64, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "65": {"text": "This code file is a Github repository reader. It retrieves the contents of a Github repository and returns a list of documents. The documents are either the contents of the files in the repository or the text extracted from the files using the parser. It uses the Github API Client to make requests to the Github API and get information about branches, trees, blobs, and commits. It also has functions to get the file extension and print if verbose. It also has a main function to test the Github Client.", "doc_id": null, "embedding": null, "extra_info": null, "index": 65, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "66": {"text": "The GithubRepositoryReader class is a Python script that retrieves the contents of a Github repository and returns a list of documents. It can be used to load data from a specific commit sha or a branch. It uses the Github API to recursively get all blob tree objects in a tree and construct their full path relative to the root of the repository. It also has the option to use a parser to extract the text from the files. It has several parameters that can be set, such as the Github token, the number of concurrent requests, and the list of file extensions and directories to ignore.", "doc_id": null, "embedding": null, "extra_info": null, "index": 66, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}, "67": {"text": "This code file is a python script that reads a Github repository and generates documents from the files in the repository. It uses a BufferedGitBlobDataIterator to iterate through the files in the repository, and it can parse supported files using a parser. It also has the ability to ignore certain directories and file extensions. The generated documents contain the file's content, its full path, and its SHA.", "doc_id": null, "embedding": null, "extra_info": null, "index": 67, "child_indices": [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], "ref_doc_id": null, "node_info": null}, "68": {"text": "This code file contains two modules, github_repository_reader.py and utils.py. The github_repository_reader.py module contains functions for reading files from a Github repository, such as loading data from a commit or branch. The utils.py module contains utility functions for the Github readers, such as a BufferedAsyncIterator class for buffering the results of an async operation, and a BufferedGitBlobDataIterator class for lazily retrieving the contents of files in a Github repository.", "doc_id": null, "embedding": null, "extra_info": null, "index": 68, "child_indices": [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59], "ref_doc_id": null, "node_info": null}, "69": {"text": "utils.py is a file in the gpt_index/readers/github_readers directory that contains functions related to retrieving blobs from a GitHub repository. The main function, _fill_buffer(), retrieves blobs in batches of size buffer_size from the blobs_and_paths list and stores them in the buffer. If verbose is set to true, the time it takes to get the blobs is printed. The results are stored in a list of GitBlobResponseModel objects and the paths are stored in a list of tuples.", "doc_id": null, "embedding": null, "extra_info": null, "index": 69, "child_indices": [60, 61, 62, 63], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"64": {"text": "This code file contains the Github API client for the GPT-Index library. It provides methods for accessing the Github API endpoints, such as getTree, getBranch, getBlob, and getCommit. It requires a Github token for authentication, which can be passed as an argument or set as an environment variable. The code also contains dataclasses for the responses from the Github API endpoints, such as GitTreeResponseModel, GitBlobResponseModel, GitCommitResponseModel, and GitBranchResponseModel.", "doc_id": null, "embedding": null, "extra_info": null, "index": 64, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "65": {"text": "This code file is a Github repository reader. It retrieves the contents of a Github repository and returns a list of documents. The documents are either the contents of the files in the repository or the text extracted from the files using the parser. It uses the Github API Client to make requests to the Github API and get information about branches, trees, blobs, and commits. It also has functions to get the file extension and print if verbose. It also has a main function to test the Github Client.", "doc_id": null, "embedding": null, "extra_info": null, "index": 65, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "66": {"text": "The GithubRepositoryReader class is a Python script that retrieves the contents of a Github repository and returns a list of documents. It can be used to load data from a specific commit sha or a branch. It uses the Github API to recursively get all blob tree objects in a tree and construct their full path relative to the root of the repository. It also has the option to use a parser to extract the text from the files. It has several parameters that can be set, such as the Github token, the number of concurrent requests, and the list of file extensions and directories to ignore.", "doc_id": null, "embedding": null, "extra_info": null, "index": 66, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}, "67": {"text": "This code file is a python script that reads a Github repository and generates documents from the files in the repository. It uses a BufferedGitBlobDataIterator to iterate through the files in the repository, and it can parse supported files using a parser. It also has the ability to ignore certain directories and file extensions. The generated documents contain the file's content, its full path, and its SHA.", "doc_id": null, "embedding": null, "extra_info": null, "index": 67, "child_indices": [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], "ref_doc_id": null, "node_info": null}, "68": {"text": "This code file contains two modules, github_repository_reader.py and utils.py. The github_repository_reader.py module contains functions for reading files from a Github repository, such as loading data from a commit or branch. The utils.py module contains utility functions for the Github readers, such as a BufferedAsyncIterator class for buffering the results of an async operation, and a BufferedGitBlobDataIterator class for lazily retrieving the contents of files in a Github repository.", "doc_id": null, "embedding": null, "extra_info": null, "index": 68, "child_indices": [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59], "ref_doc_id": null, "node_info": null}, "69": {"text": "utils.py is a file in the gpt_index/readers/github_readers directory that contains functions related to retrieving blobs from a GitHub repository. The main function, _fill_buffer(), retrieves blobs in batches of size buffer_size from the blobs_and_paths list and stores them in the buffer. If verbose is set to true, the time it takes to get the blobs is printed. The results are stored in a list of GitBlobResponseModel objects and the paths are stored in a list of tuples.", "doc_id": null, "embedding": null, "extra_info": null, "index": 69, "child_indices": [60, 61, 62, 63], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}