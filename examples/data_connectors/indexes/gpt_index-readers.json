{"index_struct": {"text": "\nThe summaries of the documents are descriptions or summaries of the functions they contain. The Slack.py document contains a reader for Slack channels, the String_iterable.py document contains a reader for an iterable of strings, the Twitter.py document contains a reader for tweets of a twitter handle, the Web.py document contains a web scraper, the gpt_index/readers/web.py file contains a SimpleWebPageReader class, the gpt_index/readers/wikipedia.py file contains a WikipediaReader class, the gpt_index/readers/youtube_transcript.py file contains a YoutubeTranscriptReader class, the discord_reader.py file contains a DiscordReader class, the download.py file contains a function that downloads a single loader from the Loader Hub, the faiss.py file contains a class that retrieves documents through an existing in-memory Faiss index, the mbox.py file contains a class that reads a set of emails saved in the mbox format, the mongo.py file contains a class that reads documents from a Mongo database, the notion.py file contains functions for reading and querying Notion pages, the obsidian.py file contains functions", "doc_id": "872e0fc1-91a0-4060-8af0-44375c8cc803", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Data Connectors for GPT Index.\n\nThis module contains the data connectors for GPT Index. Each connector inherits\nfrom a `BaseReader` class, connects to a data source, and loads Document objects\nfrom that data source.\n\nYou may also choose to construct Document objects manually, for instance\nin our `Insert How-To Guide <../how_to/insert.html>`_. See below for the API\ndefinition of a Document - the bare minimum is a `text` property.\n\n\"\"\"\n\nfrom gpt_index.readers.discord_reader import DiscordReader\nfrom gpt_index.readers.faiss import FaissReader\n\n# readers\nfrom gpt_index.readers.file.base import SimpleDirectoryReader\nfrom gpt_index.readers.github_readers.github_repository_reader import (\n    GithubRepositoryReader,\n)\nfrom gpt_index.readers.google_readers.gdocs import GoogleDocsReader\nfrom gpt_index.readers.make_com.wrapper import MakeWrapper\nfrom gpt_index.readers.mbox import MboxReader\nfrom gpt_index.readers.mongo import", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "846471c13398def5f566e12afc3117ddc30b9c88", "node_info": null}, "1": {"text": "MboxReader\nfrom gpt_index.readers.mongo import SimpleMongoReader\nfrom gpt_index.readers.notion import NotionPageReader\nfrom gpt_index.readers.obsidian import ObsidianReader\nfrom gpt_index.readers.pinecone import PineconeReader\nfrom gpt_index.readers.qdrant import QdrantReader\nfrom gpt_index.readers.schema.base import Document\nfrom gpt_index.readers.slack import SlackReader\nfrom gpt_index.readers.string_iterable import StringIterableReader\nfrom gpt_index.readers.twitter import TwitterTweetReader\nfrom gpt_index.readers.weaviate.reader import WeaviateReader\nfrom gpt_index.readers.web import (\n    BeautifulSoupWebReader,\n    RssReader,\n    SimpleWebPageReader,\n    TrafilaturaWebReader,\n)\nfrom gpt_index.readers.wikipedia import WikipediaReader\nfrom gpt_index.readers.youtube_transcript import YoutubeTranscriptReader\n\n__all__ = [\n    \"WikipediaReader\",\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/__init__.py", "file_name": "__init__.py"}, "index": 1, "child_indices": [], "ref_doc_id": "846471c13398def5f566e12afc3117ddc30b9c88", "node_info": null}, "2": {"text": "= [\n    \"WikipediaReader\",\n    \"YoutubeTranscriptReader\",\n    \"SimpleDirectoryReader\",\n    \"SimpleMongoReader\",\n    \"NotionPageReader\",\n    \"GoogleDocsReader\",\n    \"DiscordReader\",\n    \"SlackReader\",\n    \"WeaviateReader\",\n    \"PineconeReader\",\n    \"QdrantReader\",\n    \"FaissReader\",\n    \"Document\",\n    \"StringIterableReader\",\n    \"SimpleWebPageReader\",\n    \"BeautifulSoupWebReader\",\n    \"TrafilaturaWebReader\",\n    \"RssReader\",\n    \"MakeWrapper\",\n    \"TwitterTweetReader\",\n    \"ObsidianReader\",\n    \"GithubRepositoryReader\",\n    \"MboxReader\",\n]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/__init__.py", "file_name": "__init__.py"}, "index": 2, "child_indices": [], "ref_doc_id": "846471c13398def5f566e12afc3117ddc30b9c88", "node_info": null}, "3": {"text": "\"\"\"Base reader class.\"\"\"\nfrom abc import abstractmethod\nfrom typing import Any, List\n\nfrom langchain.docstore.document import Document as LCDocument\n\nfrom gpt_index.readers.schema.base import Document\n\n\nclass BaseReader:\n    \"\"\"Utilities for loading data from a directory.\"\"\"\n\n    @abstractmethod\n    def load_data(self, *args: Any, **load_kwargs: Any) -> List[Document]:\n        \"\"\"Load data from the input directory.\"\"\"\n\n    def load_langchain_documents(self, **load_kwargs: Any) -> List[LCDocument]:\n        \"\"\"Load data in LangChain document format.\"\"\"\n        docs = self.load_data(**load_kwargs)\n        return [d.to_langchain_format() for d in docs]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/base.py", "file_name": "base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "f5cc30de3365fe4a08a28f1e95a9bdd2b17e09a0", "node_info": null}, "4": {"text": "\"\"\"Database Reader.\"\"\"\n\nfrom typing import Any, List, Optional\n\nfrom sqlalchemy import text\nfrom sqlalchemy.engine import Engine\n\nfrom gpt_index.langchain_helpers.sql_wrapper import SQLDatabase\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass DatabaseReader(BaseReader):\n    \"\"\"Simple Database reader.\n\n    Concatenates each row into Document used by GPT Index.\n\n    Args:\n        sql_database (Optional[SQLDatabase]): SQL database to use,\n            including table names to specify.\n            See :ref:`Ref-Struct-Store` for more details.\n\n        OR\n\n        engine (Optional[Engine]): SQLAlchemy Engine object of the database connection.\n\n        OR\n\n        uri (Optional[str]): uri of the database connection.\n\n        OR\n\n        scheme (Optional[str]): scheme of the database", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/database.py", "file_name": "database.py"}, "index": 4, "child_indices": [], "ref_doc_id": "4098c3e4bb1452ddb95150e467c23f0d503b34e1", "node_info": null}, "5": {"text": "   scheme (Optional[str]): scheme of the database connection.\n        host (Optional[str]): host of the database connection.\n        port (Optional[int]): port of the database connection.\n        user (Optional[str]): user of the database connection.\n        password (Optional[str]): password of the database connection.\n        dbname (Optional[str]): dbname of the database connection.\n\n    Returns:\n        DatabaseReader: A DatabaseReader object.\n    \"\"\"\n\n    def __init__(\n        self,\n        sql_database: Optional[SQLDatabase] = None,\n        engine: Optional[Engine] = None,\n        uri: Optional[str] = None,\n        scheme: Optional[str] = None,\n        host: Optional[str] = None,\n        port: Optional[str] = None,\n        user:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/database.py", "file_name": "database.py"}, "index": 5, "child_indices": [], "ref_doc_id": "4098c3e4bb1452ddb95150e467c23f0d503b34e1", "node_info": null}, "6": {"text": "= None,\n        user: Optional[str] = None,\n        password: Optional[str] = None,\n        dbname: Optional[str] = None,\n        *args: Optional[Any],\n        **kwargs: Optional[Any],\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        if sql_database:\n            self.sql_database = sql_database\n        elif engine:\n            self.sql_database = SQLDatabase(engine, *args, **kwargs)\n        elif uri:\n            self.uri = uri\n            self.sql_database = SQLDatabase.from_uri(uri, *args, **kwargs)\n        elif scheme and host and port and user and password and dbname:\n            uri =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/database.py", "file_name": "database.py"}, "index": 6, "child_indices": [], "ref_doc_id": "4098c3e4bb1452ddb95150e467c23f0d503b34e1", "node_info": null}, "7": {"text": "           uri = f\"{scheme}://{user}:{password}@{host}:{port}/{dbname}\"\n            self.uri = uri\n            self.sql_database = SQLDatabase.from_uri(uri, *args, **kwargs)\n        else:\n            raise ValueError(\n                \"You must provide either a SQLDatabase, \"\n                \"a SQL Alchemy Engine, a valid connection URI, or a valid \"\n                \"set of credentials.\"\n            )\n\n    def load_data(self, query: str) -> List[Document]:\n        \"\"\"Query and load data from the Database, returning a list of Documents.\n\n        Args:\n            query (str): Query parameter to filter tables and", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/database.py", "file_name": "database.py"}, "index": 7, "child_indices": [], "ref_doc_id": "4098c3e4bb1452ddb95150e467c23f0d503b34e1", "node_info": null}, "8": {"text": "    query (str): Query parameter to filter tables and rows.\n\n        Returns:\n            List[Document]: A list of Document objects.\n        \"\"\"\n        documents = []\n        with self.sql_database.engine.connect() as connection:\n            if query is None:\n                raise ValueError(\"A query parameter is necessary to filter the data\")\n            else:\n                result = connection.execute(text(query))\n\n            for item in result.fetchall():\n                documents.append(Document(item[0]))\n        return documents\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/database.py", "file_name": "database.py"}, "index": 8, "child_indices": [], "ref_doc_id": "4098c3e4bb1452ddb95150e467c23f0d503b34e1", "node_info": null}, "9": {"text": "\"\"\"Discord reader.\n\nNote: this file is named discord_reader.py to avoid conflicts with the\ndiscord.py module.\n\n\"\"\"\n\nimport asyncio\nimport logging\nimport os\nfrom typing import List, Optional\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nasync def read_channel(\n    discord_token: str, channel_id: int, limit: Optional[int], oldest_first: bool\n) -> str:\n    \"\"\"Async read channel.\n\n    Note: This is our hack to create a synchronous interface to the\n    async discord.py API. We use the `asyncio` module to run\n    this function with `asyncio.get_event_loop().run_until_complete`.\n\n    \"\"\"\n    import discord  # noqa: F401\n\n    messages: List[discord.Message] = []\n\n    class CustomClient(discord.Client):\n        async def on_ready(self) -> None:\n          ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/discord_reader.py", "file_name": "discord_reader.py"}, "index": 9, "child_indices": [], "ref_doc_id": "8d9a229a3248b1617b67774eac291a2594ec686e", "node_info": null}, "10": {"text": "-> None:\n            try:\n                logging.info(f\"{self.user} has connected to Discord!\")\n                channel = client.get_channel(channel_id)\n                # only work for text channels for now\n                if not isinstance(channel, discord.TextChannel):\n                    raise ValueError(\n                        f\"Channel {channel_id} is not a text channel. \"\n                        \"Only text channels are supported for now.\"\n                    )\n                # thread_dict maps thread_id to thread\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/discord_reader.py", "file_name": "discord_reader.py"}, "index": 10, "child_indices": [], "ref_doc_id": "8d9a229a3248b1617b67774eac291a2594ec686e", "node_info": null}, "11": {"text": "# thread_dict maps thread_id to thread\n                thread_dict = {}\n                for thread in channel.threads:\n                    thread_dict[thread.id] = thread\n\n                async for msg in channel.history(\n                    limit=limit, oldest_first=oldest_first\n                ):\n                    messages.append(msg)\n                    if msg.id in thread_dict:\n                        thread = thread_dict[msg.id]\n                        async for thread_msg in", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/discord_reader.py", "file_name": "discord_reader.py"}, "index": 11, "child_indices": [], "ref_doc_id": "8d9a229a3248b1617b67774eac291a2594ec686e", "node_info": null}, "12": {"text": "         async for thread_msg in thread.history(\n                            limit=limit, oldest_first=oldest_first\n                        ):\n                            messages.append(thread_msg)\n            except Exception as e:\n                logging.error(\"Encountered error: \" + str(e))\n            finally:\n                await self.close()\n\n    intents = discord.Intents.default()\n    intents.message_content = True\n    client = CustomClient(intents=intents)\n    await client.start(discord_token)\n\n    msg_txt_list = [m.content for m in", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/discord_reader.py", "file_name": "discord_reader.py"}, "index": 12, "child_indices": [], "ref_doc_id": "8d9a229a3248b1617b67774eac291a2594ec686e", "node_info": null}, "13": {"text": "   msg_txt_list = [m.content for m in messages]\n\n    return \"\\n\\n\".join(msg_txt_list)\n\n\nclass DiscordReader(BaseReader):\n    \"\"\"Discord reader.\n\n    Reads conversations from channels.\n\n    Args:\n        discord_token (Optional[str]): Discord token. If not provided, we\n            assume the environment variable `DISCORD_TOKEN` is set.\n\n    \"\"\"\n\n    def __init__(self, discord_token: Optional[str] = None) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            import discord  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`discord.py` package not found, please run `pip install discord.py`\"\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/discord_reader.py", "file_name": "discord_reader.py"}, "index": 13, "child_indices": [], "ref_doc_id": "8d9a229a3248b1617b67774eac291a2594ec686e", "node_info": null}, "14": {"text": "install discord.py`\"\n            )\n        if discord_token is None:\n            discord_token = os.environ[\"DISCORD_TOKEN\"]\n            if discord_token is None:\n                raise ValueError(\n                    \"Must specify `discord_token` or set environment \"\n                    \"variable `DISCORD_TOKEN`.\"\n                )\n\n        self.discord_token = discord_token\n\n    def _read_channel(\n        self, channel_id: int, limit: Optional[int] = None, oldest_first: bool = True\n    ) -> str:\n        \"\"\"Read channel.\"\"\"\n        result =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/discord_reader.py", "file_name": "discord_reader.py"}, "index": 14, "child_indices": [], "ref_doc_id": "8d9a229a3248b1617b67774eac291a2594ec686e", "node_info": null}, "15": {"text": " \"\"\"Read channel.\"\"\"\n        result = asyncio.get_event_loop().run_until_complete(\n            read_channel(\n                self.discord_token, channel_id, limit=limit, oldest_first=oldest_first\n            )\n        )\n        return result\n\n    def load_data(\n        self,\n        channel_ids: List[int],\n        limit: Optional[int] = None,\n        oldest_first: bool = True,\n    ) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            channel_ids (List[int]): List of channel ids to read.\n            limit (Optional[int]): Maximum number of messages to read.\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/discord_reader.py", "file_name": "discord_reader.py"}, "index": 15, "child_indices": [], "ref_doc_id": "8d9a229a3248b1617b67774eac291a2594ec686e", "node_info": null}, "16": {"text": "Maximum number of messages to read.\n            oldest_first (bool): Whether to read oldest messages first.\n                Defaults to `True`.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        results: List[Document] = []\n        for channel_id in channel_ids:\n            if not isinstance(channel_id, int):\n                raise ValueError(\n                    f\"Channel id {channel_id} must be an integer, \"\n                    f\"not {type(channel_id)}.\"\n                )\n            channel_content = self._read_channel(\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/discord_reader.py", "file_name": "discord_reader.py"}, "index": 16, "child_indices": [], "ref_doc_id": "8d9a229a3248b1617b67774eac291a2594ec686e", "node_info": null}, "17": {"text": " channel_content = self._read_channel(\n                channel_id, limit=limit, oldest_first=oldest_first\n            )\n            results.append(\n                Document(channel_content, extra_info={\"channel\": channel_id})\n            )\n        return results\n\n\nif __name__ == \"__main__\":\n    reader = DiscordReader()\n    logging.info(\"initialized reader\")\n    output = reader.load_data(channel_ids=[1057178784895348746], limit=10)\n    logging.info(output)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/discord_reader.py", "file_name": "discord_reader.py"}, "index": 17, "child_indices": [], "ref_doc_id": "8d9a229a3248b1617b67774eac291a2594ec686e", "node_info": null}, "18": {"text": "\"\"\"Download loader from the Loader Hub.\"\"\"\n\nimport json\nimport os\nimport subprocess\nimport sys\nfrom importlib import util\nfrom pathlib import Path\n\nimport pkg_resources\nimport requests\nfrom pkg_resources import DistributionNotFound\n\nfrom gpt_index.readers.base import BaseReader\n\nLOADER_HUB_URL = (\n    \"https://raw.githubusercontent.com/emptycrown/loader-hub/main/loader_hub\"\n)\n\n\ndef download_loader(loader_class: str) -> BaseReader:\n    \"\"\"Download a single loader from the Loader Hub.\n\n    Args:\n        loader_class: The name of the loader class you want to download,\n            such as `SimpleWebPageReader`.\n    Returns:\n        A Loader.\n    \"\"\"\n    response = requests.get(f\"{LOADER_HUB_URL}/library.json\")\n    library = json.loads(response.text)\n\n    # Look up the loader id (e.g. `web/simple_web`)\n    loader_id =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/download.py", "file_name": "download.py"}, "index": 18, "child_indices": [], "ref_doc_id": "71585e29180e9ae11f0774b391bf9fd877e5ac54", "node_info": null}, "19": {"text": "`web/simple_web`)\n    loader_id = library[loader_class][\"id\"]\n    dirpath = \".modules\"\n    loader_filename = loader_id.replace(\"/\", \"-\")\n    loader_path = f\"{dirpath}/{loader_filename}.py\"\n    requirements_path = f\"{dirpath}/{loader_filename}_requirements.txt\"\n\n    if not os.path.exists(dirpath):\n        # Create a new directory because it does not exist\n        os.makedirs(dirpath)\n\n    if not os.path.exists(loader_path):\n        response = requests.get(f\"{LOADER_HUB_URL}/{loader_id}/base.py\")\n        with open(loader_path, \"w\") as f:\n            f.write(response.text)\n\n    if not os.path.exists(requirements_path):\n        response =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/download.py", "file_name": "download.py"}, "index": 19, "child_indices": [], "ref_doc_id": "71585e29180e9ae11f0774b391bf9fd877e5ac54", "node_info": null}, "20": {"text": "       response = requests.get(f\"{LOADER_HUB_URL}/{loader_id}/requirements.txt\")\n        if response.status_code == 200:\n            with open(requirements_path, \"w\") as f:\n                f.write(response.text)\n\n    # Install dependencies if there are any and not already installed\n    if os.path.exists(requirements_path):\n        try:\n            requirements = pkg_resources.parse_requirements(\n                Path(requirements_path).open()\n            )\n            pkg_resources.require([str(r) for r in requirements])\n        except DistributionNotFound:\n            subprocess.check_call(\n               ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/download.py", "file_name": "download.py"}, "index": 20, "child_indices": [], "ref_doc_id": "71585e29180e9ae11f0774b391bf9fd877e5ac54", "node_info": null}, "21": {"text": "               [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", requirements_path]\n            )\n\n    spec = util.spec_from_file_location(\"custom_loader\", location=loader_path)\n    if spec is None:\n        raise ValueError(f\"Could not find file: {loader_path}.\")\n    module = util.module_from_spec(spec)\n    spec.loader.exec_module(module)  # type: ignore\n\n    return getattr(module, loader_class)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/download.py", "file_name": "download.py"}, "index": 21, "child_indices": [], "ref_doc_id": "71585e29180e9ae11f0774b391bf9fd877e5ac54", "node_info": null}, "22": {"text": "\"\"\"Faiss reader.\"\"\"\n\nfrom typing import Any, Dict, List\n\nimport numpy as np\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass FaissReader(BaseReader):\n    \"\"\"Faiss reader.\n\n    Retrieves documents through an existing in-memory Faiss index.\n    These documents can then be used in a downstream GPT Index data structure.\n    If you wish use Faiss itself as an index to to organize documents,\n    insert documents, and perform queries on them, please use GPTFaissIndex.\n\n    Args:\n        faiss_index (faiss.Index): A Faiss Index object (required)\n\n    \"\"\"\n\n    def __init__(self, index: Any):\n        \"\"\"Initialize with parameters.\"\"\"\n        import_err_msg = \"\"\"\n            `faiss` package not found. For instructions on\n            how to install", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/faiss.py", "file_name": "faiss.py"}, "index": 22, "child_indices": [], "ref_doc_id": "82f076eb7558c8648d1b13d5ccec126b5cb6a5f3", "node_info": null}, "23": {"text": "on\n            how to install `faiss` please visit\n            https://github.com/facebookresearch/faiss/wiki/Installing-Faiss\n        \"\"\"\n        try:\n            import faiss  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n\n        self._index = index\n\n    def load_data(\n        self,\n        query: np.ndarray,\n        id_to_text_map: Dict[str, str],\n        k: int = 4,\n        separate_documents: bool = True,\n    ) -> List[Document]:\n        \"\"\"Load data from Faiss.\n\n        Args:\n            query (np.ndarray): A", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/faiss.py", "file_name": "faiss.py"}, "index": 23, "child_indices": [], "ref_doc_id": "82f076eb7558c8648d1b13d5ccec126b5cb6a5f3", "node_info": null}, "24": {"text": "        query (np.ndarray): A 2D numpy array of query vectors.\n            id_to_text_map (Dict[str, str]): A map from ID's to text.\n            k (int): Number of nearest neighbors to retrieve. Defaults to 4.\n            separate_documents (Optional[bool]): Whether to return separate\n                documents. Defaults to True.\n        Returns:\n            List[Document]: A list of documents.\n\n        \"\"\"\n        dists, indices = self._index.search(query, k)\n        documents = []\n        for qidx in range(indices.shape[0]):\n            for didx in range(indices.shape[1]):\n              ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/faiss.py", "file_name": "faiss.py"}, "index": 24, "child_indices": [], "ref_doc_id": "82f076eb7558c8648d1b13d5ccec126b5cb6a5f3", "node_info": null}, "25": {"text": "               doc_id = indices[qidx, didx]\n                if doc_id not in id_to_text_map:\n                    raise ValueError(\n                        f\"Document ID {doc_id} not found in id_to_text_map.\"\n                    )\n                text = id_to_text_map[doc_id]\n                documents.append(Document(text=text))\n\n        if not separate_documents:\n            # join all documents into one\n            text_list = [doc.get_text() for doc in documents]\n            text =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/faiss.py", "file_name": "faiss.py"}, "index": 25, "child_indices": [], "ref_doc_id": "82f076eb7558c8648d1b13d5ccec126b5cb6a5f3", "node_info": null}, "26": {"text": "           text = \"\\n\\n\".join(text_list)\n            documents = [Document(text=text)]\n\n        return documents\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/faiss.py", "file_name": "faiss.py"}, "index": 26, "child_indices": [], "ref_doc_id": "82f076eb7558c8648d1b13d5ccec126b5cb6a5f3", "node_info": null}, "27": {"text": "\"\"\"Simple reader for mbox (mailbox) files.\"\"\"\nimport os\nfrom pathlib import Path\nfrom typing import Any, List\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.file.mbox_parser import MboxParser\nfrom gpt_index.readers.schema.base import Document\n\n\nclass MboxReader(BaseReader):\n    \"\"\"Mbox e-mail reader.\n\n    Reads a set of e-mails saved in the mbox format.\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize.\"\"\"\n\n    def load_data(self, input_dir: str, **load_kwargs: Any) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        load_kwargs:\n            max_count (int): Maximum amount of messages to read.\n            message_format (str): Message format overriding default.\n        \"\"\"\n        docs:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/mbox.py", "file_name": "mbox.py"}, "index": 27, "child_indices": [], "ref_doc_id": "b8ba0526790bd62d1a41d878474809af816f616e", "node_info": null}, "28": {"text": "     \"\"\"\n        docs: List[Document] = []\n        for (dirpath, dirnames, filenames) in os.walk(input_dir):\n            dirnames[:] = [d for d in dirnames if not d.startswith(\".\")]\n            for filename in filenames:\n                if filename.endswith(\".mbox\"):\n                    filepath = os.path.join(dirpath, filename)\n                    content = MboxParser(**load_kwargs).parse_file(Path(filepath))\n                    for msg in content:\n                        docs.append(Document(msg))\n        return docs\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/mbox.py", "file_name": "mbox.py"}, "index": 28, "child_indices": [], "ref_doc_id": "b8ba0526790bd62d1a41d878474809af816f616e", "node_info": null}, "29": {"text": "\"\"\"Mongo client.\"\"\"\n\nfrom typing import Dict, List, Optional\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass SimpleMongoReader(BaseReader):\n    \"\"\"Simple mongo reader.\n\n    Concatenates each Mongo doc into Document used by GPT Index.\n\n    Args:\n        host (str): Mongo host.\n        port (int): Mongo port.\n        max_docs (int): Maximum number of documents to load.\n\n    \"\"\"\n\n    def __init__(self, host: str, port: int, max_docs: int = 1000) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            import pymongo  # noqa: F401\n            from pymongo import MongoClient  # noqa: F401\n        except ImportError:\n            raise", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/mongo.py", "file_name": "mongo.py"}, "index": 29, "child_indices": [], "ref_doc_id": "2bc13de0428fd82ed9d665fc257405cf2de8929a", "node_info": null}, "30": {"text": "ImportError:\n            raise ValueError(\n                \"`pymongo` package not found, please run `pip install pymongo`\"\n            )\n        self.client: MongoClient = MongoClient(host, port)\n        self.max_docs = max_docs\n\n    def load_data(\n        self, db_name: str, collection_name: str, query_dict: Optional[Dict] = None\n    ) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            db_name (str): name of the database.\n            collection_name (str): name of the collection.\n            query_dict (Optional[Dict]): query to filter documents.\n                Defaults to None\n\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/mongo.py", "file_name": "mongo.py"}, "index": 30, "child_indices": [], "ref_doc_id": "2bc13de0428fd82ed9d665fc257405cf2de8929a", "node_info": null}, "31": {"text": "       Defaults to None\n\n        Returns:\n            List[Document]: A list of documents.\n\n        \"\"\"\n        documents = []\n        db = self.client[db_name]\n        if query_dict is None:\n            cursor = db[collection_name].find()\n        else:\n            cursor = db[collection_name].find(query_dict)\n\n        for item in cursor:\n            if \"text\" not in item:\n                raise ValueError(\"`text` field not found in Mongo document.\")\n            documents.append(Document(item[\"text\"]))\n        return documents\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/mongo.py", "file_name": "mongo.py"}, "index": 31, "child_indices": [], "ref_doc_id": "2bc13de0428fd82ed9d665fc257405cf2de8929a", "node_info": null}, "32": {"text": "\"\"\"Notion reader.\"\"\"\nimport logging\nimport os\nfrom typing import Any, Dict, List, Optional\n\nimport requests  # type: ignore\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\nINTEGRATION_TOKEN_NAME = \"NOTION_INTEGRATION_TOKEN\"\nBLOCK_CHILD_URL_TMPL = \"https://api.notion.com/v1/blocks/{block_id}/children\"\nDATABASE_URL_TMPL = \"https://api.notion.com/v1/databases/{database_id}/query\"\nSEARCH_URL = \"https://api.notion.com/v1/search\"\n\n\n# TODO: Notion DB reader coming soon!\nclass NotionPageReader(BaseReader):\n    \"\"\"Notion Page reader.\n\n    Reads a set of Notion pages.\n\n    Args:\n        integration_token (str): Notion integration token.\n\n    \"\"\"\n\n    def __init__(self, integration_token: Optional[str] = None) -> None:\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/notion.py", "file_name": "notion.py"}, "index": 32, "child_indices": [], "ref_doc_id": "c7ee9e2bf5f208fdcdcdf35eaf126ee0abbefe83", "node_info": null}, "33": {"text": "integration_token: Optional[str] = None) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        if integration_token is None:\n            integration_token = os.getenv(INTEGRATION_TOKEN_NAME)\n            if integration_token is None:\n                raise ValueError(\n                    \"Must specify `integration_token` or set environment \"\n                    \"variable `NOTION_INTEGRATION_TOKEN`.\"\n                )\n        self.token = integration_token\n        self.headers = {\n            \"Authorization\": \"Bearer \" + self.token,\n            \"Content-Type\": \"application/json\",\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/notion.py", "file_name": "notion.py"}, "index": 33, "child_indices": [], "ref_doc_id": "c7ee9e2bf5f208fdcdcdf35eaf126ee0abbefe83", "node_info": null}, "34": {"text": "\"application/json\",\n            \"Notion-Version\": \"2022-06-28\",\n        }\n\n    def _read_block(self, block_id: str, num_tabs: int = 0) -> str:\n        \"\"\"Read a block.\"\"\"\n        done = False\n        result_lines_arr = []\n        cur_block_id = block_id\n        while not done:\n            block_url = BLOCK_CHILD_URL_TMPL.format(block_id=cur_block_id)\n            query_dict: Dict[str, Any] = {}\n\n            res = requests.request(\n                \"GET\", block_url, headers=self.headers, json=query_dict\n            )\n            data =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/notion.py", "file_name": "notion.py"}, "index": 34, "child_indices": [], "ref_doc_id": "c7ee9e2bf5f208fdcdcdf35eaf126ee0abbefe83", "node_info": null}, "35": {"text": " )\n            data = res.json()\n\n            for result in data[\"results\"]:\n                result_type = result[\"type\"]\n                result_obj = result[result_type]\n\n                cur_result_text_arr = []\n                if \"rich_text\" in result_obj:\n                    for rich_text in result_obj[\"rich_text\"]:\n                        # skip if doesn't have text object\n                        if \"text\" in rich_text:\n                            text = rich_text[\"text\"][\"content\"]\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/notion.py", "file_name": "notion.py"}, "index": 35, "child_indices": [], "ref_doc_id": "c7ee9e2bf5f208fdcdcdf35eaf126ee0abbefe83", "node_info": null}, "36": {"text": "                           prefix = \"\\t\" * num_tabs\n                            cur_result_text_arr.append(prefix + text)\n\n                result_block_id = result[\"id\"]\n                has_children = result[\"has_children\"]\n                if has_children:\n                    children_text = self._read_block(\n                        result_block_id, num_tabs=num_tabs + 1\n                    )\n                    cur_result_text_arr.append(children_text)\n\n        ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/notion.py", "file_name": "notion.py"}, "index": 36, "child_indices": [], "ref_doc_id": "c7ee9e2bf5f208fdcdcdf35eaf126ee0abbefe83", "node_info": null}, "37": {"text": "               cur_result_text = \"\\n\".join(cur_result_text_arr)\n                result_lines_arr.append(cur_result_text)\n\n            if data[\"next_cursor\"] is None:\n                done = True\n                break\n            else:\n                cur_block_id = data[\"next_cursor\"]\n\n        result_lines = \"\\n\".join(result_lines_arr)\n        return result_lines\n\n    def read_page(self, page_id: str) -> str:\n        \"\"\"Read a page.\"\"\"\n        return self._read_block(page_id)\n\n    def query_database(\n        self, database_id: str, query_dict:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/notion.py", "file_name": "notion.py"}, "index": 37, "child_indices": [], "ref_doc_id": "c7ee9e2bf5f208fdcdcdf35eaf126ee0abbefe83", "node_info": null}, "38": {"text": "    self, database_id: str, query_dict: Dict[str, Any] = {}\n    ) -> List[str]:\n        \"\"\"Get all the pages from a Notion database.\"\"\"\n        res = requests.post(\n            DATABASE_URL_TMPL.format(database_id=database_id),\n            headers=self.headers,\n            json=query_dict,\n        )\n        data = res.json()\n        page_ids = []\n        for result in data[\"results\"]:\n            page_id = result[\"id\"]\n            page_ids.append(page_id)\n\n        return page_ids\n\n    def search(self, query: str) -> List[str]:\n        \"\"\"Search Notion page given a text query.\"\"\"\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/notion.py", "file_name": "notion.py"}, "index": 38, "child_indices": [], "ref_doc_id": "c7ee9e2bf5f208fdcdcdf35eaf126ee0abbefe83", "node_info": null}, "39": {"text": "Notion page given a text query.\"\"\"\n        done = False\n        next_cursor: Optional[str] = None\n        page_ids = []\n        while not done:\n            query_dict = {\n                \"query\": query,\n            }\n            if next_cursor is not None:\n                query_dict[\"start_cursor\"] = next_cursor\n            res = requests.post(SEARCH_URL, headers=self.headers, json=query_dict)\n            data = res.json()\n            for result in data[\"results\"]:\n                page_id = result[\"id\"]\n               ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/notion.py", "file_name": "notion.py"}, "index": 39, "child_indices": [], "ref_doc_id": "c7ee9e2bf5f208fdcdcdf35eaf126ee0abbefe83", "node_info": null}, "40": {"text": "               page_ids.append(page_id)\n\n            if data[\"next_cursor\"] is None:\n                done = True\n                break\n            else:\n                next_cursor = data[\"next_cursor\"]\n        return page_ids\n\n    def load_data(\n        self, page_ids: List[str] = [], database_id: Optional[str] = None\n    ) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            page_ids (List[str]): List of page ids to load.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/notion.py", "file_name": "notion.py"}, "index": 40, "child_indices": [], "ref_doc_id": "c7ee9e2bf5f208fdcdcdf35eaf126ee0abbefe83", "node_info": null}, "41": {"text": "       \"\"\"\n        if not page_ids and not database_id:\n            raise ValueError(\"Must specify either `page_ids` or `database_id`.\")\n        docs = []\n        if database_id is not None:\n            # get all the pages in the database\n            page_ids = self.query_database(database_id)\n            for page_id in page_ids:\n                page_text = self.read_page(page_id)\n                docs.append(Document(page_text, extra_info={\"page_id\": page_id}))\n        else:\n            for page_id in page_ids:\n                page_text = self.read_page(page_id)\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/notion.py", "file_name": "notion.py"}, "index": 41, "child_indices": [], "ref_doc_id": "c7ee9e2bf5f208fdcdcdf35eaf126ee0abbefe83", "node_info": null}, "42": {"text": "= self.read_page(page_id)\n                docs.append(Document(page_text, extra_info={\"page_id\": page_id}))\n\n        return docs\n\n\nif __name__ == \"__main__\":\n    reader = NotionPageReader()\n    logging.info(reader.search(\"What I\"))\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/notion.py", "file_name": "notion.py"}, "index": 42, "child_indices": [], "ref_doc_id": "c7ee9e2bf5f208fdcdcdf35eaf126ee0abbefe83", "node_info": null}, "43": {"text": "\"\"\"Obsidian reader class.\n\nPass in the path to an Obsidian vault and it will parse all markdown\nfiles into a List of Documents,\nwith each Document containing text from under an Obsidian header.\n\n\"\"\"\nimport os\nfrom pathlib import Path\nfrom typing import Any, List\n\nfrom langchain.docstore.document import Document as LCDocument\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.file.markdown_parser import MarkdownParser\nfrom gpt_index.readers.schema.base import Document\n\n\nclass ObsidianReader(BaseReader):\n    \"\"\"Utilities for loading data from an Obsidian Vault.\n\n    Args:\n        input_dir (str): Path to the vault.\n\n    \"\"\"\n\n    def __init__(self, input_dir: str):\n        \"\"\"Init params.\"\"\"\n        self.input_dir = Path(input_dir)\n\n    def load_data(self, *args: Any, **load_kwargs: Any) -> List[Document]:\n        \"\"\"Load data from the input directory.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/obsidian.py", "file_name": "obsidian.py"}, "index": 43, "child_indices": [], "ref_doc_id": "1404005433f2a8f5a4aaf2f0cb40dba48a0cd9e8", "node_info": null}, "44": {"text": "      \"\"\"Load data from the input directory.\"\"\"\n        docs: List[str] = []\n        for dirpath, dirnames, filenames in os.walk(self.input_dir):\n            dirnames[:] = [d for d in dirnames if not d.startswith(\".\")]\n            for filename in filenames:\n                if filename.endswith(\".md\"):\n                    filepath = os.path.join(dirpath, filename)\n                    content = MarkdownParser().parse_file(Path(filepath))\n                    docs.extend(content)\n        return [Document(d) for d in docs]\n\n    def load_langchain_documents(self, **load_kwargs: Any) -> List[LCDocument]:\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/obsidian.py", "file_name": "obsidian.py"}, "index": 44, "child_indices": [], "ref_doc_id": "1404005433f2a8f5a4aaf2f0cb40dba48a0cd9e8", "node_info": null}, "45": {"text": "Any) -> List[LCDocument]:\n        \"\"\"Load data in LangChain document format.\"\"\"\n        docs = self.load_data(**load_kwargs)\n        return [d.to_langchain_format() for d in docs]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/obsidian.py", "file_name": "obsidian.py"}, "index": 45, "child_indices": [], "ref_doc_id": "1404005433f2a8f5a4aaf2f0cb40dba48a0cd9e8", "node_info": null}, "46": {"text": "\"\"\"Pinecone reader.\"\"\"\n\nfrom typing import Any, Dict, List, Optional\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass PineconeReader(BaseReader):\n    \"\"\"Pinecone reader.\n\n    Args:\n        api_key (str): Pinecone API key.\n        environment (str): Pinecone environment.\n    \"\"\"\n\n    def __init__(self, api_key: str, environment: str):\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            import pinecone  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`pinecone` package not found, please run `pip install pinecone-client`\"\n            )\n\n        self._api_key = api_key\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/pinecone.py", "file_name": "pinecone.py"}, "index": 46, "child_indices": [], "ref_doc_id": "c121f325702354416b70e9e38021ce04ff265381", "node_info": null}, "47": {"text": "     self._api_key = api_key\n        self._environment = environment\n        pinecone.init(api_key=api_key, environment=environment)\n\n    def load_data(\n        self,\n        index_name: str,\n        id_to_text_map: Dict[str, str],\n        vector: Optional[List[float]],\n        top_k: int,\n        separate_documents: bool = True,\n        include_values: bool = True,\n        **query_kwargs: Any\n    ) -> List[Document]:\n        \"\"\"Load data from Pinecone.\n\n        Args:\n            index_name (str): Name of the index.\n            id_to_text_map (Dict[str, str]): A map from ID's to text.\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/pinecone.py", "file_name": "pinecone.py"}, "index": 47, "child_indices": [], "ref_doc_id": "c121f325702354416b70e9e38021ce04ff265381", "node_info": null}, "48": {"text": "str]): A map from ID's to text.\n            separate_documents (Optional[bool]): Whether to return separate\n                documents per retrieved entry. Defaults to True.\n            vector (List[float]): Query vector.\n            top_k (int): Number of results to return.\n            include_values (bool): Whether to include the embedding in the response.\n                Defaults to True.\n            **query_kwargs: Keyword arguments to pass to the query.\n                Arguments are the exact same as those found in\n                Pinecone's reference documentation for the\n                query method.\n\n        Returns:\n            List[Document]:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/pinecone.py", "file_name": "pinecone.py"}, "index": 48, "child_indices": [], "ref_doc_id": "c121f325702354416b70e9e38021ce04ff265381", "node_info": null}, "49": {"text": "           List[Document]: A list of documents.\n        \"\"\"\n        import pinecone\n\n        index = pinecone.Index(index_name)\n        if \"include_values\" not in query_kwargs:\n            query_kwargs[\"include_values\"] = True\n        response = index.query(top_k=top_k, vector=vector, **query_kwargs)\n\n        documents = []\n        for match in response.matches:\n            if match.id not in id_to_text_map:\n                raise ValueError(\"ID not found in id_to_text_map.\")\n            text = id_to_text_map[match.id]\n            embedding = match.values\n            if len(embedding) ==", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/pinecone.py", "file_name": "pinecone.py"}, "index": 49, "child_indices": [], "ref_doc_id": "c121f325702354416b70e9e38021ce04ff265381", "node_info": null}, "50": {"text": "         if len(embedding) == 0:\n                embedding = None\n            documents.append(Document(text=text, embedding=embedding))\n\n        if not separate_documents:\n            text_list = [doc.get_text() for doc in documents]\n            text = \"\\n\\n\".join(text_list)\n            documents = [Document(text=text)]\n\n        return documents\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/pinecone.py", "file_name": "pinecone.py"}, "index": 50, "child_indices": [], "ref_doc_id": "c121f325702354416b70e9e38021ce04ff265381", "node_info": null}, "51": {"text": "\"\"\"Qdrant reader.\"\"\"\n\nfrom typing import List, Optional, cast\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass QdrantReader(BaseReader):\n    \"\"\"Qdrant reader.\n\n    Retrieve documents from existing Qdrant collections.\n\n    Args:\n        host: Host name of Qdrant service.\n        port: Port of the REST API interface. Default: 6333\n        grpc_port: Port of the gRPC interface. Default: 6334\n        prefer_grpc: If `true` - use gPRC interface whenever possible in custom methods.\n        https: If `true` - use HTTPS(SSL) protocol. Default: `false`\n        api_key: API key for authentication in Qdrant Cloud. Default: `None`\n        prefix:\n            If not `None` - add `prefix` to the REST URL", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/qdrant.py", "file_name": "qdrant.py"}, "index": 51, "child_indices": [], "ref_doc_id": "5088ae6098fcc16c6105a3cbf9b6b1c7dac5ec53", "node_info": null}, "52": {"text": " If not `None` - add `prefix` to the REST URL path.\n            Example: `service/v1` will result in\n            `http://localhost:6333/service/v1/{qdrant-endpoint}` for REST API.\n            Default: `None`\n        timeout:\n            Timeout for REST and gRPC API requests.\n            Default: 5.0 seconds for REST and unlimited for gRPC\n    \"\"\"\n\n    def __init__(\n        self,\n        host: str,\n        port: int = 6333,\n        grpc_port: int = 6334,\n        prefer_grpc: bool = False,\n        https: Optional[bool] = None,\n        api_key: Optional[str] = None,\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/qdrant.py", "file_name": "qdrant.py"}, "index": 52, "child_indices": [], "ref_doc_id": "5088ae6098fcc16c6105a3cbf9b6b1c7dac5ec53", "node_info": null}, "53": {"text": "  api_key: Optional[str] = None,\n        prefix: Optional[str] = None,\n        timeout: Optional[float] = None,\n    ):\n        \"\"\"Initialize with parameters.\"\"\"\n        import_err_msg = (\n            \"`qdrant-client` package not found, please run `pip install qdrant-client`\"\n        )\n        try:\n            import qdrant_client  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n\n        self._client = qdrant_client.QdrantClient(\n            host=host,\n            port=port,\n            grpc_port=grpc_port,\n         ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/qdrant.py", "file_name": "qdrant.py"}, "index": 53, "child_indices": [], "ref_doc_id": "5088ae6098fcc16c6105a3cbf9b6b1c7dac5ec53", "node_info": null}, "54": {"text": "           prefer_grpc=prefer_grpc,\n            https=https,\n            api_key=api_key,\n            prefix=prefix,\n            timeout=timeout,\n        )\n\n    def load_data(\n        self,\n        collection_name: str,\n        query_vector: List[float],\n        limit: int = 10,\n    ) -> List[Document]:\n        \"\"\"Load data from Qdrant.\n\n        Args:\n            collection_name (str): Name of the Qdrant collection.\n            query_vector (List[float]): Query vector.\n            limit (int): Number of results to return.\n\n        Returns:\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/qdrant.py", "file_name": "qdrant.py"}, "index": 54, "child_indices": [], "ref_doc_id": "5088ae6098fcc16c6105a3cbf9b6b1c7dac5ec53", "node_info": null}, "55": {"text": "return.\n\n        Returns:\n            List[Document]: A list of documents.\n        \"\"\"\n        from qdrant_client.http.models.models import Payload\n\n        response = self._client.search(\n            collection_name=collection_name,\n            query_vector=query_vector,\n            with_vectors=True,\n            with_payload=True,\n            limit=limit,\n        )\n\n        documents = []\n        for point in response:\n            payload = cast(Payload, point)\n            try:\n                vector = cast(List[float], point.vector)\n          ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/qdrant.py", "file_name": "qdrant.py"}, "index": 55, "child_indices": [], "ref_doc_id": "5088ae6098fcc16c6105a3cbf9b6b1c7dac5ec53", "node_info": null}, "56": {"text": "point.vector)\n            except ValueError as e:\n                raise ValueError(\"Could not cast vector to List[float].\") from e\n            document = Document(\n                doc_id=payload.get(\"doc_id\"),\n                text=payload.get(\"text\"),\n                embedding=vector,\n            )\n            documents.append(document)\n\n        return documents\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/qdrant.py", "file_name": "qdrant.py"}, "index": 56, "child_indices": [], "ref_doc_id": "5088ae6098fcc16c6105a3cbf9b6b1c7dac5ec53", "node_info": null}, "57": {"text": "\"\"\"Slack reader.\"\"\"\nimport logging\nimport os\nimport time\nfrom typing import List, Optional\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass SlackReader(BaseReader):\n    \"\"\"Slack reader.\n\n    Reads conversations from channels.\n\n    Args:\n        slack_token (Optional[str]): Slack token. If not provided, we\n            assume the environment variable `SLACK_BOT_TOKEN` is set.\n\n    \"\"\"\n\n    def __init__(self, slack_token: Optional[str] = None) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            from slack_sdk import WebClient\n        except ImportError:\n            raise ValueError(\n                \"`slack_sdk` package not found, please run", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/slack.py", "file_name": "slack.py"}, "index": 57, "child_indices": [], "ref_doc_id": "a0db9d42d0b3dcf85ce915984e4159a8d57fd656", "node_info": null}, "58": {"text": "  \"`slack_sdk` package not found, please run `pip install slack_sdk`\"\n            )\n        if slack_token is None:\n            slack_token = os.environ[\"SLACK_BOT_TOKEN\"]\n            if slack_token is None:\n                raise ValueError(\n                    \"Must specify `slack_token` or set environment \"\n                    \"variable `SLACK_BOT_TOKEN`.\"\n                )\n        self.client = WebClient(token=slack_token)\n        res = self.client.api_test()\n        if not res[\"ok\"]:\n            raise ValueError(f\"Error initializing", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/slack.py", "file_name": "slack.py"}, "index": 58, "child_indices": [], "ref_doc_id": "a0db9d42d0b3dcf85ce915984e4159a8d57fd656", "node_info": null}, "59": {"text": "       raise ValueError(f\"Error initializing Slack API: {res['error']}\")\n\n    def _read_message(self, channel_id: str, message_ts: str) -> str:\n        from slack_sdk.errors import SlackApiError\n\n        \"\"\"Read a message.\"\"\"\n\n        messages_text = []\n        next_cursor = None\n        while True:\n            try:\n                # https://slack.com/api/conversations.replies\n                # List all replies to a message, including the message itself.\n                result = self.client.conversations_replies(\n                    channel=channel_id, ts=message_ts, cursor=next_cursor\n             ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/slack.py", "file_name": "slack.py"}, "index": 59, "child_indices": [], "ref_doc_id": "a0db9d42d0b3dcf85ce915984e4159a8d57fd656", "node_info": null}, "60": {"text": "               )\n                messages = result[\"messages\"]\n                for message in messages:\n                    messages_text.append(message[\"text\"])\n\n                if not result[\"has_more\"]:\n                    break\n\n                next_cursor = result[\"response_metadata\"][\"next_cursor\"]\n            except SlackApiError as e:\n                if e.response[\"error\"] == \"ratelimited\":\n                    logging.error(\n                        \"Rate limit error reached, sleeping for: {}", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/slack.py", "file_name": "slack.py"}, "index": 60, "child_indices": [], "ref_doc_id": "a0db9d42d0b3dcf85ce915984e4159a8d57fd656", "node_info": null}, "61": {"text": "   \"Rate limit error reached, sleeping for: {} seconds\".format(\n                            e.response.headers[\"retry-after\"]\n                        )\n                    )\n                    time.sleep(int(e.response.headers[\"retry-after\"]))\n                else:\n                    logging.error(\"Error parsing conversation replies: {}\".format(e))\n\n        return \"\\n\\n\".join(messages_text)\n\n    def _read_channel(self, channel_id: str) -> str:\n        from slack_sdk.errors import SlackApiError\n\n        \"\"\"Read a channel.\"\"\"\n\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/slack.py", "file_name": "slack.py"}, "index": 61, "child_indices": [], "ref_doc_id": "a0db9d42d0b3dcf85ce915984e4159a8d57fd656", "node_info": null}, "62": {"text": "    \"\"\"Read a channel.\"\"\"\n\n        result_messages = []\n        next_cursor = None\n        while True:\n            try:\n                # Call the conversations.history method using the WebClient\n                # conversations.history returns the first 100 messages by default\n                # These results are paginated,\n                # see: https://api.slack.com/methods/conversations.history$pagination\n                result = self.client.conversations_history(\n                    channel=channel_id, cursor=next_cursor\n                )\n                conversation_history =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/slack.py", "file_name": "slack.py"}, "index": 62, "child_indices": [], "ref_doc_id": "a0db9d42d0b3dcf85ce915984e4159a8d57fd656", "node_info": null}, "63": {"text": "          conversation_history = result[\"messages\"]\n                # Print results\n                logging.info(\n                    \"{} messages found in {}\".format(len(conversation_history), id)\n                )\n                for message in conversation_history:\n                    result_messages.append(\n                        self._read_message(channel_id, message[\"ts\"])\n                    )\n\n                if not result[\"has_more\"]:\n                    break\n         ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/slack.py", "file_name": "slack.py"}, "index": 63, "child_indices": [], "ref_doc_id": "a0db9d42d0b3dcf85ce915984e4159a8d57fd656", "node_info": null}, "64": {"text": "     break\n                next_cursor = result[\"response_metadata\"][\"next_cursor\"]\n\n            except SlackApiError as e:\n                if e.response[\"error\"] == \"ratelimited\":\n                    logging.error(\n                        \"Rate limit error reached, sleeping for: {} seconds\".format(\n                            e.response.headers[\"retry-after\"]\n                        )\n                    )\n                    time.sleep(int(e.response.headers[\"retry-after\"]))\n         ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/slack.py", "file_name": "slack.py"}, "index": 64, "child_indices": [], "ref_doc_id": "a0db9d42d0b3dcf85ce915984e4159a8d57fd656", "node_info": null}, "65": {"text": "               else:\n                    logging.error(\"Error parsing conversation replies: {}\".format(e))\n\n        return \"\\n\\n\".join(result_messages)\n\n    def load_data(self, channel_ids: List[str]) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            channel_ids (List[str]): List of channel ids to read.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        results = []\n        for channel_id in channel_ids:\n            channel_content = self._read_channel(channel_id)\n            results.append(\n               ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/slack.py", "file_name": "slack.py"}, "index": 65, "child_indices": [], "ref_doc_id": "a0db9d42d0b3dcf85ce915984e4159a8d57fd656", "node_info": null}, "66": {"text": "               Document(channel_content, extra_info={\"channel\": channel_id})\n            )\n        return results\n\n\nif __name__ == \"__main__\":\n    reader = SlackReader()\n    logging.info(reader.load_data(channel_ids=[\"C04DC2VUY3F\"]))\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/slack.py", "file_name": "slack.py"}, "index": 66, "child_indices": [], "ref_doc_id": "a0db9d42d0b3dcf85ce915984e4159a8d57fd656", "node_info": null}, "67": {"text": "\"\"\"Simple reader that turns an iterable of strings into a list of Documents.\"\"\"\nfrom typing import List\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass StringIterableReader(BaseReader):\n    \"\"\"String Iterable Reader.\n\n    Gets a list of documents, given an iterable (e.g. list) of strings.\n\n    Example:\n        .. code-block:: python\n\n            from gpt_index import StringIterableReader, GPTTreeIndex\n\n            documents = StringIterableReader().load_data(\n                texts=[\"I went to the store\", \"I bought an apple\"])\n            index = GPTTreeIndex(documents)\n            index.query(\"what did I buy?\")\n\n            # response should be something like \"You bought an apple.\"\n    \"\"\"\n\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/string_iterable.py", "file_name": "string_iterable.py"}, "index": 67, "child_indices": [], "ref_doc_id": "62876b8e16ae2625adf3c433bdd1b3eaa70150c3", "node_info": null}, "68": {"text": "something like \"You bought an apple.\"\n    \"\"\"\n\n    def load_data(self, texts: List[str]) -> List[Document]:\n        \"\"\"Load the data.\"\"\"\n        results = []\n        for text in texts:\n            results.append(Document(text))\n\n        return results\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/string_iterable.py", "file_name": "string_iterable.py"}, "index": 68, "child_indices": [], "ref_doc_id": "62876b8e16ae2625adf3c433bdd1b3eaa70150c3", "node_info": null}, "69": {"text": "\"\"\"Simple reader that reads tweets of a twitter handle.\"\"\"\nfrom typing import Any, List, Optional\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass TwitterTweetReader(BaseReader):\n    \"\"\"Twitter tweets reader.\n\n    Read tweets of user twitter handle.\n\n    Check 'https://developer.twitter.com/en/docs/twitter-api/\\\n        getting-started/getting-access-to-the-twitter-api' \\\n        on how to get access to twitter API.\n\n    Args:\n        bearer_token (str): bearer_token that you get from twitter API.\n        num_tweets (Optional[int]): Number of tweets for each user twitter handle.\\\n            Default is 100 tweets.\n    \"\"\"\n\n    def __init__(\n        self,\n        bearer_token: str,\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/twitter.py", "file_name": "twitter.py"}, "index": 69, "child_indices": [], "ref_doc_id": "ee0e8f78b27c1a62e466e13d35e9bbeea3b425e9", "node_info": null}, "70": {"text": " bearer_token: str,\n        num_tweets: Optional[int] = 100,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        self.bearer_token = bearer_token\n        self.num_tweets = num_tweets\n\n    def load_data(\n        self, twitterhandles: List[str], **load_kwargs: Any\n    ) -> List[Document]:\n        \"\"\"Load tweets of twitter handles.\n\n        Args:\n            twitterhandles (List[str]): List of user twitter handles to read tweets.\n\n        \"\"\"\n        try:\n            import tweepy\n        except ImportError:\n            raise ValueError(\n                \"`tweepy` package not found,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/twitter.py", "file_name": "twitter.py"}, "index": 70, "child_indices": [], "ref_doc_id": "ee0e8f78b27c1a62e466e13d35e9bbeea3b425e9", "node_info": null}, "71": {"text": "     \"`tweepy` package not found, please run `pip install tweepy`\"\n            )\n\n        client = tweepy.Client(bearer_token=self.bearer_token)\n        results = []\n        for username in twitterhandles:\n            # tweets = api.user_timeline(screen_name=user, count=self.num_tweets)\n            user = client.get_user(username=username)\n            tweets = client.get_users_tweets(user.data.id, max_results=self.num_tweets)\n            response = \" \"\n            for tweet in tweets.data:\n                response = response + tweet.text + \"\\n\"\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/twitter.py", "file_name": "twitter.py"}, "index": 71, "child_indices": [], "ref_doc_id": "ee0e8f78b27c1a62e466e13d35e9bbeea3b425e9", "node_info": null}, "72": {"text": "\"\\n\"\n            results.append(Document(response))\n        return results\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/twitter.py", "file_name": "twitter.py"}, "index": 72, "child_indices": [], "ref_doc_id": "ee0e8f78b27c1a62e466e13d35e9bbeea3b425e9", "node_info": null}, "73": {"text": "\"\"\"Web scraper.\"\"\"\nimport logging\nfrom typing import Any, Callable, Dict, List, Optional, Tuple\n\nfrom langchain.utilities import RequestsWrapper\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass SimpleWebPageReader(BaseReader):\n    \"\"\"Simple web page reader.\n\n    Reads pages from the web.\n\n    Args:\n        html_to_text (bool): Whether to convert HTML to text.\n            Requires `html2text` package.\n\n    \"\"\"\n\n    def __init__(self, html_to_text: bool = False) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            import html2text  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`html2text` package not found, please run", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 73, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "74": {"text": "    \"`html2text` package not found, please run `pip install html2text`\"\n            )\n        self._html_to_text = html_to_text\n\n    def load_data(self, urls: List[str]) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            urls (List[str]): List of URLs to scrape.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        if not isinstance(urls, list):\n            raise ValueError(\"urls must be a list of strings.\")\n        requests = RequestsWrapper()\n        documents = []\n        for url in urls:\n            response = requests.run(url)\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 74, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "75": {"text": "response = requests.run(url)\n            if self._html_to_text:\n                import html2text\n\n                response = html2text.html2text(response)\n\n            documents.append(Document(response))\n\n        return documents\n\n\nclass TrafilaturaWebReader(BaseReader):\n    \"\"\"Trafilatura web page reader.\n\n    Reads pages from the web.\n    Requires the `trafilatura` package.\n\n    \"\"\"\n\n    def __init__(self, error_on_missing: bool = False) -> None:\n        \"\"\"Initialize with parameters.\n\n        Args:\n            error_on_missing (bool): Throw an error when data cannot be parsed\n        \"\"\"\n        self.error_on_missing = error_on_missing\n        try:\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 75, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "76": {"text": "       try:\n            import trafilatura  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`trafilatura` package not found, please run `pip install trafilatura`\"\n            )\n\n    def load_data(self, urls: List[str]) -> List[Document]:\n        \"\"\"Load data from the urls.\n\n        Args:\n            urls (List[str]): List of URLs to scrape.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        import trafilatura\n\n        if not isinstance(urls, list):\n            raise ValueError(\"urls must be a list of strings.\")\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 76, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "77": {"text": "raise ValueError(\"urls must be a list of strings.\")\n        documents = []\n        for url in urls:\n            downloaded = trafilatura.fetch_url(url)\n            if not downloaded:\n                if self.error_on_missing:\n                    raise ValueError(f\"Trafilatura fails to get string from url: {url}\")\n                continue\n            response = trafilatura.extract(downloaded)\n            if not response:\n                if self.error_on_missing:\n                    raise ValueError(f\"Trafilatura fails to parse page: {url}\")\n               ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 77, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "78": {"text": "               continue\n            documents.append(Document(response))\n\n        return documents\n\n\ndef _substack_reader(soup: Any) -> Tuple[str, Dict[str, Any]]:\n    \"\"\"Extract text from Substack blog post.\"\"\"\n    extra_info = {\n        \"Title of this Substack post\": soup.select_one(\"h1.post-title\").getText(),\n        \"Subtitle\": soup.select_one(\"h3.subtitle\").getText(),\n        \"Author\": soup.select_one(\"span.byline-names\").getText(),\n    }\n    text = soup.select_one(\"div.available-content\").getText()\n    return text, extra_info\n\n\nDEFAULT_WEBSITE_EXTRACTOR: Dict[str, Callable[[Any], Tuple[str, Dict[str, Any]]]] = {\n    \"substack.com\": _substack_reader,\n}\n\n\nclass", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 78, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "79": {"text": "_substack_reader,\n}\n\n\nclass BeautifulSoupWebReader(BaseReader):\n    \"\"\"BeautifulSoup web page reader.\n\n    Reads pages from the web.\n    Requires the `bs4` and `urllib` packages.\n\n    Args:\n        file_extractor (Optional[Dict[str, Callable]]): A mapping of website\n            hostname (e.g. google.com) to a function that specifies how to\n            extract text from the BeautifulSoup obj. See DEFAULT_WEBSITE_EXTRACTOR.\n    \"\"\"\n\n    def __init__(\n        self,\n        website_extractor: Optional[Dict[str, Callable]] = None,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            from urllib.parse import urlparse  # noqa: F401\n\n         ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 79, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "80": {"text": "# noqa: F401\n\n            import requests  # noqa: F401\n            from bs4 import BeautifulSoup  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`bs4`, `requests`, and `urllib` must be installed to scrape websites.\"\n                \"Please run `pip install bs4 requests urllib`.\"\n            )\n\n        self.website_extractor = website_extractor or DEFAULT_WEBSITE_EXTRACTOR\n\n    def load_data(\n        self, urls: List[str], custom_hostname: Optional[str] = None\n    ) -> List[Document]:\n        \"\"\"Load data from the urls.\n\n        Args:\n            urls", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 80, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "81": {"text": "Args:\n            urls (List[str]): List of URLs to scrape.\n            custom_hostname (Optional[str]): Force a certain hostname in the case\n                a website is displayed under custom URLs (e.g. Substack blogs)\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        from urllib.parse import urlparse\n\n        import requests\n        from bs4 import BeautifulSoup\n\n        documents = []\n        for url in urls:\n            try:\n                page = requests.get(url)\n            except Exception:\n                raise ValueError(f\"One of the inputs is not a", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 81, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "82": {"text": "  raise ValueError(f\"One of the inputs is not a valid url: {url}\")\n\n            hostname = custom_hostname or urlparse(url).hostname or \"\"\n\n            soup = BeautifulSoup(page.content, \"html.parser\")\n\n            data = \"\"\n            extra_info = {\"URL\": url}\n            if hostname in self.website_extractor:\n                data, metadata = self.website_extractor[hostname](soup)\n                extra_info.update(metadata)\n            else:\n                data = soup.getText()\n\n            documents.append(Document(data, extra_info=extra_info))\n\n        return documents\n\n\nclass RssReader(BaseReader):\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 82, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "83": {"text": " return documents\n\n\nclass RssReader(BaseReader):\n    \"\"\"RSS reader.\n\n    Reads content from an RSS feed.\n\n    \"\"\"\n\n    def __init__(self, html_to_text: bool = False) -> None:\n        \"\"\"Initialize with parameters.\n\n        Args:\n            html_to_text (bool): Whether to convert HTML to text.\n                Requires `html2text` package.\n\n        \"\"\"\n        try:\n            import feedparser  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`feedparser` package not found, please run `pip install feedparser`\"\n            )\n\n        if html_to_text:\n            try:\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 83, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "84": {"text": "           try:\n                import html2text  # noqa: F401\n            except ImportError:\n                raise ValueError(\n                    \"`html2text` package not found, please run `pip install html2text`\"\n                )\n        self._html_to_text = html_to_text\n\n    def load_data(self, urls: List[str]) -> List[Document]:\n        \"\"\"Load data from RSS feeds.\n\n        Args:\n            urls (List[str]): List of RSS URLs to load.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        import feedparser\n\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 84, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "85": {"text": "\"\"\"\n        import feedparser\n\n        if not isinstance(urls, list):\n            raise ValueError(\"urls must be a list of strings.\")\n\n        documents = []\n\n        for url in urls:\n            parsed = feedparser.parse(url)\n            for entry in parsed.entries:\n                if entry.content:\n                    data = entry.content[0].value\n                else:\n                    data = entry.description or entry.summary\n\n                if self._html_to_text:\n                    import html2text\n\n                ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 85, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "86": {"text": "                   data = html2text.html2text(data)\n\n                extra_info = {\"title\": entry.title, \"link\": entry.link}\n                documents.append(Document(data, extra_info=extra_info))\n\n        return documents\n\n\nif __name__ == \"__main__\":\n    reader = SimpleWebPageReader()\n    logging.info(reader.load_data([\"http://www.google.com\"]))\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 86, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "87": {"text": "\"\"\"Simple reader that reads wikipedia.\"\"\"\nfrom typing import Any, List\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass WikipediaReader(BaseReader):\n    \"\"\"Wikipedia reader.\n\n    Reads a page.\n\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            import wikipedia  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`wikipedia` package not found, please run `pip install wikipedia`\"\n            )\n\n    def load_data(self, pages: List[str], **load_kwargs: Any) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            pages (List[str]):", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/wikipedia.py", "file_name": "wikipedia.py"}, "index": 87, "child_indices": [], "ref_doc_id": "55400c6722fc856cbb287cec4987d87118429fb1", "node_info": null}, "88": {"text": "         pages (List[str]): List of pages to read.\n\n        \"\"\"\n        import wikipedia\n\n        results = []\n        for page in pages:\n            page_content = wikipedia.page(page, **load_kwargs).content\n            results.append(Document(page_content))\n        return results\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/wikipedia.py", "file_name": "wikipedia.py"}, "index": 88, "child_indices": [], "ref_doc_id": "55400c6722fc856cbb287cec4987d87118429fb1", "node_info": null}, "89": {"text": "\"\"\"Simple Reader that reads transcript of youtube video.\"\"\"\nfrom typing import Any, List\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass YoutubeTranscriptReader(BaseReader):\n    \"\"\"Youtube Transcript reader.\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n\n    def load_data(self, ytlinks: List[str], **load_kwargs: Any) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            pages (List[str]): List of youtube links \\\n                for which transcripts are to be read.\n\n        \"\"\"\n        try:\n            from youtube_transcript_api import YouTubeTranscriptApi\n        except ImportError:\n            raise", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/youtube_transcript.py", "file_name": "youtube_transcript.py"}, "index": 89, "child_indices": [], "ref_doc_id": "969ca38b79231bd668d28e6cdc0cbc59a2f3745d", "node_info": null}, "90": {"text": "ImportError:\n            raise ValueError(\n                \"`youtube_transcript_api` package not found, \\\n                    please run `pip install youtube-transcript-api`\"\n            )\n\n        results = []\n        for link in ytlinks:\n            video_id = link.split(\"?v=\")[-1]\n            srt = YouTubeTranscriptApi.get_transcript(video_id)\n            transcript = \"\"\n            for chunk in srt:\n                transcript = transcript + chunk[\"text\"] + \"\\n\"\n            results.append(Document(transcript))\n        return results\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/youtube_transcript.py", "file_name": "youtube_transcript.py"}, "index": 90, "child_indices": [], "ref_doc_id": "969ca38b79231bd668d28e6cdc0cbc59a2f3745d", "node_info": null}, "91": {"text": "This module contains the data connectors for GPT Index. It includes the BaseReader class, which provides utilities for loading data from a directory, and the DatabaseReader class, which concatenates each row into a Document used by GPT Index. It also includes the DiscordReader class, which provides a synchronous interface to the async discord.py API to read messages from a Discord channel.", "doc_id": null, "embedding": null, "extra_info": null, "index": 91, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "92": {"text": "\nThe discord_reader.py file is a class that reads conversations from channels using the Discord API. It takes in a Discord token and a list of channel ids, and returns a list of documents containing the messages from the channels. It also has a _read_channel() method that reads a single channel and a load_data() method that reads multiple channels. The download.py file is a function that downloads a single loader from the Loader Hub. It takes in the name of the loader class and returns a Loader. The faiss.py file is a class that retrieves documents through an existing in-memory Faiss index. It takes in a Faiss Index object and returns a list of documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 92, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "93": {"text": "The gpt_index/readers/faiss.py file contains a function that takes a 2D numpy array of query vectors, a map from ID's to text, and a number of nearest neighbors to retrieve as parameters. It then searches the index for the query vectors and returns a list of documents. The gpt_index/readers/mbox.py file contains a class that reads a set of emails saved in the mbox format. It takes an input directory, a maximum amount of messages to read, and a message format as parameters. The gpt_index/readers/mongo.py file contains a class that reads documents from a Mongo database. It takes a database name, a collection name, and a query to filter documents as parameters. The gpt_index/readers/notion.py file contains a class that reads a set of Notion pages. It takes an integration token as a parameter.", "doc_id": null, "embedding": null, "extra_info": null, "index": 93, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}, "94": {"text": "The notion.py file is a Python script that provides functions for reading and querying Notion pages. It contains functions for reading a page, querying a database, searching for a page, and loading data from a page or database. It uses the Notion API to make requests and returns a list of documents containing the page text. The obsidian.py file is a Python script that provides functions for loading data from an Obsidian vault. It contains functions for loading data from a directory and loading data in LangChain document format. The pinecone.py file is a Python script that provides functions for loading data from Pinecone. It contains functions for loading data from an index, with the option to include values and separate documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 94, "child_indices": [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], "ref_doc_id": null, "node_info": null}, "95": {"text": "This code file contains two readers, PineconeReader and QdrantReader, which are used to retrieve documents from existing Pinecone and Qdrant collections. PineconeReader takes in an index name, a map from ID's to text, a query vector, a top_k number of results to return, and keyword arguments to pass to the query. QdrantReader takes in a host name, port, grpc_port, prefer_grpc, https, api_key, prefix, and timeout. SlackReader takes in a slack token or an environment variable. All readers return a list of documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 95, "child_indices": [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59], "ref_doc_id": null, "node_info": null}, "96": {"text": "The file slack.py is a reader for the GPT-Index library. It is used to read messages from a Slack channel and turn them into a list of Documents. It uses the Slack API to access the messages and paginates the results. It also handles rate limit errors by sleeping for the specified amount of time. The load_data() method takes a list of channel ids and returns a list of Documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 96, "child_indices": [64, 65, 66, 67, 68, 69, 70, 71, 60, 61, 62, 63], "ref_doc_id": null, "node_info": null}, "97": {"text": "This code file contains two classes, SimpleWebPageReader and TrafilaturaWebReader, which are used to scrape web pages. The SimpleWebPageReader class uses the RequestsWrapper utility to scrape web pages and convert HTML to text if necessary. The TrafilaturaWebReader class uses the Trafilatura package to scrape web pages and extract text from them. The BeautifulSoupWebReader class uses the BeautifulSoup, Requests, and Urllib packages to scrape web pages and extract text from them. Finally, the RssReader class uses the Feedparser package to scrape RSS feeds and extract text from them.", "doc_id": null, "embedding": null, "extra_info": null, "index": 97, "child_indices": [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83], "ref_doc_id": null, "node_info": null}, "98": {"text": "This code file contains three readers for GPT-Index: SimpleWebPageReader, WikipediaReader, and YoutubeTranscriptReader. SimpleWebPageReader reads RSS feeds from a list of URLs and converts HTML to text if necessary. WikipediaReader reads a page from Wikipedia. YoutubeTranscriptReader reads the transcript of a YouTube video from a list of YouTube links. All readers return a list of documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 98, "child_indices": [84, 85, 86, 87, 88, 89, 90], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"91": {"text": "This module contains the data connectors for GPT Index. It includes the BaseReader class, which provides utilities for loading data from a directory, and the DatabaseReader class, which concatenates each row into a Document used by GPT Index. It also includes the DiscordReader class, which provides a synchronous interface to the async discord.py API to read messages from a Discord channel.", "doc_id": null, "embedding": null, "extra_info": null, "index": 91, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "92": {"text": "\nThe discord_reader.py file is a class that reads conversations from channels using the Discord API. It takes in a Discord token and a list of channel ids, and returns a list of documents containing the messages from the channels. It also has a _read_channel() method that reads a single channel and a load_data() method that reads multiple channels. The download.py file is a function that downloads a single loader from the Loader Hub. It takes in the name of the loader class and returns a Loader. The faiss.py file is a class that retrieves documents through an existing in-memory Faiss index. It takes in a Faiss Index object and returns a list of documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 92, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "93": {"text": "The gpt_index/readers/faiss.py file contains a function that takes a 2D numpy array of query vectors, a map from ID's to text, and a number of nearest neighbors to retrieve as parameters. It then searches the index for the query vectors and returns a list of documents. The gpt_index/readers/mbox.py file contains a class that reads a set of emails saved in the mbox format. It takes an input directory, a maximum amount of messages to read, and a message format as parameters. The gpt_index/readers/mongo.py file contains a class that reads documents from a Mongo database. It takes a database name, a collection name, and a query to filter documents as parameters. The gpt_index/readers/notion.py file contains a class that reads a set of Notion pages. It takes an integration token as a parameter.", "doc_id": null, "embedding": null, "extra_info": null, "index": 93, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}, "94": {"text": "The notion.py file is a Python script that provides functions for reading and querying Notion pages. It contains functions for reading a page, querying a database, searching for a page, and loading data from a page or database. It uses the Notion API to make requests and returns a list of documents containing the page text. The obsidian.py file is a Python script that provides functions for loading data from an Obsidian vault. It contains functions for loading data from a directory and loading data in LangChain document format. The pinecone.py file is a Python script that provides functions for loading data from Pinecone. It contains functions for loading data from an index, with the option to include values and separate documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 94, "child_indices": [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], "ref_doc_id": null, "node_info": null}, "95": {"text": "This code file contains two readers, PineconeReader and QdrantReader, which are used to retrieve documents from existing Pinecone and Qdrant collections. PineconeReader takes in an index name, a map from ID's to text, a query vector, a top_k number of results to return, and keyword arguments to pass to the query. QdrantReader takes in a host name, port, grpc_port, prefer_grpc, https, api_key, prefix, and timeout. SlackReader takes in a slack token or an environment variable. All readers return a list of documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 95, "child_indices": [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59], "ref_doc_id": null, "node_info": null}, "96": {"text": "The file slack.py is a reader for the GPT-Index library. It is used to read messages from a Slack channel and turn them into a list of Documents. It uses the Slack API to access the messages and paginates the results. It also handles rate limit errors by sleeping for the specified amount of time. The load_data() method takes a list of channel ids and returns a list of Documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 96, "child_indices": [64, 65, 66, 67, 68, 69, 70, 71, 60, 61, 62, 63], "ref_doc_id": null, "node_info": null}, "97": {"text": "This code file contains two classes, SimpleWebPageReader and TrafilaturaWebReader, which are used to scrape web pages. The SimpleWebPageReader class uses the RequestsWrapper utility to scrape web pages and convert HTML to text if necessary. The TrafilaturaWebReader class uses the Trafilatura package to scrape web pages and extract text from them. The BeautifulSoupWebReader class uses the BeautifulSoup, Requests, and Urllib packages to scrape web pages and extract text from them. Finally, the RssReader class uses the Feedparser package to scrape RSS feeds and extract text from them.", "doc_id": null, "embedding": null, "extra_info": null, "index": 97, "child_indices": [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83], "ref_doc_id": null, "node_info": null}, "98": {"text": "This code file contains three readers for GPT-Index: SimpleWebPageReader, WikipediaReader, and YoutubeTranscriptReader. SimpleWebPageReader reads RSS feeds from a list of URLs and converts HTML to text if necessary. WikipediaReader reads a page from Wikipedia. YoutubeTranscriptReader reads the transcript of a YouTube video from a list of YouTube links. All readers return a list of documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 98, "child_indices": [84, 85, 86, 87, 88, 89, 90], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"846471c13398def5f566e12afc3117ddc30b9c88": {"text": "\"\"\"Data Connectors for GPT Index.\n\nThis module contains the data connectors for GPT Index. Each connector inherits\nfrom a `BaseReader` class, connects to a data source, and loads Document objects\nfrom that data source.\n\nYou may also choose to construct Document objects manually, for instance\nin our `Insert How-To Guide <../how_to/insert.html>`_. See below for the API\ndefinition of a Document - the bare minimum is a `text` property.\n\n\"\"\"\n\nfrom gpt_index.readers.discord_reader import DiscordReader\nfrom gpt_index.readers.faiss import FaissReader\n\n# readers\nfrom gpt_index.readers.file.base import SimpleDirectoryReader\nfrom gpt_index.readers.github_readers.github_repository_reader import (\n    GithubRepositoryReader,\n)\nfrom gpt_index.readers.google_readers.gdocs import GoogleDocsReader\nfrom gpt_index.readers.make_com.wrapper import MakeWrapper\nfrom gpt_index.readers.mbox import MboxReader\nfrom gpt_index.readers.mongo import SimpleMongoReader\nfrom gpt_index.readers.notion import NotionPageReader\nfrom gpt_index.readers.obsidian import ObsidianReader\nfrom gpt_index.readers.pinecone import PineconeReader\nfrom gpt_index.readers.qdrant import QdrantReader\nfrom gpt_index.readers.schema.base import Document\nfrom gpt_index.readers.slack import SlackReader\nfrom gpt_index.readers.string_iterable import StringIterableReader\nfrom gpt_index.readers.twitter import TwitterTweetReader\nfrom gpt_index.readers.weaviate.reader import WeaviateReader\nfrom gpt_index.readers.web import (\n    BeautifulSoupWebReader,\n    RssReader,\n    SimpleWebPageReader,\n    TrafilaturaWebReader,\n)\nfrom gpt_index.readers.wikipedia import WikipediaReader\nfrom gpt_index.readers.youtube_transcript import YoutubeTranscriptReader\n\n__all__ = [\n    \"WikipediaReader\",\n    \"YoutubeTranscriptReader\",\n    \"SimpleDirectoryReader\",\n    \"SimpleMongoReader\",\n    \"NotionPageReader\",\n    \"GoogleDocsReader\",\n    \"DiscordReader\",\n    \"SlackReader\",\n    \"WeaviateReader\",\n    \"PineconeReader\",\n    \"QdrantReader\",\n    \"FaissReader\",\n    \"Document\",\n    \"StringIterableReader\",\n    \"SimpleWebPageReader\",\n    \"BeautifulSoupWebReader\",\n    \"TrafilaturaWebReader\",\n    \"RssReader\",\n    \"MakeWrapper\",\n    \"TwitterTweetReader\",\n    \"ObsidianReader\",\n    \"GithubRepositoryReader\",\n    \"MboxReader\",\n]\n", "doc_id": "846471c13398def5f566e12afc3117ddc30b9c88", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "f5cc30de3365fe4a08a28f1e95a9bdd2b17e09a0": {"text": "\"\"\"Base reader class.\"\"\"\nfrom abc import abstractmethod\nfrom typing import Any, List\n\nfrom langchain.docstore.document import Document as LCDocument\n\nfrom gpt_index.readers.schema.base import Document\n\n\nclass BaseReader:\n    \"\"\"Utilities for loading data from a directory.\"\"\"\n\n    @abstractmethod\n    def load_data(self, *args: Any, **load_kwargs: Any) -> List[Document]:\n        \"\"\"Load data from the input directory.\"\"\"\n\n    def load_langchain_documents(self, **load_kwargs: Any) -> List[LCDocument]:\n        \"\"\"Load data in LangChain document format.\"\"\"\n        docs = self.load_data(**load_kwargs)\n        return [d.to_langchain_format() for d in docs]\n", "doc_id": "f5cc30de3365fe4a08a28f1e95a9bdd2b17e09a0", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/base.py", "file_name": "base.py"}, "__type__": "Document"}, "4098c3e4bb1452ddb95150e467c23f0d503b34e1": {"text": "\"\"\"Database Reader.\"\"\"\n\nfrom typing import Any, List, Optional\n\nfrom sqlalchemy import text\nfrom sqlalchemy.engine import Engine\n\nfrom gpt_index.langchain_helpers.sql_wrapper import SQLDatabase\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass DatabaseReader(BaseReader):\n    \"\"\"Simple Database reader.\n\n    Concatenates each row into Document used by GPT Index.\n\n    Args:\n        sql_database (Optional[SQLDatabase]): SQL database to use,\n            including table names to specify.\n            See :ref:`Ref-Struct-Store` for more details.\n\n        OR\n\n        engine (Optional[Engine]): SQLAlchemy Engine object of the database connection.\n\n        OR\n\n        uri (Optional[str]): uri of the database connection.\n\n        OR\n\n        scheme (Optional[str]): scheme of the database connection.\n        host (Optional[str]): host of the database connection.\n        port (Optional[int]): port of the database connection.\n        user (Optional[str]): user of the database connection.\n        password (Optional[str]): password of the database connection.\n        dbname (Optional[str]): dbname of the database connection.\n\n    Returns:\n        DatabaseReader: A DatabaseReader object.\n    \"\"\"\n\n    def __init__(\n        self,\n        sql_database: Optional[SQLDatabase] = None,\n        engine: Optional[Engine] = None,\n        uri: Optional[str] = None,\n        scheme: Optional[str] = None,\n        host: Optional[str] = None,\n        port: Optional[str] = None,\n        user: Optional[str] = None,\n        password: Optional[str] = None,\n        dbname: Optional[str] = None,\n        *args: Optional[Any],\n        **kwargs: Optional[Any],\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        if sql_database:\n            self.sql_database = sql_database\n        elif engine:\n            self.sql_database = SQLDatabase(engine, *args, **kwargs)\n        elif uri:\n            self.uri = uri\n            self.sql_database = SQLDatabase.from_uri(uri, *args, **kwargs)\n        elif scheme and host and port and user and password and dbname:\n            uri = f\"{scheme}://{user}:{password}@{host}:{port}/{dbname}\"\n            self.uri = uri\n            self.sql_database = SQLDatabase.from_uri(uri, *args, **kwargs)\n        else:\n            raise ValueError(\n                \"You must provide either a SQLDatabase, \"\n                \"a SQL Alchemy Engine, a valid connection URI, or a valid \"\n                \"set of credentials.\"\n            )\n\n    def load_data(self, query: str) -> List[Document]:\n        \"\"\"Query and load data from the Database, returning a list of Documents.\n\n        Args:\n            query (str): Query parameter to filter tables and rows.\n\n        Returns:\n            List[Document]: A list of Document objects.\n        \"\"\"\n        documents = []\n        with self.sql_database.engine.connect() as connection:\n            if query is None:\n                raise ValueError(\"A query parameter is necessary to filter the data\")\n            else:\n                result = connection.execute(text(query))\n\n            for item in result.fetchall():\n                documents.append(Document(item[0]))\n        return documents\n", "doc_id": "4098c3e4bb1452ddb95150e467c23f0d503b34e1", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/database.py", "file_name": "database.py"}, "__type__": "Document"}, "8d9a229a3248b1617b67774eac291a2594ec686e": {"text": "\"\"\"Discord reader.\n\nNote: this file is named discord_reader.py to avoid conflicts with the\ndiscord.py module.\n\n\"\"\"\n\nimport asyncio\nimport logging\nimport os\nfrom typing import List, Optional\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nasync def read_channel(\n    discord_token: str, channel_id: int, limit: Optional[int], oldest_first: bool\n) -> str:\n    \"\"\"Async read channel.\n\n    Note: This is our hack to create a synchronous interface to the\n    async discord.py API. We use the `asyncio` module to run\n    this function with `asyncio.get_event_loop().run_until_complete`.\n\n    \"\"\"\n    import discord  # noqa: F401\n\n    messages: List[discord.Message] = []\n\n    class CustomClient(discord.Client):\n        async def on_ready(self) -> None:\n            try:\n                logging.info(f\"{self.user} has connected to Discord!\")\n                channel = client.get_channel(channel_id)\n                # only work for text channels for now\n                if not isinstance(channel, discord.TextChannel):\n                    raise ValueError(\n                        f\"Channel {channel_id} is not a text channel. \"\n                        \"Only text channels are supported for now.\"\n                    )\n                # thread_dict maps thread_id to thread\n                thread_dict = {}\n                for thread in channel.threads:\n                    thread_dict[thread.id] = thread\n\n                async for msg in channel.history(\n                    limit=limit, oldest_first=oldest_first\n                ):\n                    messages.append(msg)\n                    if msg.id in thread_dict:\n                        thread = thread_dict[msg.id]\n                        async for thread_msg in thread.history(\n                            limit=limit, oldest_first=oldest_first\n                        ):\n                            messages.append(thread_msg)\n            except Exception as e:\n                logging.error(\"Encountered error: \" + str(e))\n            finally:\n                await self.close()\n\n    intents = discord.Intents.default()\n    intents.message_content = True\n    client = CustomClient(intents=intents)\n    await client.start(discord_token)\n\n    msg_txt_list = [m.content for m in messages]\n\n    return \"\\n\\n\".join(msg_txt_list)\n\n\nclass DiscordReader(BaseReader):\n    \"\"\"Discord reader.\n\n    Reads conversations from channels.\n\n    Args:\n        discord_token (Optional[str]): Discord token. If not provided, we\n            assume the environment variable `DISCORD_TOKEN` is set.\n\n    \"\"\"\n\n    def __init__(self, discord_token: Optional[str] = None) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            import discord  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`discord.py` package not found, please run `pip install discord.py`\"\n            )\n        if discord_token is None:\n            discord_token = os.environ[\"DISCORD_TOKEN\"]\n            if discord_token is None:\n                raise ValueError(\n                    \"Must specify `discord_token` or set environment \"\n                    \"variable `DISCORD_TOKEN`.\"\n                )\n\n        self.discord_token = discord_token\n\n    def _read_channel(\n        self, channel_id: int, limit: Optional[int] = None, oldest_first: bool = True\n    ) -> str:\n        \"\"\"Read channel.\"\"\"\n        result = asyncio.get_event_loop().run_until_complete(\n            read_channel(\n                self.discord_token, channel_id, limit=limit, oldest_first=oldest_first\n            )\n        )\n        return result\n\n    def load_data(\n        self,\n        channel_ids: List[int],\n        limit: Optional[int] = None,\n        oldest_first: bool = True,\n    ) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            channel_ids (List[int]): List of channel ids to read.\n            limit (Optional[int]): Maximum number of messages to read.\n            oldest_first (bool): Whether to read oldest messages first.\n                Defaults to `True`.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        results: List[Document] = []\n        for channel_id in channel_ids:\n            if not isinstance(channel_id, int):\n                raise ValueError(\n                    f\"Channel id {channel_id} must be an integer, \"\n                    f\"not {type(channel_id)}.\"\n                )\n            channel_content = self._read_channel(\n                channel_id, limit=limit, oldest_first=oldest_first\n            )\n            results.append(\n                Document(channel_content, extra_info={\"channel\": channel_id})\n            )\n        return results\n\n\nif __name__ == \"__main__\":\n    reader = DiscordReader()\n    logging.info(\"initialized reader\")\n    output = reader.load_data(channel_ids=[1057178784895348746], limit=10)\n    logging.info(output)\n", "doc_id": "8d9a229a3248b1617b67774eac291a2594ec686e", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/discord_reader.py", "file_name": "discord_reader.py"}, "__type__": "Document"}, "71585e29180e9ae11f0774b391bf9fd877e5ac54": {"text": "\"\"\"Download loader from the Loader Hub.\"\"\"\n\nimport json\nimport os\nimport subprocess\nimport sys\nfrom importlib import util\nfrom pathlib import Path\n\nimport pkg_resources\nimport requests\nfrom pkg_resources import DistributionNotFound\n\nfrom gpt_index.readers.base import BaseReader\n\nLOADER_HUB_URL = (\n    \"https://raw.githubusercontent.com/emptycrown/loader-hub/main/loader_hub\"\n)\n\n\ndef download_loader(loader_class: str) -> BaseReader:\n    \"\"\"Download a single loader from the Loader Hub.\n\n    Args:\n        loader_class: The name of the loader class you want to download,\n            such as `SimpleWebPageReader`.\n    Returns:\n        A Loader.\n    \"\"\"\n    response = requests.get(f\"{LOADER_HUB_URL}/library.json\")\n    library = json.loads(response.text)\n\n    # Look up the loader id (e.g. `web/simple_web`)\n    loader_id = library[loader_class][\"id\"]\n    dirpath = \".modules\"\n    loader_filename = loader_id.replace(\"/\", \"-\")\n    loader_path = f\"{dirpath}/{loader_filename}.py\"\n    requirements_path = f\"{dirpath}/{loader_filename}_requirements.txt\"\n\n    if not os.path.exists(dirpath):\n        # Create a new directory because it does not exist\n        os.makedirs(dirpath)\n\n    if not os.path.exists(loader_path):\n        response = requests.get(f\"{LOADER_HUB_URL}/{loader_id}/base.py\")\n        with open(loader_path, \"w\") as f:\n            f.write(response.text)\n\n    if not os.path.exists(requirements_path):\n        response = requests.get(f\"{LOADER_HUB_URL}/{loader_id}/requirements.txt\")\n        if response.status_code == 200:\n            with open(requirements_path, \"w\") as f:\n                f.write(response.text)\n\n    # Install dependencies if there are any and not already installed\n    if os.path.exists(requirements_path):\n        try:\n            requirements = pkg_resources.parse_requirements(\n                Path(requirements_path).open()\n            )\n            pkg_resources.require([str(r) for r in requirements])\n        except DistributionNotFound:\n            subprocess.check_call(\n                [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", requirements_path]\n            )\n\n    spec = util.spec_from_file_location(\"custom_loader\", location=loader_path)\n    if spec is None:\n        raise ValueError(f\"Could not find file: {loader_path}.\")\n    module = util.module_from_spec(spec)\n    spec.loader.exec_module(module)  # type: ignore\n\n    return getattr(module, loader_class)\n", "doc_id": "71585e29180e9ae11f0774b391bf9fd877e5ac54", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/download.py", "file_name": "download.py"}, "__type__": "Document"}, "82f076eb7558c8648d1b13d5ccec126b5cb6a5f3": {"text": "\"\"\"Faiss reader.\"\"\"\n\nfrom typing import Any, Dict, List\n\nimport numpy as np\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass FaissReader(BaseReader):\n    \"\"\"Faiss reader.\n\n    Retrieves documents through an existing in-memory Faiss index.\n    These documents can then be used in a downstream GPT Index data structure.\n    If you wish use Faiss itself as an index to to organize documents,\n    insert documents, and perform queries on them, please use GPTFaissIndex.\n\n    Args:\n        faiss_index (faiss.Index): A Faiss Index object (required)\n\n    \"\"\"\n\n    def __init__(self, index: Any):\n        \"\"\"Initialize with parameters.\"\"\"\n        import_err_msg = \"\"\"\n            `faiss` package not found. For instructions on\n            how to install `faiss` please visit\n            https://github.com/facebookresearch/faiss/wiki/Installing-Faiss\n        \"\"\"\n        try:\n            import faiss  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n\n        self._index = index\n\n    def load_data(\n        self,\n        query: np.ndarray,\n        id_to_text_map: Dict[str, str],\n        k: int = 4,\n        separate_documents: bool = True,\n    ) -> List[Document]:\n        \"\"\"Load data from Faiss.\n\n        Args:\n            query (np.ndarray): A 2D numpy array of query vectors.\n            id_to_text_map (Dict[str, str]): A map from ID's to text.\n            k (int): Number of nearest neighbors to retrieve. Defaults to 4.\n            separate_documents (Optional[bool]): Whether to return separate\n                documents. Defaults to True.\n        Returns:\n            List[Document]: A list of documents.\n\n        \"\"\"\n        dists, indices = self._index.search(query, k)\n        documents = []\n        for qidx in range(indices.shape[0]):\n            for didx in range(indices.shape[1]):\n                doc_id = indices[qidx, didx]\n                if doc_id not in id_to_text_map:\n                    raise ValueError(\n                        f\"Document ID {doc_id} not found in id_to_text_map.\"\n                    )\n                text = id_to_text_map[doc_id]\n                documents.append(Document(text=text))\n\n        if not separate_documents:\n            # join all documents into one\n            text_list = [doc.get_text() for doc in documents]\n            text = \"\\n\\n\".join(text_list)\n            documents = [Document(text=text)]\n\n        return documents\n", "doc_id": "82f076eb7558c8648d1b13d5ccec126b5cb6a5f3", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/faiss.py", "file_name": "faiss.py"}, "__type__": "Document"}, "b8ba0526790bd62d1a41d878474809af816f616e": {"text": "\"\"\"Simple reader for mbox (mailbox) files.\"\"\"\nimport os\nfrom pathlib import Path\nfrom typing import Any, List\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.file.mbox_parser import MboxParser\nfrom gpt_index.readers.schema.base import Document\n\n\nclass MboxReader(BaseReader):\n    \"\"\"Mbox e-mail reader.\n\n    Reads a set of e-mails saved in the mbox format.\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize.\"\"\"\n\n    def load_data(self, input_dir: str, **load_kwargs: Any) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        load_kwargs:\n            max_count (int): Maximum amount of messages to read.\n            message_format (str): Message format overriding default.\n        \"\"\"\n        docs: List[Document] = []\n        for (dirpath, dirnames, filenames) in os.walk(input_dir):\n            dirnames[:] = [d for d in dirnames if not d.startswith(\".\")]\n            for filename in filenames:\n                if filename.endswith(\".mbox\"):\n                    filepath = os.path.join(dirpath, filename)\n                    content = MboxParser(**load_kwargs).parse_file(Path(filepath))\n                    for msg in content:\n                        docs.append(Document(msg))\n        return docs\n", "doc_id": "b8ba0526790bd62d1a41d878474809af816f616e", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/mbox.py", "file_name": "mbox.py"}, "__type__": "Document"}, "2bc13de0428fd82ed9d665fc257405cf2de8929a": {"text": "\"\"\"Mongo client.\"\"\"\n\nfrom typing import Dict, List, Optional\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass SimpleMongoReader(BaseReader):\n    \"\"\"Simple mongo reader.\n\n    Concatenates each Mongo doc into Document used by GPT Index.\n\n    Args:\n        host (str): Mongo host.\n        port (int): Mongo port.\n        max_docs (int): Maximum number of documents to load.\n\n    \"\"\"\n\n    def __init__(self, host: str, port: int, max_docs: int = 1000) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            import pymongo  # noqa: F401\n            from pymongo import MongoClient  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`pymongo` package not found, please run `pip install pymongo`\"\n            )\n        self.client: MongoClient = MongoClient(host, port)\n        self.max_docs = max_docs\n\n    def load_data(\n        self, db_name: str, collection_name: str, query_dict: Optional[Dict] = None\n    ) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            db_name (str): name of the database.\n            collection_name (str): name of the collection.\n            query_dict (Optional[Dict]): query to filter documents.\n                Defaults to None\n\n        Returns:\n            List[Document]: A list of documents.\n\n        \"\"\"\n        documents = []\n        db = self.client[db_name]\n        if query_dict is None:\n            cursor = db[collection_name].find()\n        else:\n            cursor = db[collection_name].find(query_dict)\n\n        for item in cursor:\n            if \"text\" not in item:\n                raise ValueError(\"`text` field not found in Mongo document.\")\n            documents.append(Document(item[\"text\"]))\n        return documents\n", "doc_id": "2bc13de0428fd82ed9d665fc257405cf2de8929a", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/mongo.py", "file_name": "mongo.py"}, "__type__": "Document"}, "c7ee9e2bf5f208fdcdcdf35eaf126ee0abbefe83": {"text": "\"\"\"Notion reader.\"\"\"\nimport logging\nimport os\nfrom typing import Any, Dict, List, Optional\n\nimport requests  # type: ignore\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\nINTEGRATION_TOKEN_NAME = \"NOTION_INTEGRATION_TOKEN\"\nBLOCK_CHILD_URL_TMPL = \"https://api.notion.com/v1/blocks/{block_id}/children\"\nDATABASE_URL_TMPL = \"https://api.notion.com/v1/databases/{database_id}/query\"\nSEARCH_URL = \"https://api.notion.com/v1/search\"\n\n\n# TODO: Notion DB reader coming soon!\nclass NotionPageReader(BaseReader):\n    \"\"\"Notion Page reader.\n\n    Reads a set of Notion pages.\n\n    Args:\n        integration_token (str): Notion integration token.\n\n    \"\"\"\n\n    def __init__(self, integration_token: Optional[str] = None) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        if integration_token is None:\n            integration_token = os.getenv(INTEGRATION_TOKEN_NAME)\n            if integration_token is None:\n                raise ValueError(\n                    \"Must specify `integration_token` or set environment \"\n                    \"variable `NOTION_INTEGRATION_TOKEN`.\"\n                )\n        self.token = integration_token\n        self.headers = {\n            \"Authorization\": \"Bearer \" + self.token,\n            \"Content-Type\": \"application/json\",\n            \"Notion-Version\": \"2022-06-28\",\n        }\n\n    def _read_block(self, block_id: str, num_tabs: int = 0) -> str:\n        \"\"\"Read a block.\"\"\"\n        done = False\n        result_lines_arr = []\n        cur_block_id = block_id\n        while not done:\n            block_url = BLOCK_CHILD_URL_TMPL.format(block_id=cur_block_id)\n            query_dict: Dict[str, Any] = {}\n\n            res = requests.request(\n                \"GET\", block_url, headers=self.headers, json=query_dict\n            )\n            data = res.json()\n\n            for result in data[\"results\"]:\n                result_type = result[\"type\"]\n                result_obj = result[result_type]\n\n                cur_result_text_arr = []\n                if \"rich_text\" in result_obj:\n                    for rich_text in result_obj[\"rich_text\"]:\n                        # skip if doesn't have text object\n                        if \"text\" in rich_text:\n                            text = rich_text[\"text\"][\"content\"]\n                            prefix = \"\\t\" * num_tabs\n                            cur_result_text_arr.append(prefix + text)\n\n                result_block_id = result[\"id\"]\n                has_children = result[\"has_children\"]\n                if has_children:\n                    children_text = self._read_block(\n                        result_block_id, num_tabs=num_tabs + 1\n                    )\n                    cur_result_text_arr.append(children_text)\n\n                cur_result_text = \"\\n\".join(cur_result_text_arr)\n                result_lines_arr.append(cur_result_text)\n\n            if data[\"next_cursor\"] is None:\n                done = True\n                break\n            else:\n                cur_block_id = data[\"next_cursor\"]\n\n        result_lines = \"\\n\".join(result_lines_arr)\n        return result_lines\n\n    def read_page(self, page_id: str) -> str:\n        \"\"\"Read a page.\"\"\"\n        return self._read_block(page_id)\n\n    def query_database(\n        self, database_id: str, query_dict: Dict[str, Any] = {}\n    ) -> List[str]:\n        \"\"\"Get all the pages from a Notion database.\"\"\"\n        res = requests.post(\n            DATABASE_URL_TMPL.format(database_id=database_id),\n            headers=self.headers,\n            json=query_dict,\n        )\n        data = res.json()\n        page_ids = []\n        for result in data[\"results\"]:\n            page_id = result[\"id\"]\n            page_ids.append(page_id)\n\n        return page_ids\n\n    def search(self, query: str) -> List[str]:\n        \"\"\"Search Notion page given a text query.\"\"\"\n        done = False\n        next_cursor: Optional[str] = None\n        page_ids = []\n        while not done:\n            query_dict = {\n                \"query\": query,\n            }\n            if next_cursor is not None:\n                query_dict[\"start_cursor\"] = next_cursor\n            res = requests.post(SEARCH_URL, headers=self.headers, json=query_dict)\n            data = res.json()\n            for result in data[\"results\"]:\n                page_id = result[\"id\"]\n                page_ids.append(page_id)\n\n            if data[\"next_cursor\"] is None:\n                done = True\n                break\n            else:\n                next_cursor = data[\"next_cursor\"]\n        return page_ids\n\n    def load_data(\n        self, page_ids: List[str] = [], database_id: Optional[str] = None\n    ) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            page_ids (List[str]): List of page ids to load.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        if not page_ids and not database_id:\n            raise ValueError(\"Must specify either `page_ids` or `database_id`.\")\n        docs = []\n        if database_id is not None:\n            # get all the pages in the database\n            page_ids = self.query_database(database_id)\n            for page_id in page_ids:\n                page_text = self.read_page(page_id)\n                docs.append(Document(page_text, extra_info={\"page_id\": page_id}))\n        else:\n            for page_id in page_ids:\n                page_text = self.read_page(page_id)\n                docs.append(Document(page_text, extra_info={\"page_id\": page_id}))\n\n        return docs\n\n\nif __name__ == \"__main__\":\n    reader = NotionPageReader()\n    logging.info(reader.search(\"What I\"))\n", "doc_id": "c7ee9e2bf5f208fdcdcdf35eaf126ee0abbefe83", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/notion.py", "file_name": "notion.py"}, "__type__": "Document"}, "1404005433f2a8f5a4aaf2f0cb40dba48a0cd9e8": {"text": "\"\"\"Obsidian reader class.\n\nPass in the path to an Obsidian vault and it will parse all markdown\nfiles into a List of Documents,\nwith each Document containing text from under an Obsidian header.\n\n\"\"\"\nimport os\nfrom pathlib import Path\nfrom typing import Any, List\n\nfrom langchain.docstore.document import Document as LCDocument\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.file.markdown_parser import MarkdownParser\nfrom gpt_index.readers.schema.base import Document\n\n\nclass ObsidianReader(BaseReader):\n    \"\"\"Utilities for loading data from an Obsidian Vault.\n\n    Args:\n        input_dir (str): Path to the vault.\n\n    \"\"\"\n\n    def __init__(self, input_dir: str):\n        \"\"\"Init params.\"\"\"\n        self.input_dir = Path(input_dir)\n\n    def load_data(self, *args: Any, **load_kwargs: Any) -> List[Document]:\n        \"\"\"Load data from the input directory.\"\"\"\n        docs: List[str] = []\n        for dirpath, dirnames, filenames in os.walk(self.input_dir):\n            dirnames[:] = [d for d in dirnames if not d.startswith(\".\")]\n            for filename in filenames:\n                if filename.endswith(\".md\"):\n                    filepath = os.path.join(dirpath, filename)\n                    content = MarkdownParser().parse_file(Path(filepath))\n                    docs.extend(content)\n        return [Document(d) for d in docs]\n\n    def load_langchain_documents(self, **load_kwargs: Any) -> List[LCDocument]:\n        \"\"\"Load data in LangChain document format.\"\"\"\n        docs = self.load_data(**load_kwargs)\n        return [d.to_langchain_format() for d in docs]\n", "doc_id": "1404005433f2a8f5a4aaf2f0cb40dba48a0cd9e8", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/obsidian.py", "file_name": "obsidian.py"}, "__type__": "Document"}, "c121f325702354416b70e9e38021ce04ff265381": {"text": "\"\"\"Pinecone reader.\"\"\"\n\nfrom typing import Any, Dict, List, Optional\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass PineconeReader(BaseReader):\n    \"\"\"Pinecone reader.\n\n    Args:\n        api_key (str): Pinecone API key.\n        environment (str): Pinecone environment.\n    \"\"\"\n\n    def __init__(self, api_key: str, environment: str):\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            import pinecone  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`pinecone` package not found, please run `pip install pinecone-client`\"\n            )\n\n        self._api_key = api_key\n        self._environment = environment\n        pinecone.init(api_key=api_key, environment=environment)\n\n    def load_data(\n        self,\n        index_name: str,\n        id_to_text_map: Dict[str, str],\n        vector: Optional[List[float]],\n        top_k: int,\n        separate_documents: bool = True,\n        include_values: bool = True,\n        **query_kwargs: Any\n    ) -> List[Document]:\n        \"\"\"Load data from Pinecone.\n\n        Args:\n            index_name (str): Name of the index.\n            id_to_text_map (Dict[str, str]): A map from ID's to text.\n            separate_documents (Optional[bool]): Whether to return separate\n                documents per retrieved entry. Defaults to True.\n            vector (List[float]): Query vector.\n            top_k (int): Number of results to return.\n            include_values (bool): Whether to include the embedding in the response.\n                Defaults to True.\n            **query_kwargs: Keyword arguments to pass to the query.\n                Arguments are the exact same as those found in\n                Pinecone's reference documentation for the\n                query method.\n\n        Returns:\n            List[Document]: A list of documents.\n        \"\"\"\n        import pinecone\n\n        index = pinecone.Index(index_name)\n        if \"include_values\" not in query_kwargs:\n            query_kwargs[\"include_values\"] = True\n        response = index.query(top_k=top_k, vector=vector, **query_kwargs)\n\n        documents = []\n        for match in response.matches:\n            if match.id not in id_to_text_map:\n                raise ValueError(\"ID not found in id_to_text_map.\")\n            text = id_to_text_map[match.id]\n            embedding = match.values\n            if len(embedding) == 0:\n                embedding = None\n            documents.append(Document(text=text, embedding=embedding))\n\n        if not separate_documents:\n            text_list = [doc.get_text() for doc in documents]\n            text = \"\\n\\n\".join(text_list)\n            documents = [Document(text=text)]\n\n        return documents\n", "doc_id": "c121f325702354416b70e9e38021ce04ff265381", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/pinecone.py", "file_name": "pinecone.py"}, "__type__": "Document"}, "5088ae6098fcc16c6105a3cbf9b6b1c7dac5ec53": {"text": "\"\"\"Qdrant reader.\"\"\"\n\nfrom typing import List, Optional, cast\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass QdrantReader(BaseReader):\n    \"\"\"Qdrant reader.\n\n    Retrieve documents from existing Qdrant collections.\n\n    Args:\n        host: Host name of Qdrant service.\n        port: Port of the REST API interface. Default: 6333\n        grpc_port: Port of the gRPC interface. Default: 6334\n        prefer_grpc: If `true` - use gPRC interface whenever possible in custom methods.\n        https: If `true` - use HTTPS(SSL) protocol. Default: `false`\n        api_key: API key for authentication in Qdrant Cloud. Default: `None`\n        prefix:\n            If not `None` - add `prefix` to the REST URL path.\n            Example: `service/v1` will result in\n            `http://localhost:6333/service/v1/{qdrant-endpoint}` for REST API.\n            Default: `None`\n        timeout:\n            Timeout for REST and gRPC API requests.\n            Default: 5.0 seconds for REST and unlimited for gRPC\n    \"\"\"\n\n    def __init__(\n        self,\n        host: str,\n        port: int = 6333,\n        grpc_port: int = 6334,\n        prefer_grpc: bool = False,\n        https: Optional[bool] = None,\n        api_key: Optional[str] = None,\n        prefix: Optional[str] = None,\n        timeout: Optional[float] = None,\n    ):\n        \"\"\"Initialize with parameters.\"\"\"\n        import_err_msg = (\n            \"`qdrant-client` package not found, please run `pip install qdrant-client`\"\n        )\n        try:\n            import qdrant_client  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n\n        self._client = qdrant_client.QdrantClient(\n            host=host,\n            port=port,\n            grpc_port=grpc_port,\n            prefer_grpc=prefer_grpc,\n            https=https,\n            api_key=api_key,\n            prefix=prefix,\n            timeout=timeout,\n        )\n\n    def load_data(\n        self,\n        collection_name: str,\n        query_vector: List[float],\n        limit: int = 10,\n    ) -> List[Document]:\n        \"\"\"Load data from Qdrant.\n\n        Args:\n            collection_name (str): Name of the Qdrant collection.\n            query_vector (List[float]): Query vector.\n            limit (int): Number of results to return.\n\n        Returns:\n            List[Document]: A list of documents.\n        \"\"\"\n        from qdrant_client.http.models.models import Payload\n\n        response = self._client.search(\n            collection_name=collection_name,\n            query_vector=query_vector,\n            with_vectors=True,\n            with_payload=True,\n            limit=limit,\n        )\n\n        documents = []\n        for point in response:\n            payload = cast(Payload, point)\n            try:\n                vector = cast(List[float], point.vector)\n            except ValueError as e:\n                raise ValueError(\"Could not cast vector to List[float].\") from e\n            document = Document(\n                doc_id=payload.get(\"doc_id\"),\n                text=payload.get(\"text\"),\n                embedding=vector,\n            )\n            documents.append(document)\n\n        return documents\n", "doc_id": "5088ae6098fcc16c6105a3cbf9b6b1c7dac5ec53", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/qdrant.py", "file_name": "qdrant.py"}, "__type__": "Document"}, "a0db9d42d0b3dcf85ce915984e4159a8d57fd656": {"text": "\"\"\"Slack reader.\"\"\"\nimport logging\nimport os\nimport time\nfrom typing import List, Optional\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass SlackReader(BaseReader):\n    \"\"\"Slack reader.\n\n    Reads conversations from channels.\n\n    Args:\n        slack_token (Optional[str]): Slack token. If not provided, we\n            assume the environment variable `SLACK_BOT_TOKEN` is set.\n\n    \"\"\"\n\n    def __init__(self, slack_token: Optional[str] = None) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            from slack_sdk import WebClient\n        except ImportError:\n            raise ValueError(\n                \"`slack_sdk` package not found, please run `pip install slack_sdk`\"\n            )\n        if slack_token is None:\n            slack_token = os.environ[\"SLACK_BOT_TOKEN\"]\n            if slack_token is None:\n                raise ValueError(\n                    \"Must specify `slack_token` or set environment \"\n                    \"variable `SLACK_BOT_TOKEN`.\"\n                )\n        self.client = WebClient(token=slack_token)\n        res = self.client.api_test()\n        if not res[\"ok\"]:\n            raise ValueError(f\"Error initializing Slack API: {res['error']}\")\n\n    def _read_message(self, channel_id: str, message_ts: str) -> str:\n        from slack_sdk.errors import SlackApiError\n\n        \"\"\"Read a message.\"\"\"\n\n        messages_text = []\n        next_cursor = None\n        while True:\n            try:\n                # https://slack.com/api/conversations.replies\n                # List all replies to a message, including the message itself.\n                result = self.client.conversations_replies(\n                    channel=channel_id, ts=message_ts, cursor=next_cursor\n                )\n                messages = result[\"messages\"]\n                for message in messages:\n                    messages_text.append(message[\"text\"])\n\n                if not result[\"has_more\"]:\n                    break\n\n                next_cursor = result[\"response_metadata\"][\"next_cursor\"]\n            except SlackApiError as e:\n                if e.response[\"error\"] == \"ratelimited\":\n                    logging.error(\n                        \"Rate limit error reached, sleeping for: {} seconds\".format(\n                            e.response.headers[\"retry-after\"]\n                        )\n                    )\n                    time.sleep(int(e.response.headers[\"retry-after\"]))\n                else:\n                    logging.error(\"Error parsing conversation replies: {}\".format(e))\n\n        return \"\\n\\n\".join(messages_text)\n\n    def _read_channel(self, channel_id: str) -> str:\n        from slack_sdk.errors import SlackApiError\n\n        \"\"\"Read a channel.\"\"\"\n\n        result_messages = []\n        next_cursor = None\n        while True:\n            try:\n                # Call the conversations.history method using the WebClient\n                # conversations.history returns the first 100 messages by default\n                # These results are paginated,\n                # see: https://api.slack.com/methods/conversations.history$pagination\n                result = self.client.conversations_history(\n                    channel=channel_id, cursor=next_cursor\n                )\n                conversation_history = result[\"messages\"]\n                # Print results\n                logging.info(\n                    \"{} messages found in {}\".format(len(conversation_history), id)\n                )\n                for message in conversation_history:\n                    result_messages.append(\n                        self._read_message(channel_id, message[\"ts\"])\n                    )\n\n                if not result[\"has_more\"]:\n                    break\n                next_cursor = result[\"response_metadata\"][\"next_cursor\"]\n\n            except SlackApiError as e:\n                if e.response[\"error\"] == \"ratelimited\":\n                    logging.error(\n                        \"Rate limit error reached, sleeping for: {} seconds\".format(\n                            e.response.headers[\"retry-after\"]\n                        )\n                    )\n                    time.sleep(int(e.response.headers[\"retry-after\"]))\n                else:\n                    logging.error(\"Error parsing conversation replies: {}\".format(e))\n\n        return \"\\n\\n\".join(result_messages)\n\n    def load_data(self, channel_ids: List[str]) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            channel_ids (List[str]): List of channel ids to read.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        results = []\n        for channel_id in channel_ids:\n            channel_content = self._read_channel(channel_id)\n            results.append(\n                Document(channel_content, extra_info={\"channel\": channel_id})\n            )\n        return results\n\n\nif __name__ == \"__main__\":\n    reader = SlackReader()\n    logging.info(reader.load_data(channel_ids=[\"C04DC2VUY3F\"]))\n", "doc_id": "a0db9d42d0b3dcf85ce915984e4159a8d57fd656", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/slack.py", "file_name": "slack.py"}, "__type__": "Document"}, "62876b8e16ae2625adf3c433bdd1b3eaa70150c3": {"text": "\"\"\"Simple reader that turns an iterable of strings into a list of Documents.\"\"\"\nfrom typing import List\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass StringIterableReader(BaseReader):\n    \"\"\"String Iterable Reader.\n\n    Gets a list of documents, given an iterable (e.g. list) of strings.\n\n    Example:\n        .. code-block:: python\n\n            from gpt_index import StringIterableReader, GPTTreeIndex\n\n            documents = StringIterableReader().load_data(\n                texts=[\"I went to the store\", \"I bought an apple\"])\n            index = GPTTreeIndex(documents)\n            index.query(\"what did I buy?\")\n\n            # response should be something like \"You bought an apple.\"\n    \"\"\"\n\n    def load_data(self, texts: List[str]) -> List[Document]:\n        \"\"\"Load the data.\"\"\"\n        results = []\n        for text in texts:\n            results.append(Document(text))\n\n        return results\n", "doc_id": "62876b8e16ae2625adf3c433bdd1b3eaa70150c3", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/string_iterable.py", "file_name": "string_iterable.py"}, "__type__": "Document"}, "ee0e8f78b27c1a62e466e13d35e9bbeea3b425e9": {"text": "\"\"\"Simple reader that reads tweets of a twitter handle.\"\"\"\nfrom typing import Any, List, Optional\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass TwitterTweetReader(BaseReader):\n    \"\"\"Twitter tweets reader.\n\n    Read tweets of user twitter handle.\n\n    Check 'https://developer.twitter.com/en/docs/twitter-api/\\\n        getting-started/getting-access-to-the-twitter-api' \\\n        on how to get access to twitter API.\n\n    Args:\n        bearer_token (str): bearer_token that you get from twitter API.\n        num_tweets (Optional[int]): Number of tweets for each user twitter handle.\\\n            Default is 100 tweets.\n    \"\"\"\n\n    def __init__(\n        self,\n        bearer_token: str,\n        num_tweets: Optional[int] = 100,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        self.bearer_token = bearer_token\n        self.num_tweets = num_tweets\n\n    def load_data(\n        self, twitterhandles: List[str], **load_kwargs: Any\n    ) -> List[Document]:\n        \"\"\"Load tweets of twitter handles.\n\n        Args:\n            twitterhandles (List[str]): List of user twitter handles to read tweets.\n\n        \"\"\"\n        try:\n            import tweepy\n        except ImportError:\n            raise ValueError(\n                \"`tweepy` package not found, please run `pip install tweepy`\"\n            )\n\n        client = tweepy.Client(bearer_token=self.bearer_token)\n        results = []\n        for username in twitterhandles:\n            # tweets = api.user_timeline(screen_name=user, count=self.num_tweets)\n            user = client.get_user(username=username)\n            tweets = client.get_users_tweets(user.data.id, max_results=self.num_tweets)\n            response = \" \"\n            for tweet in tweets.data:\n                response = response + tweet.text + \"\\n\"\n            results.append(Document(response))\n        return results\n", "doc_id": "ee0e8f78b27c1a62e466e13d35e9bbeea3b425e9", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/twitter.py", "file_name": "twitter.py"}, "__type__": "Document"}, "eac2fa9f945363412e1401c12d4b4c1fb14376b1": {"text": "\"\"\"Web scraper.\"\"\"\nimport logging\nfrom typing import Any, Callable, Dict, List, Optional, Tuple\n\nfrom langchain.utilities import RequestsWrapper\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass SimpleWebPageReader(BaseReader):\n    \"\"\"Simple web page reader.\n\n    Reads pages from the web.\n\n    Args:\n        html_to_text (bool): Whether to convert HTML to text.\n            Requires `html2text` package.\n\n    \"\"\"\n\n    def __init__(self, html_to_text: bool = False) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            import html2text  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`html2text` package not found, please run `pip install html2text`\"\n            )\n        self._html_to_text = html_to_text\n\n    def load_data(self, urls: List[str]) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            urls (List[str]): List of URLs to scrape.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        if not isinstance(urls, list):\n            raise ValueError(\"urls must be a list of strings.\")\n        requests = RequestsWrapper()\n        documents = []\n        for url in urls:\n            response = requests.run(url)\n            if self._html_to_text:\n                import html2text\n\n                response = html2text.html2text(response)\n\n            documents.append(Document(response))\n\n        return documents\n\n\nclass TrafilaturaWebReader(BaseReader):\n    \"\"\"Trafilatura web page reader.\n\n    Reads pages from the web.\n    Requires the `trafilatura` package.\n\n    \"\"\"\n\n    def __init__(self, error_on_missing: bool = False) -> None:\n        \"\"\"Initialize with parameters.\n\n        Args:\n            error_on_missing (bool): Throw an error when data cannot be parsed\n        \"\"\"\n        self.error_on_missing = error_on_missing\n        try:\n            import trafilatura  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`trafilatura` package not found, please run `pip install trafilatura`\"\n            )\n\n    def load_data(self, urls: List[str]) -> List[Document]:\n        \"\"\"Load data from the urls.\n\n        Args:\n            urls (List[str]): List of URLs to scrape.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        import trafilatura\n\n        if not isinstance(urls, list):\n            raise ValueError(\"urls must be a list of strings.\")\n        documents = []\n        for url in urls:\n            downloaded = trafilatura.fetch_url(url)\n            if not downloaded:\n                if self.error_on_missing:\n                    raise ValueError(f\"Trafilatura fails to get string from url: {url}\")\n                continue\n            response = trafilatura.extract(downloaded)\n            if not response:\n                if self.error_on_missing:\n                    raise ValueError(f\"Trafilatura fails to parse page: {url}\")\n                continue\n            documents.append(Document(response))\n\n        return documents\n\n\ndef _substack_reader(soup: Any) -> Tuple[str, Dict[str, Any]]:\n    \"\"\"Extract text from Substack blog post.\"\"\"\n    extra_info = {\n        \"Title of this Substack post\": soup.select_one(\"h1.post-title\").getText(),\n        \"Subtitle\": soup.select_one(\"h3.subtitle\").getText(),\n        \"Author\": soup.select_one(\"span.byline-names\").getText(),\n    }\n    text = soup.select_one(\"div.available-content\").getText()\n    return text, extra_info\n\n\nDEFAULT_WEBSITE_EXTRACTOR: Dict[str, Callable[[Any], Tuple[str, Dict[str, Any]]]] = {\n    \"substack.com\": _substack_reader,\n}\n\n\nclass BeautifulSoupWebReader(BaseReader):\n    \"\"\"BeautifulSoup web page reader.\n\n    Reads pages from the web.\n    Requires the `bs4` and `urllib` packages.\n\n    Args:\n        file_extractor (Optional[Dict[str, Callable]]): A mapping of website\n            hostname (e.g. google.com) to a function that specifies how to\n            extract text from the BeautifulSoup obj. See DEFAULT_WEBSITE_EXTRACTOR.\n    \"\"\"\n\n    def __init__(\n        self,\n        website_extractor: Optional[Dict[str, Callable]] = None,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            from urllib.parse import urlparse  # noqa: F401\n\n            import requests  # noqa: F401\n            from bs4 import BeautifulSoup  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`bs4`, `requests`, and `urllib` must be installed to scrape websites.\"\n                \"Please run `pip install bs4 requests urllib`.\"\n            )\n\n        self.website_extractor = website_extractor or DEFAULT_WEBSITE_EXTRACTOR\n\n    def load_data(\n        self, urls: List[str], custom_hostname: Optional[str] = None\n    ) -> List[Document]:\n        \"\"\"Load data from the urls.\n\n        Args:\n            urls (List[str]): List of URLs to scrape.\n            custom_hostname (Optional[str]): Force a certain hostname in the case\n                a website is displayed under custom URLs (e.g. Substack blogs)\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        from urllib.parse import urlparse\n\n        import requests\n        from bs4 import BeautifulSoup\n\n        documents = []\n        for url in urls:\n            try:\n                page = requests.get(url)\n            except Exception:\n                raise ValueError(f\"One of the inputs is not a valid url: {url}\")\n\n            hostname = custom_hostname or urlparse(url).hostname or \"\"\n\n            soup = BeautifulSoup(page.content, \"html.parser\")\n\n            data = \"\"\n            extra_info = {\"URL\": url}\n            if hostname in self.website_extractor:\n                data, metadata = self.website_extractor[hostname](soup)\n                extra_info.update(metadata)\n            else:\n                data = soup.getText()\n\n            documents.append(Document(data, extra_info=extra_info))\n\n        return documents\n\n\nclass RssReader(BaseReader):\n    \"\"\"RSS reader.\n\n    Reads content from an RSS feed.\n\n    \"\"\"\n\n    def __init__(self, html_to_text: bool = False) -> None:\n        \"\"\"Initialize with parameters.\n\n        Args:\n            html_to_text (bool): Whether to convert HTML to text.\n                Requires `html2text` package.\n\n        \"\"\"\n        try:\n            import feedparser  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`feedparser` package not found, please run `pip install feedparser`\"\n            )\n\n        if html_to_text:\n            try:\n                import html2text  # noqa: F401\n            except ImportError:\n                raise ValueError(\n                    \"`html2text` package not found, please run `pip install html2text`\"\n                )\n        self._html_to_text = html_to_text\n\n    def load_data(self, urls: List[str]) -> List[Document]:\n        \"\"\"Load data from RSS feeds.\n\n        Args:\n            urls (List[str]): List of RSS URLs to load.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        import feedparser\n\n        if not isinstance(urls, list):\n            raise ValueError(\"urls must be a list of strings.\")\n\n        documents = []\n\n        for url in urls:\n            parsed = feedparser.parse(url)\n            for entry in parsed.entries:\n                if entry.content:\n                    data = entry.content[0].value\n                else:\n                    data = entry.description or entry.summary\n\n                if self._html_to_text:\n                    import html2text\n\n                    data = html2text.html2text(data)\n\n                extra_info = {\"title\": entry.title, \"link\": entry.link}\n                documents.append(Document(data, extra_info=extra_info))\n\n        return documents\n\n\nif __name__ == \"__main__\":\n    reader = SimpleWebPageReader()\n    logging.info(reader.load_data([\"http://www.google.com\"]))\n", "doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "__type__": "Document"}, "55400c6722fc856cbb287cec4987d87118429fb1": {"text": "\"\"\"Simple reader that reads wikipedia.\"\"\"\nfrom typing import Any, List\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass WikipediaReader(BaseReader):\n    \"\"\"Wikipedia reader.\n\n    Reads a page.\n\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            import wikipedia  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`wikipedia` package not found, please run `pip install wikipedia`\"\n            )\n\n    def load_data(self, pages: List[str], **load_kwargs: Any) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            pages (List[str]): List of pages to read.\n\n        \"\"\"\n        import wikipedia\n\n        results = []\n        for page in pages:\n            page_content = wikipedia.page(page, **load_kwargs).content\n            results.append(Document(page_content))\n        return results\n", "doc_id": "55400c6722fc856cbb287cec4987d87118429fb1", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/wikipedia.py", "file_name": "wikipedia.py"}, "__type__": "Document"}, "969ca38b79231bd668d28e6cdc0cbc59a2f3745d": {"text": "\"\"\"Simple Reader that reads transcript of youtube video.\"\"\"\nfrom typing import Any, List\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass YoutubeTranscriptReader(BaseReader):\n    \"\"\"Youtube Transcript reader.\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n\n    def load_data(self, ytlinks: List[str], **load_kwargs: Any) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            pages (List[str]): List of youtube links \\\n                for which transcripts are to be read.\n\n        \"\"\"\n        try:\n            from youtube_transcript_api import YouTubeTranscriptApi\n        except ImportError:\n            raise ValueError(\n                \"`youtube_transcript_api` package not found, \\\n                    please run `pip install youtube-transcript-api`\"\n            )\n\n        results = []\n        for link in ytlinks:\n            video_id = link.split(\"?v=\")[-1]\n            srt = YouTubeTranscriptApi.get_transcript(video_id)\n            transcript = \"\"\n            for chunk in srt:\n                transcript = transcript + chunk[\"text\"] + \"\\n\"\n            results.append(Document(transcript))\n        return results\n", "doc_id": "969ca38b79231bd668d28e6cdc0cbc59a2f3745d", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/youtube_transcript.py", "file_name": "youtube_transcript.py"}, "__type__": "Document"}, "872e0fc1-91a0-4060-8af0-44375c8cc803": {"text": "\nThe summaries of the documents are descriptions or summaries of the functions they contain. The Slack.py document contains a reader for Slack channels, the String_iterable.py document contains a reader for an iterable of strings, the Twitter.py document contains a reader for tweets of a twitter handle, the Web.py document contains a web scraper, the gpt_index/readers/web.py file contains a SimpleWebPageReader class, the gpt_index/readers/wikipedia.py file contains a WikipediaReader class, the gpt_index/readers/youtube_transcript.py file contains a YoutubeTranscriptReader class, the discord_reader.py file contains a DiscordReader class, the download.py file contains a function that downloads a single loader from the Loader Hub, the faiss.py file contains a class that retrieves documents through an existing in-memory Faiss index, the mbox.py file contains a class that reads a set of emails saved in the mbox format, the mongo.py file contains a class that reads documents from a Mongo database, the notion.py file contains functions for reading and querying Notion pages, the obsidian.py file contains functions", "doc_id": "872e0fc1-91a0-4060-8af0-44375c8cc803", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Data Connectors for GPT Index.\n\nThis module contains the data connectors for GPT Index. Each connector inherits\nfrom a `BaseReader` class, connects to a data source, and loads Document objects\nfrom that data source.\n\nYou may also choose to construct Document objects manually, for instance\nin our `Insert How-To Guide <../how_to/insert.html>`_. See below for the API\ndefinition of a Document - the bare minimum is a `text` property.\n\n\"\"\"\n\nfrom gpt_index.readers.discord_reader import DiscordReader\nfrom gpt_index.readers.faiss import FaissReader\n\n# readers\nfrom gpt_index.readers.file.base import SimpleDirectoryReader\nfrom gpt_index.readers.github_readers.github_repository_reader import (\n    GithubRepositoryReader,\n)\nfrom gpt_index.readers.google_readers.gdocs import GoogleDocsReader\nfrom gpt_index.readers.make_com.wrapper import MakeWrapper\nfrom gpt_index.readers.mbox import MboxReader\nfrom gpt_index.readers.mongo import", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "846471c13398def5f566e12afc3117ddc30b9c88", "node_info": null}, "1": {"text": "MboxReader\nfrom gpt_index.readers.mongo import SimpleMongoReader\nfrom gpt_index.readers.notion import NotionPageReader\nfrom gpt_index.readers.obsidian import ObsidianReader\nfrom gpt_index.readers.pinecone import PineconeReader\nfrom gpt_index.readers.qdrant import QdrantReader\nfrom gpt_index.readers.schema.base import Document\nfrom gpt_index.readers.slack import SlackReader\nfrom gpt_index.readers.string_iterable import StringIterableReader\nfrom gpt_index.readers.twitter import TwitterTweetReader\nfrom gpt_index.readers.weaviate.reader import WeaviateReader\nfrom gpt_index.readers.web import (\n    BeautifulSoupWebReader,\n    RssReader,\n    SimpleWebPageReader,\n    TrafilaturaWebReader,\n)\nfrom gpt_index.readers.wikipedia import WikipediaReader\nfrom gpt_index.readers.youtube_transcript import YoutubeTranscriptReader\n\n__all__ = [\n    \"WikipediaReader\",\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/__init__.py", "file_name": "__init__.py"}, "index": 1, "child_indices": [], "ref_doc_id": "846471c13398def5f566e12afc3117ddc30b9c88", "node_info": null}, "2": {"text": "= [\n    \"WikipediaReader\",\n    \"YoutubeTranscriptReader\",\n    \"SimpleDirectoryReader\",\n    \"SimpleMongoReader\",\n    \"NotionPageReader\",\n    \"GoogleDocsReader\",\n    \"DiscordReader\",\n    \"SlackReader\",\n    \"WeaviateReader\",\n    \"PineconeReader\",\n    \"QdrantReader\",\n    \"FaissReader\",\n    \"Document\",\n    \"StringIterableReader\",\n    \"SimpleWebPageReader\",\n    \"BeautifulSoupWebReader\",\n    \"TrafilaturaWebReader\",\n    \"RssReader\",\n    \"MakeWrapper\",\n    \"TwitterTweetReader\",\n    \"ObsidianReader\",\n    \"GithubRepositoryReader\",\n    \"MboxReader\",\n]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/__init__.py", "file_name": "__init__.py"}, "index": 2, "child_indices": [], "ref_doc_id": "846471c13398def5f566e12afc3117ddc30b9c88", "node_info": null}, "3": {"text": "\"\"\"Base reader class.\"\"\"\nfrom abc import abstractmethod\nfrom typing import Any, List\n\nfrom langchain.docstore.document import Document as LCDocument\n\nfrom gpt_index.readers.schema.base import Document\n\n\nclass BaseReader:\n    \"\"\"Utilities for loading data from a directory.\"\"\"\n\n    @abstractmethod\n    def load_data(self, *args: Any, **load_kwargs: Any) -> List[Document]:\n        \"\"\"Load data from the input directory.\"\"\"\n\n    def load_langchain_documents(self, **load_kwargs: Any) -> List[LCDocument]:\n        \"\"\"Load data in LangChain document format.\"\"\"\n        docs = self.load_data(**load_kwargs)\n        return [d.to_langchain_format() for d in docs]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/base.py", "file_name": "base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "f5cc30de3365fe4a08a28f1e95a9bdd2b17e09a0", "node_info": null}, "4": {"text": "\"\"\"Database Reader.\"\"\"\n\nfrom typing import Any, List, Optional\n\nfrom sqlalchemy import text\nfrom sqlalchemy.engine import Engine\n\nfrom gpt_index.langchain_helpers.sql_wrapper import SQLDatabase\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass DatabaseReader(BaseReader):\n    \"\"\"Simple Database reader.\n\n    Concatenates each row into Document used by GPT Index.\n\n    Args:\n        sql_database (Optional[SQLDatabase]): SQL database to use,\n            including table names to specify.\n            See :ref:`Ref-Struct-Store` for more details.\n\n        OR\n\n        engine (Optional[Engine]): SQLAlchemy Engine object of the database connection.\n\n        OR\n\n        uri (Optional[str]): uri of the database connection.\n\n        OR\n\n        scheme (Optional[str]): scheme of the database", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/database.py", "file_name": "database.py"}, "index": 4, "child_indices": [], "ref_doc_id": "4098c3e4bb1452ddb95150e467c23f0d503b34e1", "node_info": null}, "5": {"text": "   scheme (Optional[str]): scheme of the database connection.\n        host (Optional[str]): host of the database connection.\n        port (Optional[int]): port of the database connection.\n        user (Optional[str]): user of the database connection.\n        password (Optional[str]): password of the database connection.\n        dbname (Optional[str]): dbname of the database connection.\n\n    Returns:\n        DatabaseReader: A DatabaseReader object.\n    \"\"\"\n\n    def __init__(\n        self,\n        sql_database: Optional[SQLDatabase] = None,\n        engine: Optional[Engine] = None,\n        uri: Optional[str] = None,\n        scheme: Optional[str] = None,\n        host: Optional[str] = None,\n        port: Optional[str] = None,\n        user:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/database.py", "file_name": "database.py"}, "index": 5, "child_indices": [], "ref_doc_id": "4098c3e4bb1452ddb95150e467c23f0d503b34e1", "node_info": null}, "6": {"text": "= None,\n        user: Optional[str] = None,\n        password: Optional[str] = None,\n        dbname: Optional[str] = None,\n        *args: Optional[Any],\n        **kwargs: Optional[Any],\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        if sql_database:\n            self.sql_database = sql_database\n        elif engine:\n            self.sql_database = SQLDatabase(engine, *args, **kwargs)\n        elif uri:\n            self.uri = uri\n            self.sql_database = SQLDatabase.from_uri(uri, *args, **kwargs)\n        elif scheme and host and port and user and password and dbname:\n            uri =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/database.py", "file_name": "database.py"}, "index": 6, "child_indices": [], "ref_doc_id": "4098c3e4bb1452ddb95150e467c23f0d503b34e1", "node_info": null}, "7": {"text": "           uri = f\"{scheme}://{user}:{password}@{host}:{port}/{dbname}\"\n            self.uri = uri\n            self.sql_database = SQLDatabase.from_uri(uri, *args, **kwargs)\n        else:\n            raise ValueError(\n                \"You must provide either a SQLDatabase, \"\n                \"a SQL Alchemy Engine, a valid connection URI, or a valid \"\n                \"set of credentials.\"\n            )\n\n    def load_data(self, query: str) -> List[Document]:\n        \"\"\"Query and load data from the Database, returning a list of Documents.\n\n        Args:\n            query (str): Query parameter to filter tables and", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/database.py", "file_name": "database.py"}, "index": 7, "child_indices": [], "ref_doc_id": "4098c3e4bb1452ddb95150e467c23f0d503b34e1", "node_info": null}, "8": {"text": "    query (str): Query parameter to filter tables and rows.\n\n        Returns:\n            List[Document]: A list of Document objects.\n        \"\"\"\n        documents = []\n        with self.sql_database.engine.connect() as connection:\n            if query is None:\n                raise ValueError(\"A query parameter is necessary to filter the data\")\n            else:\n                result = connection.execute(text(query))\n\n            for item in result.fetchall():\n                documents.append(Document(item[0]))\n        return documents\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/database.py", "file_name": "database.py"}, "index": 8, "child_indices": [], "ref_doc_id": "4098c3e4bb1452ddb95150e467c23f0d503b34e1", "node_info": null}, "9": {"text": "\"\"\"Discord reader.\n\nNote: this file is named discord_reader.py to avoid conflicts with the\ndiscord.py module.\n\n\"\"\"\n\nimport asyncio\nimport logging\nimport os\nfrom typing import List, Optional\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nasync def read_channel(\n    discord_token: str, channel_id: int, limit: Optional[int], oldest_first: bool\n) -> str:\n    \"\"\"Async read channel.\n\n    Note: This is our hack to create a synchronous interface to the\n    async discord.py API. We use the `asyncio` module to run\n    this function with `asyncio.get_event_loop().run_until_complete`.\n\n    \"\"\"\n    import discord  # noqa: F401\n\n    messages: List[discord.Message] = []\n\n    class CustomClient(discord.Client):\n        async def on_ready(self) -> None:\n          ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/discord_reader.py", "file_name": "discord_reader.py"}, "index": 9, "child_indices": [], "ref_doc_id": "8d9a229a3248b1617b67774eac291a2594ec686e", "node_info": null}, "10": {"text": "-> None:\n            try:\n                logging.info(f\"{self.user} has connected to Discord!\")\n                channel = client.get_channel(channel_id)\n                # only work for text channels for now\n                if not isinstance(channel, discord.TextChannel):\n                    raise ValueError(\n                        f\"Channel {channel_id} is not a text channel. \"\n                        \"Only text channels are supported for now.\"\n                    )\n                # thread_dict maps thread_id to thread\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/discord_reader.py", "file_name": "discord_reader.py"}, "index": 10, "child_indices": [], "ref_doc_id": "8d9a229a3248b1617b67774eac291a2594ec686e", "node_info": null}, "11": {"text": "# thread_dict maps thread_id to thread\n                thread_dict = {}\n                for thread in channel.threads:\n                    thread_dict[thread.id] = thread\n\n                async for msg in channel.history(\n                    limit=limit, oldest_first=oldest_first\n                ):\n                    messages.append(msg)\n                    if msg.id in thread_dict:\n                        thread = thread_dict[msg.id]\n                        async for thread_msg in", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/discord_reader.py", "file_name": "discord_reader.py"}, "index": 11, "child_indices": [], "ref_doc_id": "8d9a229a3248b1617b67774eac291a2594ec686e", "node_info": null}, "12": {"text": "         async for thread_msg in thread.history(\n                            limit=limit, oldest_first=oldest_first\n                        ):\n                            messages.append(thread_msg)\n            except Exception as e:\n                logging.error(\"Encountered error: \" + str(e))\n            finally:\n                await self.close()\n\n    intents = discord.Intents.default()\n    intents.message_content = True\n    client = CustomClient(intents=intents)\n    await client.start(discord_token)\n\n    msg_txt_list = [m.content for m in", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/discord_reader.py", "file_name": "discord_reader.py"}, "index": 12, "child_indices": [], "ref_doc_id": "8d9a229a3248b1617b67774eac291a2594ec686e", "node_info": null}, "13": {"text": "   msg_txt_list = [m.content for m in messages]\n\n    return \"\\n\\n\".join(msg_txt_list)\n\n\nclass DiscordReader(BaseReader):\n    \"\"\"Discord reader.\n\n    Reads conversations from channels.\n\n    Args:\n        discord_token (Optional[str]): Discord token. If not provided, we\n            assume the environment variable `DISCORD_TOKEN` is set.\n\n    \"\"\"\n\n    def __init__(self, discord_token: Optional[str] = None) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            import discord  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`discord.py` package not found, please run `pip install discord.py`\"\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/discord_reader.py", "file_name": "discord_reader.py"}, "index": 13, "child_indices": [], "ref_doc_id": "8d9a229a3248b1617b67774eac291a2594ec686e", "node_info": null}, "14": {"text": "install discord.py`\"\n            )\n        if discord_token is None:\n            discord_token = os.environ[\"DISCORD_TOKEN\"]\n            if discord_token is None:\n                raise ValueError(\n                    \"Must specify `discord_token` or set environment \"\n                    \"variable `DISCORD_TOKEN`.\"\n                )\n\n        self.discord_token = discord_token\n\n    def _read_channel(\n        self, channel_id: int, limit: Optional[int] = None, oldest_first: bool = True\n    ) -> str:\n        \"\"\"Read channel.\"\"\"\n        result =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/discord_reader.py", "file_name": "discord_reader.py"}, "index": 14, "child_indices": [], "ref_doc_id": "8d9a229a3248b1617b67774eac291a2594ec686e", "node_info": null}, "15": {"text": " \"\"\"Read channel.\"\"\"\n        result = asyncio.get_event_loop().run_until_complete(\n            read_channel(\n                self.discord_token, channel_id, limit=limit, oldest_first=oldest_first\n            )\n        )\n        return result\n\n    def load_data(\n        self,\n        channel_ids: List[int],\n        limit: Optional[int] = None,\n        oldest_first: bool = True,\n    ) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            channel_ids (List[int]): List of channel ids to read.\n            limit (Optional[int]): Maximum number of messages to read.\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/discord_reader.py", "file_name": "discord_reader.py"}, "index": 15, "child_indices": [], "ref_doc_id": "8d9a229a3248b1617b67774eac291a2594ec686e", "node_info": null}, "16": {"text": "Maximum number of messages to read.\n            oldest_first (bool): Whether to read oldest messages first.\n                Defaults to `True`.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        results: List[Document] = []\n        for channel_id in channel_ids:\n            if not isinstance(channel_id, int):\n                raise ValueError(\n                    f\"Channel id {channel_id} must be an integer, \"\n                    f\"not {type(channel_id)}.\"\n                )\n            channel_content = self._read_channel(\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/discord_reader.py", "file_name": "discord_reader.py"}, "index": 16, "child_indices": [], "ref_doc_id": "8d9a229a3248b1617b67774eac291a2594ec686e", "node_info": null}, "17": {"text": " channel_content = self._read_channel(\n                channel_id, limit=limit, oldest_first=oldest_first\n            )\n            results.append(\n                Document(channel_content, extra_info={\"channel\": channel_id})\n            )\n        return results\n\n\nif __name__ == \"__main__\":\n    reader = DiscordReader()\n    logging.info(\"initialized reader\")\n    output = reader.load_data(channel_ids=[1057178784895348746], limit=10)\n    logging.info(output)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/discord_reader.py", "file_name": "discord_reader.py"}, "index": 17, "child_indices": [], "ref_doc_id": "8d9a229a3248b1617b67774eac291a2594ec686e", "node_info": null}, "18": {"text": "\"\"\"Download loader from the Loader Hub.\"\"\"\n\nimport json\nimport os\nimport subprocess\nimport sys\nfrom importlib import util\nfrom pathlib import Path\n\nimport pkg_resources\nimport requests\nfrom pkg_resources import DistributionNotFound\n\nfrom gpt_index.readers.base import BaseReader\n\nLOADER_HUB_URL = (\n    \"https://raw.githubusercontent.com/emptycrown/loader-hub/main/loader_hub\"\n)\n\n\ndef download_loader(loader_class: str) -> BaseReader:\n    \"\"\"Download a single loader from the Loader Hub.\n\n    Args:\n        loader_class: The name of the loader class you want to download,\n            such as `SimpleWebPageReader`.\n    Returns:\n        A Loader.\n    \"\"\"\n    response = requests.get(f\"{LOADER_HUB_URL}/library.json\")\n    library = json.loads(response.text)\n\n    # Look up the loader id (e.g. `web/simple_web`)\n    loader_id =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/download.py", "file_name": "download.py"}, "index": 18, "child_indices": [], "ref_doc_id": "71585e29180e9ae11f0774b391bf9fd877e5ac54", "node_info": null}, "19": {"text": "`web/simple_web`)\n    loader_id = library[loader_class][\"id\"]\n    dirpath = \".modules\"\n    loader_filename = loader_id.replace(\"/\", \"-\")\n    loader_path = f\"{dirpath}/{loader_filename}.py\"\n    requirements_path = f\"{dirpath}/{loader_filename}_requirements.txt\"\n\n    if not os.path.exists(dirpath):\n        # Create a new directory because it does not exist\n        os.makedirs(dirpath)\n\n    if not os.path.exists(loader_path):\n        response = requests.get(f\"{LOADER_HUB_URL}/{loader_id}/base.py\")\n        with open(loader_path, \"w\") as f:\n            f.write(response.text)\n\n    if not os.path.exists(requirements_path):\n        response =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/download.py", "file_name": "download.py"}, "index": 19, "child_indices": [], "ref_doc_id": "71585e29180e9ae11f0774b391bf9fd877e5ac54", "node_info": null}, "20": {"text": "       response = requests.get(f\"{LOADER_HUB_URL}/{loader_id}/requirements.txt\")\n        if response.status_code == 200:\n            with open(requirements_path, \"w\") as f:\n                f.write(response.text)\n\n    # Install dependencies if there are any and not already installed\n    if os.path.exists(requirements_path):\n        try:\n            requirements = pkg_resources.parse_requirements(\n                Path(requirements_path).open()\n            )\n            pkg_resources.require([str(r) for r in requirements])\n        except DistributionNotFound:\n            subprocess.check_call(\n               ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/download.py", "file_name": "download.py"}, "index": 20, "child_indices": [], "ref_doc_id": "71585e29180e9ae11f0774b391bf9fd877e5ac54", "node_info": null}, "21": {"text": "               [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", requirements_path]\n            )\n\n    spec = util.spec_from_file_location(\"custom_loader\", location=loader_path)\n    if spec is None:\n        raise ValueError(f\"Could not find file: {loader_path}.\")\n    module = util.module_from_spec(spec)\n    spec.loader.exec_module(module)  # type: ignore\n\n    return getattr(module, loader_class)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/download.py", "file_name": "download.py"}, "index": 21, "child_indices": [], "ref_doc_id": "71585e29180e9ae11f0774b391bf9fd877e5ac54", "node_info": null}, "22": {"text": "\"\"\"Faiss reader.\"\"\"\n\nfrom typing import Any, Dict, List\n\nimport numpy as np\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass FaissReader(BaseReader):\n    \"\"\"Faiss reader.\n\n    Retrieves documents through an existing in-memory Faiss index.\n    These documents can then be used in a downstream GPT Index data structure.\n    If you wish use Faiss itself as an index to to organize documents,\n    insert documents, and perform queries on them, please use GPTFaissIndex.\n\n    Args:\n        faiss_index (faiss.Index): A Faiss Index object (required)\n\n    \"\"\"\n\n    def __init__(self, index: Any):\n        \"\"\"Initialize with parameters.\"\"\"\n        import_err_msg = \"\"\"\n            `faiss` package not found. For instructions on\n            how to install", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/faiss.py", "file_name": "faiss.py"}, "index": 22, "child_indices": [], "ref_doc_id": "82f076eb7558c8648d1b13d5ccec126b5cb6a5f3", "node_info": null}, "23": {"text": "on\n            how to install `faiss` please visit\n            https://github.com/facebookresearch/faiss/wiki/Installing-Faiss\n        \"\"\"\n        try:\n            import faiss  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n\n        self._index = index\n\n    def load_data(\n        self,\n        query: np.ndarray,\n        id_to_text_map: Dict[str, str],\n        k: int = 4,\n        separate_documents: bool = True,\n    ) -> List[Document]:\n        \"\"\"Load data from Faiss.\n\n        Args:\n            query (np.ndarray): A", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/faiss.py", "file_name": "faiss.py"}, "index": 23, "child_indices": [], "ref_doc_id": "82f076eb7558c8648d1b13d5ccec126b5cb6a5f3", "node_info": null}, "24": {"text": "        query (np.ndarray): A 2D numpy array of query vectors.\n            id_to_text_map (Dict[str, str]): A map from ID's to text.\n            k (int): Number of nearest neighbors to retrieve. Defaults to 4.\n            separate_documents (Optional[bool]): Whether to return separate\n                documents. Defaults to True.\n        Returns:\n            List[Document]: A list of documents.\n\n        \"\"\"\n        dists, indices = self._index.search(query, k)\n        documents = []\n        for qidx in range(indices.shape[0]):\n            for didx in range(indices.shape[1]):\n              ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/faiss.py", "file_name": "faiss.py"}, "index": 24, "child_indices": [], "ref_doc_id": "82f076eb7558c8648d1b13d5ccec126b5cb6a5f3", "node_info": null}, "25": {"text": "               doc_id = indices[qidx, didx]\n                if doc_id not in id_to_text_map:\n                    raise ValueError(\n                        f\"Document ID {doc_id} not found in id_to_text_map.\"\n                    )\n                text = id_to_text_map[doc_id]\n                documents.append(Document(text=text))\n\n        if not separate_documents:\n            # join all documents into one\n            text_list = [doc.get_text() for doc in documents]\n            text =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/faiss.py", "file_name": "faiss.py"}, "index": 25, "child_indices": [], "ref_doc_id": "82f076eb7558c8648d1b13d5ccec126b5cb6a5f3", "node_info": null}, "26": {"text": "           text = \"\\n\\n\".join(text_list)\n            documents = [Document(text=text)]\n\n        return documents\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/faiss.py", "file_name": "faiss.py"}, "index": 26, "child_indices": [], "ref_doc_id": "82f076eb7558c8648d1b13d5ccec126b5cb6a5f3", "node_info": null}, "27": {"text": "\"\"\"Simple reader for mbox (mailbox) files.\"\"\"\nimport os\nfrom pathlib import Path\nfrom typing import Any, List\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.file.mbox_parser import MboxParser\nfrom gpt_index.readers.schema.base import Document\n\n\nclass MboxReader(BaseReader):\n    \"\"\"Mbox e-mail reader.\n\n    Reads a set of e-mails saved in the mbox format.\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize.\"\"\"\n\n    def load_data(self, input_dir: str, **load_kwargs: Any) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        load_kwargs:\n            max_count (int): Maximum amount of messages to read.\n            message_format (str): Message format overriding default.\n        \"\"\"\n        docs:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/mbox.py", "file_name": "mbox.py"}, "index": 27, "child_indices": [], "ref_doc_id": "b8ba0526790bd62d1a41d878474809af816f616e", "node_info": null}, "28": {"text": "     \"\"\"\n        docs: List[Document] = []\n        for (dirpath, dirnames, filenames) in os.walk(input_dir):\n            dirnames[:] = [d for d in dirnames if not d.startswith(\".\")]\n            for filename in filenames:\n                if filename.endswith(\".mbox\"):\n                    filepath = os.path.join(dirpath, filename)\n                    content = MboxParser(**load_kwargs).parse_file(Path(filepath))\n                    for msg in content:\n                        docs.append(Document(msg))\n        return docs\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/mbox.py", "file_name": "mbox.py"}, "index": 28, "child_indices": [], "ref_doc_id": "b8ba0526790bd62d1a41d878474809af816f616e", "node_info": null}, "29": {"text": "\"\"\"Mongo client.\"\"\"\n\nfrom typing import Dict, List, Optional\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass SimpleMongoReader(BaseReader):\n    \"\"\"Simple mongo reader.\n\n    Concatenates each Mongo doc into Document used by GPT Index.\n\n    Args:\n        host (str): Mongo host.\n        port (int): Mongo port.\n        max_docs (int): Maximum number of documents to load.\n\n    \"\"\"\n\n    def __init__(self, host: str, port: int, max_docs: int = 1000) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            import pymongo  # noqa: F401\n            from pymongo import MongoClient  # noqa: F401\n        except ImportError:\n            raise", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/mongo.py", "file_name": "mongo.py"}, "index": 29, "child_indices": [], "ref_doc_id": "2bc13de0428fd82ed9d665fc257405cf2de8929a", "node_info": null}, "30": {"text": "ImportError:\n            raise ValueError(\n                \"`pymongo` package not found, please run `pip install pymongo`\"\n            )\n        self.client: MongoClient = MongoClient(host, port)\n        self.max_docs = max_docs\n\n    def load_data(\n        self, db_name: str, collection_name: str, query_dict: Optional[Dict] = None\n    ) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            db_name (str): name of the database.\n            collection_name (str): name of the collection.\n            query_dict (Optional[Dict]): query to filter documents.\n                Defaults to None\n\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/mongo.py", "file_name": "mongo.py"}, "index": 30, "child_indices": [], "ref_doc_id": "2bc13de0428fd82ed9d665fc257405cf2de8929a", "node_info": null}, "31": {"text": "       Defaults to None\n\n        Returns:\n            List[Document]: A list of documents.\n\n        \"\"\"\n        documents = []\n        db = self.client[db_name]\n        if query_dict is None:\n            cursor = db[collection_name].find()\n        else:\n            cursor = db[collection_name].find(query_dict)\n\n        for item in cursor:\n            if \"text\" not in item:\n                raise ValueError(\"`text` field not found in Mongo document.\")\n            documents.append(Document(item[\"text\"]))\n        return documents\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/mongo.py", "file_name": "mongo.py"}, "index": 31, "child_indices": [], "ref_doc_id": "2bc13de0428fd82ed9d665fc257405cf2de8929a", "node_info": null}, "32": {"text": "\"\"\"Notion reader.\"\"\"\nimport logging\nimport os\nfrom typing import Any, Dict, List, Optional\n\nimport requests  # type: ignore\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\nINTEGRATION_TOKEN_NAME = \"NOTION_INTEGRATION_TOKEN\"\nBLOCK_CHILD_URL_TMPL = \"https://api.notion.com/v1/blocks/{block_id}/children\"\nDATABASE_URL_TMPL = \"https://api.notion.com/v1/databases/{database_id}/query\"\nSEARCH_URL = \"https://api.notion.com/v1/search\"\n\n\n# TODO: Notion DB reader coming soon!\nclass NotionPageReader(BaseReader):\n    \"\"\"Notion Page reader.\n\n    Reads a set of Notion pages.\n\n    Args:\n        integration_token (str): Notion integration token.\n\n    \"\"\"\n\n    def __init__(self, integration_token: Optional[str] = None) -> None:\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/notion.py", "file_name": "notion.py"}, "index": 32, "child_indices": [], "ref_doc_id": "c7ee9e2bf5f208fdcdcdf35eaf126ee0abbefe83", "node_info": null}, "33": {"text": "integration_token: Optional[str] = None) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        if integration_token is None:\n            integration_token = os.getenv(INTEGRATION_TOKEN_NAME)\n            if integration_token is None:\n                raise ValueError(\n                    \"Must specify `integration_token` or set environment \"\n                    \"variable `NOTION_INTEGRATION_TOKEN`.\"\n                )\n        self.token = integration_token\n        self.headers = {\n            \"Authorization\": \"Bearer \" + self.token,\n            \"Content-Type\": \"application/json\",\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/notion.py", "file_name": "notion.py"}, "index": 33, "child_indices": [], "ref_doc_id": "c7ee9e2bf5f208fdcdcdf35eaf126ee0abbefe83", "node_info": null}, "34": {"text": "\"application/json\",\n            \"Notion-Version\": \"2022-06-28\",\n        }\n\n    def _read_block(self, block_id: str, num_tabs: int = 0) -> str:\n        \"\"\"Read a block.\"\"\"\n        done = False\n        result_lines_arr = []\n        cur_block_id = block_id\n        while not done:\n            block_url = BLOCK_CHILD_URL_TMPL.format(block_id=cur_block_id)\n            query_dict: Dict[str, Any] = {}\n\n            res = requests.request(\n                \"GET\", block_url, headers=self.headers, json=query_dict\n            )\n            data =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/notion.py", "file_name": "notion.py"}, "index": 34, "child_indices": [], "ref_doc_id": "c7ee9e2bf5f208fdcdcdf35eaf126ee0abbefe83", "node_info": null}, "35": {"text": " )\n            data = res.json()\n\n            for result in data[\"results\"]:\n                result_type = result[\"type\"]\n                result_obj = result[result_type]\n\n                cur_result_text_arr = []\n                if \"rich_text\" in result_obj:\n                    for rich_text in result_obj[\"rich_text\"]:\n                        # skip if doesn't have text object\n                        if \"text\" in rich_text:\n                            text = rich_text[\"text\"][\"content\"]\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/notion.py", "file_name": "notion.py"}, "index": 35, "child_indices": [], "ref_doc_id": "c7ee9e2bf5f208fdcdcdf35eaf126ee0abbefe83", "node_info": null}, "36": {"text": "                           prefix = \"\\t\" * num_tabs\n                            cur_result_text_arr.append(prefix + text)\n\n                result_block_id = result[\"id\"]\n                has_children = result[\"has_children\"]\n                if has_children:\n                    children_text = self._read_block(\n                        result_block_id, num_tabs=num_tabs + 1\n                    )\n                    cur_result_text_arr.append(children_text)\n\n        ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/notion.py", "file_name": "notion.py"}, "index": 36, "child_indices": [], "ref_doc_id": "c7ee9e2bf5f208fdcdcdf35eaf126ee0abbefe83", "node_info": null}, "37": {"text": "               cur_result_text = \"\\n\".join(cur_result_text_arr)\n                result_lines_arr.append(cur_result_text)\n\n            if data[\"next_cursor\"] is None:\n                done = True\n                break\n            else:\n                cur_block_id = data[\"next_cursor\"]\n\n        result_lines = \"\\n\".join(result_lines_arr)\n        return result_lines\n\n    def read_page(self, page_id: str) -> str:\n        \"\"\"Read a page.\"\"\"\n        return self._read_block(page_id)\n\n    def query_database(\n        self, database_id: str, query_dict:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/notion.py", "file_name": "notion.py"}, "index": 37, "child_indices": [], "ref_doc_id": "c7ee9e2bf5f208fdcdcdf35eaf126ee0abbefe83", "node_info": null}, "38": {"text": "    self, database_id: str, query_dict: Dict[str, Any] = {}\n    ) -> List[str]:\n        \"\"\"Get all the pages from a Notion database.\"\"\"\n        res = requests.post(\n            DATABASE_URL_TMPL.format(database_id=database_id),\n            headers=self.headers,\n            json=query_dict,\n        )\n        data = res.json()\n        page_ids = []\n        for result in data[\"results\"]:\n            page_id = result[\"id\"]\n            page_ids.append(page_id)\n\n        return page_ids\n\n    def search(self, query: str) -> List[str]:\n        \"\"\"Search Notion page given a text query.\"\"\"\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/notion.py", "file_name": "notion.py"}, "index": 38, "child_indices": [], "ref_doc_id": "c7ee9e2bf5f208fdcdcdf35eaf126ee0abbefe83", "node_info": null}, "39": {"text": "Notion page given a text query.\"\"\"\n        done = False\n        next_cursor: Optional[str] = None\n        page_ids = []\n        while not done:\n            query_dict = {\n                \"query\": query,\n            }\n            if next_cursor is not None:\n                query_dict[\"start_cursor\"] = next_cursor\n            res = requests.post(SEARCH_URL, headers=self.headers, json=query_dict)\n            data = res.json()\n            for result in data[\"results\"]:\n                page_id = result[\"id\"]\n               ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/notion.py", "file_name": "notion.py"}, "index": 39, "child_indices": [], "ref_doc_id": "c7ee9e2bf5f208fdcdcdf35eaf126ee0abbefe83", "node_info": null}, "40": {"text": "               page_ids.append(page_id)\n\n            if data[\"next_cursor\"] is None:\n                done = True\n                break\n            else:\n                next_cursor = data[\"next_cursor\"]\n        return page_ids\n\n    def load_data(\n        self, page_ids: List[str] = [], database_id: Optional[str] = None\n    ) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            page_ids (List[str]): List of page ids to load.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/notion.py", "file_name": "notion.py"}, "index": 40, "child_indices": [], "ref_doc_id": "c7ee9e2bf5f208fdcdcdf35eaf126ee0abbefe83", "node_info": null}, "41": {"text": "       \"\"\"\n        if not page_ids and not database_id:\n            raise ValueError(\"Must specify either `page_ids` or `database_id`.\")\n        docs = []\n        if database_id is not None:\n            # get all the pages in the database\n            page_ids = self.query_database(database_id)\n            for page_id in page_ids:\n                page_text = self.read_page(page_id)\n                docs.append(Document(page_text, extra_info={\"page_id\": page_id}))\n        else:\n            for page_id in page_ids:\n                page_text = self.read_page(page_id)\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/notion.py", "file_name": "notion.py"}, "index": 41, "child_indices": [], "ref_doc_id": "c7ee9e2bf5f208fdcdcdf35eaf126ee0abbefe83", "node_info": null}, "42": {"text": "= self.read_page(page_id)\n                docs.append(Document(page_text, extra_info={\"page_id\": page_id}))\n\n        return docs\n\n\nif __name__ == \"__main__\":\n    reader = NotionPageReader()\n    logging.info(reader.search(\"What I\"))\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/notion.py", "file_name": "notion.py"}, "index": 42, "child_indices": [], "ref_doc_id": "c7ee9e2bf5f208fdcdcdf35eaf126ee0abbefe83", "node_info": null}, "43": {"text": "\"\"\"Obsidian reader class.\n\nPass in the path to an Obsidian vault and it will parse all markdown\nfiles into a List of Documents,\nwith each Document containing text from under an Obsidian header.\n\n\"\"\"\nimport os\nfrom pathlib import Path\nfrom typing import Any, List\n\nfrom langchain.docstore.document import Document as LCDocument\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.file.markdown_parser import MarkdownParser\nfrom gpt_index.readers.schema.base import Document\n\n\nclass ObsidianReader(BaseReader):\n    \"\"\"Utilities for loading data from an Obsidian Vault.\n\n    Args:\n        input_dir (str): Path to the vault.\n\n    \"\"\"\n\n    def __init__(self, input_dir: str):\n        \"\"\"Init params.\"\"\"\n        self.input_dir = Path(input_dir)\n\n    def load_data(self, *args: Any, **load_kwargs: Any) -> List[Document]:\n        \"\"\"Load data from the input directory.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/obsidian.py", "file_name": "obsidian.py"}, "index": 43, "child_indices": [], "ref_doc_id": "1404005433f2a8f5a4aaf2f0cb40dba48a0cd9e8", "node_info": null}, "44": {"text": "      \"\"\"Load data from the input directory.\"\"\"\n        docs: List[str] = []\n        for dirpath, dirnames, filenames in os.walk(self.input_dir):\n            dirnames[:] = [d for d in dirnames if not d.startswith(\".\")]\n            for filename in filenames:\n                if filename.endswith(\".md\"):\n                    filepath = os.path.join(dirpath, filename)\n                    content = MarkdownParser().parse_file(Path(filepath))\n                    docs.extend(content)\n        return [Document(d) for d in docs]\n\n    def load_langchain_documents(self, **load_kwargs: Any) -> List[LCDocument]:\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/obsidian.py", "file_name": "obsidian.py"}, "index": 44, "child_indices": [], "ref_doc_id": "1404005433f2a8f5a4aaf2f0cb40dba48a0cd9e8", "node_info": null}, "45": {"text": "Any) -> List[LCDocument]:\n        \"\"\"Load data in LangChain document format.\"\"\"\n        docs = self.load_data(**load_kwargs)\n        return [d.to_langchain_format() for d in docs]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/obsidian.py", "file_name": "obsidian.py"}, "index": 45, "child_indices": [], "ref_doc_id": "1404005433f2a8f5a4aaf2f0cb40dba48a0cd9e8", "node_info": null}, "46": {"text": "\"\"\"Pinecone reader.\"\"\"\n\nfrom typing import Any, Dict, List, Optional\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass PineconeReader(BaseReader):\n    \"\"\"Pinecone reader.\n\n    Args:\n        api_key (str): Pinecone API key.\n        environment (str): Pinecone environment.\n    \"\"\"\n\n    def __init__(self, api_key: str, environment: str):\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            import pinecone  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`pinecone` package not found, please run `pip install pinecone-client`\"\n            )\n\n        self._api_key = api_key\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/pinecone.py", "file_name": "pinecone.py"}, "index": 46, "child_indices": [], "ref_doc_id": "c121f325702354416b70e9e38021ce04ff265381", "node_info": null}, "47": {"text": "     self._api_key = api_key\n        self._environment = environment\n        pinecone.init(api_key=api_key, environment=environment)\n\n    def load_data(\n        self,\n        index_name: str,\n        id_to_text_map: Dict[str, str],\n        vector: Optional[List[float]],\n        top_k: int,\n        separate_documents: bool = True,\n        include_values: bool = True,\n        **query_kwargs: Any\n    ) -> List[Document]:\n        \"\"\"Load data from Pinecone.\n\n        Args:\n            index_name (str): Name of the index.\n            id_to_text_map (Dict[str, str]): A map from ID's to text.\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/pinecone.py", "file_name": "pinecone.py"}, "index": 47, "child_indices": [], "ref_doc_id": "c121f325702354416b70e9e38021ce04ff265381", "node_info": null}, "48": {"text": "str]): A map from ID's to text.\n            separate_documents (Optional[bool]): Whether to return separate\n                documents per retrieved entry. Defaults to True.\n            vector (List[float]): Query vector.\n            top_k (int): Number of results to return.\n            include_values (bool): Whether to include the embedding in the response.\n                Defaults to True.\n            **query_kwargs: Keyword arguments to pass to the query.\n                Arguments are the exact same as those found in\n                Pinecone's reference documentation for the\n                query method.\n\n        Returns:\n            List[Document]:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/pinecone.py", "file_name": "pinecone.py"}, "index": 48, "child_indices": [], "ref_doc_id": "c121f325702354416b70e9e38021ce04ff265381", "node_info": null}, "49": {"text": "           List[Document]: A list of documents.\n        \"\"\"\n        import pinecone\n\n        index = pinecone.Index(index_name)\n        if \"include_values\" not in query_kwargs:\n            query_kwargs[\"include_values\"] = True\n        response = index.query(top_k=top_k, vector=vector, **query_kwargs)\n\n        documents = []\n        for match in response.matches:\n            if match.id not in id_to_text_map:\n                raise ValueError(\"ID not found in id_to_text_map.\")\n            text = id_to_text_map[match.id]\n            embedding = match.values\n            if len(embedding) ==", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/pinecone.py", "file_name": "pinecone.py"}, "index": 49, "child_indices": [], "ref_doc_id": "c121f325702354416b70e9e38021ce04ff265381", "node_info": null}, "50": {"text": "         if len(embedding) == 0:\n                embedding = None\n            documents.append(Document(text=text, embedding=embedding))\n\n        if not separate_documents:\n            text_list = [doc.get_text() for doc in documents]\n            text = \"\\n\\n\".join(text_list)\n            documents = [Document(text=text)]\n\n        return documents\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/pinecone.py", "file_name": "pinecone.py"}, "index": 50, "child_indices": [], "ref_doc_id": "c121f325702354416b70e9e38021ce04ff265381", "node_info": null}, "51": {"text": "\"\"\"Qdrant reader.\"\"\"\n\nfrom typing import List, Optional, cast\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass QdrantReader(BaseReader):\n    \"\"\"Qdrant reader.\n\n    Retrieve documents from existing Qdrant collections.\n\n    Args:\n        host: Host name of Qdrant service.\n        port: Port of the REST API interface. Default: 6333\n        grpc_port: Port of the gRPC interface. Default: 6334\n        prefer_grpc: If `true` - use gPRC interface whenever possible in custom methods.\n        https: If `true` - use HTTPS(SSL) protocol. Default: `false`\n        api_key: API key for authentication in Qdrant Cloud. Default: `None`\n        prefix:\n            If not `None` - add `prefix` to the REST URL", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/qdrant.py", "file_name": "qdrant.py"}, "index": 51, "child_indices": [], "ref_doc_id": "5088ae6098fcc16c6105a3cbf9b6b1c7dac5ec53", "node_info": null}, "52": {"text": " If not `None` - add `prefix` to the REST URL path.\n            Example: `service/v1` will result in\n            `http://localhost:6333/service/v1/{qdrant-endpoint}` for REST API.\n            Default: `None`\n        timeout:\n            Timeout for REST and gRPC API requests.\n            Default: 5.0 seconds for REST and unlimited for gRPC\n    \"\"\"\n\n    def __init__(\n        self,\n        host: str,\n        port: int = 6333,\n        grpc_port: int = 6334,\n        prefer_grpc: bool = False,\n        https: Optional[bool] = None,\n        api_key: Optional[str] = None,\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/qdrant.py", "file_name": "qdrant.py"}, "index": 52, "child_indices": [], "ref_doc_id": "5088ae6098fcc16c6105a3cbf9b6b1c7dac5ec53", "node_info": null}, "53": {"text": "  api_key: Optional[str] = None,\n        prefix: Optional[str] = None,\n        timeout: Optional[float] = None,\n    ):\n        \"\"\"Initialize with parameters.\"\"\"\n        import_err_msg = (\n            \"`qdrant-client` package not found, please run `pip install qdrant-client`\"\n        )\n        try:\n            import qdrant_client  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n\n        self._client = qdrant_client.QdrantClient(\n            host=host,\n            port=port,\n            grpc_port=grpc_port,\n         ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/qdrant.py", "file_name": "qdrant.py"}, "index": 53, "child_indices": [], "ref_doc_id": "5088ae6098fcc16c6105a3cbf9b6b1c7dac5ec53", "node_info": null}, "54": {"text": "           prefer_grpc=prefer_grpc,\n            https=https,\n            api_key=api_key,\n            prefix=prefix,\n            timeout=timeout,\n        )\n\n    def load_data(\n        self,\n        collection_name: str,\n        query_vector: List[float],\n        limit: int = 10,\n    ) -> List[Document]:\n        \"\"\"Load data from Qdrant.\n\n        Args:\n            collection_name (str): Name of the Qdrant collection.\n            query_vector (List[float]): Query vector.\n            limit (int): Number of results to return.\n\n        Returns:\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/qdrant.py", "file_name": "qdrant.py"}, "index": 54, "child_indices": [], "ref_doc_id": "5088ae6098fcc16c6105a3cbf9b6b1c7dac5ec53", "node_info": null}, "55": {"text": "return.\n\n        Returns:\n            List[Document]: A list of documents.\n        \"\"\"\n        from qdrant_client.http.models.models import Payload\n\n        response = self._client.search(\n            collection_name=collection_name,\n            query_vector=query_vector,\n            with_vectors=True,\n            with_payload=True,\n            limit=limit,\n        )\n\n        documents = []\n        for point in response:\n            payload = cast(Payload, point)\n            try:\n                vector = cast(List[float], point.vector)\n          ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/qdrant.py", "file_name": "qdrant.py"}, "index": 55, "child_indices": [], "ref_doc_id": "5088ae6098fcc16c6105a3cbf9b6b1c7dac5ec53", "node_info": null}, "56": {"text": "point.vector)\n            except ValueError as e:\n                raise ValueError(\"Could not cast vector to List[float].\") from e\n            document = Document(\n                doc_id=payload.get(\"doc_id\"),\n                text=payload.get(\"text\"),\n                embedding=vector,\n            )\n            documents.append(document)\n\n        return documents\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/qdrant.py", "file_name": "qdrant.py"}, "index": 56, "child_indices": [], "ref_doc_id": "5088ae6098fcc16c6105a3cbf9b6b1c7dac5ec53", "node_info": null}, "57": {"text": "\"\"\"Slack reader.\"\"\"\nimport logging\nimport os\nimport time\nfrom typing import List, Optional\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass SlackReader(BaseReader):\n    \"\"\"Slack reader.\n\n    Reads conversations from channels.\n\n    Args:\n        slack_token (Optional[str]): Slack token. If not provided, we\n            assume the environment variable `SLACK_BOT_TOKEN` is set.\n\n    \"\"\"\n\n    def __init__(self, slack_token: Optional[str] = None) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            from slack_sdk import WebClient\n        except ImportError:\n            raise ValueError(\n                \"`slack_sdk` package not found, please run", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/slack.py", "file_name": "slack.py"}, "index": 57, "child_indices": [], "ref_doc_id": "a0db9d42d0b3dcf85ce915984e4159a8d57fd656", "node_info": null}, "58": {"text": "  \"`slack_sdk` package not found, please run `pip install slack_sdk`\"\n            )\n        if slack_token is None:\n            slack_token = os.environ[\"SLACK_BOT_TOKEN\"]\n            if slack_token is None:\n                raise ValueError(\n                    \"Must specify `slack_token` or set environment \"\n                    \"variable `SLACK_BOT_TOKEN`.\"\n                )\n        self.client = WebClient(token=slack_token)\n        res = self.client.api_test()\n        if not res[\"ok\"]:\n            raise ValueError(f\"Error initializing", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/slack.py", "file_name": "slack.py"}, "index": 58, "child_indices": [], "ref_doc_id": "a0db9d42d0b3dcf85ce915984e4159a8d57fd656", "node_info": null}, "59": {"text": "       raise ValueError(f\"Error initializing Slack API: {res['error']}\")\n\n    def _read_message(self, channel_id: str, message_ts: str) -> str:\n        from slack_sdk.errors import SlackApiError\n\n        \"\"\"Read a message.\"\"\"\n\n        messages_text = []\n        next_cursor = None\n        while True:\n            try:\n                # https://slack.com/api/conversations.replies\n                # List all replies to a message, including the message itself.\n                result = self.client.conversations_replies(\n                    channel=channel_id, ts=message_ts, cursor=next_cursor\n             ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/slack.py", "file_name": "slack.py"}, "index": 59, "child_indices": [], "ref_doc_id": "a0db9d42d0b3dcf85ce915984e4159a8d57fd656", "node_info": null}, "60": {"text": "               )\n                messages = result[\"messages\"]\n                for message in messages:\n                    messages_text.append(message[\"text\"])\n\n                if not result[\"has_more\"]:\n                    break\n\n                next_cursor = result[\"response_metadata\"][\"next_cursor\"]\n            except SlackApiError as e:\n                if e.response[\"error\"] == \"ratelimited\":\n                    logging.error(\n                        \"Rate limit error reached, sleeping for: {}", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/slack.py", "file_name": "slack.py"}, "index": 60, "child_indices": [], "ref_doc_id": "a0db9d42d0b3dcf85ce915984e4159a8d57fd656", "node_info": null}, "61": {"text": "   \"Rate limit error reached, sleeping for: {} seconds\".format(\n                            e.response.headers[\"retry-after\"]\n                        )\n                    )\n                    time.sleep(int(e.response.headers[\"retry-after\"]))\n                else:\n                    logging.error(\"Error parsing conversation replies: {}\".format(e))\n\n        return \"\\n\\n\".join(messages_text)\n\n    def _read_channel(self, channel_id: str) -> str:\n        from slack_sdk.errors import SlackApiError\n\n        \"\"\"Read a channel.\"\"\"\n\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/slack.py", "file_name": "slack.py"}, "index": 61, "child_indices": [], "ref_doc_id": "a0db9d42d0b3dcf85ce915984e4159a8d57fd656", "node_info": null}, "62": {"text": "    \"\"\"Read a channel.\"\"\"\n\n        result_messages = []\n        next_cursor = None\n        while True:\n            try:\n                # Call the conversations.history method using the WebClient\n                # conversations.history returns the first 100 messages by default\n                # These results are paginated,\n                # see: https://api.slack.com/methods/conversations.history$pagination\n                result = self.client.conversations_history(\n                    channel=channel_id, cursor=next_cursor\n                )\n                conversation_history =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/slack.py", "file_name": "slack.py"}, "index": 62, "child_indices": [], "ref_doc_id": "a0db9d42d0b3dcf85ce915984e4159a8d57fd656", "node_info": null}, "63": {"text": "          conversation_history = result[\"messages\"]\n                # Print results\n                logging.info(\n                    \"{} messages found in {}\".format(len(conversation_history), id)\n                )\n                for message in conversation_history:\n                    result_messages.append(\n                        self._read_message(channel_id, message[\"ts\"])\n                    )\n\n                if not result[\"has_more\"]:\n                    break\n         ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/slack.py", "file_name": "slack.py"}, "index": 63, "child_indices": [], "ref_doc_id": "a0db9d42d0b3dcf85ce915984e4159a8d57fd656", "node_info": null}, "64": {"text": "     break\n                next_cursor = result[\"response_metadata\"][\"next_cursor\"]\n\n            except SlackApiError as e:\n                if e.response[\"error\"] == \"ratelimited\":\n                    logging.error(\n                        \"Rate limit error reached, sleeping for: {} seconds\".format(\n                            e.response.headers[\"retry-after\"]\n                        )\n                    )\n                    time.sleep(int(e.response.headers[\"retry-after\"]))\n         ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/slack.py", "file_name": "slack.py"}, "index": 64, "child_indices": [], "ref_doc_id": "a0db9d42d0b3dcf85ce915984e4159a8d57fd656", "node_info": null}, "65": {"text": "               else:\n                    logging.error(\"Error parsing conversation replies: {}\".format(e))\n\n        return \"\\n\\n\".join(result_messages)\n\n    def load_data(self, channel_ids: List[str]) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            channel_ids (List[str]): List of channel ids to read.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        results = []\n        for channel_id in channel_ids:\n            channel_content = self._read_channel(channel_id)\n            results.append(\n               ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/slack.py", "file_name": "slack.py"}, "index": 65, "child_indices": [], "ref_doc_id": "a0db9d42d0b3dcf85ce915984e4159a8d57fd656", "node_info": null}, "66": {"text": "               Document(channel_content, extra_info={\"channel\": channel_id})\n            )\n        return results\n\n\nif __name__ == \"__main__\":\n    reader = SlackReader()\n    logging.info(reader.load_data(channel_ids=[\"C04DC2VUY3F\"]))\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/slack.py", "file_name": "slack.py"}, "index": 66, "child_indices": [], "ref_doc_id": "a0db9d42d0b3dcf85ce915984e4159a8d57fd656", "node_info": null}, "67": {"text": "\"\"\"Simple reader that turns an iterable of strings into a list of Documents.\"\"\"\nfrom typing import List\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass StringIterableReader(BaseReader):\n    \"\"\"String Iterable Reader.\n\n    Gets a list of documents, given an iterable (e.g. list) of strings.\n\n    Example:\n        .. code-block:: python\n\n            from gpt_index import StringIterableReader, GPTTreeIndex\n\n            documents = StringIterableReader().load_data(\n                texts=[\"I went to the store\", \"I bought an apple\"])\n            index = GPTTreeIndex(documents)\n            index.query(\"what did I buy?\")\n\n            # response should be something like \"You bought an apple.\"\n    \"\"\"\n\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/string_iterable.py", "file_name": "string_iterable.py"}, "index": 67, "child_indices": [], "ref_doc_id": "62876b8e16ae2625adf3c433bdd1b3eaa70150c3", "node_info": null}, "68": {"text": "something like \"You bought an apple.\"\n    \"\"\"\n\n    def load_data(self, texts: List[str]) -> List[Document]:\n        \"\"\"Load the data.\"\"\"\n        results = []\n        for text in texts:\n            results.append(Document(text))\n\n        return results\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/string_iterable.py", "file_name": "string_iterable.py"}, "index": 68, "child_indices": [], "ref_doc_id": "62876b8e16ae2625adf3c433bdd1b3eaa70150c3", "node_info": null}, "69": {"text": "\"\"\"Simple reader that reads tweets of a twitter handle.\"\"\"\nfrom typing import Any, List, Optional\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass TwitterTweetReader(BaseReader):\n    \"\"\"Twitter tweets reader.\n\n    Read tweets of user twitter handle.\n\n    Check 'https://developer.twitter.com/en/docs/twitter-api/\\\n        getting-started/getting-access-to-the-twitter-api' \\\n        on how to get access to twitter API.\n\n    Args:\n        bearer_token (str): bearer_token that you get from twitter API.\n        num_tweets (Optional[int]): Number of tweets for each user twitter handle.\\\n            Default is 100 tweets.\n    \"\"\"\n\n    def __init__(\n        self,\n        bearer_token: str,\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/twitter.py", "file_name": "twitter.py"}, "index": 69, "child_indices": [], "ref_doc_id": "ee0e8f78b27c1a62e466e13d35e9bbeea3b425e9", "node_info": null}, "70": {"text": " bearer_token: str,\n        num_tweets: Optional[int] = 100,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        self.bearer_token = bearer_token\n        self.num_tweets = num_tweets\n\n    def load_data(\n        self, twitterhandles: List[str], **load_kwargs: Any\n    ) -> List[Document]:\n        \"\"\"Load tweets of twitter handles.\n\n        Args:\n            twitterhandles (List[str]): List of user twitter handles to read tweets.\n\n        \"\"\"\n        try:\n            import tweepy\n        except ImportError:\n            raise ValueError(\n                \"`tweepy` package not found,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/twitter.py", "file_name": "twitter.py"}, "index": 70, "child_indices": [], "ref_doc_id": "ee0e8f78b27c1a62e466e13d35e9bbeea3b425e9", "node_info": null}, "71": {"text": "     \"`tweepy` package not found, please run `pip install tweepy`\"\n            )\n\n        client = tweepy.Client(bearer_token=self.bearer_token)\n        results = []\n        for username in twitterhandles:\n            # tweets = api.user_timeline(screen_name=user, count=self.num_tweets)\n            user = client.get_user(username=username)\n            tweets = client.get_users_tweets(user.data.id, max_results=self.num_tweets)\n            response = \" \"\n            for tweet in tweets.data:\n                response = response + tweet.text + \"\\n\"\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/twitter.py", "file_name": "twitter.py"}, "index": 71, "child_indices": [], "ref_doc_id": "ee0e8f78b27c1a62e466e13d35e9bbeea3b425e9", "node_info": null}, "72": {"text": "\"\\n\"\n            results.append(Document(response))\n        return results\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/twitter.py", "file_name": "twitter.py"}, "index": 72, "child_indices": [], "ref_doc_id": "ee0e8f78b27c1a62e466e13d35e9bbeea3b425e9", "node_info": null}, "73": {"text": "\"\"\"Web scraper.\"\"\"\nimport logging\nfrom typing import Any, Callable, Dict, List, Optional, Tuple\n\nfrom langchain.utilities import RequestsWrapper\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass SimpleWebPageReader(BaseReader):\n    \"\"\"Simple web page reader.\n\n    Reads pages from the web.\n\n    Args:\n        html_to_text (bool): Whether to convert HTML to text.\n            Requires `html2text` package.\n\n    \"\"\"\n\n    def __init__(self, html_to_text: bool = False) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            import html2text  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`html2text` package not found, please run", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 73, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "74": {"text": "    \"`html2text` package not found, please run `pip install html2text`\"\n            )\n        self._html_to_text = html_to_text\n\n    def load_data(self, urls: List[str]) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            urls (List[str]): List of URLs to scrape.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        if not isinstance(urls, list):\n            raise ValueError(\"urls must be a list of strings.\")\n        requests = RequestsWrapper()\n        documents = []\n        for url in urls:\n            response = requests.run(url)\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 74, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "75": {"text": "response = requests.run(url)\n            if self._html_to_text:\n                import html2text\n\n                response = html2text.html2text(response)\n\n            documents.append(Document(response))\n\n        return documents\n\n\nclass TrafilaturaWebReader(BaseReader):\n    \"\"\"Trafilatura web page reader.\n\n    Reads pages from the web.\n    Requires the `trafilatura` package.\n\n    \"\"\"\n\n    def __init__(self, error_on_missing: bool = False) -> None:\n        \"\"\"Initialize with parameters.\n\n        Args:\n            error_on_missing (bool): Throw an error when data cannot be parsed\n        \"\"\"\n        self.error_on_missing = error_on_missing\n        try:\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 75, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "76": {"text": "       try:\n            import trafilatura  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`trafilatura` package not found, please run `pip install trafilatura`\"\n            )\n\n    def load_data(self, urls: List[str]) -> List[Document]:\n        \"\"\"Load data from the urls.\n\n        Args:\n            urls (List[str]): List of URLs to scrape.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        import trafilatura\n\n        if not isinstance(urls, list):\n            raise ValueError(\"urls must be a list of strings.\")\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 76, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "77": {"text": "raise ValueError(\"urls must be a list of strings.\")\n        documents = []\n        for url in urls:\n            downloaded = trafilatura.fetch_url(url)\n            if not downloaded:\n                if self.error_on_missing:\n                    raise ValueError(f\"Trafilatura fails to get string from url: {url}\")\n                continue\n            response = trafilatura.extract(downloaded)\n            if not response:\n                if self.error_on_missing:\n                    raise ValueError(f\"Trafilatura fails to parse page: {url}\")\n               ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 77, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "78": {"text": "               continue\n            documents.append(Document(response))\n\n        return documents\n\n\ndef _substack_reader(soup: Any) -> Tuple[str, Dict[str, Any]]:\n    \"\"\"Extract text from Substack blog post.\"\"\"\n    extra_info = {\n        \"Title of this Substack post\": soup.select_one(\"h1.post-title\").getText(),\n        \"Subtitle\": soup.select_one(\"h3.subtitle\").getText(),\n        \"Author\": soup.select_one(\"span.byline-names\").getText(),\n    }\n    text = soup.select_one(\"div.available-content\").getText()\n    return text, extra_info\n\n\nDEFAULT_WEBSITE_EXTRACTOR: Dict[str, Callable[[Any], Tuple[str, Dict[str, Any]]]] = {\n    \"substack.com\": _substack_reader,\n}\n\n\nclass", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 78, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "79": {"text": "_substack_reader,\n}\n\n\nclass BeautifulSoupWebReader(BaseReader):\n    \"\"\"BeautifulSoup web page reader.\n\n    Reads pages from the web.\n    Requires the `bs4` and `urllib` packages.\n\n    Args:\n        file_extractor (Optional[Dict[str, Callable]]): A mapping of website\n            hostname (e.g. google.com) to a function that specifies how to\n            extract text from the BeautifulSoup obj. See DEFAULT_WEBSITE_EXTRACTOR.\n    \"\"\"\n\n    def __init__(\n        self,\n        website_extractor: Optional[Dict[str, Callable]] = None,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            from urllib.parse import urlparse  # noqa: F401\n\n         ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 79, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "80": {"text": "# noqa: F401\n\n            import requests  # noqa: F401\n            from bs4 import BeautifulSoup  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`bs4`, `requests`, and `urllib` must be installed to scrape websites.\"\n                \"Please run `pip install bs4 requests urllib`.\"\n            )\n\n        self.website_extractor = website_extractor or DEFAULT_WEBSITE_EXTRACTOR\n\n    def load_data(\n        self, urls: List[str], custom_hostname: Optional[str] = None\n    ) -> List[Document]:\n        \"\"\"Load data from the urls.\n\n        Args:\n            urls", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 80, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "81": {"text": "Args:\n            urls (List[str]): List of URLs to scrape.\n            custom_hostname (Optional[str]): Force a certain hostname in the case\n                a website is displayed under custom URLs (e.g. Substack blogs)\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        from urllib.parse import urlparse\n\n        import requests\n        from bs4 import BeautifulSoup\n\n        documents = []\n        for url in urls:\n            try:\n                page = requests.get(url)\n            except Exception:\n                raise ValueError(f\"One of the inputs is not a", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 81, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "82": {"text": "  raise ValueError(f\"One of the inputs is not a valid url: {url}\")\n\n            hostname = custom_hostname or urlparse(url).hostname or \"\"\n\n            soup = BeautifulSoup(page.content, \"html.parser\")\n\n            data = \"\"\n            extra_info = {\"URL\": url}\n            if hostname in self.website_extractor:\n                data, metadata = self.website_extractor[hostname](soup)\n                extra_info.update(metadata)\n            else:\n                data = soup.getText()\n\n            documents.append(Document(data, extra_info=extra_info))\n\n        return documents\n\n\nclass RssReader(BaseReader):\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 82, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "83": {"text": " return documents\n\n\nclass RssReader(BaseReader):\n    \"\"\"RSS reader.\n\n    Reads content from an RSS feed.\n\n    \"\"\"\n\n    def __init__(self, html_to_text: bool = False) -> None:\n        \"\"\"Initialize with parameters.\n\n        Args:\n            html_to_text (bool): Whether to convert HTML to text.\n                Requires `html2text` package.\n\n        \"\"\"\n        try:\n            import feedparser  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`feedparser` package not found, please run `pip install feedparser`\"\n            )\n\n        if html_to_text:\n            try:\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 83, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "84": {"text": "           try:\n                import html2text  # noqa: F401\n            except ImportError:\n                raise ValueError(\n                    \"`html2text` package not found, please run `pip install html2text`\"\n                )\n        self._html_to_text = html_to_text\n\n    def load_data(self, urls: List[str]) -> List[Document]:\n        \"\"\"Load data from RSS feeds.\n\n        Args:\n            urls (List[str]): List of RSS URLs to load.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        import feedparser\n\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 84, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "85": {"text": "\"\"\"\n        import feedparser\n\n        if not isinstance(urls, list):\n            raise ValueError(\"urls must be a list of strings.\")\n\n        documents = []\n\n        for url in urls:\n            parsed = feedparser.parse(url)\n            for entry in parsed.entries:\n                if entry.content:\n                    data = entry.content[0].value\n                else:\n                    data = entry.description or entry.summary\n\n                if self._html_to_text:\n                    import html2text\n\n                ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 85, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "86": {"text": "                   data = html2text.html2text(data)\n\n                extra_info = {\"title\": entry.title, \"link\": entry.link}\n                documents.append(Document(data, extra_info=extra_info))\n\n        return documents\n\n\nif __name__ == \"__main__\":\n    reader = SimpleWebPageReader()\n    logging.info(reader.load_data([\"http://www.google.com\"]))\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 86, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "87": {"text": "\"\"\"Simple reader that reads wikipedia.\"\"\"\nfrom typing import Any, List\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass WikipediaReader(BaseReader):\n    \"\"\"Wikipedia reader.\n\n    Reads a page.\n\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            import wikipedia  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`wikipedia` package not found, please run `pip install wikipedia`\"\n            )\n\n    def load_data(self, pages: List[str], **load_kwargs: Any) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            pages (List[str]):", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/wikipedia.py", "file_name": "wikipedia.py"}, "index": 87, "child_indices": [], "ref_doc_id": "55400c6722fc856cbb287cec4987d87118429fb1", "node_info": null}, "88": {"text": "         pages (List[str]): List of pages to read.\n\n        \"\"\"\n        import wikipedia\n\n        results = []\n        for page in pages:\n            page_content = wikipedia.page(page, **load_kwargs).content\n            results.append(Document(page_content))\n        return results\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/wikipedia.py", "file_name": "wikipedia.py"}, "index": 88, "child_indices": [], "ref_doc_id": "55400c6722fc856cbb287cec4987d87118429fb1", "node_info": null}, "89": {"text": "\"\"\"Simple Reader that reads transcript of youtube video.\"\"\"\nfrom typing import Any, List\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass YoutubeTranscriptReader(BaseReader):\n    \"\"\"Youtube Transcript reader.\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n\n    def load_data(self, ytlinks: List[str], **load_kwargs: Any) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            pages (List[str]): List of youtube links \\\n                for which transcripts are to be read.\n\n        \"\"\"\n        try:\n            from youtube_transcript_api import YouTubeTranscriptApi\n        except ImportError:\n            raise", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/youtube_transcript.py", "file_name": "youtube_transcript.py"}, "index": 89, "child_indices": [], "ref_doc_id": "969ca38b79231bd668d28e6cdc0cbc59a2f3745d", "node_info": null}, "90": {"text": "ImportError:\n            raise ValueError(\n                \"`youtube_transcript_api` package not found, \\\n                    please run `pip install youtube-transcript-api`\"\n            )\n\n        results = []\n        for link in ytlinks:\n            video_id = link.split(\"?v=\")[-1]\n            srt = YouTubeTranscriptApi.get_transcript(video_id)\n            transcript = \"\"\n            for chunk in srt:\n                transcript = transcript + chunk[\"text\"] + \"\\n\"\n            results.append(Document(transcript))\n        return results\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/youtube_transcript.py", "file_name": "youtube_transcript.py"}, "index": 90, "child_indices": [], "ref_doc_id": "969ca38b79231bd668d28e6cdc0cbc59a2f3745d", "node_info": null}, "91": {"text": "This module contains the data connectors for GPT Index. It includes the BaseReader class, which provides utilities for loading data from a directory, and the DatabaseReader class, which concatenates each row into a Document used by GPT Index. It also includes the DiscordReader class, which provides a synchronous interface to the async discord.py API to read messages from a Discord channel.", "doc_id": null, "embedding": null, "extra_info": null, "index": 91, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "92": {"text": "\nThe discord_reader.py file is a class that reads conversations from channels using the Discord API. It takes in a Discord token and a list of channel ids, and returns a list of documents containing the messages from the channels. It also has a _read_channel() method that reads a single channel and a load_data() method that reads multiple channels. The download.py file is a function that downloads a single loader from the Loader Hub. It takes in the name of the loader class and returns a Loader. The faiss.py file is a class that retrieves documents through an existing in-memory Faiss index. It takes in a Faiss Index object and returns a list of documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 92, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "93": {"text": "The gpt_index/readers/faiss.py file contains a function that takes a 2D numpy array of query vectors, a map from ID's to text, and a number of nearest neighbors to retrieve as parameters. It then searches the index for the query vectors and returns a list of documents. The gpt_index/readers/mbox.py file contains a class that reads a set of emails saved in the mbox format. It takes an input directory, a maximum amount of messages to read, and a message format as parameters. The gpt_index/readers/mongo.py file contains a class that reads documents from a Mongo database. It takes a database name, a collection name, and a query to filter documents as parameters. The gpt_index/readers/notion.py file contains a class that reads a set of Notion pages. It takes an integration token as a parameter.", "doc_id": null, "embedding": null, "extra_info": null, "index": 93, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}, "94": {"text": "The notion.py file is a Python script that provides functions for reading and querying Notion pages. It contains functions for reading a page, querying a database, searching for a page, and loading data from a page or database. It uses the Notion API to make requests and returns a list of documents containing the page text. The obsidian.py file is a Python script that provides functions for loading data from an Obsidian vault. It contains functions for loading data from a directory and loading data in LangChain document format. The pinecone.py file is a Python script that provides functions for loading data from Pinecone. It contains functions for loading data from an index, with the option to include values and separate documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 94, "child_indices": [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], "ref_doc_id": null, "node_info": null}, "95": {"text": "This code file contains two readers, PineconeReader and QdrantReader, which are used to retrieve documents from existing Pinecone and Qdrant collections. PineconeReader takes in an index name, a map from ID's to text, a query vector, a top_k number of results to return, and keyword arguments to pass to the query. QdrantReader takes in a host name, port, grpc_port, prefer_grpc, https, api_key, prefix, and timeout. SlackReader takes in a slack token or an environment variable. All readers return a list of documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 95, "child_indices": [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59], "ref_doc_id": null, "node_info": null}, "96": {"text": "The file slack.py is a reader for the GPT-Index library. It is used to read messages from a Slack channel and turn them into a list of Documents. It uses the Slack API to access the messages and paginates the results. It also handles rate limit errors by sleeping for the specified amount of time. The load_data() method takes a list of channel ids and returns a list of Documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 96, "child_indices": [64, 65, 66, 67, 68, 69, 70, 71, 60, 61, 62, 63], "ref_doc_id": null, "node_info": null}, "97": {"text": "This code file contains two classes, SimpleWebPageReader and TrafilaturaWebReader, which are used to scrape web pages. The SimpleWebPageReader class uses the RequestsWrapper utility to scrape web pages and convert HTML to text if necessary. The TrafilaturaWebReader class uses the Trafilatura package to scrape web pages and extract text from them. The BeautifulSoupWebReader class uses the BeautifulSoup, Requests, and Urllib packages to scrape web pages and extract text from them. Finally, the RssReader class uses the Feedparser package to scrape RSS feeds and extract text from them.", "doc_id": null, "embedding": null, "extra_info": null, "index": 97, "child_indices": [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83], "ref_doc_id": null, "node_info": null}, "98": {"text": "This code file contains three readers for GPT-Index: SimpleWebPageReader, WikipediaReader, and YoutubeTranscriptReader. SimpleWebPageReader reads RSS feeds from a list of URLs and converts HTML to text if necessary. WikipediaReader reads a page from Wikipedia. YoutubeTranscriptReader reads the transcript of a YouTube video from a list of YouTube links. All readers return a list of documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 98, "child_indices": [84, 85, 86, 87, 88, 89, 90], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"91": {"text": "This module contains the data connectors for GPT Index. It includes the BaseReader class, which provides utilities for loading data from a directory, and the DatabaseReader class, which concatenates each row into a Document used by GPT Index. It also includes the DiscordReader class, which provides a synchronous interface to the async discord.py API to read messages from a Discord channel.", "doc_id": null, "embedding": null, "extra_info": null, "index": 91, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "92": {"text": "\nThe discord_reader.py file is a class that reads conversations from channels using the Discord API. It takes in a Discord token and a list of channel ids, and returns a list of documents containing the messages from the channels. It also has a _read_channel() method that reads a single channel and a load_data() method that reads multiple channels. The download.py file is a function that downloads a single loader from the Loader Hub. It takes in the name of the loader class and returns a Loader. The faiss.py file is a class that retrieves documents through an existing in-memory Faiss index. It takes in a Faiss Index object and returns a list of documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 92, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "93": {"text": "The gpt_index/readers/faiss.py file contains a function that takes a 2D numpy array of query vectors, a map from ID's to text, and a number of nearest neighbors to retrieve as parameters. It then searches the index for the query vectors and returns a list of documents. The gpt_index/readers/mbox.py file contains a class that reads a set of emails saved in the mbox format. It takes an input directory, a maximum amount of messages to read, and a message format as parameters. The gpt_index/readers/mongo.py file contains a class that reads documents from a Mongo database. It takes a database name, a collection name, and a query to filter documents as parameters. The gpt_index/readers/notion.py file contains a class that reads a set of Notion pages. It takes an integration token as a parameter.", "doc_id": null, "embedding": null, "extra_info": null, "index": 93, "child_indices": [32, 33, 34, 35, 24, 25, 26, 27, 28, 29, 30, 31], "ref_doc_id": null, "node_info": null}, "94": {"text": "The notion.py file is a Python script that provides functions for reading and querying Notion pages. It contains functions for reading a page, querying a database, searching for a page, and loading data from a page or database. It uses the Notion API to make requests and returns a list of documents containing the page text. The obsidian.py file is a Python script that provides functions for loading data from an Obsidian vault. It contains functions for loading data from a directory and loading data in LangChain document format. The pinecone.py file is a Python script that provides functions for loading data from Pinecone. It contains functions for loading data from an index, with the option to include values and separate documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 94, "child_indices": [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], "ref_doc_id": null, "node_info": null}, "95": {"text": "This code file contains two readers, PineconeReader and QdrantReader, which are used to retrieve documents from existing Pinecone and Qdrant collections. PineconeReader takes in an index name, a map from ID's to text, a query vector, a top_k number of results to return, and keyword arguments to pass to the query. QdrantReader takes in a host name, port, grpc_port, prefer_grpc, https, api_key, prefix, and timeout. SlackReader takes in a slack token or an environment variable. All readers return a list of documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 95, "child_indices": [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59], "ref_doc_id": null, "node_info": null}, "96": {"text": "The file slack.py is a reader for the GPT-Index library. It is used to read messages from a Slack channel and turn them into a list of Documents. It uses the Slack API to access the messages and paginates the results. It also handles rate limit errors by sleeping for the specified amount of time. The load_data() method takes a list of channel ids and returns a list of Documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 96, "child_indices": [64, 65, 66, 67, 68, 69, 70, 71, 60, 61, 62, 63], "ref_doc_id": null, "node_info": null}, "97": {"text": "This code file contains two classes, SimpleWebPageReader and TrafilaturaWebReader, which are used to scrape web pages. The SimpleWebPageReader class uses the RequestsWrapper utility to scrape web pages and convert HTML to text if necessary. The TrafilaturaWebReader class uses the Trafilatura package to scrape web pages and extract text from them. The BeautifulSoupWebReader class uses the BeautifulSoup, Requests, and Urllib packages to scrape web pages and extract text from them. Finally, the RssReader class uses the Feedparser package to scrape RSS feeds and extract text from them.", "doc_id": null, "embedding": null, "extra_info": null, "index": 97, "child_indices": [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83], "ref_doc_id": null, "node_info": null}, "98": {"text": "This code file contains three readers for GPT-Index: SimpleWebPageReader, WikipediaReader, and YoutubeTranscriptReader. SimpleWebPageReader reads RSS feeds from a list of URLs and converts HTML to text if necessary. WikipediaReader reads a page from Wikipedia. YoutubeTranscriptReader reads the transcript of a YouTube video from a list of YouTube links. All readers return a list of documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 98, "child_indices": [84, 85, 86, 87, 88, 89, 90], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}