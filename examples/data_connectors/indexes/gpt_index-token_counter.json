{"index_struct": {"text": "\nThere is no answer to this question as the context information does not provide any summaries of the documents.", "doc_id": "5fc314c7-f8a0-495d-a728-84b7a4178350", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "node_info": null}, "1": {"text": "\"\"\"Mock chain wrapper.\"\"\"\n\nfrom typing import Any, Dict\n\nfrom gpt_index.constants import NUM_OUTPUTS\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.prompts.prompt_type import PromptType\nfrom gpt_index.token_counter.utils import mock_extract_keywords_response\nfrom gpt_index.utils import globals_helper\n\n# TODO: consolidate with unit tests in tests/mock_utils/mock_predict.py\n\n\ndef _mock_summary_predict(max_tokens: int, prompt_args: Dict) -> str:\n    \"\"\"Mock summary predict.\"\"\"\n    # tokens in response shouldn't be larger than tokens in `context_str`\n    num_text_tokens = len(globals_helper.tokenizer(prompt_args[\"context_str\"]))\n    token_limit = min(num_text_tokens,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "index": 1, "child_indices": [], "ref_doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "node_info": null}, "2": {"text": "  token_limit = min(num_text_tokens, max_tokens)\n    return \" \".join([\"summary\"] * token_limit)\n\n\ndef _mock_insert_predict() -> str:\n    \"\"\"Mock insert predict.\"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_query_select() -> str:\n    \"\"\"Mock query select.\"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_query_select_multiple(num_chunks: int) -> str:\n    \"\"\"Mock query select.\"\"\"\n    nums_str = \", \".join([str(i) for i in range(num_chunks)])\n    return f\"ANSWER: {nums_str}\"\n\n\ndef _mock_answer(max_tokens: int, prompt_args: Dict) -> str:\n    \"\"\"Mock answer.\"\"\"\n    # tokens in response shouldn't be larger than tokens in `text`\n    num_ctx_tokens =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "index": 2, "child_indices": [], "ref_doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "node_info": null}, "3": {"text": "in `text`\n    num_ctx_tokens = len(globals_helper.tokenizer(prompt_args[\"context_str\"]))\n    token_limit = min(num_ctx_tokens, max_tokens)\n    return \" \".join([\"answer\"] * token_limit)\n\n\ndef _mock_refine(max_tokens: int, prompt: Prompt, prompt_args: Dict) -> str:\n    \"\"\"Mock refine.\"\"\"\n    # tokens in response shouldn't be larger than tokens in\n    # `existing_answer` + `context_msg`\n    # NOTE: if existing_answer is not in prompt_args, we need to get it from the prompt\n    if \"existing_answer\" not in prompt_args:\n        existing_answer = prompt.partial_dict[\"existing_answer\"]\n    else:\n        existing_answer = prompt_args[\"existing_answer\"]\n    num_ctx_tokens =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "index": 3, "child_indices": [], "ref_doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "node_info": null}, "4": {"text": "   num_ctx_tokens = len(globals_helper.tokenizer(prompt_args[\"context_msg\"]))\n    num_exist_tokens = len(globals_helper.tokenizer(existing_answer))\n    token_limit = min(num_ctx_tokens + num_exist_tokens, max_tokens)\n    return \" \".join([\"answer\"] * token_limit)\n\n\ndef _mock_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"text\"])\n\n\ndef _mock_query_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock query keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"question\"])\n\n\nclass MockLLMPredictor(LLMPredictor):\n    \"\"\"Mock LLM Predictor.\"\"\"\n\n    def __init__(self,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "index": 4, "child_indices": [], "ref_doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "node_info": null}, "5": {"text": "Predictor.\"\"\"\n\n    def __init__(self, max_tokens: int = NUM_OUTPUTS) -> None:\n        \"\"\"Initialize params.\"\"\"\n        # NOTE: don't call super, we don't want to instantiate LLM\n        self.max_tokens = max_tokens\n        self._total_tokens_used = 0\n        self.flag = True\n        self._last_token_usage = None\n\n    def _predict(self, prompt: Prompt, **prompt_args: Any) -> str:\n        \"\"\"Mock predict.\"\"\"\n        prompt_str = prompt.prompt_type\n        if prompt_str == PromptType.SUMMARY:\n            return _mock_summary_predict(self.max_tokens, prompt_args)\n        elif prompt_str", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "index": 5, "child_indices": [], "ref_doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "node_info": null}, "6": {"text": "       elif prompt_str == PromptType.TREE_INSERT:\n            return _mock_insert_predict()\n        elif prompt_str == PromptType.TREE_SELECT:\n            return _mock_query_select()\n        elif prompt_str == PromptType.TREE_SELECT_MULTIPLE:\n            return _mock_query_select_multiple(prompt_args[\"num_chunks\"])\n        elif prompt_str == PromptType.REFINE:\n            return _mock_refine(self.max_tokens, prompt, prompt_args)\n        elif prompt_str == PromptType.QUESTION_ANSWER:\n            return _mock_answer(self.max_tokens, prompt_args)\n        elif", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "index": 6, "child_indices": [], "ref_doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "node_info": null}, "7": {"text": "prompt_args)\n        elif prompt_str == PromptType.KEYWORD_EXTRACT:\n            return _mock_keyword_extract(prompt_args)\n        elif prompt_str == PromptType.QUERY_KEYWORD_EXTRACT:\n            return _mock_query_keyword_extract(prompt_args)\n        else:\n            raise ValueError(\"Invalid prompt type.\")\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "index": 7, "child_indices": [], "ref_doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "node_info": null}, "8": {"text": "\"\"\"Mock embedding model.\"\"\"\n\nfrom typing import Any, List\n\nfrom gpt_index.embeddings.base import BaseEmbedding\n\n\nclass MockEmbedding(BaseEmbedding):\n    \"\"\"Mock embedding.\n\n    Used for token prediction.\n\n    Args:\n        embed_dim (int): embedding dimension\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.embed_dim = embed_dim\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding.\"\"\"\n        return [0.5] * self.embed_dim\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get text embedding.\"\"\"\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_embed_model.py", "file_name": "mock_embed_model.py"}, "index": 8, "child_indices": [], "ref_doc_id": "9ac4e9006efa30143675c5320e66fe9c1c7bf4a4", "node_info": null}, "9": {"text": "      \"\"\"Get text embedding.\"\"\"\n        return [0.5] * self.embed_dim\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_embed_model.py", "file_name": "mock_embed_model.py"}, "index": 9, "child_indices": [], "ref_doc_id": "9ac4e9006efa30143675c5320e66fe9c1c7bf4a4", "node_info": null}, "10": {"text": "\"\"\"Token counter function.\"\"\"\n\nimport logging\nfrom typing import Any, Callable, cast\n\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\n\n\ndef llm_token_counter(method_name_str: str) -> Callable:\n    \"\"\"\n    Use this as a decorator for methods in index/query classes that make calls to LLMs.\n\n    At the moment, this decorator can only be used on class instance methods with a\n    `_llm_predictor` attribute.\n\n    Do not use this on abstract methods.\n\n    For example, consider the class below:\n        .. code-block:: python\n            class GPTTreeIndexBuilder:\n            ...\n            @llm_token_counter(\"build_from_text\")\n            def build_from_text(self, documents: Sequence[BaseDocument]) ->", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/token_counter.py", "file_name": "token_counter.py"}, "index": 10, "child_indices": [], "ref_doc_id": "ee4b9ac3b34abb65842a80d4ae2390733c1542c1", "node_info": null}, "11": {"text": "documents: Sequence[BaseDocument]) -> IndexGraph:\n                ...\n\n    If you run `build_from_text()`, it will print the output in the form below:\n\n    ```\n    [build_from_text] Total token usage: <some-number> tokens\n    ```\n    \"\"\"\n\n    def wrap(f: Callable) -> Callable:\n        def wrapped_llm_predict(_self: Any, *args: Any, **kwargs: Any) -> Any:\n            llm_predictor = getattr(_self, \"_llm_predictor\", None)\n            if llm_predictor is None:\n                raise ValueError(\n                    \"Cannot use llm_token_counter on an instance \"\n                    \"without a", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/token_counter.py", "file_name": "token_counter.py"}, "index": 11, "child_indices": [], "ref_doc_id": "ee4b9ac3b34abb65842a80d4ae2390733c1542c1", "node_info": null}, "12": {"text": "             \"without a _llm_predictor attribute.\"\n                )\n            llm_predictor = cast(LLMPredictor, llm_predictor)\n\n            embed_model = getattr(_self, \"_embed_model\", None)\n            if embed_model is None:\n                raise ValueError(\n                    \"Cannot use llm_token_counter on an instance \"\n                    \"without a _embed_model attribute.\"\n                )\n            embed_model = cast(BaseEmbedding, embed_model)\n\n            start_token_ct = llm_predictor.total_tokens_used\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/token_counter.py", "file_name": "token_counter.py"}, "index": 12, "child_indices": [], "ref_doc_id": "ee4b9ac3b34abb65842a80d4ae2390733c1542c1", "node_info": null}, "13": {"text": "llm_predictor.total_tokens_used\n            start_embed_token_ct = embed_model.total_tokens_used\n\n            f_return_val = f(_self, *args, **kwargs)\n\n            net_tokens = llm_predictor.total_tokens_used - start_token_ct\n            llm_predictor.last_token_usage = net_tokens\n            net_embed_tokens = embed_model.total_tokens_used - start_embed_token_ct\n            embed_model.last_token_usage = net_embed_tokens\n\n            # print outputs\n            logging.info(\n                f\"> [{method_name_str}] Total LLM token usage: {net_tokens}", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/token_counter.py", "file_name": "token_counter.py"}, "index": 13, "child_indices": [], "ref_doc_id": "ee4b9ac3b34abb65842a80d4ae2390733c1542c1", "node_info": null}, "14": {"text": "Total LLM token usage: {net_tokens} tokens\"\n            )\n            logging.info(\n                f\"> [{method_name_str}] Total embedding token usage: \"\n                f\"{net_embed_tokens} tokens\"\n            )\n\n            return f_return_val\n\n        return wrapped_llm_predict\n\n    return wrap\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/token_counter.py", "file_name": "token_counter.py"}, "index": 14, "child_indices": [], "ref_doc_id": "ee4b9ac3b34abb65842a80d4ae2390733c1542c1", "node_info": null}, "15": {"text": "\"\"\"Token predictor utils.\"\"\"\nfrom typing import Optional\n\nfrom gpt_index.indices.keyword_table.utils import simple_extract_keywords\n\n\ndef mock_extract_keywords_response(\n    text_chunk: str, max_keywords: Optional[int] = None, filter_stopwords: bool = True\n) -> str:\n    \"\"\"Extract keywords mock response.\n\n    Same as simple_extract_keywords but without filtering stopwords.\n\n    \"\"\"\n    return \",\".join(\n        simple_extract_keywords(\n            text_chunk, max_keywords=max_keywords, filter_stopwords=False\n        )\n    )\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/utils.py", "file_name": "utils.py"}, "index": 15, "child_indices": [], "ref_doc_id": "fe6d373401194b3dab89e747b4126550d5204a27", "node_info": null}, "16": {"text": "This code file contains two files: __init__.py and mock_chain_wrapper.py. The __init__.py file is an init file, while the mock_chain_wrapper.py file contains functions to mock the chain wrapper, such as _mock_summary_predict, _mock_insert_predict, _mock_query_select, _mock_query_select_multiple, _mock_answer, _mock_refine, _mock_keyword_extract, _mock_query_keyword_extract, and MockLLMPredictor. Additionally, the mock_embed_model.py file contains a MockEmbedding class and the token_counter.py file contains a llm_token_counter function. The purpose of this code is to provide mock functions and classes to test the chain wrapper and embedding model.", "doc_id": null, "embedding": null, "extra_info": null, "index": 16, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "17": {"text": "The token_counter.py file is a Python script that is used to track the total number of tokens used by an LLM predictor and an embedding model. It wraps a function with a decorator that records the total number of tokens used before and after the function is called, and then prints the total number of tokens used for each model. The utils.py file contains a mock response for the simple_extract_keywords function that does not filter out stopwords.", "doc_id": null, "embedding": null, "extra_info": null, "index": 17, "child_indices": [12, 13, 14, 15], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"16": {"text": "This code file contains two files: __init__.py and mock_chain_wrapper.py. The __init__.py file is an init file, while the mock_chain_wrapper.py file contains functions to mock the chain wrapper, such as _mock_summary_predict, _mock_insert_predict, _mock_query_select, _mock_query_select_multiple, _mock_answer, _mock_refine, _mock_keyword_extract, _mock_query_keyword_extract, and MockLLMPredictor. Additionally, the mock_embed_model.py file contains a MockEmbedding class and the token_counter.py file contains a llm_token_counter function. The purpose of this code is to provide mock functions and classes to test the chain wrapper and embedding model.", "doc_id": null, "embedding": null, "extra_info": null, "index": 16, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "17": {"text": "The token_counter.py file is a Python script that is used to track the total number of tokens used by an LLM predictor and an embedding model. It wraps a function with a decorator that records the total number of tokens used before and after the function is called, and then prints the total number of tokens used for each model. The utils.py file contains a mock response for the simple_extract_keywords function that does not filter out stopwords.", "doc_id": null, "embedding": null, "extra_info": null, "index": 17, "child_indices": [12, 13, 14, 15], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"1d4640565ae2765d9ca96a509dc9809217f62f2f": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d": {"text": "\"\"\"Mock chain wrapper.\"\"\"\n\nfrom typing import Any, Dict\n\nfrom gpt_index.constants import NUM_OUTPUTS\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.prompts.prompt_type import PromptType\nfrom gpt_index.token_counter.utils import mock_extract_keywords_response\nfrom gpt_index.utils import globals_helper\n\n# TODO: consolidate with unit tests in tests/mock_utils/mock_predict.py\n\n\ndef _mock_summary_predict(max_tokens: int, prompt_args: Dict) -> str:\n    \"\"\"Mock summary predict.\"\"\"\n    # tokens in response shouldn't be larger than tokens in `context_str`\n    num_text_tokens = len(globals_helper.tokenizer(prompt_args[\"context_str\"]))\n    token_limit = min(num_text_tokens, max_tokens)\n    return \" \".join([\"summary\"] * token_limit)\n\n\ndef _mock_insert_predict() -> str:\n    \"\"\"Mock insert predict.\"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_query_select() -> str:\n    \"\"\"Mock query select.\"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_query_select_multiple(num_chunks: int) -> str:\n    \"\"\"Mock query select.\"\"\"\n    nums_str = \", \".join([str(i) for i in range(num_chunks)])\n    return f\"ANSWER: {nums_str}\"\n\n\ndef _mock_answer(max_tokens: int, prompt_args: Dict) -> str:\n    \"\"\"Mock answer.\"\"\"\n    # tokens in response shouldn't be larger than tokens in `text`\n    num_ctx_tokens = len(globals_helper.tokenizer(prompt_args[\"context_str\"]))\n    token_limit = min(num_ctx_tokens, max_tokens)\n    return \" \".join([\"answer\"] * token_limit)\n\n\ndef _mock_refine(max_tokens: int, prompt: Prompt, prompt_args: Dict) -> str:\n    \"\"\"Mock refine.\"\"\"\n    # tokens in response shouldn't be larger than tokens in\n    # `existing_answer` + `context_msg`\n    # NOTE: if existing_answer is not in prompt_args, we need to get it from the prompt\n    if \"existing_answer\" not in prompt_args:\n        existing_answer = prompt.partial_dict[\"existing_answer\"]\n    else:\n        existing_answer = prompt_args[\"existing_answer\"]\n    num_ctx_tokens = len(globals_helper.tokenizer(prompt_args[\"context_msg\"]))\n    num_exist_tokens = len(globals_helper.tokenizer(existing_answer))\n    token_limit = min(num_ctx_tokens + num_exist_tokens, max_tokens)\n    return \" \".join([\"answer\"] * token_limit)\n\n\ndef _mock_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"text\"])\n\n\ndef _mock_query_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock query keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"question\"])\n\n\nclass MockLLMPredictor(LLMPredictor):\n    \"\"\"Mock LLM Predictor.\"\"\"\n\n    def __init__(self, max_tokens: int = NUM_OUTPUTS) -> None:\n        \"\"\"Initialize params.\"\"\"\n        # NOTE: don't call super, we don't want to instantiate LLM\n        self.max_tokens = max_tokens\n        self._total_tokens_used = 0\n        self.flag = True\n        self._last_token_usage = None\n\n    def _predict(self, prompt: Prompt, **prompt_args: Any) -> str:\n        \"\"\"Mock predict.\"\"\"\n        prompt_str = prompt.prompt_type\n        if prompt_str == PromptType.SUMMARY:\n            return _mock_summary_predict(self.max_tokens, prompt_args)\n        elif prompt_str == PromptType.TREE_INSERT:\n            return _mock_insert_predict()\n        elif prompt_str == PromptType.TREE_SELECT:\n            return _mock_query_select()\n        elif prompt_str == PromptType.TREE_SELECT_MULTIPLE:\n            return _mock_query_select_multiple(prompt_args[\"num_chunks\"])\n        elif prompt_str == PromptType.REFINE:\n            return _mock_refine(self.max_tokens, prompt, prompt_args)\n        elif prompt_str == PromptType.QUESTION_ANSWER:\n            return _mock_answer(self.max_tokens, prompt_args)\n        elif prompt_str == PromptType.KEYWORD_EXTRACT:\n            return _mock_keyword_extract(prompt_args)\n        elif prompt_str == PromptType.QUERY_KEYWORD_EXTRACT:\n            return _mock_query_keyword_extract(prompt_args)\n        else:\n            raise ValueError(\"Invalid prompt type.\")\n", "doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "__type__": "Document"}, "9ac4e9006efa30143675c5320e66fe9c1c7bf4a4": {"text": "\"\"\"Mock embedding model.\"\"\"\n\nfrom typing import Any, List\n\nfrom gpt_index.embeddings.base import BaseEmbedding\n\n\nclass MockEmbedding(BaseEmbedding):\n    \"\"\"Mock embedding.\n\n    Used for token prediction.\n\n    Args:\n        embed_dim (int): embedding dimension\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.embed_dim = embed_dim\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding.\"\"\"\n        return [0.5] * self.embed_dim\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get text embedding.\"\"\"\n        return [0.5] * self.embed_dim\n", "doc_id": "9ac4e9006efa30143675c5320e66fe9c1c7bf4a4", "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_embed_model.py", "file_name": "mock_embed_model.py"}, "__type__": "Document"}, "ee4b9ac3b34abb65842a80d4ae2390733c1542c1": {"text": "\"\"\"Token counter function.\"\"\"\n\nimport logging\nfrom typing import Any, Callable, cast\n\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\n\n\ndef llm_token_counter(method_name_str: str) -> Callable:\n    \"\"\"\n    Use this as a decorator for methods in index/query classes that make calls to LLMs.\n\n    At the moment, this decorator can only be used on class instance methods with a\n    `_llm_predictor` attribute.\n\n    Do not use this on abstract methods.\n\n    For example, consider the class below:\n        .. code-block:: python\n            class GPTTreeIndexBuilder:\n            ...\n            @llm_token_counter(\"build_from_text\")\n            def build_from_text(self, documents: Sequence[BaseDocument]) -> IndexGraph:\n                ...\n\n    If you run `build_from_text()`, it will print the output in the form below:\n\n    ```\n    [build_from_text] Total token usage: <some-number> tokens\n    ```\n    \"\"\"\n\n    def wrap(f: Callable) -> Callable:\n        def wrapped_llm_predict(_self: Any, *args: Any, **kwargs: Any) -> Any:\n            llm_predictor = getattr(_self, \"_llm_predictor\", None)\n            if llm_predictor is None:\n                raise ValueError(\n                    \"Cannot use llm_token_counter on an instance \"\n                    \"without a _llm_predictor attribute.\"\n                )\n            llm_predictor = cast(LLMPredictor, llm_predictor)\n\n            embed_model = getattr(_self, \"_embed_model\", None)\n            if embed_model is None:\n                raise ValueError(\n                    \"Cannot use llm_token_counter on an instance \"\n                    \"without a _embed_model attribute.\"\n                )\n            embed_model = cast(BaseEmbedding, embed_model)\n\n            start_token_ct = llm_predictor.total_tokens_used\n            start_embed_token_ct = embed_model.total_tokens_used\n\n            f_return_val = f(_self, *args, **kwargs)\n\n            net_tokens = llm_predictor.total_tokens_used - start_token_ct\n            llm_predictor.last_token_usage = net_tokens\n            net_embed_tokens = embed_model.total_tokens_used - start_embed_token_ct\n            embed_model.last_token_usage = net_embed_tokens\n\n            # print outputs\n            logging.info(\n                f\"> [{method_name_str}] Total LLM token usage: {net_tokens} tokens\"\n            )\n            logging.info(\n                f\"> [{method_name_str}] Total embedding token usage: \"\n                f\"{net_embed_tokens} tokens\"\n            )\n\n            return f_return_val\n\n        return wrapped_llm_predict\n\n    return wrap\n", "doc_id": "ee4b9ac3b34abb65842a80d4ae2390733c1542c1", "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/token_counter.py", "file_name": "token_counter.py"}, "__type__": "Document"}, "fe6d373401194b3dab89e747b4126550d5204a27": {"text": "\"\"\"Token predictor utils.\"\"\"\nfrom typing import Optional\n\nfrom gpt_index.indices.keyword_table.utils import simple_extract_keywords\n\n\ndef mock_extract_keywords_response(\n    text_chunk: str, max_keywords: Optional[int] = None, filter_stopwords: bool = True\n) -> str:\n    \"\"\"Extract keywords mock response.\n\n    Same as simple_extract_keywords but without filtering stopwords.\n\n    \"\"\"\n    return \",\".join(\n        simple_extract_keywords(\n            text_chunk, max_keywords=max_keywords, filter_stopwords=False\n        )\n    )\n", "doc_id": "fe6d373401194b3dab89e747b4126550d5204a27", "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/utils.py", "file_name": "utils.py"}, "__type__": "Document"}, "5fc314c7-f8a0-495d-a728-84b7a4178350": {"text": "\nThere is no answer to this question as the context information does not provide any summaries of the documents.", "doc_id": "5fc314c7-f8a0-495d-a728-84b7a4178350", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "node_info": null}, "1": {"text": "\"\"\"Mock chain wrapper.\"\"\"\n\nfrom typing import Any, Dict\n\nfrom gpt_index.constants import NUM_OUTPUTS\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.prompts.prompt_type import PromptType\nfrom gpt_index.token_counter.utils import mock_extract_keywords_response\nfrom gpt_index.utils import globals_helper\n\n# TODO: consolidate with unit tests in tests/mock_utils/mock_predict.py\n\n\ndef _mock_summary_predict(max_tokens: int, prompt_args: Dict) -> str:\n    \"\"\"Mock summary predict.\"\"\"\n    # tokens in response shouldn't be larger than tokens in `context_str`\n    num_text_tokens = len(globals_helper.tokenizer(prompt_args[\"context_str\"]))\n    token_limit = min(num_text_tokens,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "index": 1, "child_indices": [], "ref_doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "node_info": null}, "2": {"text": "  token_limit = min(num_text_tokens, max_tokens)\n    return \" \".join([\"summary\"] * token_limit)\n\n\ndef _mock_insert_predict() -> str:\n    \"\"\"Mock insert predict.\"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_query_select() -> str:\n    \"\"\"Mock query select.\"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_query_select_multiple(num_chunks: int) -> str:\n    \"\"\"Mock query select.\"\"\"\n    nums_str = \", \".join([str(i) for i in range(num_chunks)])\n    return f\"ANSWER: {nums_str}\"\n\n\ndef _mock_answer(max_tokens: int, prompt_args: Dict) -> str:\n    \"\"\"Mock answer.\"\"\"\n    # tokens in response shouldn't be larger than tokens in `text`\n    num_ctx_tokens =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "index": 2, "child_indices": [], "ref_doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "node_info": null}, "3": {"text": "in `text`\n    num_ctx_tokens = len(globals_helper.tokenizer(prompt_args[\"context_str\"]))\n    token_limit = min(num_ctx_tokens, max_tokens)\n    return \" \".join([\"answer\"] * token_limit)\n\n\ndef _mock_refine(max_tokens: int, prompt: Prompt, prompt_args: Dict) -> str:\n    \"\"\"Mock refine.\"\"\"\n    # tokens in response shouldn't be larger than tokens in\n    # `existing_answer` + `context_msg`\n    # NOTE: if existing_answer is not in prompt_args, we need to get it from the prompt\n    if \"existing_answer\" not in prompt_args:\n        existing_answer = prompt.partial_dict[\"existing_answer\"]\n    else:\n        existing_answer = prompt_args[\"existing_answer\"]\n    num_ctx_tokens =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "index": 3, "child_indices": [], "ref_doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "node_info": null}, "4": {"text": "   num_ctx_tokens = len(globals_helper.tokenizer(prompt_args[\"context_msg\"]))\n    num_exist_tokens = len(globals_helper.tokenizer(existing_answer))\n    token_limit = min(num_ctx_tokens + num_exist_tokens, max_tokens)\n    return \" \".join([\"answer\"] * token_limit)\n\n\ndef _mock_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"text\"])\n\n\ndef _mock_query_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock query keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"question\"])\n\n\nclass MockLLMPredictor(LLMPredictor):\n    \"\"\"Mock LLM Predictor.\"\"\"\n\n    def __init__(self,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "index": 4, "child_indices": [], "ref_doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "node_info": null}, "5": {"text": "Predictor.\"\"\"\n\n    def __init__(self, max_tokens: int = NUM_OUTPUTS) -> None:\n        \"\"\"Initialize params.\"\"\"\n        # NOTE: don't call super, we don't want to instantiate LLM\n        self.max_tokens = max_tokens\n        self._total_tokens_used = 0\n        self.flag = True\n        self._last_token_usage = None\n\n    def _predict(self, prompt: Prompt, **prompt_args: Any) -> str:\n        \"\"\"Mock predict.\"\"\"\n        prompt_str = prompt.prompt_type\n        if prompt_str == PromptType.SUMMARY:\n            return _mock_summary_predict(self.max_tokens, prompt_args)\n        elif prompt_str", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "index": 5, "child_indices": [], "ref_doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "node_info": null}, "6": {"text": "       elif prompt_str == PromptType.TREE_INSERT:\n            return _mock_insert_predict()\n        elif prompt_str == PromptType.TREE_SELECT:\n            return _mock_query_select()\n        elif prompt_str == PromptType.TREE_SELECT_MULTIPLE:\n            return _mock_query_select_multiple(prompt_args[\"num_chunks\"])\n        elif prompt_str == PromptType.REFINE:\n            return _mock_refine(self.max_tokens, prompt, prompt_args)\n        elif prompt_str == PromptType.QUESTION_ANSWER:\n            return _mock_answer(self.max_tokens, prompt_args)\n        elif", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "index": 6, "child_indices": [], "ref_doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "node_info": null}, "7": {"text": "prompt_args)\n        elif prompt_str == PromptType.KEYWORD_EXTRACT:\n            return _mock_keyword_extract(prompt_args)\n        elif prompt_str == PromptType.QUERY_KEYWORD_EXTRACT:\n            return _mock_query_keyword_extract(prompt_args)\n        else:\n            raise ValueError(\"Invalid prompt type.\")\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "index": 7, "child_indices": [], "ref_doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "node_info": null}, "8": {"text": "\"\"\"Mock embedding model.\"\"\"\n\nfrom typing import Any, List\n\nfrom gpt_index.embeddings.base import BaseEmbedding\n\n\nclass MockEmbedding(BaseEmbedding):\n    \"\"\"Mock embedding.\n\n    Used for token prediction.\n\n    Args:\n        embed_dim (int): embedding dimension\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.embed_dim = embed_dim\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding.\"\"\"\n        return [0.5] * self.embed_dim\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get text embedding.\"\"\"\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_embed_model.py", "file_name": "mock_embed_model.py"}, "index": 8, "child_indices": [], "ref_doc_id": "9ac4e9006efa30143675c5320e66fe9c1c7bf4a4", "node_info": null}, "9": {"text": "      \"\"\"Get text embedding.\"\"\"\n        return [0.5] * self.embed_dim\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_embed_model.py", "file_name": "mock_embed_model.py"}, "index": 9, "child_indices": [], "ref_doc_id": "9ac4e9006efa30143675c5320e66fe9c1c7bf4a4", "node_info": null}, "10": {"text": "\"\"\"Token counter function.\"\"\"\n\nimport logging\nfrom typing import Any, Callable, cast\n\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\n\n\ndef llm_token_counter(method_name_str: str) -> Callable:\n    \"\"\"\n    Use this as a decorator for methods in index/query classes that make calls to LLMs.\n\n    At the moment, this decorator can only be used on class instance methods with a\n    `_llm_predictor` attribute.\n\n    Do not use this on abstract methods.\n\n    For example, consider the class below:\n        .. code-block:: python\n            class GPTTreeIndexBuilder:\n            ...\n            @llm_token_counter(\"build_from_text\")\n            def build_from_text(self, documents: Sequence[BaseDocument]) ->", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/token_counter.py", "file_name": "token_counter.py"}, "index": 10, "child_indices": [], "ref_doc_id": "ee4b9ac3b34abb65842a80d4ae2390733c1542c1", "node_info": null}, "11": {"text": "documents: Sequence[BaseDocument]) -> IndexGraph:\n                ...\n\n    If you run `build_from_text()`, it will print the output in the form below:\n\n    ```\n    [build_from_text] Total token usage: <some-number> tokens\n    ```\n    \"\"\"\n\n    def wrap(f: Callable) -> Callable:\n        def wrapped_llm_predict(_self: Any, *args: Any, **kwargs: Any) -> Any:\n            llm_predictor = getattr(_self, \"_llm_predictor\", None)\n            if llm_predictor is None:\n                raise ValueError(\n                    \"Cannot use llm_token_counter on an instance \"\n                    \"without a", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/token_counter.py", "file_name": "token_counter.py"}, "index": 11, "child_indices": [], "ref_doc_id": "ee4b9ac3b34abb65842a80d4ae2390733c1542c1", "node_info": null}, "12": {"text": "             \"without a _llm_predictor attribute.\"\n                )\n            llm_predictor = cast(LLMPredictor, llm_predictor)\n\n            embed_model = getattr(_self, \"_embed_model\", None)\n            if embed_model is None:\n                raise ValueError(\n                    \"Cannot use llm_token_counter on an instance \"\n                    \"without a _embed_model attribute.\"\n                )\n            embed_model = cast(BaseEmbedding, embed_model)\n\n            start_token_ct = llm_predictor.total_tokens_used\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/token_counter.py", "file_name": "token_counter.py"}, "index": 12, "child_indices": [], "ref_doc_id": "ee4b9ac3b34abb65842a80d4ae2390733c1542c1", "node_info": null}, "13": {"text": "llm_predictor.total_tokens_used\n            start_embed_token_ct = embed_model.total_tokens_used\n\n            f_return_val = f(_self, *args, **kwargs)\n\n            net_tokens = llm_predictor.total_tokens_used - start_token_ct\n            llm_predictor.last_token_usage = net_tokens\n            net_embed_tokens = embed_model.total_tokens_used - start_embed_token_ct\n            embed_model.last_token_usage = net_embed_tokens\n\n            # print outputs\n            logging.info(\n                f\"> [{method_name_str}] Total LLM token usage: {net_tokens}", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/token_counter.py", "file_name": "token_counter.py"}, "index": 13, "child_indices": [], "ref_doc_id": "ee4b9ac3b34abb65842a80d4ae2390733c1542c1", "node_info": null}, "14": {"text": "Total LLM token usage: {net_tokens} tokens\"\n            )\n            logging.info(\n                f\"> [{method_name_str}] Total embedding token usage: \"\n                f\"{net_embed_tokens} tokens\"\n            )\n\n            return f_return_val\n\n        return wrapped_llm_predict\n\n    return wrap\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/token_counter.py", "file_name": "token_counter.py"}, "index": 14, "child_indices": [], "ref_doc_id": "ee4b9ac3b34abb65842a80d4ae2390733c1542c1", "node_info": null}, "15": {"text": "\"\"\"Token predictor utils.\"\"\"\nfrom typing import Optional\n\nfrom gpt_index.indices.keyword_table.utils import simple_extract_keywords\n\n\ndef mock_extract_keywords_response(\n    text_chunk: str, max_keywords: Optional[int] = None, filter_stopwords: bool = True\n) -> str:\n    \"\"\"Extract keywords mock response.\n\n    Same as simple_extract_keywords but without filtering stopwords.\n\n    \"\"\"\n    return \",\".join(\n        simple_extract_keywords(\n            text_chunk, max_keywords=max_keywords, filter_stopwords=False\n        )\n    )\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/utils.py", "file_name": "utils.py"}, "index": 15, "child_indices": [], "ref_doc_id": "fe6d373401194b3dab89e747b4126550d5204a27", "node_info": null}, "16": {"text": "This code file contains two files: __init__.py and mock_chain_wrapper.py. The __init__.py file is an init file, while the mock_chain_wrapper.py file contains functions to mock the chain wrapper, such as _mock_summary_predict, _mock_insert_predict, _mock_query_select, _mock_query_select_multiple, _mock_answer, _mock_refine, _mock_keyword_extract, _mock_query_keyword_extract, and MockLLMPredictor. Additionally, the mock_embed_model.py file contains a MockEmbedding class and the token_counter.py file contains a llm_token_counter function. The purpose of this code is to provide mock functions and classes to test the chain wrapper and embedding model.", "doc_id": null, "embedding": null, "extra_info": null, "index": 16, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "17": {"text": "The token_counter.py file is a Python script that is used to track the total number of tokens used by an LLM predictor and an embedding model. It wraps a function with a decorator that records the total number of tokens used before and after the function is called, and then prints the total number of tokens used for each model. The utils.py file contains a mock response for the simple_extract_keywords function that does not filter out stopwords.", "doc_id": null, "embedding": null, "extra_info": null, "index": 17, "child_indices": [12, 13, 14, 15], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"16": {"text": "This code file contains two files: __init__.py and mock_chain_wrapper.py. The __init__.py file is an init file, while the mock_chain_wrapper.py file contains functions to mock the chain wrapper, such as _mock_summary_predict, _mock_insert_predict, _mock_query_select, _mock_query_select_multiple, _mock_answer, _mock_refine, _mock_keyword_extract, _mock_query_keyword_extract, and MockLLMPredictor. Additionally, the mock_embed_model.py file contains a MockEmbedding class and the token_counter.py file contains a llm_token_counter function. The purpose of this code is to provide mock functions and classes to test the chain wrapper and embedding model.", "doc_id": null, "embedding": null, "extra_info": null, "index": 16, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "17": {"text": "The token_counter.py file is a Python script that is used to track the total number of tokens used by an LLM predictor and an embedding model. It wraps a function with a decorator that records the total number of tokens used before and after the function is called, and then prints the total number of tokens used for each model. The utils.py file contains a mock response for the simple_extract_keywords function that does not filter out stopwords.", "doc_id": null, "embedding": null, "extra_info": null, "index": 17, "child_indices": [12, 13, 14, 15], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}