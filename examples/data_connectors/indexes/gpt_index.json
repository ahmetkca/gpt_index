{"index_struct": {"text": "\nThe __init__.py file for the GPT Index library contains the necessary imports and constants, the DocumentStore class for storing documents, and the __all__ list of classes and functions. The docstore.py file contains the DocumentStore class for managing documents. The schema.py file contains the BaseDocument class for capturing index structs and documents. The utils.py file contains the GlobalsHelper class for retrieving tokenizers and stopwords, the get_new_id() and get_new_int_id() functions for generating IDs, and the temp_set_attrs() context manager for setting temporary values for attributes. It also contains the retry_on_exceptions_with_backoff() function for executing a lambda function with retries and exponential backoff.", "doc_id": "1801dcbd-c19e-4492-ba4d-5765d27eb9eb", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "0.4.1\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/VERSION", "file_name": "VERSION"}, "index": 0, "child_indices": [], "ref_doc_id": "267577d47e497a0630bc454b3f74c4fd9a10ced4", "node_info": null}, "1": {"text": "\"\"\"Init file of GPT Index.\"\"\"\nfrom pathlib import Path\n\nwith open(Path(__file__).absolute().parents[0] / \"VERSION\") as _f:\n    __version__ = _f.read().strip()\n\n\nfrom gpt_index.data_structs.struct_type import IndexStructType\n\n# embeddings\nfrom gpt_index.embeddings.langchain import LangchainEmbedding\nfrom gpt_index.embeddings.openai import OpenAIEmbedding\n\n# structured\nfrom gpt_index.indices.common.struct_store.base import SQLContextBuilder\n\n# indices\nfrom gpt_index.indices.keyword_table import (\n    GPTKeywordTableIndex,\n    GPTRAKEKeywordTableIndex,\n    GPTSimpleKeywordTableIndex,\n)\nfrom gpt_index.indices.list import GPTListIndex\n\n# prompt helper\nfrom gpt_index.indices.prompt_helper import PromptHelper\n\n# for composability\nfrom gpt_index.indices.query.schema import QueryConfig, QueryMode\nfrom", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/__init__.py", "file_name": "__init__.py"}, "index": 1, "child_indices": [], "ref_doc_id": "0054ff473fdab69e9283aafee2e4579f884d9e7b", "node_info": null}, "2": {"text": "import QueryConfig, QueryMode\nfrom gpt_index.indices.struct_store.sql import GPTSQLStructStoreIndex\nfrom gpt_index.indices.tree import GPTTreeIndex\nfrom gpt_index.indices.vector_store import (\n    GPTFaissIndex,\n    GPTPineconeIndex,\n    GPTQdrantIndex,\n    GPTSimpleVectorIndex,\n    GPTWeaviateIndex,\n)\n\n# langchain helper\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.memory_wrapper import GPTIndexMemory\nfrom gpt_index.langchain_helpers.sql_wrapper import SQLDatabase\n\n# prompts\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.prompts.prompts import (\n    KeywordExtractPrompt,\n    QueryKeywordExtractPrompt,\n    QuestionAnswerPrompt,\n    RefinePrompt,\n    SummaryPrompt,\n    TreeInsertPrompt,\n    TreeSelectMultiplePrompt,\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/__init__.py", "file_name": "__init__.py"}, "index": 2, "child_indices": [], "ref_doc_id": "0054ff473fdab69e9283aafee2e4579f884d9e7b", "node_info": null}, "3": {"text": "TreeInsertPrompt,\n    TreeSelectMultiplePrompt,\n    TreeSelectPrompt,\n)\n\n# readers\nfrom gpt_index.readers import (\n    BeautifulSoupWebReader,\n    DiscordReader,\n    Document,\n    FaissReader,\n    GithubRepositoryReader,\n    GoogleDocsReader,\n    MboxReader,\n    NotionPageReader,\n    ObsidianReader,\n    PineconeReader,\n    QdrantReader,\n    RssReader,\n    SimpleDirectoryReader,\n    SimpleMongoReader,\n    SimpleWebPageReader,\n    SlackReader,\n    StringIterableReader,\n    TrafilaturaWebReader,\n    TwitterTweetReader,\n    WeaviateReader,\n    WikipediaReader,\n)\nfrom gpt_index.readers.download import download_loader\n\n# token predictor\nfrom gpt_index.token_counter.mock_chain_wrapper import MockLLMPredictor\nfrom gpt_index.token_counter.mock_embed_model", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/__init__.py", "file_name": "__init__.py"}, "index": 3, "child_indices": [], "ref_doc_id": "0054ff473fdab69e9283aafee2e4579f884d9e7b", "node_info": null}, "4": {"text": "gpt_index.token_counter.mock_embed_model import MockEmbedding\n\n__all__ = [\n    \"GPTKeywordTableIndex\",\n    \"GPTSimpleKeywordTableIndex\",\n    \"GPTRAKEKeywordTableIndex\",\n    \"GPTListIndex\",\n    \"GPTTreeIndex\",\n    \"GPTFaissIndex\",\n    \"GPTSimpleVectorIndex\",\n    \"GPTWeaviateIndex\",\n    \"GPTPineconeIndex\",\n    \"GPTQdrantIndex\",\n    \"GPTSQLStructStoreIndex\",\n    \"Prompt\",\n    \"LangchainEmbedding\",\n    \"OpenAIEmbedding\",\n    \"SummaryPrompt\",\n    \"TreeInsertPrompt\",\n    \"TreeSelectPrompt\",\n    \"TreeSelectMultiplePrompt\",\n    \"RefinePrompt\",\n    \"QuestionAnswerPrompt\",\n    \"KeywordExtractPrompt\",\n    \"QueryKeywordExtractPrompt\",\n    \"WikipediaReader\",\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/__init__.py", "file_name": "__init__.py"}, "index": 4, "child_indices": [], "ref_doc_id": "0054ff473fdab69e9283aafee2e4579f884d9e7b", "node_info": null}, "5": {"text": "   \"WikipediaReader\",\n    \"ObsidianReader\",\n    \"Document\",\n    \"SimpleDirectoryReader\",\n    \"SimpleMongoReader\",\n    \"NotionPageReader\",\n    \"GoogleDocsReader\",\n    \"MboxReader\",\n    \"SlackReader\",\n    \"StringIterableReader\",\n    \"WeaviateReader\",\n    \"FaissReader\",\n    \"PineconeReader\",\n    \"QdrantReader\",\n    \"DiscordReader\",\n    \"SimpleWebPageReader\",\n    \"RssReader\",\n    \"BeautifulSoupWebReader\",\n    \"TrafilaturaWebReader\",\n    \"LLMPredictor\",\n    \"MockLLMPredictor\",\n    \"MockEmbedding\",\n    \"SQLDatabase\",\n    \"GPTIndexMemory\",\n    \"SQLContextBuilder\",\n    \"PromptHelper\",\n    \"QueryConfig\",\n    \"QueryMode\",\n    \"IndexStructType\",\n    \"TwitterTweetReader\",\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/__init__.py", "file_name": "__init__.py"}, "index": 5, "child_indices": [], "ref_doc_id": "0054ff473fdab69e9283aafee2e4579f884d9e7b", "node_info": null}, "6": {"text": "   \"TwitterTweetReader\",\n    \"download_loader\",\n    \"GithubRepositoryReader\",\n]\n\nimport logging\nfrom logging import NullHandler\n\n# best practices for library logging:\n# https://docs.python.org/3/howto/logging.html#configuring-logging-for-a-library\nlogging.getLogger(__name__).addHandler(NullHandler())\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/__init__.py", "file_name": "__init__.py"}, "index": 6, "child_indices": [], "ref_doc_id": "0054ff473fdab69e9283aafee2e4579f884d9e7b", "node_info": null}, "7": {"text": "\"\"\"Set of constants.\"\"\"\n\nMAX_CHUNK_SIZE = 3900\nMAX_CHUNK_OVERLAP = 200\nNUM_OUTPUTS = 256\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/constants.py", "file_name": "constants.py"}, "index": 7, "child_indices": [], "ref_doc_id": "d619a2462ae933d7bdde127f86bde58dac29b79b", "node_info": null}, "8": {"text": "\"\"\"Document store.\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional, Type, Union\n\nfrom dataclasses_json import DataClassJsonMixin\n\nfrom gpt_index.data_structs.data_structs import IndexStruct\nfrom gpt_index.readers.schema.base import Document\nfrom gpt_index.utils import get_new_id\n\nDOC_TYPE = Union[IndexStruct, Document]\n\n# type key: used to store type of document\nTYPE_KEY = \"__type__\"\n\n\n@dataclass\nclass DocumentStore(DataClassJsonMixin):\n    \"\"\"Document store.\"\"\"\n\n    docs: Dict[str, DOC_TYPE] = field(default_factory=dict)\n\n    def serialize_to_dict(self) -> Dict[str, Any]:\n        \"\"\"Serialize to dict.\"\"\"\n        docs_dict = {}\n        for doc_id, doc in self.docs.items():\n            doc_dict = doc.to_dict()\n         ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/docstore.py", "file_name": "docstore.py"}, "index": 8, "child_indices": [], "ref_doc_id": "523dc2d189441cf16be18c8f3896fae904dc3473", "node_info": null}, "9": {"text": "doc.to_dict()\n            doc_dict[TYPE_KEY] = doc.get_type()\n            docs_dict[doc_id] = doc_dict\n        return {\"docs\": docs_dict}\n\n    def contains_index_struct(self, exclude_ids: Optional[List[str]] = None) -> bool:\n        \"\"\"Check if contains index struct.\"\"\"\n        exclude_ids = exclude_ids or []\n        for doc in self.docs.values():\n            if isinstance(doc, IndexStruct) and doc.get_doc_id() not in exclude_ids:\n                return True\n        return False\n\n    @classmethod\n    def load_from_dict(\n        cls,\n        docs_dict: Dict[str, Any],\n        type_to_struct:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/docstore.py", "file_name": "docstore.py"}, "index": 9, "child_indices": [], "ref_doc_id": "523dc2d189441cf16be18c8f3896fae904dc3473", "node_info": null}, "10": {"text": "Any],\n        type_to_struct: Optional[Dict[str, Type[IndexStruct]]] = None,\n    ) -> \"DocumentStore\":\n        \"\"\"Load from dict.\"\"\"\n        docs_obj_dict = {}\n        for doc_id, doc_dict in docs_dict[\"docs\"].items():\n            doc_type = doc_dict.pop(TYPE_KEY, None)\n            if doc_type == \"Document\" or doc_type is None:\n                doc: DOC_TYPE = Document.from_dict(doc_dict)\n            else:\n                if type_to_struct is None:\n                    raise ValueError(\n                        \"type_to_struct must be provided if type is index struct.\"\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/docstore.py", "file_name": "docstore.py"}, "index": 10, "child_indices": [], "ref_doc_id": "523dc2d189441cf16be18c8f3896fae904dc3473", "node_info": null}, "11": {"text": "must be provided if type is index struct.\"\n                    )\n                # try using IndexStructType to retrieve documents\n                if doc_type not in type_to_struct:\n                    raise ValueError(\n                        f\"doc_type {doc_type} not found in type_to_struct. \"\n                        \"Make sure that it was registered in the index registry.\"\n                    )\n                doc = type_to_struct[doc_type].from_dict(doc_dict)\n                # doc = index_struct_cls.from_dict(doc_dict)\n        ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/docstore.py", "file_name": "docstore.py"}, "index": 11, "child_indices": [], "ref_doc_id": "523dc2d189441cf16be18c8f3896fae904dc3473", "node_info": null}, "12": {"text": "           docs_obj_dict[doc_id] = doc\n        return cls(docs=docs_obj_dict)\n\n    @classmethod\n    def from_documents(cls, docs: List[DOC_TYPE]) -> \"DocumentStore\":\n        \"\"\"Create from documents.\"\"\"\n        obj = cls()\n        obj.add_documents(docs)\n        return obj\n\n    def get_new_id(self) -> str:\n        \"\"\"Get a new ID.\"\"\"\n        return get_new_id(set(self.docs.keys()))\n\n    def update_docstore(self, other: \"DocumentStore\") -> None:\n        \"\"\"Update docstore.\"\"\"\n        self.docs.update(other.docs)\n\n    def add_documents(self, docs: List[DOC_TYPE], generate_id: bool = True) -> None:\n        \"\"\"Add a document to the store.\n\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/docstore.py", "file_name": "docstore.py"}, "index": 12, "child_indices": [], "ref_doc_id": "523dc2d189441cf16be18c8f3896fae904dc3473", "node_info": null}, "13": {"text": "     \"\"\"Add a document to the store.\n\n        If generate_id = True, then generate id for doc if doc_id doesn't exist.\n\n        \"\"\"\n        for doc in docs:\n            if doc.is_doc_id_none:\n                if generate_id:\n                    doc.doc_id = self.get_new_id()\n                else:\n                    raise ValueError(\n                        \"doc_id not set (to generate id, please set generate_id=True).\"\n                    )\n\n            # NOTE: doc could already exist in the store, but we overwrite it\n        ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/docstore.py", "file_name": "docstore.py"}, "index": 13, "child_indices": [], "ref_doc_id": "523dc2d189441cf16be18c8f3896fae904dc3473", "node_info": null}, "14": {"text": "store, but we overwrite it\n            self.docs[doc.get_doc_id()] = doc\n\n    def get_document(self, doc_id: str, raise_error: bool = True) -> Optional[DOC_TYPE]:\n        \"\"\"Get a document from the store.\"\"\"\n        doc = self.docs.get(doc_id, None)\n        if doc is None and raise_error:\n            raise ValueError(f\"doc_id {doc_id} not found.\")\n        return doc\n\n    def document_exists(self, doc_id: str) -> bool:\n        \"\"\"Check if document exists.\"\"\"\n        return doc_id in self.docs\n\n    def delete_document(\n        self, doc_id: str, raise_error: bool = True\n    ) -> Optional[DOC_TYPE]:\n        \"\"\"Delete a document from the store.\"\"\"\n        doc =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/docstore.py", "file_name": "docstore.py"}, "index": 14, "child_indices": [], "ref_doc_id": "523dc2d189441cf16be18c8f3896fae904dc3473", "node_info": null}, "15": {"text": "document from the store.\"\"\"\n        doc = self.docs.pop(doc_id, None)\n        if doc is None and raise_error:\n            raise ValueError(f\"doc_id {doc_id} not found.\")\n        return doc\n\n    def __len__(self) -> int:\n        \"\"\"Get length.\"\"\"\n        return len(self.docs.keys())\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/docstore.py", "file_name": "docstore.py"}, "index": 15, "child_indices": [], "ref_doc_id": "523dc2d189441cf16be18c8f3896fae904dc3473", "node_info": null}, "16": {"text": "\"\"\"Base schema for data structures.\"\"\"\nfrom abc import abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional\n\nfrom dataclasses_json import DataClassJsonMixin\n\n\n@dataclass\nclass BaseDocument(DataClassJsonMixin):\n    \"\"\"Base document.\n\n    Generic abstract interfaces that captures both index structs\n    as well as documents.\n\n    \"\"\"\n\n    # TODO: consolidate fields from Document/IndexStruct into base class\n    text: Optional[str] = None\n    doc_id: Optional[str] = None\n    embedding: Optional[List[float]] = None\n\n    # extra fields\n    extra_info: Optional[Dict[str, Any]] = None\n\n    @classmethod\n    @abstractmethod\n    def get_type(cls) -> str:\n        \"\"\"Get Document type.\"\"\"\n\n    def get_text(self) -> str:\n        \"\"\"Get text.\"\"\"\n        if self.text is None:\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/schema.py", "file_name": "schema.py"}, "index": 16, "child_indices": [], "ref_doc_id": "ec467e5a74f81e11b21a7965c94c1986708445d6", "node_info": null}, "17": {"text": "   if self.text is None:\n            raise ValueError(\"text field not set.\")\n        return self.text\n\n    def get_doc_id(self) -> str:\n        \"\"\"Get doc_id.\"\"\"\n        if self.doc_id is None:\n            raise ValueError(\"doc_id not set.\")\n        return self.doc_id\n\n    @property\n    def is_doc_id_none(self) -> bool:\n        \"\"\"Check if doc_id is None.\"\"\"\n        return self.doc_id is None\n\n    def get_embedding(self) -> List[float]:\n        \"\"\"Get embedding.\n\n        Errors if embedding is None.\n\n        \"\"\"\n        if self.embedding is None:\n            raise ValueError(\"embedding not set.\")\n        return self.embedding\n\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/schema.py", "file_name": "schema.py"}, "index": 17, "child_indices": [], "ref_doc_id": "ec467e5a74f81e11b21a7965c94c1986708445d6", "node_info": null}, "18": {"text": "set.\")\n        return self.embedding\n\n    @property\n    def extra_info_str(self) -> Optional[str]:\n        \"\"\"Extra info string.\"\"\"\n        if self.extra_info is None:\n            return None\n\n        return \"\\n\".join([f\"{k}: {str(v)}\" for k, v in self.extra_info.items()])\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/schema.py", "file_name": "schema.py"}, "index": 18, "child_indices": [], "ref_doc_id": "ec467e5a74f81e11b21a7965c94c1986708445d6", "node_info": null}, "19": {"text": "\"\"\"General utils functions.\"\"\"\n\nimport random\nimport sys\nimport time\nimport traceback\nimport uuid\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, Generator, List, Optional, Set, Type, cast\n\nimport nltk\n\n\nclass GlobalsHelper:\n    \"\"\"Helper to retrieve globals.\n\n    Helpful for global caching of certain variables that can be expensive to load.\n    (e.g. tokenization)\n\n    \"\"\"\n\n    _tokenizer: Optional[Callable[[str], List]] = None\n    _stopwords: Optional[List[str]] = None\n\n    @property\n    def tokenizer(self) -> Callable[[str], List]:\n        \"\"\"Get tokenizer.\"\"\"\n        if self._tokenizer is None:\n            # if python version >= 3.9, then use tiktoken\n            # else use GPT2TokenizerFast\n            if sys.version_info >= (3, 9):\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/utils.py", "file_name": "utils.py"}, "index": 19, "child_indices": [], "ref_doc_id": "c070558fc2f2babc0f4f950c573000ba295a80c7", "node_info": null}, "20": {"text": "  if sys.version_info >= (3, 9):\n                tiktoken_import_err = (\n                    \"`tiktoken` package not found, please run `pip install tiktoken`\"\n                )\n                try:\n                    import tiktoken\n                except ImportError:\n                    raise ValueError(tiktoken_import_err)\n                enc = tiktoken.get_encoding(\"gpt2\")\n                self._tokenizer = cast(Callable[[str], List], enc.encode)\n            else:\n                import", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/utils.py", "file_name": "utils.py"}, "index": 20, "child_indices": [], "ref_doc_id": "c070558fc2f2babc0f4f950c573000ba295a80c7", "node_info": null}, "21": {"text": "               import transformers\n\n                tokenizer = transformers.GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\n                def tokenizer_fn(text: str) -> List:\n                    return tokenizer(text)[\"input_ids\"]\n\n                self._tokenizer = tokenizer_fn\n        return self._tokenizer\n\n    @property\n    def stopwords(self) -> List[str]:\n        \"\"\"Get stopwords.\"\"\"\n        if self._stopwords is None:\n            try:\n                from nltk.corpus import stopwords\n            except ImportError:\n                raise ValueError(\n        ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/utils.py", "file_name": "utils.py"}, "index": 21, "child_indices": [], "ref_doc_id": "c070558fc2f2babc0f4f950c573000ba295a80c7", "node_info": null}, "22": {"text": "   raise ValueError(\n                    \"`nltk` package not found, please run `pip install nltk`\"\n                )\n            nltk.download(\"stopwords\")\n            self._stopwords = stopwords.words(\"english\")\n        return self._stopwords\n\n\nglobals_helper = GlobalsHelper()\n\n\ndef get_new_id(d: Set) -> str:\n    \"\"\"Get a new ID.\"\"\"\n    while True:\n        new_id = str(uuid.uuid4())\n        if new_id not in d:\n            break\n    return new_id\n\n\ndef get_new_int_id(d: Set) -> int:\n    \"\"\"Get a new integer ID.\"\"\"\n    while True:\n        new_id = random.randint(0, sys.maxsize)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/utils.py", "file_name": "utils.py"}, "index": 22, "child_indices": [], "ref_doc_id": "c070558fc2f2babc0f4f950c573000ba295a80c7", "node_info": null}, "23": {"text": "= random.randint(0, sys.maxsize)\n        if new_id not in d:\n            break\n    return new_id\n\n\n@contextmanager\ndef temp_set_attrs(obj: Any, **kwargs: Any) -> Generator:\n    \"\"\"Temporary setter.\n\n    Utility class for setting a temporary value for an attribute on a class.\n    Taken from: https://tinyurl.com/2p89xymh\n\n    \"\"\"\n    prev_values = {k: getattr(obj, k) for k in kwargs}\n    for k, v in kwargs.items():\n        setattr(obj, k, v)\n    try:\n        yield\n    finally:\n        for k, v in prev_values.items():\n            setattr(obj, k, v)\n\n\n@dataclass\nclass ErrorToRetry:\n    \"\"\"Exception types that should be retried.\n\n    Args:\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/utils.py", "file_name": "utils.py"}, "index": 23, "child_indices": [], "ref_doc_id": "c070558fc2f2babc0f4f950c573000ba295a80c7", "node_info": null}, "24": {"text": "retried.\n\n    Args:\n        exception_cls (Type[Exception]): Class of exception.\n        check_fn (Optional[Callable[[Any]], bool]]):\n            A function that takes an exception instance as input and returns\n            whether to retry.\n\n    \"\"\"\n\n    exception_cls: Type[Exception]\n    check_fn: Optional[Callable[[Any], bool]] = None\n\n\ndef retry_on_exceptions_with_backoff(\n    lambda_fn: Callable,\n    errors_to_retry: List[ErrorToRetry],\n    max_tries: int = 10,\n    min_backoff_secs: float = 0.5,\n    max_backoff_secs: float = 60.0,\n) -> Any:\n    \"\"\"Execute lambda function with retries and exponential backoff.\n\n    Args:\n        lambda_fn (Callable): Function to be called and output we want.\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/utils.py", "file_name": "utils.py"}, "index": 24, "child_indices": [], "ref_doc_id": "c070558fc2f2babc0f4f950c573000ba295a80c7", "node_info": null}, "25": {"text": "Function to be called and output we want.\n        errors_to_retry (List[ErrorToRetry]): List of errors to retry.\n            At least one needs to be provided.\n        max_tries (int): Maximum number of tries, including the first. Defaults to 10.\n        min_backoff_secs (float): Minimum amount of backoff time between attempts.\n            Defaults to 0.5.\n        max_backoff_secs (float): Maximum amount of backoff time between attempts.\n            Defaults to 60.\n\n    \"\"\"\n    if not errors_to_retry:\n        raise ValueError(\"At least one error to retry needs to be provided\")\n\n    error_checks = {\n        error_to_retry.exception_cls: error_to_retry.check_fn\n        for error_to_retry in errors_to_retry\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/utils.py", "file_name": "utils.py"}, "index": 25, "child_indices": [], "ref_doc_id": "c070558fc2f2babc0f4f950c573000ba295a80c7", "node_info": null}, "26": {"text": "for error_to_retry in errors_to_retry\n    }\n    exception_class_tuples = tuple(error_checks.keys())\n\n    backoff_secs = min_backoff_secs\n    tries = 0\n\n    while True:\n        try:\n            return lambda_fn()\n        except exception_class_tuples as e:\n            traceback.print_exc()\n            tries += 1\n            if tries >= max_tries:\n                raise\n            check_fn = error_checks.get(e.__class__)\n            if check_fn and not check_fn(e):\n                raise\n            time.sleep(backoff_secs)\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/utils.py", "file_name": "utils.py"}, "index": 26, "child_indices": [], "ref_doc_id": "c070558fc2f2babc0f4f950c573000ba295a80c7", "node_info": null}, "27": {"text": "           backoff_secs = min(backoff_secs * 2, max_backoff_secs)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/utils.py", "file_name": "utils.py"}, "index": 27, "child_indices": [], "ref_doc_id": "c070558fc2f2babc0f4f950c573000ba295a80c7", "node_info": null}, "28": {"text": "This code file is the __init__.py file for the GPT Index library. It contains the necessary imports and constants for the library, as well as the DocumentStore class which is used to store documents. It also contains the __all__ list which contains all the classes and functions that are part of the library. Finally, it contains the logging configuration for the library.", "doc_id": null, "embedding": null, "extra_info": null, "index": 28, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "29": {"text": "\nThe docstore.py file contains a DocumentStore class which provides methods for managing documents. It allows for the creation of a DocumentStore from a list of documents, the addition of documents to the store, the retrieval of documents from the store, the deletion of documents from the store, and the updating of the store with another DocumentStore. The schema.py file contains a BaseDocument class which provides generic abstract interfaces for capturing both index structs and documents. It provides methods for getting the text, doc_id, and embedding of a document, as well as a method for getting the extra info string. The utils.py file contains a GlobalsHelper class which provides methods for retrieving tokenizers and stopwords, a get_new_id() function for generating a new ID, a get_new_int_id() function for generating a new integer ID, and a temp_set_attrs() context manager for setting a temporary value for an attribute on a class. It also contains an ErrorToRetry dataclass which provides a list of exception types that should be retried.", "doc_id": null, "embedding": null, "extra_info": null, "index": 29, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "30": {"text": "This code file, utils.py, contains a function, retry_on_exceptions_with_backoff, which is used to execute a given lambda function with retries and exponential backoff. It takes in a list of errors to retry, a maximum number of tries, a minimum and maximum backoff time, and a lambda function as arguments. It then attempts to execute the lambda function, and if an exception is raised, it checks if it is one of the errors to retry and if so, retries the function with an exponentially increasing backoff time. If the maximum number of tries is exceeded, the exception is raised.", "doc_id": null, "embedding": null, "extra_info": null, "index": 30, "child_indices": [24, 25, 26, 27], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"28": {"text": "This code file is the __init__.py file for the GPT Index library. It contains the necessary imports and constants for the library, as well as the DocumentStore class which is used to store documents. It also contains the __all__ list which contains all the classes and functions that are part of the library. Finally, it contains the logging configuration for the library.", "doc_id": null, "embedding": null, "extra_info": null, "index": 28, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "29": {"text": "\nThe docstore.py file contains a DocumentStore class which provides methods for managing documents. It allows for the creation of a DocumentStore from a list of documents, the addition of documents to the store, the retrieval of documents from the store, the deletion of documents from the store, and the updating of the store with another DocumentStore. The schema.py file contains a BaseDocument class which provides generic abstract interfaces for capturing both index structs and documents. It provides methods for getting the text, doc_id, and embedding of a document, as well as a method for getting the extra info string. The utils.py file contains a GlobalsHelper class which provides methods for retrieving tokenizers and stopwords, a get_new_id() function for generating a new ID, a get_new_int_id() function for generating a new integer ID, and a temp_set_attrs() context manager for setting a temporary value for an attribute on a class. It also contains an ErrorToRetry dataclass which provides a list of exception types that should be retried.", "doc_id": null, "embedding": null, "extra_info": null, "index": 29, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "30": {"text": "This code file, utils.py, contains a function, retry_on_exceptions_with_backoff, which is used to execute a given lambda function with retries and exponential backoff. It takes in a list of errors to retry, a maximum number of tries, a minimum and maximum backoff time, and a lambda function as arguments. It then attempts to execute the lambda function, and if an exception is raised, it checks if it is one of the errors to retry and if so, retries the function with an exponentially increasing backoff time. If the maximum number of tries is exceeded, the exception is raised.", "doc_id": null, "embedding": null, "extra_info": null, "index": 30, "child_indices": [24, 25, 26, 27], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"267577d47e497a0630bc454b3f74c4fd9a10ced4": {"text": "0.4.1\n", "doc_id": "267577d47e497a0630bc454b3f74c4fd9a10ced4", "embedding": null, "extra_info": {"file_path": "gpt_index/VERSION", "file_name": "VERSION"}, "__type__": "Document"}, "0054ff473fdab69e9283aafee2e4579f884d9e7b": {"text": "\"\"\"Init file of GPT Index.\"\"\"\nfrom pathlib import Path\n\nwith open(Path(__file__).absolute().parents[0] / \"VERSION\") as _f:\n    __version__ = _f.read().strip()\n\n\nfrom gpt_index.data_structs.struct_type import IndexStructType\n\n# embeddings\nfrom gpt_index.embeddings.langchain import LangchainEmbedding\nfrom gpt_index.embeddings.openai import OpenAIEmbedding\n\n# structured\nfrom gpt_index.indices.common.struct_store.base import SQLContextBuilder\n\n# indices\nfrom gpt_index.indices.keyword_table import (\n    GPTKeywordTableIndex,\n    GPTRAKEKeywordTableIndex,\n    GPTSimpleKeywordTableIndex,\n)\nfrom gpt_index.indices.list import GPTListIndex\n\n# prompt helper\nfrom gpt_index.indices.prompt_helper import PromptHelper\n\n# for composability\nfrom gpt_index.indices.query.schema import QueryConfig, QueryMode\nfrom gpt_index.indices.struct_store.sql import GPTSQLStructStoreIndex\nfrom gpt_index.indices.tree import GPTTreeIndex\nfrom gpt_index.indices.vector_store import (\n    GPTFaissIndex,\n    GPTPineconeIndex,\n    GPTQdrantIndex,\n    GPTSimpleVectorIndex,\n    GPTWeaviateIndex,\n)\n\n# langchain helper\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.memory_wrapper import GPTIndexMemory\nfrom gpt_index.langchain_helpers.sql_wrapper import SQLDatabase\n\n# prompts\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.prompts.prompts import (\n    KeywordExtractPrompt,\n    QueryKeywordExtractPrompt,\n    QuestionAnswerPrompt,\n    RefinePrompt,\n    SummaryPrompt,\n    TreeInsertPrompt,\n    TreeSelectMultiplePrompt,\n    TreeSelectPrompt,\n)\n\n# readers\nfrom gpt_index.readers import (\n    BeautifulSoupWebReader,\n    DiscordReader,\n    Document,\n    FaissReader,\n    GithubRepositoryReader,\n    GoogleDocsReader,\n    MboxReader,\n    NotionPageReader,\n    ObsidianReader,\n    PineconeReader,\n    QdrantReader,\n    RssReader,\n    SimpleDirectoryReader,\n    SimpleMongoReader,\n    SimpleWebPageReader,\n    SlackReader,\n    StringIterableReader,\n    TrafilaturaWebReader,\n    TwitterTweetReader,\n    WeaviateReader,\n    WikipediaReader,\n)\nfrom gpt_index.readers.download import download_loader\n\n# token predictor\nfrom gpt_index.token_counter.mock_chain_wrapper import MockLLMPredictor\nfrom gpt_index.token_counter.mock_embed_model import MockEmbedding\n\n__all__ = [\n    \"GPTKeywordTableIndex\",\n    \"GPTSimpleKeywordTableIndex\",\n    \"GPTRAKEKeywordTableIndex\",\n    \"GPTListIndex\",\n    \"GPTTreeIndex\",\n    \"GPTFaissIndex\",\n    \"GPTSimpleVectorIndex\",\n    \"GPTWeaviateIndex\",\n    \"GPTPineconeIndex\",\n    \"GPTQdrantIndex\",\n    \"GPTSQLStructStoreIndex\",\n    \"Prompt\",\n    \"LangchainEmbedding\",\n    \"OpenAIEmbedding\",\n    \"SummaryPrompt\",\n    \"TreeInsertPrompt\",\n    \"TreeSelectPrompt\",\n    \"TreeSelectMultiplePrompt\",\n    \"RefinePrompt\",\n    \"QuestionAnswerPrompt\",\n    \"KeywordExtractPrompt\",\n    \"QueryKeywordExtractPrompt\",\n    \"WikipediaReader\",\n    \"ObsidianReader\",\n    \"Document\",\n    \"SimpleDirectoryReader\",\n    \"SimpleMongoReader\",\n    \"NotionPageReader\",\n    \"GoogleDocsReader\",\n    \"MboxReader\",\n    \"SlackReader\",\n    \"StringIterableReader\",\n    \"WeaviateReader\",\n    \"FaissReader\",\n    \"PineconeReader\",\n    \"QdrantReader\",\n    \"DiscordReader\",\n    \"SimpleWebPageReader\",\n    \"RssReader\",\n    \"BeautifulSoupWebReader\",\n    \"TrafilaturaWebReader\",\n    \"LLMPredictor\",\n    \"MockLLMPredictor\",\n    \"MockEmbedding\",\n    \"SQLDatabase\",\n    \"GPTIndexMemory\",\n    \"SQLContextBuilder\",\n    \"PromptHelper\",\n    \"QueryConfig\",\n    \"QueryMode\",\n    \"IndexStructType\",\n    \"TwitterTweetReader\",\n    \"download_loader\",\n    \"GithubRepositoryReader\",\n]\n\nimport logging\nfrom logging import NullHandler\n\n# best practices for library logging:\n# https://docs.python.org/3/howto/logging.html#configuring-logging-for-a-library\nlogging.getLogger(__name__).addHandler(NullHandler())\n", "doc_id": "0054ff473fdab69e9283aafee2e4579f884d9e7b", "embedding": null, "extra_info": {"file_path": "gpt_index/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "d619a2462ae933d7bdde127f86bde58dac29b79b": {"text": "\"\"\"Set of constants.\"\"\"\n\nMAX_CHUNK_SIZE = 3900\nMAX_CHUNK_OVERLAP = 200\nNUM_OUTPUTS = 256\n", "doc_id": "d619a2462ae933d7bdde127f86bde58dac29b79b", "embedding": null, "extra_info": {"file_path": "gpt_index/constants.py", "file_name": "constants.py"}, "__type__": "Document"}, "523dc2d189441cf16be18c8f3896fae904dc3473": {"text": "\"\"\"Document store.\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional, Type, Union\n\nfrom dataclasses_json import DataClassJsonMixin\n\nfrom gpt_index.data_structs.data_structs import IndexStruct\nfrom gpt_index.readers.schema.base import Document\nfrom gpt_index.utils import get_new_id\n\nDOC_TYPE = Union[IndexStruct, Document]\n\n# type key: used to store type of document\nTYPE_KEY = \"__type__\"\n\n\n@dataclass\nclass DocumentStore(DataClassJsonMixin):\n    \"\"\"Document store.\"\"\"\n\n    docs: Dict[str, DOC_TYPE] = field(default_factory=dict)\n\n    def serialize_to_dict(self) -> Dict[str, Any]:\n        \"\"\"Serialize to dict.\"\"\"\n        docs_dict = {}\n        for doc_id, doc in self.docs.items():\n            doc_dict = doc.to_dict()\n            doc_dict[TYPE_KEY] = doc.get_type()\n            docs_dict[doc_id] = doc_dict\n        return {\"docs\": docs_dict}\n\n    def contains_index_struct(self, exclude_ids: Optional[List[str]] = None) -> bool:\n        \"\"\"Check if contains index struct.\"\"\"\n        exclude_ids = exclude_ids or []\n        for doc in self.docs.values():\n            if isinstance(doc, IndexStruct) and doc.get_doc_id() not in exclude_ids:\n                return True\n        return False\n\n    @classmethod\n    def load_from_dict(\n        cls,\n        docs_dict: Dict[str, Any],\n        type_to_struct: Optional[Dict[str, Type[IndexStruct]]] = None,\n    ) -> \"DocumentStore\":\n        \"\"\"Load from dict.\"\"\"\n        docs_obj_dict = {}\n        for doc_id, doc_dict in docs_dict[\"docs\"].items():\n            doc_type = doc_dict.pop(TYPE_KEY, None)\n            if doc_type == \"Document\" or doc_type is None:\n                doc: DOC_TYPE = Document.from_dict(doc_dict)\n            else:\n                if type_to_struct is None:\n                    raise ValueError(\n                        \"type_to_struct must be provided if type is index struct.\"\n                    )\n                # try using IndexStructType to retrieve documents\n                if doc_type not in type_to_struct:\n                    raise ValueError(\n                        f\"doc_type {doc_type} not found in type_to_struct. \"\n                        \"Make sure that it was registered in the index registry.\"\n                    )\n                doc = type_to_struct[doc_type].from_dict(doc_dict)\n                # doc = index_struct_cls.from_dict(doc_dict)\n            docs_obj_dict[doc_id] = doc\n        return cls(docs=docs_obj_dict)\n\n    @classmethod\n    def from_documents(cls, docs: List[DOC_TYPE]) -> \"DocumentStore\":\n        \"\"\"Create from documents.\"\"\"\n        obj = cls()\n        obj.add_documents(docs)\n        return obj\n\n    def get_new_id(self) -> str:\n        \"\"\"Get a new ID.\"\"\"\n        return get_new_id(set(self.docs.keys()))\n\n    def update_docstore(self, other: \"DocumentStore\") -> None:\n        \"\"\"Update docstore.\"\"\"\n        self.docs.update(other.docs)\n\n    def add_documents(self, docs: List[DOC_TYPE], generate_id: bool = True) -> None:\n        \"\"\"Add a document to the store.\n\n        If generate_id = True, then generate id for doc if doc_id doesn't exist.\n\n        \"\"\"\n        for doc in docs:\n            if doc.is_doc_id_none:\n                if generate_id:\n                    doc.doc_id = self.get_new_id()\n                else:\n                    raise ValueError(\n                        \"doc_id not set (to generate id, please set generate_id=True).\"\n                    )\n\n            # NOTE: doc could already exist in the store, but we overwrite it\n            self.docs[doc.get_doc_id()] = doc\n\n    def get_document(self, doc_id: str, raise_error: bool = True) -> Optional[DOC_TYPE]:\n        \"\"\"Get a document from the store.\"\"\"\n        doc = self.docs.get(doc_id, None)\n        if doc is None and raise_error:\n            raise ValueError(f\"doc_id {doc_id} not found.\")\n        return doc\n\n    def document_exists(self, doc_id: str) -> bool:\n        \"\"\"Check if document exists.\"\"\"\n        return doc_id in self.docs\n\n    def delete_document(\n        self, doc_id: str, raise_error: bool = True\n    ) -> Optional[DOC_TYPE]:\n        \"\"\"Delete a document from the store.\"\"\"\n        doc = self.docs.pop(doc_id, None)\n        if doc is None and raise_error:\n            raise ValueError(f\"doc_id {doc_id} not found.\")\n        return doc\n\n    def __len__(self) -> int:\n        \"\"\"Get length.\"\"\"\n        return len(self.docs.keys())\n", "doc_id": "523dc2d189441cf16be18c8f3896fae904dc3473", "embedding": null, "extra_info": {"file_path": "gpt_index/docstore.py", "file_name": "docstore.py"}, "__type__": "Document"}, "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391": {"text": "", "doc_id": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391", "embedding": null, "extra_info": {"file_path": "gpt_index/py.typed", "file_name": "py.typed"}, "__type__": "Document"}, "ec467e5a74f81e11b21a7965c94c1986708445d6": {"text": "\"\"\"Base schema for data structures.\"\"\"\nfrom abc import abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional\n\nfrom dataclasses_json import DataClassJsonMixin\n\n\n@dataclass\nclass BaseDocument(DataClassJsonMixin):\n    \"\"\"Base document.\n\n    Generic abstract interfaces that captures both index structs\n    as well as documents.\n\n    \"\"\"\n\n    # TODO: consolidate fields from Document/IndexStruct into base class\n    text: Optional[str] = None\n    doc_id: Optional[str] = None\n    embedding: Optional[List[float]] = None\n\n    # extra fields\n    extra_info: Optional[Dict[str, Any]] = None\n\n    @classmethod\n    @abstractmethod\n    def get_type(cls) -> str:\n        \"\"\"Get Document type.\"\"\"\n\n    def get_text(self) -> str:\n        \"\"\"Get text.\"\"\"\n        if self.text is None:\n            raise ValueError(\"text field not set.\")\n        return self.text\n\n    def get_doc_id(self) -> str:\n        \"\"\"Get doc_id.\"\"\"\n        if self.doc_id is None:\n            raise ValueError(\"doc_id not set.\")\n        return self.doc_id\n\n    @property\n    def is_doc_id_none(self) -> bool:\n        \"\"\"Check if doc_id is None.\"\"\"\n        return self.doc_id is None\n\n    def get_embedding(self) -> List[float]:\n        \"\"\"Get embedding.\n\n        Errors if embedding is None.\n\n        \"\"\"\n        if self.embedding is None:\n            raise ValueError(\"embedding not set.\")\n        return self.embedding\n\n    @property\n    def extra_info_str(self) -> Optional[str]:\n        \"\"\"Extra info string.\"\"\"\n        if self.extra_info is None:\n            return None\n\n        return \"\\n\".join([f\"{k}: {str(v)}\" for k, v in self.extra_info.items()])\n", "doc_id": "ec467e5a74f81e11b21a7965c94c1986708445d6", "embedding": null, "extra_info": {"file_path": "gpt_index/schema.py", "file_name": "schema.py"}, "__type__": "Document"}, "c070558fc2f2babc0f4f950c573000ba295a80c7": {"text": "\"\"\"General utils functions.\"\"\"\n\nimport random\nimport sys\nimport time\nimport traceback\nimport uuid\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, Generator, List, Optional, Set, Type, cast\n\nimport nltk\n\n\nclass GlobalsHelper:\n    \"\"\"Helper to retrieve globals.\n\n    Helpful for global caching of certain variables that can be expensive to load.\n    (e.g. tokenization)\n\n    \"\"\"\n\n    _tokenizer: Optional[Callable[[str], List]] = None\n    _stopwords: Optional[List[str]] = None\n\n    @property\n    def tokenizer(self) -> Callable[[str], List]:\n        \"\"\"Get tokenizer.\"\"\"\n        if self._tokenizer is None:\n            # if python version >= 3.9, then use tiktoken\n            # else use GPT2TokenizerFast\n            if sys.version_info >= (3, 9):\n                tiktoken_import_err = (\n                    \"`tiktoken` package not found, please run `pip install tiktoken`\"\n                )\n                try:\n                    import tiktoken\n                except ImportError:\n                    raise ValueError(tiktoken_import_err)\n                enc = tiktoken.get_encoding(\"gpt2\")\n                self._tokenizer = cast(Callable[[str], List], enc.encode)\n            else:\n                import transformers\n\n                tokenizer = transformers.GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\n                def tokenizer_fn(text: str) -> List:\n                    return tokenizer(text)[\"input_ids\"]\n\n                self._tokenizer = tokenizer_fn\n        return self._tokenizer\n\n    @property\n    def stopwords(self) -> List[str]:\n        \"\"\"Get stopwords.\"\"\"\n        if self._stopwords is None:\n            try:\n                from nltk.corpus import stopwords\n            except ImportError:\n                raise ValueError(\n                    \"`nltk` package not found, please run `pip install nltk`\"\n                )\n            nltk.download(\"stopwords\")\n            self._stopwords = stopwords.words(\"english\")\n        return self._stopwords\n\n\nglobals_helper = GlobalsHelper()\n\n\ndef get_new_id(d: Set) -> str:\n    \"\"\"Get a new ID.\"\"\"\n    while True:\n        new_id = str(uuid.uuid4())\n        if new_id not in d:\n            break\n    return new_id\n\n\ndef get_new_int_id(d: Set) -> int:\n    \"\"\"Get a new integer ID.\"\"\"\n    while True:\n        new_id = random.randint(0, sys.maxsize)\n        if new_id not in d:\n            break\n    return new_id\n\n\n@contextmanager\ndef temp_set_attrs(obj: Any, **kwargs: Any) -> Generator:\n    \"\"\"Temporary setter.\n\n    Utility class for setting a temporary value for an attribute on a class.\n    Taken from: https://tinyurl.com/2p89xymh\n\n    \"\"\"\n    prev_values = {k: getattr(obj, k) for k in kwargs}\n    for k, v in kwargs.items():\n        setattr(obj, k, v)\n    try:\n        yield\n    finally:\n        for k, v in prev_values.items():\n            setattr(obj, k, v)\n\n\n@dataclass\nclass ErrorToRetry:\n    \"\"\"Exception types that should be retried.\n\n    Args:\n        exception_cls (Type[Exception]): Class of exception.\n        check_fn (Optional[Callable[[Any]], bool]]):\n            A function that takes an exception instance as input and returns\n            whether to retry.\n\n    \"\"\"\n\n    exception_cls: Type[Exception]\n    check_fn: Optional[Callable[[Any], bool]] = None\n\n\ndef retry_on_exceptions_with_backoff(\n    lambda_fn: Callable,\n    errors_to_retry: List[ErrorToRetry],\n    max_tries: int = 10,\n    min_backoff_secs: float = 0.5,\n    max_backoff_secs: float = 60.0,\n) -> Any:\n    \"\"\"Execute lambda function with retries and exponential backoff.\n\n    Args:\n        lambda_fn (Callable): Function to be called and output we want.\n        errors_to_retry (List[ErrorToRetry]): List of errors to retry.\n            At least one needs to be provided.\n        max_tries (int): Maximum number of tries, including the first. Defaults to 10.\n        min_backoff_secs (float): Minimum amount of backoff time between attempts.\n            Defaults to 0.5.\n        max_backoff_secs (float): Maximum amount of backoff time between attempts.\n            Defaults to 60.\n\n    \"\"\"\n    if not errors_to_retry:\n        raise ValueError(\"At least one error to retry needs to be provided\")\n\n    error_checks = {\n        error_to_retry.exception_cls: error_to_retry.check_fn\n        for error_to_retry in errors_to_retry\n    }\n    exception_class_tuples = tuple(error_checks.keys())\n\n    backoff_secs = min_backoff_secs\n    tries = 0\n\n    while True:\n        try:\n            return lambda_fn()\n        except exception_class_tuples as e:\n            traceback.print_exc()\n            tries += 1\n            if tries >= max_tries:\n                raise\n            check_fn = error_checks.get(e.__class__)\n            if check_fn and not check_fn(e):\n                raise\n            time.sleep(backoff_secs)\n            backoff_secs = min(backoff_secs * 2, max_backoff_secs)\n", "doc_id": "c070558fc2f2babc0f4f950c573000ba295a80c7", "embedding": null, "extra_info": {"file_path": "gpt_index/utils.py", "file_name": "utils.py"}, "__type__": "Document"}, "1801dcbd-c19e-4492-ba4d-5765d27eb9eb": {"text": "\nThe __init__.py file for the GPT Index library contains the necessary imports and constants, the DocumentStore class for storing documents, and the __all__ list of classes and functions. The docstore.py file contains the DocumentStore class for managing documents. The schema.py file contains the BaseDocument class for capturing index structs and documents. The utils.py file contains the GlobalsHelper class for retrieving tokenizers and stopwords, the get_new_id() and get_new_int_id() functions for generating IDs, and the temp_set_attrs() context manager for setting temporary values for attributes. It also contains the retry_on_exceptions_with_backoff() function for executing a lambda function with retries and exponential backoff.", "doc_id": "1801dcbd-c19e-4492-ba4d-5765d27eb9eb", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "0.4.1\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/VERSION", "file_name": "VERSION"}, "index": 0, "child_indices": [], "ref_doc_id": "267577d47e497a0630bc454b3f74c4fd9a10ced4", "node_info": null}, "1": {"text": "\"\"\"Init file of GPT Index.\"\"\"\nfrom pathlib import Path\n\nwith open(Path(__file__).absolute().parents[0] / \"VERSION\") as _f:\n    __version__ = _f.read().strip()\n\n\nfrom gpt_index.data_structs.struct_type import IndexStructType\n\n# embeddings\nfrom gpt_index.embeddings.langchain import LangchainEmbedding\nfrom gpt_index.embeddings.openai import OpenAIEmbedding\n\n# structured\nfrom gpt_index.indices.common.struct_store.base import SQLContextBuilder\n\n# indices\nfrom gpt_index.indices.keyword_table import (\n    GPTKeywordTableIndex,\n    GPTRAKEKeywordTableIndex,\n    GPTSimpleKeywordTableIndex,\n)\nfrom gpt_index.indices.list import GPTListIndex\n\n# prompt helper\nfrom gpt_index.indices.prompt_helper import PromptHelper\n\n# for composability\nfrom gpt_index.indices.query.schema import QueryConfig, QueryMode\nfrom", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/__init__.py", "file_name": "__init__.py"}, "index": 1, "child_indices": [], "ref_doc_id": "0054ff473fdab69e9283aafee2e4579f884d9e7b", "node_info": null}, "2": {"text": "import QueryConfig, QueryMode\nfrom gpt_index.indices.struct_store.sql import GPTSQLStructStoreIndex\nfrom gpt_index.indices.tree import GPTTreeIndex\nfrom gpt_index.indices.vector_store import (\n    GPTFaissIndex,\n    GPTPineconeIndex,\n    GPTQdrantIndex,\n    GPTSimpleVectorIndex,\n    GPTWeaviateIndex,\n)\n\n# langchain helper\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.memory_wrapper import GPTIndexMemory\nfrom gpt_index.langchain_helpers.sql_wrapper import SQLDatabase\n\n# prompts\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.prompts.prompts import (\n    KeywordExtractPrompt,\n    QueryKeywordExtractPrompt,\n    QuestionAnswerPrompt,\n    RefinePrompt,\n    SummaryPrompt,\n    TreeInsertPrompt,\n    TreeSelectMultiplePrompt,\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/__init__.py", "file_name": "__init__.py"}, "index": 2, "child_indices": [], "ref_doc_id": "0054ff473fdab69e9283aafee2e4579f884d9e7b", "node_info": null}, "3": {"text": "TreeInsertPrompt,\n    TreeSelectMultiplePrompt,\n    TreeSelectPrompt,\n)\n\n# readers\nfrom gpt_index.readers import (\n    BeautifulSoupWebReader,\n    DiscordReader,\n    Document,\n    FaissReader,\n    GithubRepositoryReader,\n    GoogleDocsReader,\n    MboxReader,\n    NotionPageReader,\n    ObsidianReader,\n    PineconeReader,\n    QdrantReader,\n    RssReader,\n    SimpleDirectoryReader,\n    SimpleMongoReader,\n    SimpleWebPageReader,\n    SlackReader,\n    StringIterableReader,\n    TrafilaturaWebReader,\n    TwitterTweetReader,\n    WeaviateReader,\n    WikipediaReader,\n)\nfrom gpt_index.readers.download import download_loader\n\n# token predictor\nfrom gpt_index.token_counter.mock_chain_wrapper import MockLLMPredictor\nfrom gpt_index.token_counter.mock_embed_model", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/__init__.py", "file_name": "__init__.py"}, "index": 3, "child_indices": [], "ref_doc_id": "0054ff473fdab69e9283aafee2e4579f884d9e7b", "node_info": null}, "4": {"text": "gpt_index.token_counter.mock_embed_model import MockEmbedding\n\n__all__ = [\n    \"GPTKeywordTableIndex\",\n    \"GPTSimpleKeywordTableIndex\",\n    \"GPTRAKEKeywordTableIndex\",\n    \"GPTListIndex\",\n    \"GPTTreeIndex\",\n    \"GPTFaissIndex\",\n    \"GPTSimpleVectorIndex\",\n    \"GPTWeaviateIndex\",\n    \"GPTPineconeIndex\",\n    \"GPTQdrantIndex\",\n    \"GPTSQLStructStoreIndex\",\n    \"Prompt\",\n    \"LangchainEmbedding\",\n    \"OpenAIEmbedding\",\n    \"SummaryPrompt\",\n    \"TreeInsertPrompt\",\n    \"TreeSelectPrompt\",\n    \"TreeSelectMultiplePrompt\",\n    \"RefinePrompt\",\n    \"QuestionAnswerPrompt\",\n    \"KeywordExtractPrompt\",\n    \"QueryKeywordExtractPrompt\",\n    \"WikipediaReader\",\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/__init__.py", "file_name": "__init__.py"}, "index": 4, "child_indices": [], "ref_doc_id": "0054ff473fdab69e9283aafee2e4579f884d9e7b", "node_info": null}, "5": {"text": "   \"WikipediaReader\",\n    \"ObsidianReader\",\n    \"Document\",\n    \"SimpleDirectoryReader\",\n    \"SimpleMongoReader\",\n    \"NotionPageReader\",\n    \"GoogleDocsReader\",\n    \"MboxReader\",\n    \"SlackReader\",\n    \"StringIterableReader\",\n    \"WeaviateReader\",\n    \"FaissReader\",\n    \"PineconeReader\",\n    \"QdrantReader\",\n    \"DiscordReader\",\n    \"SimpleWebPageReader\",\n    \"RssReader\",\n    \"BeautifulSoupWebReader\",\n    \"TrafilaturaWebReader\",\n    \"LLMPredictor\",\n    \"MockLLMPredictor\",\n    \"MockEmbedding\",\n    \"SQLDatabase\",\n    \"GPTIndexMemory\",\n    \"SQLContextBuilder\",\n    \"PromptHelper\",\n    \"QueryConfig\",\n    \"QueryMode\",\n    \"IndexStructType\",\n    \"TwitterTweetReader\",\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/__init__.py", "file_name": "__init__.py"}, "index": 5, "child_indices": [], "ref_doc_id": "0054ff473fdab69e9283aafee2e4579f884d9e7b", "node_info": null}, "6": {"text": "   \"TwitterTweetReader\",\n    \"download_loader\",\n    \"GithubRepositoryReader\",\n]\n\nimport logging\nfrom logging import NullHandler\n\n# best practices for library logging:\n# https://docs.python.org/3/howto/logging.html#configuring-logging-for-a-library\nlogging.getLogger(__name__).addHandler(NullHandler())\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/__init__.py", "file_name": "__init__.py"}, "index": 6, "child_indices": [], "ref_doc_id": "0054ff473fdab69e9283aafee2e4579f884d9e7b", "node_info": null}, "7": {"text": "\"\"\"Set of constants.\"\"\"\n\nMAX_CHUNK_SIZE = 3900\nMAX_CHUNK_OVERLAP = 200\nNUM_OUTPUTS = 256\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/constants.py", "file_name": "constants.py"}, "index": 7, "child_indices": [], "ref_doc_id": "d619a2462ae933d7bdde127f86bde58dac29b79b", "node_info": null}, "8": {"text": "\"\"\"Document store.\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional, Type, Union\n\nfrom dataclasses_json import DataClassJsonMixin\n\nfrom gpt_index.data_structs.data_structs import IndexStruct\nfrom gpt_index.readers.schema.base import Document\nfrom gpt_index.utils import get_new_id\n\nDOC_TYPE = Union[IndexStruct, Document]\n\n# type key: used to store type of document\nTYPE_KEY = \"__type__\"\n\n\n@dataclass\nclass DocumentStore(DataClassJsonMixin):\n    \"\"\"Document store.\"\"\"\n\n    docs: Dict[str, DOC_TYPE] = field(default_factory=dict)\n\n    def serialize_to_dict(self) -> Dict[str, Any]:\n        \"\"\"Serialize to dict.\"\"\"\n        docs_dict = {}\n        for doc_id, doc in self.docs.items():\n            doc_dict = doc.to_dict()\n         ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/docstore.py", "file_name": "docstore.py"}, "index": 8, "child_indices": [], "ref_doc_id": "523dc2d189441cf16be18c8f3896fae904dc3473", "node_info": null}, "9": {"text": "doc.to_dict()\n            doc_dict[TYPE_KEY] = doc.get_type()\n            docs_dict[doc_id] = doc_dict\n        return {\"docs\": docs_dict}\n\n    def contains_index_struct(self, exclude_ids: Optional[List[str]] = None) -> bool:\n        \"\"\"Check if contains index struct.\"\"\"\n        exclude_ids = exclude_ids or []\n        for doc in self.docs.values():\n            if isinstance(doc, IndexStruct) and doc.get_doc_id() not in exclude_ids:\n                return True\n        return False\n\n    @classmethod\n    def load_from_dict(\n        cls,\n        docs_dict: Dict[str, Any],\n        type_to_struct:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/docstore.py", "file_name": "docstore.py"}, "index": 9, "child_indices": [], "ref_doc_id": "523dc2d189441cf16be18c8f3896fae904dc3473", "node_info": null}, "10": {"text": "Any],\n        type_to_struct: Optional[Dict[str, Type[IndexStruct]]] = None,\n    ) -> \"DocumentStore\":\n        \"\"\"Load from dict.\"\"\"\n        docs_obj_dict = {}\n        for doc_id, doc_dict in docs_dict[\"docs\"].items():\n            doc_type = doc_dict.pop(TYPE_KEY, None)\n            if doc_type == \"Document\" or doc_type is None:\n                doc: DOC_TYPE = Document.from_dict(doc_dict)\n            else:\n                if type_to_struct is None:\n                    raise ValueError(\n                        \"type_to_struct must be provided if type is index struct.\"\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/docstore.py", "file_name": "docstore.py"}, "index": 10, "child_indices": [], "ref_doc_id": "523dc2d189441cf16be18c8f3896fae904dc3473", "node_info": null}, "11": {"text": "must be provided if type is index struct.\"\n                    )\n                # try using IndexStructType to retrieve documents\n                if doc_type not in type_to_struct:\n                    raise ValueError(\n                        f\"doc_type {doc_type} not found in type_to_struct. \"\n                        \"Make sure that it was registered in the index registry.\"\n                    )\n                doc = type_to_struct[doc_type].from_dict(doc_dict)\n                # doc = index_struct_cls.from_dict(doc_dict)\n        ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/docstore.py", "file_name": "docstore.py"}, "index": 11, "child_indices": [], "ref_doc_id": "523dc2d189441cf16be18c8f3896fae904dc3473", "node_info": null}, "12": {"text": "           docs_obj_dict[doc_id] = doc\n        return cls(docs=docs_obj_dict)\n\n    @classmethod\n    def from_documents(cls, docs: List[DOC_TYPE]) -> \"DocumentStore\":\n        \"\"\"Create from documents.\"\"\"\n        obj = cls()\n        obj.add_documents(docs)\n        return obj\n\n    def get_new_id(self) -> str:\n        \"\"\"Get a new ID.\"\"\"\n        return get_new_id(set(self.docs.keys()))\n\n    def update_docstore(self, other: \"DocumentStore\") -> None:\n        \"\"\"Update docstore.\"\"\"\n        self.docs.update(other.docs)\n\n    def add_documents(self, docs: List[DOC_TYPE], generate_id: bool = True) -> None:\n        \"\"\"Add a document to the store.\n\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/docstore.py", "file_name": "docstore.py"}, "index": 12, "child_indices": [], "ref_doc_id": "523dc2d189441cf16be18c8f3896fae904dc3473", "node_info": null}, "13": {"text": "     \"\"\"Add a document to the store.\n\n        If generate_id = True, then generate id for doc if doc_id doesn't exist.\n\n        \"\"\"\n        for doc in docs:\n            if doc.is_doc_id_none:\n                if generate_id:\n                    doc.doc_id = self.get_new_id()\n                else:\n                    raise ValueError(\n                        \"doc_id not set (to generate id, please set generate_id=True).\"\n                    )\n\n            # NOTE: doc could already exist in the store, but we overwrite it\n        ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/docstore.py", "file_name": "docstore.py"}, "index": 13, "child_indices": [], "ref_doc_id": "523dc2d189441cf16be18c8f3896fae904dc3473", "node_info": null}, "14": {"text": "store, but we overwrite it\n            self.docs[doc.get_doc_id()] = doc\n\n    def get_document(self, doc_id: str, raise_error: bool = True) -> Optional[DOC_TYPE]:\n        \"\"\"Get a document from the store.\"\"\"\n        doc = self.docs.get(doc_id, None)\n        if doc is None and raise_error:\n            raise ValueError(f\"doc_id {doc_id} not found.\")\n        return doc\n\n    def document_exists(self, doc_id: str) -> bool:\n        \"\"\"Check if document exists.\"\"\"\n        return doc_id in self.docs\n\n    def delete_document(\n        self, doc_id: str, raise_error: bool = True\n    ) -> Optional[DOC_TYPE]:\n        \"\"\"Delete a document from the store.\"\"\"\n        doc =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/docstore.py", "file_name": "docstore.py"}, "index": 14, "child_indices": [], "ref_doc_id": "523dc2d189441cf16be18c8f3896fae904dc3473", "node_info": null}, "15": {"text": "document from the store.\"\"\"\n        doc = self.docs.pop(doc_id, None)\n        if doc is None and raise_error:\n            raise ValueError(f\"doc_id {doc_id} not found.\")\n        return doc\n\n    def __len__(self) -> int:\n        \"\"\"Get length.\"\"\"\n        return len(self.docs.keys())\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/docstore.py", "file_name": "docstore.py"}, "index": 15, "child_indices": [], "ref_doc_id": "523dc2d189441cf16be18c8f3896fae904dc3473", "node_info": null}, "16": {"text": "\"\"\"Base schema for data structures.\"\"\"\nfrom abc import abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional\n\nfrom dataclasses_json import DataClassJsonMixin\n\n\n@dataclass\nclass BaseDocument(DataClassJsonMixin):\n    \"\"\"Base document.\n\n    Generic abstract interfaces that captures both index structs\n    as well as documents.\n\n    \"\"\"\n\n    # TODO: consolidate fields from Document/IndexStruct into base class\n    text: Optional[str] = None\n    doc_id: Optional[str] = None\n    embedding: Optional[List[float]] = None\n\n    # extra fields\n    extra_info: Optional[Dict[str, Any]] = None\n\n    @classmethod\n    @abstractmethod\n    def get_type(cls) -> str:\n        \"\"\"Get Document type.\"\"\"\n\n    def get_text(self) -> str:\n        \"\"\"Get text.\"\"\"\n        if self.text is None:\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/schema.py", "file_name": "schema.py"}, "index": 16, "child_indices": [], "ref_doc_id": "ec467e5a74f81e11b21a7965c94c1986708445d6", "node_info": null}, "17": {"text": "   if self.text is None:\n            raise ValueError(\"text field not set.\")\n        return self.text\n\n    def get_doc_id(self) -> str:\n        \"\"\"Get doc_id.\"\"\"\n        if self.doc_id is None:\n            raise ValueError(\"doc_id not set.\")\n        return self.doc_id\n\n    @property\n    def is_doc_id_none(self) -> bool:\n        \"\"\"Check if doc_id is None.\"\"\"\n        return self.doc_id is None\n\n    def get_embedding(self) -> List[float]:\n        \"\"\"Get embedding.\n\n        Errors if embedding is None.\n\n        \"\"\"\n        if self.embedding is None:\n            raise ValueError(\"embedding not set.\")\n        return self.embedding\n\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/schema.py", "file_name": "schema.py"}, "index": 17, "child_indices": [], "ref_doc_id": "ec467e5a74f81e11b21a7965c94c1986708445d6", "node_info": null}, "18": {"text": "set.\")\n        return self.embedding\n\n    @property\n    def extra_info_str(self) -> Optional[str]:\n        \"\"\"Extra info string.\"\"\"\n        if self.extra_info is None:\n            return None\n\n        return \"\\n\".join([f\"{k}: {str(v)}\" for k, v in self.extra_info.items()])\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/schema.py", "file_name": "schema.py"}, "index": 18, "child_indices": [], "ref_doc_id": "ec467e5a74f81e11b21a7965c94c1986708445d6", "node_info": null}, "19": {"text": "\"\"\"General utils functions.\"\"\"\n\nimport random\nimport sys\nimport time\nimport traceback\nimport uuid\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, Generator, List, Optional, Set, Type, cast\n\nimport nltk\n\n\nclass GlobalsHelper:\n    \"\"\"Helper to retrieve globals.\n\n    Helpful for global caching of certain variables that can be expensive to load.\n    (e.g. tokenization)\n\n    \"\"\"\n\n    _tokenizer: Optional[Callable[[str], List]] = None\n    _stopwords: Optional[List[str]] = None\n\n    @property\n    def tokenizer(self) -> Callable[[str], List]:\n        \"\"\"Get tokenizer.\"\"\"\n        if self._tokenizer is None:\n            # if python version >= 3.9, then use tiktoken\n            # else use GPT2TokenizerFast\n            if sys.version_info >= (3, 9):\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/utils.py", "file_name": "utils.py"}, "index": 19, "child_indices": [], "ref_doc_id": "c070558fc2f2babc0f4f950c573000ba295a80c7", "node_info": null}, "20": {"text": "  if sys.version_info >= (3, 9):\n                tiktoken_import_err = (\n                    \"`tiktoken` package not found, please run `pip install tiktoken`\"\n                )\n                try:\n                    import tiktoken\n                except ImportError:\n                    raise ValueError(tiktoken_import_err)\n                enc = tiktoken.get_encoding(\"gpt2\")\n                self._tokenizer = cast(Callable[[str], List], enc.encode)\n            else:\n                import", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/utils.py", "file_name": "utils.py"}, "index": 20, "child_indices": [], "ref_doc_id": "c070558fc2f2babc0f4f950c573000ba295a80c7", "node_info": null}, "21": {"text": "               import transformers\n\n                tokenizer = transformers.GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\n                def tokenizer_fn(text: str) -> List:\n                    return tokenizer(text)[\"input_ids\"]\n\n                self._tokenizer = tokenizer_fn\n        return self._tokenizer\n\n    @property\n    def stopwords(self) -> List[str]:\n        \"\"\"Get stopwords.\"\"\"\n        if self._stopwords is None:\n            try:\n                from nltk.corpus import stopwords\n            except ImportError:\n                raise ValueError(\n        ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/utils.py", "file_name": "utils.py"}, "index": 21, "child_indices": [], "ref_doc_id": "c070558fc2f2babc0f4f950c573000ba295a80c7", "node_info": null}, "22": {"text": "   raise ValueError(\n                    \"`nltk` package not found, please run `pip install nltk`\"\n                )\n            nltk.download(\"stopwords\")\n            self._stopwords = stopwords.words(\"english\")\n        return self._stopwords\n\n\nglobals_helper = GlobalsHelper()\n\n\ndef get_new_id(d: Set) -> str:\n    \"\"\"Get a new ID.\"\"\"\n    while True:\n        new_id = str(uuid.uuid4())\n        if new_id not in d:\n            break\n    return new_id\n\n\ndef get_new_int_id(d: Set) -> int:\n    \"\"\"Get a new integer ID.\"\"\"\n    while True:\n        new_id = random.randint(0, sys.maxsize)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/utils.py", "file_name": "utils.py"}, "index": 22, "child_indices": [], "ref_doc_id": "c070558fc2f2babc0f4f950c573000ba295a80c7", "node_info": null}, "23": {"text": "= random.randint(0, sys.maxsize)\n        if new_id not in d:\n            break\n    return new_id\n\n\n@contextmanager\ndef temp_set_attrs(obj: Any, **kwargs: Any) -> Generator:\n    \"\"\"Temporary setter.\n\n    Utility class for setting a temporary value for an attribute on a class.\n    Taken from: https://tinyurl.com/2p89xymh\n\n    \"\"\"\n    prev_values = {k: getattr(obj, k) for k in kwargs}\n    for k, v in kwargs.items():\n        setattr(obj, k, v)\n    try:\n        yield\n    finally:\n        for k, v in prev_values.items():\n            setattr(obj, k, v)\n\n\n@dataclass\nclass ErrorToRetry:\n    \"\"\"Exception types that should be retried.\n\n    Args:\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/utils.py", "file_name": "utils.py"}, "index": 23, "child_indices": [], "ref_doc_id": "c070558fc2f2babc0f4f950c573000ba295a80c7", "node_info": null}, "24": {"text": "retried.\n\n    Args:\n        exception_cls (Type[Exception]): Class of exception.\n        check_fn (Optional[Callable[[Any]], bool]]):\n            A function that takes an exception instance as input and returns\n            whether to retry.\n\n    \"\"\"\n\n    exception_cls: Type[Exception]\n    check_fn: Optional[Callable[[Any], bool]] = None\n\n\ndef retry_on_exceptions_with_backoff(\n    lambda_fn: Callable,\n    errors_to_retry: List[ErrorToRetry],\n    max_tries: int = 10,\n    min_backoff_secs: float = 0.5,\n    max_backoff_secs: float = 60.0,\n) -> Any:\n    \"\"\"Execute lambda function with retries and exponential backoff.\n\n    Args:\n        lambda_fn (Callable): Function to be called and output we want.\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/utils.py", "file_name": "utils.py"}, "index": 24, "child_indices": [], "ref_doc_id": "c070558fc2f2babc0f4f950c573000ba295a80c7", "node_info": null}, "25": {"text": "Function to be called and output we want.\n        errors_to_retry (List[ErrorToRetry]): List of errors to retry.\n            At least one needs to be provided.\n        max_tries (int): Maximum number of tries, including the first. Defaults to 10.\n        min_backoff_secs (float): Minimum amount of backoff time between attempts.\n            Defaults to 0.5.\n        max_backoff_secs (float): Maximum amount of backoff time between attempts.\n            Defaults to 60.\n\n    \"\"\"\n    if not errors_to_retry:\n        raise ValueError(\"At least one error to retry needs to be provided\")\n\n    error_checks = {\n        error_to_retry.exception_cls: error_to_retry.check_fn\n        for error_to_retry in errors_to_retry\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/utils.py", "file_name": "utils.py"}, "index": 25, "child_indices": [], "ref_doc_id": "c070558fc2f2babc0f4f950c573000ba295a80c7", "node_info": null}, "26": {"text": "for error_to_retry in errors_to_retry\n    }\n    exception_class_tuples = tuple(error_checks.keys())\n\n    backoff_secs = min_backoff_secs\n    tries = 0\n\n    while True:\n        try:\n            return lambda_fn()\n        except exception_class_tuples as e:\n            traceback.print_exc()\n            tries += 1\n            if tries >= max_tries:\n                raise\n            check_fn = error_checks.get(e.__class__)\n            if check_fn and not check_fn(e):\n                raise\n            time.sleep(backoff_secs)\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/utils.py", "file_name": "utils.py"}, "index": 26, "child_indices": [], "ref_doc_id": "c070558fc2f2babc0f4f950c573000ba295a80c7", "node_info": null}, "27": {"text": "           backoff_secs = min(backoff_secs * 2, max_backoff_secs)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/utils.py", "file_name": "utils.py"}, "index": 27, "child_indices": [], "ref_doc_id": "c070558fc2f2babc0f4f950c573000ba295a80c7", "node_info": null}, "28": {"text": "This code file is the __init__.py file for the GPT Index library. It contains the necessary imports and constants for the library, as well as the DocumentStore class which is used to store documents. It also contains the __all__ list which contains all the classes and functions that are part of the library. Finally, it contains the logging configuration for the library.", "doc_id": null, "embedding": null, "extra_info": null, "index": 28, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "29": {"text": "\nThe docstore.py file contains a DocumentStore class which provides methods for managing documents. It allows for the creation of a DocumentStore from a list of documents, the addition of documents to the store, the retrieval of documents from the store, the deletion of documents from the store, and the updating of the store with another DocumentStore. The schema.py file contains a BaseDocument class which provides generic abstract interfaces for capturing both index structs and documents. It provides methods for getting the text, doc_id, and embedding of a document, as well as a method for getting the extra info string. The utils.py file contains a GlobalsHelper class which provides methods for retrieving tokenizers and stopwords, a get_new_id() function for generating a new ID, a get_new_int_id() function for generating a new integer ID, and a temp_set_attrs() context manager for setting a temporary value for an attribute on a class. It also contains an ErrorToRetry dataclass which provides a list of exception types that should be retried.", "doc_id": null, "embedding": null, "extra_info": null, "index": 29, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "30": {"text": "This code file, utils.py, contains a function, retry_on_exceptions_with_backoff, which is used to execute a given lambda function with retries and exponential backoff. It takes in a list of errors to retry, a maximum number of tries, a minimum and maximum backoff time, and a lambda function as arguments. It then attempts to execute the lambda function, and if an exception is raised, it checks if it is one of the errors to retry and if so, retries the function with an exponentially increasing backoff time. If the maximum number of tries is exceeded, the exception is raised.", "doc_id": null, "embedding": null, "extra_info": null, "index": 30, "child_indices": [24, 25, 26, 27], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"28": {"text": "This code file is the __init__.py file for the GPT Index library. It contains the necessary imports and constants for the library, as well as the DocumentStore class which is used to store documents. It also contains the __all__ list which contains all the classes and functions that are part of the library. Finally, it contains the logging configuration for the library.", "doc_id": null, "embedding": null, "extra_info": null, "index": 28, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "29": {"text": "\nThe docstore.py file contains a DocumentStore class which provides methods for managing documents. It allows for the creation of a DocumentStore from a list of documents, the addition of documents to the store, the retrieval of documents from the store, the deletion of documents from the store, and the updating of the store with another DocumentStore. The schema.py file contains a BaseDocument class which provides generic abstract interfaces for capturing both index structs and documents. It provides methods for getting the text, doc_id, and embedding of a document, as well as a method for getting the extra info string. The utils.py file contains a GlobalsHelper class which provides methods for retrieving tokenizers and stopwords, a get_new_id() function for generating a new ID, a get_new_int_id() function for generating a new integer ID, and a temp_set_attrs() context manager for setting a temporary value for an attribute on a class. It also contains an ErrorToRetry dataclass which provides a list of exception types that should be retried.", "doc_id": null, "embedding": null, "extra_info": null, "index": 29, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "30": {"text": "This code file, utils.py, contains a function, retry_on_exceptions_with_backoff, which is used to execute a given lambda function with retries and exponential backoff. It takes in a list of errors to retry, a maximum number of tries, a minimum and maximum backoff time, and a lambda function as arguments. It then attempts to execute the lambda function, and if an exception is raised, it checks if it is one of the errors to retry and if so, retries the function with an exponentially increasing backoff time. If the maximum number of tries is exceeded, the exception is raised.", "doc_id": null, "embedding": null, "extra_info": null, "index": 30, "child_indices": [24, 25, 26, 27], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}