{"index_struct": {"text": "\nThe summaries of these documents are that they contain code for testing the GPTSQLStructStoreIndex, such as setting table context dictionaries, sql context builders, and context documents dictionaries, as well as querying the index with SQL and natural language. The code also includes a mock output parser, a fixture for index and query keyword arguments, and a test for the default output parser.", "doc_id": "37ec5b3b-eb95-48a3-8ba1-06ca6a637fe1", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Init params.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "c637335013c599b07de054fba07b47ecb86ad3e8", "node_info": null}, "1": {"text": "\"\"\"Test struct store indices.\"\"\"\n\nimport re\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport pytest\nfrom sqlalchemy import (\n    Column,\n    Integer,\n    MetaData,\n    String,\n    Table,\n    column,\n    create_engine,\n    delete,\n    select,\n)\n\nfrom gpt_index.indices.common.struct_store.base import SQLContextBuilder\nfrom gpt_index.indices.struct_store.base import default_output_parser\nfrom gpt_index.indices.struct_store.sql import GPTSQLStructStoreIndex\nfrom gpt_index.langchain_helpers.sql_wrapper import SQLDatabase\nfrom gpt_index.readers.schema.base import Document\nfrom gpt_index.schema import BaseDocument\nfrom tests.mock_utils.mock_decorator import patch_common\nfrom tests.mock_utils.mock_prompts import (\n    MOCK_REFINE_PROMPT,\n    MOCK_SCHEMA_EXTRACT_PROMPT,\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/test_base.py", "file_name": "test_base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94", "node_info": null}, "2": {"text": "   MOCK_TABLE_CONTEXT_PROMPT,\n    MOCK_TEXT_QA_PROMPT,\n)\n\n\ndef _mock_output_parser(output: str) -> Optional[Dict[str, Any]]:\n    \"\"\"Mock output parser.\n\n    Split via commas instead of newlines, in order to fit\n    the format of the mock test document (newlines create\n    separate text chunks in the testing code).\n\n    \"\"\"\n    tups = output.split(\",\")\n\n    fields = {}\n    for tup in tups:\n        if \":\" in tup:\n            tokens = tup.split(\":\")\n            field = re.sub(r\"\\W+\", \"\", tokens[0])\n            value = re.sub(r\"\\W+\", \"\", tokens[1])\n            fields[field] = value\n    return", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/test_base.py", "file_name": "test_base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94", "node_info": null}, "3": {"text": "     fields[field] = value\n    return fields\n\n\n@pytest.fixture\ndef struct_kwargs() -> Tuple[Dict, Dict]:\n    \"\"\"Index kwargs.\"\"\"\n    # NOTE: QuestionAnswer and Refine templates aren't technically used\n    index_kwargs = {\n        \"schema_extract_prompt\": MOCK_SCHEMA_EXTRACT_PROMPT,\n        \"output_parser\": _mock_output_parser,\n    }\n    query_kwargs = {\n        \"text_qa_template\": MOCK_TEXT_QA_PROMPT,\n        \"refine_template\": MOCK_REFINE_PROMPT,\n    }\n    return index_kwargs, query_kwargs\n\n\n@patch_common\ndef test_sql_index(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/test_base.py", "file_name": "test_base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94", "node_info": null}, "4": {"text": "Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    struct_kwargs: Tuple[Dict, Dict],\n) -> None:\n    \"\"\"Test GPTSQLStructStoreIndex.\"\"\"\n    engine = create_engine(\"sqlite:///:memory:\")\n    metadata_obj = MetaData(bind=engine)\n    table_name = \"test_table\"\n    test_table = Table(\n        table_name,\n        metadata_obj,\n        Column(\"user_id\", Integer, primary_key=True),\n        Column(\"foo\", String(16), nullable=False),\n    )\n    metadata_obj.create_all()\n    # NOTE: we can use the default output parser for this\n    index_kwargs, _ = struct_kwargs\n    docs = [Document(text=\"user_id:2,foo:bar\"), Document(text=\"user_id:8,foo:hello\")]\n    sql_database", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/test_base.py", "file_name": "test_base.py"}, "index": 4, "child_indices": [], "ref_doc_id": "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94", "node_info": null}, "5": {"text": "   sql_database = SQLDatabase(engine)\n    index = GPTSQLStructStoreIndex(\n        docs, sql_database=sql_database, table_name=table_name, **index_kwargs\n    )\n\n    # test that the document is inserted\n    stmt = select([column(\"user_id\"), column(\"foo\")]).select_from(test_table)\n    engine = index.sql_database.engine\n    with engine.connect() as connection:\n        results = connection.execute(stmt).fetchall()\n        assert results == [(2, \"bar\"), (8, \"hello\")]\n\n    # try with documents with more text chunks\n    delete_stmt = delete(test_table)\n    with engine.connect() as connection:\n        connection.execute(delete_stmt)\n    docs = [Document(text=\"user_id:2\\nfoo:bar\"), Document(text=\"user_id:8\\nfoo:hello\")]\n    index = GPTSQLStructStoreIndex(\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/test_base.py", "file_name": "test_base.py"}, "index": 5, "child_indices": [], "ref_doc_id": "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94", "node_info": null}, "6": {"text": "   index = GPTSQLStructStoreIndex(\n        docs, sql_database=sql_database, table_name=table_name, **index_kwargs\n    )\n    # test that the document is inserted\n    stmt = select([column(\"user_id\"), column(\"foo\")]).select_from(test_table)\n    engine = index.sql_database.engine\n    with engine.connect() as connection:\n        results = connection.execute(stmt).fetchall()\n        assert results == [(2, \"bar\"), (8, \"hello\")]\n\n\n@patch_common\ndef test_sql_index_with_context(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    struct_kwargs: Tuple[Dict, Dict],\n) -> None:\n    \"\"\"Test", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/test_base.py", "file_name": "test_base.py"}, "index": 6, "child_indices": [], "ref_doc_id": "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94", "node_info": null}, "7": {"text": "Dict],\n) -> None:\n    \"\"\"Test GPTSQLStructStoreIndex.\"\"\"\n    # test setting table_context_dict\n    engine = create_engine(\"sqlite:///:memory:\")\n    metadata_obj = MetaData(bind=engine)\n    table_name = \"test_table\"\n    test_table = Table(\n        table_name,\n        metadata_obj,\n        Column(\"user_id\", Integer, primary_key=True),\n        Column(\"foo\", String(16), nullable=False),\n    )\n    metadata_obj.create_all()\n    # NOTE: we can use the default output parser for this\n    index_kwargs, _ = struct_kwargs\n    docs = [Document(text=\"user_id:2,foo:bar\"), Document(text=\"user_id:8,foo:hello\")]\n    sql_database = SQLDatabase(engine)\n    table_context_dict = {\"test_table\": \"test_table_context\"}\n    index =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/test_base.py", "file_name": "test_base.py"}, "index": 7, "child_indices": [], "ref_doc_id": "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94", "node_info": null}, "8": {"text": "\"test_table_context\"}\n    index = GPTSQLStructStoreIndex(\n        docs,\n        sql_database=sql_database,\n        table_name=table_name,\n        table_context_dict=table_context_dict,\n        **index_kwargs\n    )\n    assert index._index_struct.context_dict == table_context_dict\n\n    # test setting sql_context_builder\n    delete_stmt = delete(test_table)\n    with engine.connect() as connection:\n        connection.execute(delete_stmt)\n    sql_database = SQLDatabase(engine)\n    # this should cause the mock QuestionAnswer prompt to run\n    sql_context_builder = SQLContextBuilder(\n        sql_database,\n        table_context_prompt=MOCK_TABLE_CONTEXT_PROMPT,\n        table_context_task=\"extract_test\",\n    )\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/test_base.py", "file_name": "test_base.py"}, "index": 8, "child_indices": [], "ref_doc_id": "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94", "node_info": null}, "9": {"text": "   )\n    context_documents_dict: Dict[str, List[BaseDocument]] = {\n        \"test_table\": [Document(\"test_table_context\")]\n    }\n    index = GPTSQLStructStoreIndex(\n        docs,\n        sql_database=sql_database,\n        table_name=table_name,\n        sql_context_builder=sql_context_builder,\n        context_documents_dict=context_documents_dict,\n        **index_kwargs\n    )\n    assert index._index_struct.context_dict == {\n        \"test_table\": \"extract_test:test_table_context\"\n    }\n\n    # test error if both are set\n    # TODO:\n\n\n@patch_common\ndef test_sql_index_query(\n    _mock_init: Any,\n    _mock_predict: Any,\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/test_base.py", "file_name": "test_base.py"}, "index": 9, "child_indices": [], "ref_doc_id": "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94", "node_info": null}, "10": {"text": "   _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    struct_kwargs: Tuple[Dict, Dict],\n) -> None:\n    \"\"\"Test GPTSQLStructStoreIndex.\"\"\"\n    index_kwargs, query_kwargs = struct_kwargs\n    docs = [Document(text=\"user_id:2,foo:bar\"), Document(text=\"user_id:8,foo:hello\")]\n    engine = create_engine(\"sqlite:///:memory:\")\n    metadata_obj = MetaData(bind=engine)\n    table_name = \"test_table\"\n    # NOTE: table is created by tying to metadata_obj\n    Table(\n        table_name,\n        metadata_obj,\n        Column(\"user_id\", Integer, primary_key=True),\n        Column(\"foo\", String(16),", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/test_base.py", "file_name": "test_base.py"}, "index": 10, "child_indices": [], "ref_doc_id": "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94", "node_info": null}, "11": {"text": "       Column(\"foo\", String(16), nullable=False),\n    )\n    metadata_obj.create_all()\n    sql_database = SQLDatabase(engine)\n    # NOTE: we can use the default output parser for this\n    index = GPTSQLStructStoreIndex(\n        docs, sql_database=sql_database, table_name=table_name, **index_kwargs\n    )\n\n    # query the index with SQL\n    response = index.query(\n        \"SELECT user_id, foo FROM test_table\", mode=\"sql\", **query_kwargs\n    )\n    assert response.response == \"[(2, 'bar'), (8, 'hello')]\"\n\n    # query the index with natural language\n    response = index.query(\"test_table:user_id,foo\", mode=\"default\", **query_kwargs)\n    assert response.response == \"[(2, 'bar'), (8, 'hello')]\"\n\n\ndef test_default_output_parser() -> None:\n    \"\"\"Test default output parser.\"\"\"\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/test_base.py", "file_name": "test_base.py"}, "index": 11, "child_indices": [], "ref_doc_id": "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94", "node_info": null}, "12": {"text": "-> None:\n    \"\"\"Test default output parser.\"\"\"\n    test_str = \"user_id:2\\n\" \"foo:bar\\n\" \",,testing:testing2..\\n\" \"number:123,456,789\\n\"\n    fields = default_output_parser(test_str)\n    assert fields == {\n        \"user_id\": \"2\",\n        \"foo\": \"bar\",\n        \"testing\": \"testing2\",\n        \"number\": \"123456789\",\n    }\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/test_base.py", "file_name": "test_base.py"}, "index": 12, "child_indices": [], "ref_doc_id": "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94", "node_info": null}, "13": {"text": "This code file contains tests for the GPTSQLStructStoreIndex class, which is used to store structured data in a SQL database. The tests cover setting table context dictionaries, setting SQL context builders, and querying the index with SQL and natural language. The code also includes a mock output parser, which splits via commas instead of newlines, and a fixture for index and query keyword arguments. Finally, the code includes a test for the default output parser.", "doc_id": null, "embedding": null, "extra_info": null, "index": 13, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "14": {"text": "This code file, test_base.py, tests the default output parser. It creates a test string with various fields and values, and then passes it to the default output parser. The expected output is a dictionary with the fields and values from the test string. The code then asserts that the output is as expected.", "doc_id": null, "embedding": null, "extra_info": null, "index": 14, "child_indices": [12], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"13": {"text": "This code file contains tests for the GPTSQLStructStoreIndex class, which is used to store structured data in a SQL database. The tests cover setting table context dictionaries, setting SQL context builders, and querying the index with SQL and natural language. The code also includes a mock output parser, which splits via commas instead of newlines, and a fixture for index and query keyword arguments. Finally, the code includes a test for the default output parser.", "doc_id": null, "embedding": null, "extra_info": null, "index": 13, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "14": {"text": "This code file, test_base.py, tests the default output parser. It creates a test string with various fields and values, and then passes it to the default output parser. The expected output is a dictionary with the fields and values from the test string. The code then asserts that the output is as expected.", "doc_id": null, "embedding": null, "extra_info": null, "index": 14, "child_indices": [12], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"c637335013c599b07de054fba07b47ecb86ad3e8": {"text": "\"\"\"Init params.\"\"\"\n", "doc_id": "c637335013c599b07de054fba07b47ecb86ad3e8", "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94": {"text": "\"\"\"Test struct store indices.\"\"\"\n\nimport re\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport pytest\nfrom sqlalchemy import (\n    Column,\n    Integer,\n    MetaData,\n    String,\n    Table,\n    column,\n    create_engine,\n    delete,\n    select,\n)\n\nfrom gpt_index.indices.common.struct_store.base import SQLContextBuilder\nfrom gpt_index.indices.struct_store.base import default_output_parser\nfrom gpt_index.indices.struct_store.sql import GPTSQLStructStoreIndex\nfrom gpt_index.langchain_helpers.sql_wrapper import SQLDatabase\nfrom gpt_index.readers.schema.base import Document\nfrom gpt_index.schema import BaseDocument\nfrom tests.mock_utils.mock_decorator import patch_common\nfrom tests.mock_utils.mock_prompts import (\n    MOCK_REFINE_PROMPT,\n    MOCK_SCHEMA_EXTRACT_PROMPT,\n    MOCK_TABLE_CONTEXT_PROMPT,\n    MOCK_TEXT_QA_PROMPT,\n)\n\n\ndef _mock_output_parser(output: str) -> Optional[Dict[str, Any]]:\n    \"\"\"Mock output parser.\n\n    Split via commas instead of newlines, in order to fit\n    the format of the mock test document (newlines create\n    separate text chunks in the testing code).\n\n    \"\"\"\n    tups = output.split(\",\")\n\n    fields = {}\n    for tup in tups:\n        if \":\" in tup:\n            tokens = tup.split(\":\")\n            field = re.sub(r\"\\W+\", \"\", tokens[0])\n            value = re.sub(r\"\\W+\", \"\", tokens[1])\n            fields[field] = value\n    return fields\n\n\n@pytest.fixture\ndef struct_kwargs() -> Tuple[Dict, Dict]:\n    \"\"\"Index kwargs.\"\"\"\n    # NOTE: QuestionAnswer and Refine templates aren't technically used\n    index_kwargs = {\n        \"schema_extract_prompt\": MOCK_SCHEMA_EXTRACT_PROMPT,\n        \"output_parser\": _mock_output_parser,\n    }\n    query_kwargs = {\n        \"text_qa_template\": MOCK_TEXT_QA_PROMPT,\n        \"refine_template\": MOCK_REFINE_PROMPT,\n    }\n    return index_kwargs, query_kwargs\n\n\n@patch_common\ndef test_sql_index(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    struct_kwargs: Tuple[Dict, Dict],\n) -> None:\n    \"\"\"Test GPTSQLStructStoreIndex.\"\"\"\n    engine = create_engine(\"sqlite:///:memory:\")\n    metadata_obj = MetaData(bind=engine)\n    table_name = \"test_table\"\n    test_table = Table(\n        table_name,\n        metadata_obj,\n        Column(\"user_id\", Integer, primary_key=True),\n        Column(\"foo\", String(16), nullable=False),\n    )\n    metadata_obj.create_all()\n    # NOTE: we can use the default output parser for this\n    index_kwargs, _ = struct_kwargs\n    docs = [Document(text=\"user_id:2,foo:bar\"), Document(text=\"user_id:8,foo:hello\")]\n    sql_database = SQLDatabase(engine)\n    index = GPTSQLStructStoreIndex(\n        docs, sql_database=sql_database, table_name=table_name, **index_kwargs\n    )\n\n    # test that the document is inserted\n    stmt = select([column(\"user_id\"), column(\"foo\")]).select_from(test_table)\n    engine = index.sql_database.engine\n    with engine.connect() as connection:\n        results = connection.execute(stmt).fetchall()\n        assert results == [(2, \"bar\"), (8, \"hello\")]\n\n    # try with documents with more text chunks\n    delete_stmt = delete(test_table)\n    with engine.connect() as connection:\n        connection.execute(delete_stmt)\n    docs = [Document(text=\"user_id:2\\nfoo:bar\"), Document(text=\"user_id:8\\nfoo:hello\")]\n    index = GPTSQLStructStoreIndex(\n        docs, sql_database=sql_database, table_name=table_name, **index_kwargs\n    )\n    # test that the document is inserted\n    stmt = select([column(\"user_id\"), column(\"foo\")]).select_from(test_table)\n    engine = index.sql_database.engine\n    with engine.connect() as connection:\n        results = connection.execute(stmt).fetchall()\n        assert results == [(2, \"bar\"), (8, \"hello\")]\n\n\n@patch_common\ndef test_sql_index_with_context(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    struct_kwargs: Tuple[Dict, Dict],\n) -> None:\n    \"\"\"Test GPTSQLStructStoreIndex.\"\"\"\n    # test setting table_context_dict\n    engine = create_engine(\"sqlite:///:memory:\")\n    metadata_obj = MetaData(bind=engine)\n    table_name = \"test_table\"\n    test_table = Table(\n        table_name,\n        metadata_obj,\n        Column(\"user_id\", Integer, primary_key=True),\n        Column(\"foo\", String(16), nullable=False),\n    )\n    metadata_obj.create_all()\n    # NOTE: we can use the default output parser for this\n    index_kwargs, _ = struct_kwargs\n    docs = [Document(text=\"user_id:2,foo:bar\"), Document(text=\"user_id:8,foo:hello\")]\n    sql_database = SQLDatabase(engine)\n    table_context_dict = {\"test_table\": \"test_table_context\"}\n    index = GPTSQLStructStoreIndex(\n        docs,\n        sql_database=sql_database,\n        table_name=table_name,\n        table_context_dict=table_context_dict,\n        **index_kwargs\n    )\n    assert index._index_struct.context_dict == table_context_dict\n\n    # test setting sql_context_builder\n    delete_stmt = delete(test_table)\n    with engine.connect() as connection:\n        connection.execute(delete_stmt)\n    sql_database = SQLDatabase(engine)\n    # this should cause the mock QuestionAnswer prompt to run\n    sql_context_builder = SQLContextBuilder(\n        sql_database,\n        table_context_prompt=MOCK_TABLE_CONTEXT_PROMPT,\n        table_context_task=\"extract_test\",\n    )\n    context_documents_dict: Dict[str, List[BaseDocument]] = {\n        \"test_table\": [Document(\"test_table_context\")]\n    }\n    index = GPTSQLStructStoreIndex(\n        docs,\n        sql_database=sql_database,\n        table_name=table_name,\n        sql_context_builder=sql_context_builder,\n        context_documents_dict=context_documents_dict,\n        **index_kwargs\n    )\n    assert index._index_struct.context_dict == {\n        \"test_table\": \"extract_test:test_table_context\"\n    }\n\n    # test error if both are set\n    # TODO:\n\n\n@patch_common\ndef test_sql_index_query(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    struct_kwargs: Tuple[Dict, Dict],\n) -> None:\n    \"\"\"Test GPTSQLStructStoreIndex.\"\"\"\n    index_kwargs, query_kwargs = struct_kwargs\n    docs = [Document(text=\"user_id:2,foo:bar\"), Document(text=\"user_id:8,foo:hello\")]\n    engine = create_engine(\"sqlite:///:memory:\")\n    metadata_obj = MetaData(bind=engine)\n    table_name = \"test_table\"\n    # NOTE: table is created by tying to metadata_obj\n    Table(\n        table_name,\n        metadata_obj,\n        Column(\"user_id\", Integer, primary_key=True),\n        Column(\"foo\", String(16), nullable=False),\n    )\n    metadata_obj.create_all()\n    sql_database = SQLDatabase(engine)\n    # NOTE: we can use the default output parser for this\n    index = GPTSQLStructStoreIndex(\n        docs, sql_database=sql_database, table_name=table_name, **index_kwargs\n    )\n\n    # query the index with SQL\n    response = index.query(\n        \"SELECT user_id, foo FROM test_table\", mode=\"sql\", **query_kwargs\n    )\n    assert response.response == \"[(2, 'bar'), (8, 'hello')]\"\n\n    # query the index with natural language\n    response = index.query(\"test_table:user_id,foo\", mode=\"default\", **query_kwargs)\n    assert response.response == \"[(2, 'bar'), (8, 'hello')]\"\n\n\ndef test_default_output_parser() -> None:\n    \"\"\"Test default output parser.\"\"\"\n    test_str = \"user_id:2\\n\" \"foo:bar\\n\" \",,testing:testing2..\\n\" \"number:123,456,789\\n\"\n    fields = default_output_parser(test_str)\n    assert fields == {\n        \"user_id\": \"2\",\n        \"foo\": \"bar\",\n        \"testing\": \"testing2\",\n        \"number\": \"123456789\",\n    }\n", "doc_id": "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94", "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/test_base.py", "file_name": "test_base.py"}, "__type__": "Document"}, "37ec5b3b-eb95-48a3-8ba1-06ca6a637fe1": {"text": "\nThe summaries of these documents are that they contain code for testing the GPTSQLStructStoreIndex, such as setting table context dictionaries, sql context builders, and context documents dictionaries, as well as querying the index with SQL and natural language. The code also includes a mock output parser, a fixture for index and query keyword arguments, and a test for the default output parser.", "doc_id": "37ec5b3b-eb95-48a3-8ba1-06ca6a637fe1", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Init params.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "c637335013c599b07de054fba07b47ecb86ad3e8", "node_info": null}, "1": {"text": "\"\"\"Test struct store indices.\"\"\"\n\nimport re\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport pytest\nfrom sqlalchemy import (\n    Column,\n    Integer,\n    MetaData,\n    String,\n    Table,\n    column,\n    create_engine,\n    delete,\n    select,\n)\n\nfrom gpt_index.indices.common.struct_store.base import SQLContextBuilder\nfrom gpt_index.indices.struct_store.base import default_output_parser\nfrom gpt_index.indices.struct_store.sql import GPTSQLStructStoreIndex\nfrom gpt_index.langchain_helpers.sql_wrapper import SQLDatabase\nfrom gpt_index.readers.schema.base import Document\nfrom gpt_index.schema import BaseDocument\nfrom tests.mock_utils.mock_decorator import patch_common\nfrom tests.mock_utils.mock_prompts import (\n    MOCK_REFINE_PROMPT,\n    MOCK_SCHEMA_EXTRACT_PROMPT,\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/test_base.py", "file_name": "test_base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94", "node_info": null}, "2": {"text": "   MOCK_TABLE_CONTEXT_PROMPT,\n    MOCK_TEXT_QA_PROMPT,\n)\n\n\ndef _mock_output_parser(output: str) -> Optional[Dict[str, Any]]:\n    \"\"\"Mock output parser.\n\n    Split via commas instead of newlines, in order to fit\n    the format of the mock test document (newlines create\n    separate text chunks in the testing code).\n\n    \"\"\"\n    tups = output.split(\",\")\n\n    fields = {}\n    for tup in tups:\n        if \":\" in tup:\n            tokens = tup.split(\":\")\n            field = re.sub(r\"\\W+\", \"\", tokens[0])\n            value = re.sub(r\"\\W+\", \"\", tokens[1])\n            fields[field] = value\n    return", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/test_base.py", "file_name": "test_base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94", "node_info": null}, "3": {"text": "     fields[field] = value\n    return fields\n\n\n@pytest.fixture\ndef struct_kwargs() -> Tuple[Dict, Dict]:\n    \"\"\"Index kwargs.\"\"\"\n    # NOTE: QuestionAnswer and Refine templates aren't technically used\n    index_kwargs = {\n        \"schema_extract_prompt\": MOCK_SCHEMA_EXTRACT_PROMPT,\n        \"output_parser\": _mock_output_parser,\n    }\n    query_kwargs = {\n        \"text_qa_template\": MOCK_TEXT_QA_PROMPT,\n        \"refine_template\": MOCK_REFINE_PROMPT,\n    }\n    return index_kwargs, query_kwargs\n\n\n@patch_common\ndef test_sql_index(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/test_base.py", "file_name": "test_base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94", "node_info": null}, "4": {"text": "Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    struct_kwargs: Tuple[Dict, Dict],\n) -> None:\n    \"\"\"Test GPTSQLStructStoreIndex.\"\"\"\n    engine = create_engine(\"sqlite:///:memory:\")\n    metadata_obj = MetaData(bind=engine)\n    table_name = \"test_table\"\n    test_table = Table(\n        table_name,\n        metadata_obj,\n        Column(\"user_id\", Integer, primary_key=True),\n        Column(\"foo\", String(16), nullable=False),\n    )\n    metadata_obj.create_all()\n    # NOTE: we can use the default output parser for this\n    index_kwargs, _ = struct_kwargs\n    docs = [Document(text=\"user_id:2,foo:bar\"), Document(text=\"user_id:8,foo:hello\")]\n    sql_database", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/test_base.py", "file_name": "test_base.py"}, "index": 4, "child_indices": [], "ref_doc_id": "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94", "node_info": null}, "5": {"text": "   sql_database = SQLDatabase(engine)\n    index = GPTSQLStructStoreIndex(\n        docs, sql_database=sql_database, table_name=table_name, **index_kwargs\n    )\n\n    # test that the document is inserted\n    stmt = select([column(\"user_id\"), column(\"foo\")]).select_from(test_table)\n    engine = index.sql_database.engine\n    with engine.connect() as connection:\n        results = connection.execute(stmt).fetchall()\n        assert results == [(2, \"bar\"), (8, \"hello\")]\n\n    # try with documents with more text chunks\n    delete_stmt = delete(test_table)\n    with engine.connect() as connection:\n        connection.execute(delete_stmt)\n    docs = [Document(text=\"user_id:2\\nfoo:bar\"), Document(text=\"user_id:8\\nfoo:hello\")]\n    index = GPTSQLStructStoreIndex(\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/test_base.py", "file_name": "test_base.py"}, "index": 5, "child_indices": [], "ref_doc_id": "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94", "node_info": null}, "6": {"text": "   index = GPTSQLStructStoreIndex(\n        docs, sql_database=sql_database, table_name=table_name, **index_kwargs\n    )\n    # test that the document is inserted\n    stmt = select([column(\"user_id\"), column(\"foo\")]).select_from(test_table)\n    engine = index.sql_database.engine\n    with engine.connect() as connection:\n        results = connection.execute(stmt).fetchall()\n        assert results == [(2, \"bar\"), (8, \"hello\")]\n\n\n@patch_common\ndef test_sql_index_with_context(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    struct_kwargs: Tuple[Dict, Dict],\n) -> None:\n    \"\"\"Test", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/test_base.py", "file_name": "test_base.py"}, "index": 6, "child_indices": [], "ref_doc_id": "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94", "node_info": null}, "7": {"text": "Dict],\n) -> None:\n    \"\"\"Test GPTSQLStructStoreIndex.\"\"\"\n    # test setting table_context_dict\n    engine = create_engine(\"sqlite:///:memory:\")\n    metadata_obj = MetaData(bind=engine)\n    table_name = \"test_table\"\n    test_table = Table(\n        table_name,\n        metadata_obj,\n        Column(\"user_id\", Integer, primary_key=True),\n        Column(\"foo\", String(16), nullable=False),\n    )\n    metadata_obj.create_all()\n    # NOTE: we can use the default output parser for this\n    index_kwargs, _ = struct_kwargs\n    docs = [Document(text=\"user_id:2,foo:bar\"), Document(text=\"user_id:8,foo:hello\")]\n    sql_database = SQLDatabase(engine)\n    table_context_dict = {\"test_table\": \"test_table_context\"}\n    index =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/test_base.py", "file_name": "test_base.py"}, "index": 7, "child_indices": [], "ref_doc_id": "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94", "node_info": null}, "8": {"text": "\"test_table_context\"}\n    index = GPTSQLStructStoreIndex(\n        docs,\n        sql_database=sql_database,\n        table_name=table_name,\n        table_context_dict=table_context_dict,\n        **index_kwargs\n    )\n    assert index._index_struct.context_dict == table_context_dict\n\n    # test setting sql_context_builder\n    delete_stmt = delete(test_table)\n    with engine.connect() as connection:\n        connection.execute(delete_stmt)\n    sql_database = SQLDatabase(engine)\n    # this should cause the mock QuestionAnswer prompt to run\n    sql_context_builder = SQLContextBuilder(\n        sql_database,\n        table_context_prompt=MOCK_TABLE_CONTEXT_PROMPT,\n        table_context_task=\"extract_test\",\n    )\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/test_base.py", "file_name": "test_base.py"}, "index": 8, "child_indices": [], "ref_doc_id": "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94", "node_info": null}, "9": {"text": "   )\n    context_documents_dict: Dict[str, List[BaseDocument]] = {\n        \"test_table\": [Document(\"test_table_context\")]\n    }\n    index = GPTSQLStructStoreIndex(\n        docs,\n        sql_database=sql_database,\n        table_name=table_name,\n        sql_context_builder=sql_context_builder,\n        context_documents_dict=context_documents_dict,\n        **index_kwargs\n    )\n    assert index._index_struct.context_dict == {\n        \"test_table\": \"extract_test:test_table_context\"\n    }\n\n    # test error if both are set\n    # TODO:\n\n\n@patch_common\ndef test_sql_index_query(\n    _mock_init: Any,\n    _mock_predict: Any,\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/test_base.py", "file_name": "test_base.py"}, "index": 9, "child_indices": [], "ref_doc_id": "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94", "node_info": null}, "10": {"text": "   _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    struct_kwargs: Tuple[Dict, Dict],\n) -> None:\n    \"\"\"Test GPTSQLStructStoreIndex.\"\"\"\n    index_kwargs, query_kwargs = struct_kwargs\n    docs = [Document(text=\"user_id:2,foo:bar\"), Document(text=\"user_id:8,foo:hello\")]\n    engine = create_engine(\"sqlite:///:memory:\")\n    metadata_obj = MetaData(bind=engine)\n    table_name = \"test_table\"\n    # NOTE: table is created by tying to metadata_obj\n    Table(\n        table_name,\n        metadata_obj,\n        Column(\"user_id\", Integer, primary_key=True),\n        Column(\"foo\", String(16),", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/test_base.py", "file_name": "test_base.py"}, "index": 10, "child_indices": [], "ref_doc_id": "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94", "node_info": null}, "11": {"text": "       Column(\"foo\", String(16), nullable=False),\n    )\n    metadata_obj.create_all()\n    sql_database = SQLDatabase(engine)\n    # NOTE: we can use the default output parser for this\n    index = GPTSQLStructStoreIndex(\n        docs, sql_database=sql_database, table_name=table_name, **index_kwargs\n    )\n\n    # query the index with SQL\n    response = index.query(\n        \"SELECT user_id, foo FROM test_table\", mode=\"sql\", **query_kwargs\n    )\n    assert response.response == \"[(2, 'bar'), (8, 'hello')]\"\n\n    # query the index with natural language\n    response = index.query(\"test_table:user_id,foo\", mode=\"default\", **query_kwargs)\n    assert response.response == \"[(2, 'bar'), (8, 'hello')]\"\n\n\ndef test_default_output_parser() -> None:\n    \"\"\"Test default output parser.\"\"\"\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/test_base.py", "file_name": "test_base.py"}, "index": 11, "child_indices": [], "ref_doc_id": "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94", "node_info": null}, "12": {"text": "-> None:\n    \"\"\"Test default output parser.\"\"\"\n    test_str = \"user_id:2\\n\" \"foo:bar\\n\" \",,testing:testing2..\\n\" \"number:123,456,789\\n\"\n    fields = default_output_parser(test_str)\n    assert fields == {\n        \"user_id\": \"2\",\n        \"foo\": \"bar\",\n        \"testing\": \"testing2\",\n        \"number\": \"123456789\",\n    }\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/test_base.py", "file_name": "test_base.py"}, "index": 12, "child_indices": [], "ref_doc_id": "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94", "node_info": null}, "13": {"text": "This code file contains tests for the GPTSQLStructStoreIndex class, which is used to store structured data in a SQL database. The tests cover setting table context dictionaries, setting SQL context builders, and querying the index with SQL and natural language. The code also includes a mock output parser, which splits via commas instead of newlines, and a fixture for index and query keyword arguments. Finally, the code includes a test for the default output parser.", "doc_id": null, "embedding": null, "extra_info": null, "index": 13, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "14": {"text": "This code file, test_base.py, tests the default output parser. It creates a test string with various fields and values, and then passes it to the default output parser. The expected output is a dictionary with the fields and values from the test string. The code then asserts that the output is as expected.", "doc_id": null, "embedding": null, "extra_info": null, "index": 14, "child_indices": [12], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"13": {"text": "This code file contains tests for the GPTSQLStructStoreIndex class, which is used to store structured data in a SQL database. The tests cover setting table context dictionaries, setting SQL context builders, and querying the index with SQL and natural language. The code also includes a mock output parser, which splits via commas instead of newlines, and a fixture for index and query keyword arguments. Finally, the code includes a test for the default output parser.", "doc_id": null, "embedding": null, "extra_info": null, "index": 13, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "14": {"text": "This code file, test_base.py, tests the default output parser. It creates a test string with various fields and values, and then passes it to the default output parser. The expected output is a dictionary with the fields and values from the test string. The code then asserts that the output is as expected.", "doc_id": null, "embedding": null, "extra_info": null, "index": 14, "child_indices": [12], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}