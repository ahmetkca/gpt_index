{"index_struct": {"text": "\nThe summaries of these documents are:\n1. Test file containing a fixture for a text splitter and a list of documents.\n2. Test file containing tests for the get_nodes_from_document function.\n3. Test file containing tests for the PromptHelper class.\n4. Test file testing the ResponseMode.TREE_SUMMARIZE feature of the ResponseBuilder class.\n5. Test file testing the expand_tokens_with_subtokens() function.", "doc_id": "dd176b17-4a38-4b69-b938-738b739f01d9", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "node_info": null}, "1": {"text": "\"\"\"Test node utils.\"\"\"\n\nfrom typing import List\n\nimport pytest\n\nfrom gpt_index.indices.node_utils import get_nodes_from_document\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.readers.schema.base import Document\n\n\n@pytest.fixture\ndef text_splitter() -> TokenTextSplitter:\n    \"\"\"Get text splitter.\"\"\"\n    return TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n\n\n@pytest.fixture\ndef documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"\n    # NOTE: one document for now\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    return [\n        Document(doc_text, doc_id=\"test_doc_id\",", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_node_utils.py", "file_name": "test_node_utils.py"}, "index": 1, "child_indices": [], "ref_doc_id": "2c621db7da92be19edfbf5fb7f66fe0d8609091f", "node_info": null}, "2": {"text": "Document(doc_text, doc_id=\"test_doc_id\", extra_info={\"test_key\": \"test_val\"})\n    ]\n\n\ndef test_get_nodes_from_document(\n    documents: List[Document], text_splitter: TokenTextSplitter\n) -> None:\n    \"\"\"Test get nodes from document have desired chunk size.\"\"\"\n    nodes = get_nodes_from_document(\n        documents[0],\n        text_splitter,\n        start_idx=0,\n        include_extra_info=False,\n    )\n    assert len(nodes) == 2\n    actual_chunk_sizes = [\n        len(text_splitter.tokenizer(node.get_text())) for node in nodes\n    ]\n    assert all(\n        chunk_size <= text_splitter._chunk_size for chunk_size in actual_chunk_sizes\n    )\n\n\ndef", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_node_utils.py", "file_name": "test_node_utils.py"}, "index": 2, "child_indices": [], "ref_doc_id": "2c621db7da92be19edfbf5fb7f66fe0d8609091f", "node_info": null}, "3": {"text": "in actual_chunk_sizes\n    )\n\n\ndef test_get_nodes_from_document_with_extra_info(\n    documents: List[Document], text_splitter: TokenTextSplitter\n) -> None:\n    \"\"\"Test get nodes from document with extra info have desired chunk size.\"\"\"\n    nodes = get_nodes_from_document(\n        documents[0],\n        text_splitter,\n        start_idx=0,\n        include_extra_info=True,\n    )\n    assert len(nodes) == 3\n    actual_chunk_sizes = [\n        len(text_splitter.tokenizer(node.get_text())) for node in nodes\n    ]\n    assert all(\n        chunk_size <= text_splitter._chunk_size for chunk_size in actual_chunk_sizes\n    )\n    assert all([\"test_key: test_val\" in", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_node_utils.py", "file_name": "test_node_utils.py"}, "index": 3, "child_indices": [], "ref_doc_id": "2c621db7da92be19edfbf5fb7f66fe0d8609091f", "node_info": null}, "4": {"text": "   assert all([\"test_key: test_val\" in n.get_text() for n in nodes])\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_node_utils.py", "file_name": "test_node_utils.py"}, "index": 4, "child_indices": [], "ref_doc_id": "2c621db7da92be19edfbf5fb7f66fe0d8609091f", "node_info": null}, "5": {"text": "\"\"\"Test PromptHelper.\"\"\"\nfrom typing import List\n\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.prompts.base import Prompt\nfrom tests.mock_utils.mock_utils import mock_tokenizer\n\n\nclass TestPrompt(Prompt):\n    \"\"\"Test prompt class.\"\"\"\n\n    input_variables: List[str] = [\"text\"]\n\n\ndef test_get_chunk_size() -> None:\n    \"\"\"Test get chunk size given prompt.\"\"\"\n    # test with 1 chunk\n    empty_prompt_text = \"This is the prompt\"\n    prompt_helper = PromptHelper(\n        max_input_size=11, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    chunk_size = prompt_helper.get_chunk_size_given_prompt(\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_prompt_helper.py", "file_name": "test_prompt_helper.py"}, "index": 5, "child_indices": [], "ref_doc_id": "726d88a507fa99e69d3f782fb2697d21049f82e0", "node_info": null}, "6": {"text": "       empty_prompt_text, 1, padding=0\n    )\n    assert chunk_size == 6\n\n    # test having 2 chunks\n    prompt_helper = PromptHelper(\n        max_input_size=11, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    chunk_size = prompt_helper.get_chunk_size_given_prompt(\n        empty_prompt_text, 2, padding=0\n    )\n    assert chunk_size == 3\n\n    # test with 2 chunks, and with chunk_size_limit\n    prompt_helper = PromptHelper(\n        max_input_size=11,\n        num_output=1,\n        max_chunk_overlap=0,\n        tokenizer=mock_tokenizer,\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_prompt_helper.py", "file_name": "test_prompt_helper.py"}, "index": 6, "child_indices": [], "ref_doc_id": "726d88a507fa99e69d3f782fb2697d21049f82e0", "node_info": null}, "7": {"text": "       chunk_size_limit=2,\n    )\n    chunk_size = prompt_helper.get_chunk_size_given_prompt(\n        empty_prompt_text, 2, padding=0\n    )\n    assert chunk_size == 2\n\n    # test padding\n    prompt_helper = PromptHelper(\n        max_input_size=11, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    chunk_size = prompt_helper.get_chunk_size_given_prompt(\n        empty_prompt_text, 2, padding=1\n    )\n    assert chunk_size == 2\n\n\ndef test_get_text_splitter() -> None:\n    \"\"\"Test get text splitter.\"\"\"\n    test_prompt_text = \"This is the prompt{text}\"\n    test_prompt =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_prompt_helper.py", "file_name": "test_prompt_helper.py"}, "index": 7, "child_indices": [], "ref_doc_id": "726d88a507fa99e69d3f782fb2697d21049f82e0", "node_info": null}, "8": {"text": "is the prompt{text}\"\n    test_prompt = TestPrompt(test_prompt_text)\n    prompt_helper = PromptHelper(\n        max_input_size=11, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    text_splitter = prompt_helper.get_text_splitter_given_prompt(\n        test_prompt, 2, padding=1\n    )\n    assert text_splitter._chunk_size == 2\n    test_text = \"Hello world foo Hello world bar\"\n    text_chunks = text_splitter.split_text(test_text)\n    assert text_chunks == [\"Hello world\", \"foo Hello\", \"world bar\"]\n    truncated_text = text_splitter.truncate_text(test_text)\n    assert truncated_text == \"Hello world\"\n\n    # test with chunk_size_limit\n    prompt_helper = PromptHelper(\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_prompt_helper.py", "file_name": "test_prompt_helper.py"}, "index": 8, "child_indices": [], "ref_doc_id": "726d88a507fa99e69d3f782fb2697d21049f82e0", "node_info": null}, "9": {"text": "   prompt_helper = PromptHelper(\n        max_input_size=11,\n        num_output=1,\n        max_chunk_overlap=0,\n        tokenizer=mock_tokenizer,\n        chunk_size_limit=1,\n    )\n    text_splitter = prompt_helper.get_text_splitter_given_prompt(\n        test_prompt, 2, padding=1\n    )\n    text_chunks = text_splitter.split_text(test_text)\n    assert text_chunks == [\"Hello\", \"world\", \"foo\", \"Hello\", \"world\", \"bar\"]\n\n\ndef test_get_text_splitter_partial() -> None:\n    \"\"\"Test get text splitter with a partially formatted prompt.\"\"\"\n\n    class TestPromptFoo(Prompt):\n        \"\"\"Test prompt class.\"\"\"\n\n        input_variables:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_prompt_helper.py", "file_name": "test_prompt_helper.py"}, "index": 9, "child_indices": [], "ref_doc_id": "726d88a507fa99e69d3f782fb2697d21049f82e0", "node_info": null}, "10": {"text": "class.\"\"\"\n\n        input_variables: List[str] = [\"foo\", \"text\"]\n\n    # test without partially formatting\n    test_prompt_text = \"This is the {foo} prompt{text}\"\n    test_prompt = TestPromptFoo(test_prompt_text)\n    prompt_helper = PromptHelper(\n        max_input_size=11, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    text_splitter = prompt_helper.get_text_splitter_given_prompt(\n        test_prompt, 2, padding=1\n    )\n    test_text = \"Hello world foo Hello world bar\"\n    text_chunks = text_splitter.split_text(test_text)\n    assert text_chunks == [\"Hello world\", \"foo Hello\", \"world bar\"]\n    truncated_text = text_splitter.truncate_text(test_text)\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_prompt_helper.py", "file_name": "test_prompt_helper.py"}, "index": 10, "child_indices": [], "ref_doc_id": "726d88a507fa99e69d3f782fb2697d21049f82e0", "node_info": null}, "11": {"text": "   assert truncated_text == \"Hello world\"\n\n    # test with partially formatting\n    test_prompt = TestPromptFoo(test_prompt_text)\n    test_prompt = test_prompt.partial_format(foo=\"bar\")\n    prompt_helper = PromptHelper(\n        max_input_size=12, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    assert prompt_helper._get_empty_prompt_txt(test_prompt) == \"This is the bar prompt\"\n    text_splitter = prompt_helper.get_text_splitter_given_prompt(\n        test_prompt, 2, padding=1\n    )\n    test_text = \"Hello world foo Hello world bar\"\n    text_chunks = text_splitter.split_text(test_text)\n    assert text_chunks == [\"Hello world\", \"foo Hello\", \"world bar\"]\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_prompt_helper.py", "file_name": "test_prompt_helper.py"}, "index": 11, "child_indices": [], "ref_doc_id": "726d88a507fa99e69d3f782fb2697d21049f82e0", "node_info": null}, "12": {"text": "[\"Hello world\", \"foo Hello\", \"world bar\"]\n    truncated_text = text_splitter.truncate_text(test_text)\n    assert truncated_text == \"Hello world\"\n\n\ndef test_get_text_from_nodes() -> None:\n    \"\"\"Test get_text_from_nodes.\"\"\"\n    # test prompt uses up one token\n    test_prompt_txt = \"test{text}\"\n    test_prompt = TestPrompt(test_prompt_txt)\n    # set max_input_size=11\n    # For each text chunk, there's 4 tokens for text + 1 for the padding\n    prompt_helper = PromptHelper(\n        max_input_size=11, num_output=0, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    node1 = Node(text=\"This is a test foo bar\")\n    node2 = Node(text=\"Hello world bar foo\")\n\n    response =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_prompt_helper.py", "file_name": "test_prompt_helper.py"}, "index": 12, "child_indices": [], "ref_doc_id": "726d88a507fa99e69d3f782fb2697d21049f82e0", "node_info": null}, "13": {"text": "= Node(text=\"Hello world bar foo\")\n\n    response = prompt_helper.get_text_from_nodes([node1, node2], prompt=test_prompt)\n    assert str(response) == (\"This is a test\\n\" \"Hello world bar foo\")\n\n\ndef test_get_numbered_text_from_nodes() -> None:\n    \"\"\"Test get_text_from_nodes.\"\"\"\n    # test prompt uses up one token\n    test_prompt_txt = \"test{text}\"\n    test_prompt = TestPrompt(test_prompt_txt)\n    # set max_input_size=17\n    # For each text chunk, there's 3 for text, 5 for padding (including number)\n    prompt_helper = PromptHelper(\n        max_input_size=17, num_output=0, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    node1 = Node(text=\"This is a test foo bar\")\n    node2 = Node(text=\"Hello world bar", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_prompt_helper.py", "file_name": "test_prompt_helper.py"}, "index": 13, "child_indices": [], "ref_doc_id": "726d88a507fa99e69d3f782fb2697d21049f82e0", "node_info": null}, "14": {"text": "bar\")\n    node2 = Node(text=\"Hello world bar foo\")\n\n    response = prompt_helper.get_numbered_text_from_nodes(\n        [node1, node2], prompt=test_prompt\n    )\n    assert str(response) == (\"(1) This is a\\n\\n(2) Hello world bar\")\n\n\ndef test_compact_text() -> None:\n    \"\"\"Test compact text.\"\"\"\n    test_prompt_text = \"This is the prompt{text}\"\n    test_prompt = TestPrompt(test_prompt_text)\n    prompt_helper = PromptHelper(\n        max_input_size=9,\n        num_output=1,\n        max_chunk_overlap=0,\n        tokenizer=mock_tokenizer,\n        separator=\"\\n\\n\",\n    )\n    text_chunks = [\"Hello\", \"world\", \"foo\",", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_prompt_helper.py", "file_name": "test_prompt_helper.py"}, "index": 14, "child_indices": [], "ref_doc_id": "726d88a507fa99e69d3f782fb2697d21049f82e0", "node_info": null}, "15": {"text": "  text_chunks = [\"Hello\", \"world\", \"foo\", \"Hello\", \"world\", \"bar\"]\n    compacted_chunks = prompt_helper.compact_text_chunks(test_prompt, text_chunks)\n    assert compacted_chunks == [\"Hello\\n\\nworld\\n\\nfoo\", \"Hello\\n\\nworld\\n\\nbar\"]\n\n\ndef test_get_biggest_prompt() -> None:\n    \"\"\"Test get_biggest_prompt from PromptHelper.\"\"\"\n    # NOTE: inputs don't matter\n    prompt_helper = PromptHelper(max_input_size=1, num_output=1, max_chunk_overlap=0)\n    prompt1 = TestPrompt(\"This is the prompt{text}\")\n    prompt2 = TestPrompt(\"This is the longer prompt{text}\")\n    prompt3 = TestPrompt(\"This is the {text}\")\n    biggest_prompt = prompt_helper.get_biggest_prompt([prompt1, prompt2,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_prompt_helper.py", "file_name": "test_prompt_helper.py"}, "index": 15, "child_indices": [], "ref_doc_id": "726d88a507fa99e69d3f782fb2697d21049f82e0", "node_info": null}, "16": {"text": "prompt2, prompt3])\n    assert biggest_prompt.prompt.template == prompt2.prompt.template\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_prompt_helper.py", "file_name": "test_prompt_helper.py"}, "index": 16, "child_indices": [], "ref_doc_id": "726d88a507fa99e69d3f782fb2697d21049f82e0", "node_info": null}, "17": {"text": "\"\"\"Test response utils.\"\"\"\n\nfrom typing import Any, List\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom gpt_index.constants import MAX_CHUNK_OVERLAP, MAX_CHUNK_SIZE, NUM_OUTPUTS\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.response.builder import ResponseBuilder, ResponseMode, TextChunk\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\nfrom gpt_index.readers.schema.base import Document\nfrom tests.mock_utils.mock_decorator import patch_common\nfrom tests.mock_utils.mock_predict import mock_llmpredictor_predict\nfrom tests.mock_utils.mock_prompts import MOCK_REFINE_PROMPT, MOCK_TEXT_QA_PROMPT\n\n\n@pytest.fixture\ndef documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"\n    # NOTE: one document for", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 17, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "18": {"text": "\"\"\"Get documents.\"\"\"\n    # NOTE: one document for now\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    return [Document(doc_text)]\n\n\ndef mock_tokenizer(text: str) -> List[str]:\n    \"\"\"Mock tokenizer.\"\"\"\n    if text == \"\":\n        return []\n    tokens = text.split(\" \")\n    return tokens\n\n\n@patch_common\ndef test_give_response(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test give", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 18, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "19": {"text": "List[Document],\n) -> None:\n    \"\"\"Test give response.\"\"\"\n    prompt_helper = PromptHelper(MAX_CHUNK_SIZE, NUM_OUTPUTS, MAX_CHUNK_OVERLAP)\n    llm_predictor = LLMPredictor()\n    query_str = \"What is?\"\n\n    # test single line\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n        MOCK_TEXT_QA_PROMPT,\n        MOCK_REFINE_PROMPT,\n        texts=[TextChunk(\"This is a single line.\")],\n    )\n    response = builder.get_response(query_str)\n    assert str(response) == \"What is?:This is a single line.\"\n\n    # test multiple lines\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 19, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "20": {"text": " llm_predictor,\n        MOCK_TEXT_QA_PROMPT,\n        MOCK_REFINE_PROMPT,\n        texts=[TextChunk(documents[0].get_text())],\n    )\n    response = builder.get_response(query_str)\n    assert str(response) == \"What is?:Hello world.\"\n\n\n@patch.object(LLMPredictor, \"total_tokens_used\", return_value=0)\n@patch.object(LLMPredictor, \"predict\", side_effect=mock_llmpredictor_predict)\n@patch.object(LLMPredictor, \"__init__\", return_value=None)\ndef test_compact_response(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test give response.\"\"\"\n    # test response with ResponseMode.COMPACT\n    # NOTE: here", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 20, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "21": {"text": "response with ResponseMode.COMPACT\n    # NOTE: here we want to guarante that prompts have 0 extra tokens\n    mock_refine_prompt_tmpl = \"{query_str}{existing_answer}{context_msg}\"\n    mock_refine_prompt = RefinePrompt(mock_refine_prompt_tmpl)\n\n    mock_qa_prompt_tmpl = \"{context_str}{query_str}\"\n    mock_qa_prompt = QuestionAnswerPrompt(mock_qa_prompt_tmpl)\n\n    # max input size is 11, prompt is two tokens (the query) --> 9 tokens\n    # --> padding is 1 --> 8 tokens\n    prompt_helper = PromptHelper(\n        11, 0, 0, tokenizer=mock_tokenizer, separator=\"\\n\\n\", chunk_size_limit=4\n    )\n    cur_chunk_size = prompt_helper.get_chunk_size_given_prompt(\"\", 1, padding=1)\n    # outside of", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 21, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "22": {"text": "1, padding=1)\n    # outside of compact, assert that chunk size is 4\n    assert cur_chunk_size == 4\n\n    # within compact, make sure that chunk size is 8\n    llm_predictor = LLMPredictor()\n    query_str = \"What is?\"\n    texts = [\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nbar\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\nanother\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nfoo\"),\n    ]\n\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n        mock_qa_prompt,\n        mock_refine_prompt,\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 22, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "23": {"text": "       texts=texts,\n    )\n    response = builder.get_response(query_str, mode=ResponseMode.COMPACT)\n    assert str(response) == (\n        \"What is?:\" \"This\\n\\nis\\n\\na\\n\\nbar\\n\\n\" \"This\\n\\nis\\n\\na\\n\\ntest\"\n    )\n\n\n@patch.object(LLMPredictor, \"total_tokens_used\", return_value=0)\n@patch.object(LLMPredictor, \"predict\", side_effect=mock_llmpredictor_predict)\n@patch.object(LLMPredictor, \"__init__\", return_value=None)\ndef test_tree_summarize_response(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test give response.\"\"\"\n    # test response with ResponseMode.TREE_SUMMARIZE\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 23, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "24": {"text": "# test response with ResponseMode.TREE_SUMMARIZE\n    # NOTE: here we want to guarante that prompts have 0 extra tokens\n    mock_refine_prompt_tmpl = \"{query_str}{existing_answer}{context_msg}\"\n    mock_refine_prompt = RefinePrompt(mock_refine_prompt_tmpl)\n\n    mock_qa_prompt_tmpl = \"{context_str}{query_str}\"\n    mock_qa_prompt = QuestionAnswerPrompt(mock_qa_prompt_tmpl)\n\n    # max input size is 12, prompt tokens is 2 (query_str)\n    # --> 10 tokens for 2 chunks -->\n    # 5 tokens per chunk, 1 is padding --> 4 tokens per chunk\n    prompt_helper = PromptHelper(12, 0, 0, tokenizer=mock_tokenizer, separator=\"\\n\\n\")\n\n    # within tree_summarize, make sure that chunk size is 8\n    llm_predictor = LLMPredictor()\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 24, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "25": {"text": "  llm_predictor = LLMPredictor()\n    query_str = \"What is?\"\n    texts = [\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nbar\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\nanother\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nfoo\"),\n    ]\n\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n        mock_qa_prompt,\n        mock_refine_prompt,\n        texts=texts,\n    )\n    response = builder.get_response(\n        query_str, mode=ResponseMode.TREE_SUMMARIZE, num_children=2\n    )\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 25, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "26": {"text": "num_children=2\n    )\n    # TODO: fix this output, the \\n join appends unnecessary results at the end\n    assert str(response) == (\n        \"What is?:This\\n\\nis\\n\\na\\n\\nbar\\nThis\\n\" \"This\\n\\nis\\n\\nanother\\n\\ntest\\nThis\"\n    )\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 26, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "27": {"text": "\"\"\"Test indices/utils.py.\"\"\"\nfrom gpt_index.indices.utils import expand_tokens_with_subtokens\n\n\ndef test_expand_tokens_with_subtokens() -> None:\n    \"\"\"Test expand tokens.\"\"\"\n    tokens = {\"foo bar\", \"baz\", \"hello hello world bye\"}\n    keywords = expand_tokens_with_subtokens(tokens)\n    assert keywords == {\n        \"foo bar\",\n        \"foo\",\n        \"bar\",\n        \"baz\",\n        \"hello hello world bye\",\n        \"hello\",\n        \"world\",\n        \"bye\",\n    }\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_utils.py", "file_name": "test_utils.py"}, "index": 27, "child_indices": [], "ref_doc_id": "8027ba56fe884bcf3ec48d3b08a69aac608a1252", "node_info": null}, "28": {"text": "This code file contains tests for two functions in the gpt_index library. The first function, get_nodes_from_document, takes a document and a text splitter as input and returns a list of nodes. The second function, get_text_splitter, takes a prompt and returns a text splitter. The code also tests the functionality of these functions, such as ensuring that the chunk size is within the specified limit and that the extra info is included when specified.", "doc_id": null, "embedding": null, "extra_info": null, "index": 28, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "29": {"text": "This file contains tests for the PromptHelper class, which is used to help build responses from text chunks. The tests cover the methods get_text_from_nodes, get_numbered_text_from_nodes, compact_text, and get_biggest_prompt. Additionally, the file contains tests for the ResponseBuilder class, which is used to build responses from prompts and text chunks. The tests cover the methods give_response, compact_response, and tree_summarize_response.", "doc_id": null, "embedding": null, "extra_info": null, "index": 29, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "30": {"text": "test_response.py is a code file that tests the ResponseMode.TREE_SUMMARIZE feature of the ResponseBuilder class. It creates a mock tokenizer, prompt helper, and LLMPredictor, and then uses them to build a response to a query string. The test_utils.py file tests the expand_tokens_with_subtokens() function, which takes a set of tokens and returns a set of tokens and their subtokens.", "doc_id": null, "embedding": null, "extra_info": null, "index": 30, "child_indices": [24, 25, 26, 27], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"28": {"text": "This code file contains tests for two functions in the gpt_index library. The first function, get_nodes_from_document, takes a document and a text splitter as input and returns a list of nodes. The second function, get_text_splitter, takes a prompt and returns a text splitter. The code also tests the functionality of these functions, such as ensuring that the chunk size is within the specified limit and that the extra info is included when specified.", "doc_id": null, "embedding": null, "extra_info": null, "index": 28, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "29": {"text": "This file contains tests for the PromptHelper class, which is used to help build responses from text chunks. The tests cover the methods get_text_from_nodes, get_numbered_text_from_nodes, compact_text, and get_biggest_prompt. Additionally, the file contains tests for the ResponseBuilder class, which is used to build responses from prompts and text chunks. The tests cover the methods give_response, compact_response, and tree_summarize_response.", "doc_id": null, "embedding": null, "extra_info": null, "index": 29, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "30": {"text": "test_response.py is a code file that tests the ResponseMode.TREE_SUMMARIZE feature of the ResponseBuilder class. It creates a mock tokenizer, prompt helper, and LLMPredictor, and then uses them to build a response to a query string. The test_utils.py file tests the expand_tokens_with_subtokens() function, which takes a set of tokens and returns a set of tokens and their subtokens.", "doc_id": null, "embedding": null, "extra_info": null, "index": 30, "child_indices": [24, 25, 26, 27], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"1d4640565ae2765d9ca96a509dc9809217f62f2f": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "embedding": null, "extra_info": {"file_path": "tests/indices/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "2c621db7da92be19edfbf5fb7f66fe0d8609091f": {"text": "\"\"\"Test node utils.\"\"\"\n\nfrom typing import List\n\nimport pytest\n\nfrom gpt_index.indices.node_utils import get_nodes_from_document\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.readers.schema.base import Document\n\n\n@pytest.fixture\ndef text_splitter() -> TokenTextSplitter:\n    \"\"\"Get text splitter.\"\"\"\n    return TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n\n\n@pytest.fixture\ndef documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"\n    # NOTE: one document for now\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    return [\n        Document(doc_text, doc_id=\"test_doc_id\", extra_info={\"test_key\": \"test_val\"})\n    ]\n\n\ndef test_get_nodes_from_document(\n    documents: List[Document], text_splitter: TokenTextSplitter\n) -> None:\n    \"\"\"Test get nodes from document have desired chunk size.\"\"\"\n    nodes = get_nodes_from_document(\n        documents[0],\n        text_splitter,\n        start_idx=0,\n        include_extra_info=False,\n    )\n    assert len(nodes) == 2\n    actual_chunk_sizes = [\n        len(text_splitter.tokenizer(node.get_text())) for node in nodes\n    ]\n    assert all(\n        chunk_size <= text_splitter._chunk_size for chunk_size in actual_chunk_sizes\n    )\n\n\ndef test_get_nodes_from_document_with_extra_info(\n    documents: List[Document], text_splitter: TokenTextSplitter\n) -> None:\n    \"\"\"Test get nodes from document with extra info have desired chunk size.\"\"\"\n    nodes = get_nodes_from_document(\n        documents[0],\n        text_splitter,\n        start_idx=0,\n        include_extra_info=True,\n    )\n    assert len(nodes) == 3\n    actual_chunk_sizes = [\n        len(text_splitter.tokenizer(node.get_text())) for node in nodes\n    ]\n    assert all(\n        chunk_size <= text_splitter._chunk_size for chunk_size in actual_chunk_sizes\n    )\n    assert all([\"test_key: test_val\" in n.get_text() for n in nodes])\n", "doc_id": "2c621db7da92be19edfbf5fb7f66fe0d8609091f", "embedding": null, "extra_info": {"file_path": "tests/indices/test_node_utils.py", "file_name": "test_node_utils.py"}, "__type__": "Document"}, "726d88a507fa99e69d3f782fb2697d21049f82e0": {"text": "\"\"\"Test PromptHelper.\"\"\"\nfrom typing import List\n\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.prompts.base import Prompt\nfrom tests.mock_utils.mock_utils import mock_tokenizer\n\n\nclass TestPrompt(Prompt):\n    \"\"\"Test prompt class.\"\"\"\n\n    input_variables: List[str] = [\"text\"]\n\n\ndef test_get_chunk_size() -> None:\n    \"\"\"Test get chunk size given prompt.\"\"\"\n    # test with 1 chunk\n    empty_prompt_text = \"This is the prompt\"\n    prompt_helper = PromptHelper(\n        max_input_size=11, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    chunk_size = prompt_helper.get_chunk_size_given_prompt(\n        empty_prompt_text, 1, padding=0\n    )\n    assert chunk_size == 6\n\n    # test having 2 chunks\n    prompt_helper = PromptHelper(\n        max_input_size=11, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    chunk_size = prompt_helper.get_chunk_size_given_prompt(\n        empty_prompt_text, 2, padding=0\n    )\n    assert chunk_size == 3\n\n    # test with 2 chunks, and with chunk_size_limit\n    prompt_helper = PromptHelper(\n        max_input_size=11,\n        num_output=1,\n        max_chunk_overlap=0,\n        tokenizer=mock_tokenizer,\n        chunk_size_limit=2,\n    )\n    chunk_size = prompt_helper.get_chunk_size_given_prompt(\n        empty_prompt_text, 2, padding=0\n    )\n    assert chunk_size == 2\n\n    # test padding\n    prompt_helper = PromptHelper(\n        max_input_size=11, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    chunk_size = prompt_helper.get_chunk_size_given_prompt(\n        empty_prompt_text, 2, padding=1\n    )\n    assert chunk_size == 2\n\n\ndef test_get_text_splitter() -> None:\n    \"\"\"Test get text splitter.\"\"\"\n    test_prompt_text = \"This is the prompt{text}\"\n    test_prompt = TestPrompt(test_prompt_text)\n    prompt_helper = PromptHelper(\n        max_input_size=11, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    text_splitter = prompt_helper.get_text_splitter_given_prompt(\n        test_prompt, 2, padding=1\n    )\n    assert text_splitter._chunk_size == 2\n    test_text = \"Hello world foo Hello world bar\"\n    text_chunks = text_splitter.split_text(test_text)\n    assert text_chunks == [\"Hello world\", \"foo Hello\", \"world bar\"]\n    truncated_text = text_splitter.truncate_text(test_text)\n    assert truncated_text == \"Hello world\"\n\n    # test with chunk_size_limit\n    prompt_helper = PromptHelper(\n        max_input_size=11,\n        num_output=1,\n        max_chunk_overlap=0,\n        tokenizer=mock_tokenizer,\n        chunk_size_limit=1,\n    )\n    text_splitter = prompt_helper.get_text_splitter_given_prompt(\n        test_prompt, 2, padding=1\n    )\n    text_chunks = text_splitter.split_text(test_text)\n    assert text_chunks == [\"Hello\", \"world\", \"foo\", \"Hello\", \"world\", \"bar\"]\n\n\ndef test_get_text_splitter_partial() -> None:\n    \"\"\"Test get text splitter with a partially formatted prompt.\"\"\"\n\n    class TestPromptFoo(Prompt):\n        \"\"\"Test prompt class.\"\"\"\n\n        input_variables: List[str] = [\"foo\", \"text\"]\n\n    # test without partially formatting\n    test_prompt_text = \"This is the {foo} prompt{text}\"\n    test_prompt = TestPromptFoo(test_prompt_text)\n    prompt_helper = PromptHelper(\n        max_input_size=11, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    text_splitter = prompt_helper.get_text_splitter_given_prompt(\n        test_prompt, 2, padding=1\n    )\n    test_text = \"Hello world foo Hello world bar\"\n    text_chunks = text_splitter.split_text(test_text)\n    assert text_chunks == [\"Hello world\", \"foo Hello\", \"world bar\"]\n    truncated_text = text_splitter.truncate_text(test_text)\n    assert truncated_text == \"Hello world\"\n\n    # test with partially formatting\n    test_prompt = TestPromptFoo(test_prompt_text)\n    test_prompt = test_prompt.partial_format(foo=\"bar\")\n    prompt_helper = PromptHelper(\n        max_input_size=12, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    assert prompt_helper._get_empty_prompt_txt(test_prompt) == \"This is the bar prompt\"\n    text_splitter = prompt_helper.get_text_splitter_given_prompt(\n        test_prompt, 2, padding=1\n    )\n    test_text = \"Hello world foo Hello world bar\"\n    text_chunks = text_splitter.split_text(test_text)\n    assert text_chunks == [\"Hello world\", \"foo Hello\", \"world bar\"]\n    truncated_text = text_splitter.truncate_text(test_text)\n    assert truncated_text == \"Hello world\"\n\n\ndef test_get_text_from_nodes() -> None:\n    \"\"\"Test get_text_from_nodes.\"\"\"\n    # test prompt uses up one token\n    test_prompt_txt = \"test{text}\"\n    test_prompt = TestPrompt(test_prompt_txt)\n    # set max_input_size=11\n    # For each text chunk, there's 4 tokens for text + 1 for the padding\n    prompt_helper = PromptHelper(\n        max_input_size=11, num_output=0, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    node1 = Node(text=\"This is a test foo bar\")\n    node2 = Node(text=\"Hello world bar foo\")\n\n    response = prompt_helper.get_text_from_nodes([node1, node2], prompt=test_prompt)\n    assert str(response) == (\"This is a test\\n\" \"Hello world bar foo\")\n\n\ndef test_get_numbered_text_from_nodes() -> None:\n    \"\"\"Test get_text_from_nodes.\"\"\"\n    # test prompt uses up one token\n    test_prompt_txt = \"test{text}\"\n    test_prompt = TestPrompt(test_prompt_txt)\n    # set max_input_size=17\n    # For each text chunk, there's 3 for text, 5 for padding (including number)\n    prompt_helper = PromptHelper(\n        max_input_size=17, num_output=0, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    node1 = Node(text=\"This is a test foo bar\")\n    node2 = Node(text=\"Hello world bar foo\")\n\n    response = prompt_helper.get_numbered_text_from_nodes(\n        [node1, node2], prompt=test_prompt\n    )\n    assert str(response) == (\"(1) This is a\\n\\n(2) Hello world bar\")\n\n\ndef test_compact_text() -> None:\n    \"\"\"Test compact text.\"\"\"\n    test_prompt_text = \"This is the prompt{text}\"\n    test_prompt = TestPrompt(test_prompt_text)\n    prompt_helper = PromptHelper(\n        max_input_size=9,\n        num_output=1,\n        max_chunk_overlap=0,\n        tokenizer=mock_tokenizer,\n        separator=\"\\n\\n\",\n    )\n    text_chunks = [\"Hello\", \"world\", \"foo\", \"Hello\", \"world\", \"bar\"]\n    compacted_chunks = prompt_helper.compact_text_chunks(test_prompt, text_chunks)\n    assert compacted_chunks == [\"Hello\\n\\nworld\\n\\nfoo\", \"Hello\\n\\nworld\\n\\nbar\"]\n\n\ndef test_get_biggest_prompt() -> None:\n    \"\"\"Test get_biggest_prompt from PromptHelper.\"\"\"\n    # NOTE: inputs don't matter\n    prompt_helper = PromptHelper(max_input_size=1, num_output=1, max_chunk_overlap=0)\n    prompt1 = TestPrompt(\"This is the prompt{text}\")\n    prompt2 = TestPrompt(\"This is the longer prompt{text}\")\n    prompt3 = TestPrompt(\"This is the {text}\")\n    biggest_prompt = prompt_helper.get_biggest_prompt([prompt1, prompt2, prompt3])\n    assert biggest_prompt.prompt.template == prompt2.prompt.template\n", "doc_id": "726d88a507fa99e69d3f782fb2697d21049f82e0", "embedding": null, "extra_info": {"file_path": "tests/indices/test_prompt_helper.py", "file_name": "test_prompt_helper.py"}, "__type__": "Document"}, "c2a0685b15adc5262f0ab5349e856d6210e6e7bb": {"text": "\"\"\"Test response utils.\"\"\"\n\nfrom typing import Any, List\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom gpt_index.constants import MAX_CHUNK_OVERLAP, MAX_CHUNK_SIZE, NUM_OUTPUTS\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.response.builder import ResponseBuilder, ResponseMode, TextChunk\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\nfrom gpt_index.readers.schema.base import Document\nfrom tests.mock_utils.mock_decorator import patch_common\nfrom tests.mock_utils.mock_predict import mock_llmpredictor_predict\nfrom tests.mock_utils.mock_prompts import MOCK_REFINE_PROMPT, MOCK_TEXT_QA_PROMPT\n\n\n@pytest.fixture\ndef documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"\n    # NOTE: one document for now\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    return [Document(doc_text)]\n\n\ndef mock_tokenizer(text: str) -> List[str]:\n    \"\"\"Mock tokenizer.\"\"\"\n    if text == \"\":\n        return []\n    tokens = text.split(\" \")\n    return tokens\n\n\n@patch_common\ndef test_give_response(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test give response.\"\"\"\n    prompt_helper = PromptHelper(MAX_CHUNK_SIZE, NUM_OUTPUTS, MAX_CHUNK_OVERLAP)\n    llm_predictor = LLMPredictor()\n    query_str = \"What is?\"\n\n    # test single line\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n        MOCK_TEXT_QA_PROMPT,\n        MOCK_REFINE_PROMPT,\n        texts=[TextChunk(\"This is a single line.\")],\n    )\n    response = builder.get_response(query_str)\n    assert str(response) == \"What is?:This is a single line.\"\n\n    # test multiple lines\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n        MOCK_TEXT_QA_PROMPT,\n        MOCK_REFINE_PROMPT,\n        texts=[TextChunk(documents[0].get_text())],\n    )\n    response = builder.get_response(query_str)\n    assert str(response) == \"What is?:Hello world.\"\n\n\n@patch.object(LLMPredictor, \"total_tokens_used\", return_value=0)\n@patch.object(LLMPredictor, \"predict\", side_effect=mock_llmpredictor_predict)\n@patch.object(LLMPredictor, \"__init__\", return_value=None)\ndef test_compact_response(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test give response.\"\"\"\n    # test response with ResponseMode.COMPACT\n    # NOTE: here we want to guarante that prompts have 0 extra tokens\n    mock_refine_prompt_tmpl = \"{query_str}{existing_answer}{context_msg}\"\n    mock_refine_prompt = RefinePrompt(mock_refine_prompt_tmpl)\n\n    mock_qa_prompt_tmpl = \"{context_str}{query_str}\"\n    mock_qa_prompt = QuestionAnswerPrompt(mock_qa_prompt_tmpl)\n\n    # max input size is 11, prompt is two tokens (the query) --> 9 tokens\n    # --> padding is 1 --> 8 tokens\n    prompt_helper = PromptHelper(\n        11, 0, 0, tokenizer=mock_tokenizer, separator=\"\\n\\n\", chunk_size_limit=4\n    )\n    cur_chunk_size = prompt_helper.get_chunk_size_given_prompt(\"\", 1, padding=1)\n    # outside of compact, assert that chunk size is 4\n    assert cur_chunk_size == 4\n\n    # within compact, make sure that chunk size is 8\n    llm_predictor = LLMPredictor()\n    query_str = \"What is?\"\n    texts = [\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nbar\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\nanother\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nfoo\"),\n    ]\n\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n        mock_qa_prompt,\n        mock_refine_prompt,\n        texts=texts,\n    )\n    response = builder.get_response(query_str, mode=ResponseMode.COMPACT)\n    assert str(response) == (\n        \"What is?:\" \"This\\n\\nis\\n\\na\\n\\nbar\\n\\n\" \"This\\n\\nis\\n\\na\\n\\ntest\"\n    )\n\n\n@patch.object(LLMPredictor, \"total_tokens_used\", return_value=0)\n@patch.object(LLMPredictor, \"predict\", side_effect=mock_llmpredictor_predict)\n@patch.object(LLMPredictor, \"__init__\", return_value=None)\ndef test_tree_summarize_response(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test give response.\"\"\"\n    # test response with ResponseMode.TREE_SUMMARIZE\n    # NOTE: here we want to guarante that prompts have 0 extra tokens\n    mock_refine_prompt_tmpl = \"{query_str}{existing_answer}{context_msg}\"\n    mock_refine_prompt = RefinePrompt(mock_refine_prompt_tmpl)\n\n    mock_qa_prompt_tmpl = \"{context_str}{query_str}\"\n    mock_qa_prompt = QuestionAnswerPrompt(mock_qa_prompt_tmpl)\n\n    # max input size is 12, prompt tokens is 2 (query_str)\n    # --> 10 tokens for 2 chunks -->\n    # 5 tokens per chunk, 1 is padding --> 4 tokens per chunk\n    prompt_helper = PromptHelper(12, 0, 0, tokenizer=mock_tokenizer, separator=\"\\n\\n\")\n\n    # within tree_summarize, make sure that chunk size is 8\n    llm_predictor = LLMPredictor()\n    query_str = \"What is?\"\n    texts = [\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nbar\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\nanother\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nfoo\"),\n    ]\n\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n        mock_qa_prompt,\n        mock_refine_prompt,\n        texts=texts,\n    )\n    response = builder.get_response(\n        query_str, mode=ResponseMode.TREE_SUMMARIZE, num_children=2\n    )\n    # TODO: fix this output, the \\n join appends unnecessary results at the end\n    assert str(response) == (\n        \"What is?:This\\n\\nis\\n\\na\\n\\nbar\\nThis\\n\" \"This\\n\\nis\\n\\nanother\\n\\ntest\\nThis\"\n    )\n", "doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "__type__": "Document"}, "8027ba56fe884bcf3ec48d3b08a69aac608a1252": {"text": "\"\"\"Test indices/utils.py.\"\"\"\nfrom gpt_index.indices.utils import expand_tokens_with_subtokens\n\n\ndef test_expand_tokens_with_subtokens() -> None:\n    \"\"\"Test expand tokens.\"\"\"\n    tokens = {\"foo bar\", \"baz\", \"hello hello world bye\"}\n    keywords = expand_tokens_with_subtokens(tokens)\n    assert keywords == {\n        \"foo bar\",\n        \"foo\",\n        \"bar\",\n        \"baz\",\n        \"hello hello world bye\",\n        \"hello\",\n        \"world\",\n        \"bye\",\n    }\n", "doc_id": "8027ba56fe884bcf3ec48d3b08a69aac608a1252", "embedding": null, "extra_info": {"file_path": "tests/indices/test_utils.py", "file_name": "test_utils.py"}, "__type__": "Document"}, "dd176b17-4a38-4b69-b938-738b739f01d9": {"text": "\nThe summaries of these documents are:\n1. Test file containing a fixture for a text splitter and a list of documents.\n2. Test file containing tests for the get_nodes_from_document function.\n3. Test file containing tests for the PromptHelper class.\n4. Test file testing the ResponseMode.TREE_SUMMARIZE feature of the ResponseBuilder class.\n5. Test file testing the expand_tokens_with_subtokens() function.", "doc_id": "dd176b17-4a38-4b69-b938-738b739f01d9", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "node_info": null}, "1": {"text": "\"\"\"Test node utils.\"\"\"\n\nfrom typing import List\n\nimport pytest\n\nfrom gpt_index.indices.node_utils import get_nodes_from_document\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.readers.schema.base import Document\n\n\n@pytest.fixture\ndef text_splitter() -> TokenTextSplitter:\n    \"\"\"Get text splitter.\"\"\"\n    return TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n\n\n@pytest.fixture\ndef documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"\n    # NOTE: one document for now\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    return [\n        Document(doc_text, doc_id=\"test_doc_id\",", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_node_utils.py", "file_name": "test_node_utils.py"}, "index": 1, "child_indices": [], "ref_doc_id": "2c621db7da92be19edfbf5fb7f66fe0d8609091f", "node_info": null}, "2": {"text": "Document(doc_text, doc_id=\"test_doc_id\", extra_info={\"test_key\": \"test_val\"})\n    ]\n\n\ndef test_get_nodes_from_document(\n    documents: List[Document], text_splitter: TokenTextSplitter\n) -> None:\n    \"\"\"Test get nodes from document have desired chunk size.\"\"\"\n    nodes = get_nodes_from_document(\n        documents[0],\n        text_splitter,\n        start_idx=0,\n        include_extra_info=False,\n    )\n    assert len(nodes) == 2\n    actual_chunk_sizes = [\n        len(text_splitter.tokenizer(node.get_text())) for node in nodes\n    ]\n    assert all(\n        chunk_size <= text_splitter._chunk_size for chunk_size in actual_chunk_sizes\n    )\n\n\ndef", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_node_utils.py", "file_name": "test_node_utils.py"}, "index": 2, "child_indices": [], "ref_doc_id": "2c621db7da92be19edfbf5fb7f66fe0d8609091f", "node_info": null}, "3": {"text": "in actual_chunk_sizes\n    )\n\n\ndef test_get_nodes_from_document_with_extra_info(\n    documents: List[Document], text_splitter: TokenTextSplitter\n) -> None:\n    \"\"\"Test get nodes from document with extra info have desired chunk size.\"\"\"\n    nodes = get_nodes_from_document(\n        documents[0],\n        text_splitter,\n        start_idx=0,\n        include_extra_info=True,\n    )\n    assert len(nodes) == 3\n    actual_chunk_sizes = [\n        len(text_splitter.tokenizer(node.get_text())) for node in nodes\n    ]\n    assert all(\n        chunk_size <= text_splitter._chunk_size for chunk_size in actual_chunk_sizes\n    )\n    assert all([\"test_key: test_val\" in", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_node_utils.py", "file_name": "test_node_utils.py"}, "index": 3, "child_indices": [], "ref_doc_id": "2c621db7da92be19edfbf5fb7f66fe0d8609091f", "node_info": null}, "4": {"text": "   assert all([\"test_key: test_val\" in n.get_text() for n in nodes])\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_node_utils.py", "file_name": "test_node_utils.py"}, "index": 4, "child_indices": [], "ref_doc_id": "2c621db7da92be19edfbf5fb7f66fe0d8609091f", "node_info": null}, "5": {"text": "\"\"\"Test PromptHelper.\"\"\"\nfrom typing import List\n\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.prompts.base import Prompt\nfrom tests.mock_utils.mock_utils import mock_tokenizer\n\n\nclass TestPrompt(Prompt):\n    \"\"\"Test prompt class.\"\"\"\n\n    input_variables: List[str] = [\"text\"]\n\n\ndef test_get_chunk_size() -> None:\n    \"\"\"Test get chunk size given prompt.\"\"\"\n    # test with 1 chunk\n    empty_prompt_text = \"This is the prompt\"\n    prompt_helper = PromptHelper(\n        max_input_size=11, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    chunk_size = prompt_helper.get_chunk_size_given_prompt(\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_prompt_helper.py", "file_name": "test_prompt_helper.py"}, "index": 5, "child_indices": [], "ref_doc_id": "726d88a507fa99e69d3f782fb2697d21049f82e0", "node_info": null}, "6": {"text": "       empty_prompt_text, 1, padding=0\n    )\n    assert chunk_size == 6\n\n    # test having 2 chunks\n    prompt_helper = PromptHelper(\n        max_input_size=11, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    chunk_size = prompt_helper.get_chunk_size_given_prompt(\n        empty_prompt_text, 2, padding=0\n    )\n    assert chunk_size == 3\n\n    # test with 2 chunks, and with chunk_size_limit\n    prompt_helper = PromptHelper(\n        max_input_size=11,\n        num_output=1,\n        max_chunk_overlap=0,\n        tokenizer=mock_tokenizer,\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_prompt_helper.py", "file_name": "test_prompt_helper.py"}, "index": 6, "child_indices": [], "ref_doc_id": "726d88a507fa99e69d3f782fb2697d21049f82e0", "node_info": null}, "7": {"text": "       chunk_size_limit=2,\n    )\n    chunk_size = prompt_helper.get_chunk_size_given_prompt(\n        empty_prompt_text, 2, padding=0\n    )\n    assert chunk_size == 2\n\n    # test padding\n    prompt_helper = PromptHelper(\n        max_input_size=11, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    chunk_size = prompt_helper.get_chunk_size_given_prompt(\n        empty_prompt_text, 2, padding=1\n    )\n    assert chunk_size == 2\n\n\ndef test_get_text_splitter() -> None:\n    \"\"\"Test get text splitter.\"\"\"\n    test_prompt_text = \"This is the prompt{text}\"\n    test_prompt =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_prompt_helper.py", "file_name": "test_prompt_helper.py"}, "index": 7, "child_indices": [], "ref_doc_id": "726d88a507fa99e69d3f782fb2697d21049f82e0", "node_info": null}, "8": {"text": "is the prompt{text}\"\n    test_prompt = TestPrompt(test_prompt_text)\n    prompt_helper = PromptHelper(\n        max_input_size=11, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    text_splitter = prompt_helper.get_text_splitter_given_prompt(\n        test_prompt, 2, padding=1\n    )\n    assert text_splitter._chunk_size == 2\n    test_text = \"Hello world foo Hello world bar\"\n    text_chunks = text_splitter.split_text(test_text)\n    assert text_chunks == [\"Hello world\", \"foo Hello\", \"world bar\"]\n    truncated_text = text_splitter.truncate_text(test_text)\n    assert truncated_text == \"Hello world\"\n\n    # test with chunk_size_limit\n    prompt_helper = PromptHelper(\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_prompt_helper.py", "file_name": "test_prompt_helper.py"}, "index": 8, "child_indices": [], "ref_doc_id": "726d88a507fa99e69d3f782fb2697d21049f82e0", "node_info": null}, "9": {"text": "   prompt_helper = PromptHelper(\n        max_input_size=11,\n        num_output=1,\n        max_chunk_overlap=0,\n        tokenizer=mock_tokenizer,\n        chunk_size_limit=1,\n    )\n    text_splitter = prompt_helper.get_text_splitter_given_prompt(\n        test_prompt, 2, padding=1\n    )\n    text_chunks = text_splitter.split_text(test_text)\n    assert text_chunks == [\"Hello\", \"world\", \"foo\", \"Hello\", \"world\", \"bar\"]\n\n\ndef test_get_text_splitter_partial() -> None:\n    \"\"\"Test get text splitter with a partially formatted prompt.\"\"\"\n\n    class TestPromptFoo(Prompt):\n        \"\"\"Test prompt class.\"\"\"\n\n        input_variables:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_prompt_helper.py", "file_name": "test_prompt_helper.py"}, "index": 9, "child_indices": [], "ref_doc_id": "726d88a507fa99e69d3f782fb2697d21049f82e0", "node_info": null}, "10": {"text": "class.\"\"\"\n\n        input_variables: List[str] = [\"foo\", \"text\"]\n\n    # test without partially formatting\n    test_prompt_text = \"This is the {foo} prompt{text}\"\n    test_prompt = TestPromptFoo(test_prompt_text)\n    prompt_helper = PromptHelper(\n        max_input_size=11, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    text_splitter = prompt_helper.get_text_splitter_given_prompt(\n        test_prompt, 2, padding=1\n    )\n    test_text = \"Hello world foo Hello world bar\"\n    text_chunks = text_splitter.split_text(test_text)\n    assert text_chunks == [\"Hello world\", \"foo Hello\", \"world bar\"]\n    truncated_text = text_splitter.truncate_text(test_text)\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_prompt_helper.py", "file_name": "test_prompt_helper.py"}, "index": 10, "child_indices": [], "ref_doc_id": "726d88a507fa99e69d3f782fb2697d21049f82e0", "node_info": null}, "11": {"text": "   assert truncated_text == \"Hello world\"\n\n    # test with partially formatting\n    test_prompt = TestPromptFoo(test_prompt_text)\n    test_prompt = test_prompt.partial_format(foo=\"bar\")\n    prompt_helper = PromptHelper(\n        max_input_size=12, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    assert prompt_helper._get_empty_prompt_txt(test_prompt) == \"This is the bar prompt\"\n    text_splitter = prompt_helper.get_text_splitter_given_prompt(\n        test_prompt, 2, padding=1\n    )\n    test_text = \"Hello world foo Hello world bar\"\n    text_chunks = text_splitter.split_text(test_text)\n    assert text_chunks == [\"Hello world\", \"foo Hello\", \"world bar\"]\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_prompt_helper.py", "file_name": "test_prompt_helper.py"}, "index": 11, "child_indices": [], "ref_doc_id": "726d88a507fa99e69d3f782fb2697d21049f82e0", "node_info": null}, "12": {"text": "[\"Hello world\", \"foo Hello\", \"world bar\"]\n    truncated_text = text_splitter.truncate_text(test_text)\n    assert truncated_text == \"Hello world\"\n\n\ndef test_get_text_from_nodes() -> None:\n    \"\"\"Test get_text_from_nodes.\"\"\"\n    # test prompt uses up one token\n    test_prompt_txt = \"test{text}\"\n    test_prompt = TestPrompt(test_prompt_txt)\n    # set max_input_size=11\n    # For each text chunk, there's 4 tokens for text + 1 for the padding\n    prompt_helper = PromptHelper(\n        max_input_size=11, num_output=0, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    node1 = Node(text=\"This is a test foo bar\")\n    node2 = Node(text=\"Hello world bar foo\")\n\n    response =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_prompt_helper.py", "file_name": "test_prompt_helper.py"}, "index": 12, "child_indices": [], "ref_doc_id": "726d88a507fa99e69d3f782fb2697d21049f82e0", "node_info": null}, "13": {"text": "= Node(text=\"Hello world bar foo\")\n\n    response = prompt_helper.get_text_from_nodes([node1, node2], prompt=test_prompt)\n    assert str(response) == (\"This is a test\\n\" \"Hello world bar foo\")\n\n\ndef test_get_numbered_text_from_nodes() -> None:\n    \"\"\"Test get_text_from_nodes.\"\"\"\n    # test prompt uses up one token\n    test_prompt_txt = \"test{text}\"\n    test_prompt = TestPrompt(test_prompt_txt)\n    # set max_input_size=17\n    # For each text chunk, there's 3 for text, 5 for padding (including number)\n    prompt_helper = PromptHelper(\n        max_input_size=17, num_output=0, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    node1 = Node(text=\"This is a test foo bar\")\n    node2 = Node(text=\"Hello world bar", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_prompt_helper.py", "file_name": "test_prompt_helper.py"}, "index": 13, "child_indices": [], "ref_doc_id": "726d88a507fa99e69d3f782fb2697d21049f82e0", "node_info": null}, "14": {"text": "bar\")\n    node2 = Node(text=\"Hello world bar foo\")\n\n    response = prompt_helper.get_numbered_text_from_nodes(\n        [node1, node2], prompt=test_prompt\n    )\n    assert str(response) == (\"(1) This is a\\n\\n(2) Hello world bar\")\n\n\ndef test_compact_text() -> None:\n    \"\"\"Test compact text.\"\"\"\n    test_prompt_text = \"This is the prompt{text}\"\n    test_prompt = TestPrompt(test_prompt_text)\n    prompt_helper = PromptHelper(\n        max_input_size=9,\n        num_output=1,\n        max_chunk_overlap=0,\n        tokenizer=mock_tokenizer,\n        separator=\"\\n\\n\",\n    )\n    text_chunks = [\"Hello\", \"world\", \"foo\",", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_prompt_helper.py", "file_name": "test_prompt_helper.py"}, "index": 14, "child_indices": [], "ref_doc_id": "726d88a507fa99e69d3f782fb2697d21049f82e0", "node_info": null}, "15": {"text": "  text_chunks = [\"Hello\", \"world\", \"foo\", \"Hello\", \"world\", \"bar\"]\n    compacted_chunks = prompt_helper.compact_text_chunks(test_prompt, text_chunks)\n    assert compacted_chunks == [\"Hello\\n\\nworld\\n\\nfoo\", \"Hello\\n\\nworld\\n\\nbar\"]\n\n\ndef test_get_biggest_prompt() -> None:\n    \"\"\"Test get_biggest_prompt from PromptHelper.\"\"\"\n    # NOTE: inputs don't matter\n    prompt_helper = PromptHelper(max_input_size=1, num_output=1, max_chunk_overlap=0)\n    prompt1 = TestPrompt(\"This is the prompt{text}\")\n    prompt2 = TestPrompt(\"This is the longer prompt{text}\")\n    prompt3 = TestPrompt(\"This is the {text}\")\n    biggest_prompt = prompt_helper.get_biggest_prompt([prompt1, prompt2,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_prompt_helper.py", "file_name": "test_prompt_helper.py"}, "index": 15, "child_indices": [], "ref_doc_id": "726d88a507fa99e69d3f782fb2697d21049f82e0", "node_info": null}, "16": {"text": "prompt2, prompt3])\n    assert biggest_prompt.prompt.template == prompt2.prompt.template\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_prompt_helper.py", "file_name": "test_prompt_helper.py"}, "index": 16, "child_indices": [], "ref_doc_id": "726d88a507fa99e69d3f782fb2697d21049f82e0", "node_info": null}, "17": {"text": "\"\"\"Test response utils.\"\"\"\n\nfrom typing import Any, List\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom gpt_index.constants import MAX_CHUNK_OVERLAP, MAX_CHUNK_SIZE, NUM_OUTPUTS\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.response.builder import ResponseBuilder, ResponseMode, TextChunk\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\nfrom gpt_index.readers.schema.base import Document\nfrom tests.mock_utils.mock_decorator import patch_common\nfrom tests.mock_utils.mock_predict import mock_llmpredictor_predict\nfrom tests.mock_utils.mock_prompts import MOCK_REFINE_PROMPT, MOCK_TEXT_QA_PROMPT\n\n\n@pytest.fixture\ndef documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"\n    # NOTE: one document for", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 17, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "18": {"text": "\"\"\"Get documents.\"\"\"\n    # NOTE: one document for now\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    return [Document(doc_text)]\n\n\ndef mock_tokenizer(text: str) -> List[str]:\n    \"\"\"Mock tokenizer.\"\"\"\n    if text == \"\":\n        return []\n    tokens = text.split(\" \")\n    return tokens\n\n\n@patch_common\ndef test_give_response(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test give", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 18, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "19": {"text": "List[Document],\n) -> None:\n    \"\"\"Test give response.\"\"\"\n    prompt_helper = PromptHelper(MAX_CHUNK_SIZE, NUM_OUTPUTS, MAX_CHUNK_OVERLAP)\n    llm_predictor = LLMPredictor()\n    query_str = \"What is?\"\n\n    # test single line\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n        MOCK_TEXT_QA_PROMPT,\n        MOCK_REFINE_PROMPT,\n        texts=[TextChunk(\"This is a single line.\")],\n    )\n    response = builder.get_response(query_str)\n    assert str(response) == \"What is?:This is a single line.\"\n\n    # test multiple lines\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 19, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "20": {"text": " llm_predictor,\n        MOCK_TEXT_QA_PROMPT,\n        MOCK_REFINE_PROMPT,\n        texts=[TextChunk(documents[0].get_text())],\n    )\n    response = builder.get_response(query_str)\n    assert str(response) == \"What is?:Hello world.\"\n\n\n@patch.object(LLMPredictor, \"total_tokens_used\", return_value=0)\n@patch.object(LLMPredictor, \"predict\", side_effect=mock_llmpredictor_predict)\n@patch.object(LLMPredictor, \"__init__\", return_value=None)\ndef test_compact_response(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test give response.\"\"\"\n    # test response with ResponseMode.COMPACT\n    # NOTE: here", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 20, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "21": {"text": "response with ResponseMode.COMPACT\n    # NOTE: here we want to guarante that prompts have 0 extra tokens\n    mock_refine_prompt_tmpl = \"{query_str}{existing_answer}{context_msg}\"\n    mock_refine_prompt = RefinePrompt(mock_refine_prompt_tmpl)\n\n    mock_qa_prompt_tmpl = \"{context_str}{query_str}\"\n    mock_qa_prompt = QuestionAnswerPrompt(mock_qa_prompt_tmpl)\n\n    # max input size is 11, prompt is two tokens (the query) --> 9 tokens\n    # --> padding is 1 --> 8 tokens\n    prompt_helper = PromptHelper(\n        11, 0, 0, tokenizer=mock_tokenizer, separator=\"\\n\\n\", chunk_size_limit=4\n    )\n    cur_chunk_size = prompt_helper.get_chunk_size_given_prompt(\"\", 1, padding=1)\n    # outside of", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 21, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "22": {"text": "1, padding=1)\n    # outside of compact, assert that chunk size is 4\n    assert cur_chunk_size == 4\n\n    # within compact, make sure that chunk size is 8\n    llm_predictor = LLMPredictor()\n    query_str = \"What is?\"\n    texts = [\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nbar\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\nanother\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nfoo\"),\n    ]\n\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n        mock_qa_prompt,\n        mock_refine_prompt,\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 22, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "23": {"text": "       texts=texts,\n    )\n    response = builder.get_response(query_str, mode=ResponseMode.COMPACT)\n    assert str(response) == (\n        \"What is?:\" \"This\\n\\nis\\n\\na\\n\\nbar\\n\\n\" \"This\\n\\nis\\n\\na\\n\\ntest\"\n    )\n\n\n@patch.object(LLMPredictor, \"total_tokens_used\", return_value=0)\n@patch.object(LLMPredictor, \"predict\", side_effect=mock_llmpredictor_predict)\n@patch.object(LLMPredictor, \"__init__\", return_value=None)\ndef test_tree_summarize_response(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test give response.\"\"\"\n    # test response with ResponseMode.TREE_SUMMARIZE\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 23, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "24": {"text": "# test response with ResponseMode.TREE_SUMMARIZE\n    # NOTE: here we want to guarante that prompts have 0 extra tokens\n    mock_refine_prompt_tmpl = \"{query_str}{existing_answer}{context_msg}\"\n    mock_refine_prompt = RefinePrompt(mock_refine_prompt_tmpl)\n\n    mock_qa_prompt_tmpl = \"{context_str}{query_str}\"\n    mock_qa_prompt = QuestionAnswerPrompt(mock_qa_prompt_tmpl)\n\n    # max input size is 12, prompt tokens is 2 (query_str)\n    # --> 10 tokens for 2 chunks -->\n    # 5 tokens per chunk, 1 is padding --> 4 tokens per chunk\n    prompt_helper = PromptHelper(12, 0, 0, tokenizer=mock_tokenizer, separator=\"\\n\\n\")\n\n    # within tree_summarize, make sure that chunk size is 8\n    llm_predictor = LLMPredictor()\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 24, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "25": {"text": "  llm_predictor = LLMPredictor()\n    query_str = \"What is?\"\n    texts = [\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nbar\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\nanother\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nfoo\"),\n    ]\n\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n        mock_qa_prompt,\n        mock_refine_prompt,\n        texts=texts,\n    )\n    response = builder.get_response(\n        query_str, mode=ResponseMode.TREE_SUMMARIZE, num_children=2\n    )\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 25, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "26": {"text": "num_children=2\n    )\n    # TODO: fix this output, the \\n join appends unnecessary results at the end\n    assert str(response) == (\n        \"What is?:This\\n\\nis\\n\\na\\n\\nbar\\nThis\\n\" \"This\\n\\nis\\n\\nanother\\n\\ntest\\nThis\"\n    )\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 26, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "27": {"text": "\"\"\"Test indices/utils.py.\"\"\"\nfrom gpt_index.indices.utils import expand_tokens_with_subtokens\n\n\ndef test_expand_tokens_with_subtokens() -> None:\n    \"\"\"Test expand tokens.\"\"\"\n    tokens = {\"foo bar\", \"baz\", \"hello hello world bye\"}\n    keywords = expand_tokens_with_subtokens(tokens)\n    assert keywords == {\n        \"foo bar\",\n        \"foo\",\n        \"bar\",\n        \"baz\",\n        \"hello hello world bye\",\n        \"hello\",\n        \"world\",\n        \"bye\",\n    }\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_utils.py", "file_name": "test_utils.py"}, "index": 27, "child_indices": [], "ref_doc_id": "8027ba56fe884bcf3ec48d3b08a69aac608a1252", "node_info": null}, "28": {"text": "This code file contains tests for two functions in the gpt_index library. The first function, get_nodes_from_document, takes a document and a text splitter as input and returns a list of nodes. The second function, get_text_splitter, takes a prompt and returns a text splitter. The code also tests the functionality of these functions, such as ensuring that the chunk size is within the specified limit and that the extra info is included when specified.", "doc_id": null, "embedding": null, "extra_info": null, "index": 28, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "29": {"text": "This file contains tests for the PromptHelper class, which is used to help build responses from text chunks. The tests cover the methods get_text_from_nodes, get_numbered_text_from_nodes, compact_text, and get_biggest_prompt. Additionally, the file contains tests for the ResponseBuilder class, which is used to build responses from prompts and text chunks. The tests cover the methods give_response, compact_response, and tree_summarize_response.", "doc_id": null, "embedding": null, "extra_info": null, "index": 29, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "30": {"text": "test_response.py is a code file that tests the ResponseMode.TREE_SUMMARIZE feature of the ResponseBuilder class. It creates a mock tokenizer, prompt helper, and LLMPredictor, and then uses them to build a response to a query string. The test_utils.py file tests the expand_tokens_with_subtokens() function, which takes a set of tokens and returns a set of tokens and their subtokens.", "doc_id": null, "embedding": null, "extra_info": null, "index": 30, "child_indices": [24, 25, 26, 27], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"28": {"text": "This code file contains tests for two functions in the gpt_index library. The first function, get_nodes_from_document, takes a document and a text splitter as input and returns a list of nodes. The second function, get_text_splitter, takes a prompt and returns a text splitter. The code also tests the functionality of these functions, such as ensuring that the chunk size is within the specified limit and that the extra info is included when specified.", "doc_id": null, "embedding": null, "extra_info": null, "index": 28, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "29": {"text": "This file contains tests for the PromptHelper class, which is used to help build responses from text chunks. The tests cover the methods get_text_from_nodes, get_numbered_text_from_nodes, compact_text, and get_biggest_prompt. Additionally, the file contains tests for the ResponseBuilder class, which is used to build responses from prompts and text chunks. The tests cover the methods give_response, compact_response, and tree_summarize_response.", "doc_id": null, "embedding": null, "extra_info": null, "index": 29, "child_indices": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], "ref_doc_id": null, "node_info": null}, "30": {"text": "test_response.py is a code file that tests the ResponseMode.TREE_SUMMARIZE feature of the ResponseBuilder class. It creates a mock tokenizer, prompt helper, and LLMPredictor, and then uses them to build a response to a query string. The test_utils.py file tests the expand_tokens_with_subtokens() function, which takes a set of tokens and returns a set of tokens and their subtokens.", "doc_id": null, "embedding": null, "extra_info": null, "index": 30, "child_indices": [24, 25, 26, 27], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}