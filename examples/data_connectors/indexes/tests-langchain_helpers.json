{"index_struct": {"text": "\nThese documents are tests that check the functionality of the TokenTextSplitter class from the gpt_index.langchain_helpers module. The tests verify that the split_text and truncate_text methods return the expected results, as well as the ability to split a long token and split while considering the chunk size used by an extra info string.", "doc_id": "cf9ae30c-4bb8-4a82-83a6-46d4ceb5a4be", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Init params.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/langchain_helpers/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "c637335013c599b07de054fba07b47ecb86ad3e8", "node_info": null}, "1": {"text": "\"\"\"Test text splitter.\"\"\"\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\n\n\ndef test_split_token() -> None:\n    \"\"\"Test split normal token.\"\"\"\n    # tiktoken will say length is ~5k\n    token = \"foo bar\"\n    text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)\n    chunks = text_splitter.split_text(token)\n    assert chunks == [\"foo\", \"bar\"]\n\n    token = \"foo bar hello world\"\n    text_splitter = TokenTextSplitter(chunk_size=2, chunk_overlap=1)\n    chunks = text_splitter.split_text(token)\n    assert chunks == [\"foo bar\", \"bar hello\", \"hello world\"]\n\n\ndef test_truncate_token() -> None:\n    \"\"\"Test truncate normal token.\"\"\"\n    # tiktoken will say length is ~5k\n    token = \"foo bar\"\n    text_splitter =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/langchain_helpers/test_text_splitter.py", "file_name": "test_text_splitter.py"}, "index": 1, "child_indices": [], "ref_doc_id": "ddb2540512e86c257e1680bbd058b930257b3b9e", "node_info": null}, "2": {"text": " token = \"foo bar\"\n    text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)\n    chunks = text_splitter.truncate_text(token)\n    assert chunks == \"foo\"\n\n\ndef test_split_long_token() -> None:\n    \"\"\"Test split a really long token.\"\"\"\n    # tiktoken will say length is ~5k\n    token = \"a\" * 100\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    text_splitter.split_text(token)\n\n    token = (\"a\" * 49) + \"\\n\" + (\"a\" * 50)\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    chunks = text_splitter.split_text(token)\n    assert len(chunks[0]) == 49\n    assert len(chunks[1]) == 50\n\n\ndef test_split_with_extra_info_str() ->", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/langchain_helpers/test_text_splitter.py", "file_name": "test_text_splitter.py"}, "index": 2, "child_indices": [], "ref_doc_id": "ddb2540512e86c257e1680bbd058b930257b3b9e", "node_info": null}, "3": {"text": "test_split_with_extra_info_str() -> None:\n    \"\"\"Test split while taking into account chunk size used by extra info str.\"\"\"\n    text = \" \".join([\"foo\"] * 20)\n    extra_info_str = \"test_extra_info_str\"\n\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    chunks = text_splitter.split_text(text)\n    assert len(chunks) == 1\n\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    chunks = text_splitter.split_text(text, extra_info_str=extra_info_str)\n    assert len(chunks) == 2\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/langchain_helpers/test_text_splitter.py", "file_name": "test_text_splitter.py"}, "index": 3, "child_indices": [], "ref_doc_id": "ddb2540512e86c257e1680bbd058b930257b3b9e", "node_info": null}, "4": {"text": "This code file tests the functionality of the TokenTextSplitter class from the gpt_index.langchain_helpers module. It tests the split_text and truncate_text methods, as well as the ability to split a really long token and split while taking into account the chunk size used by an extra info string. The tests assert that the expected output is returned for each of the methods.", "doc_id": null, "embedding": null, "extra_info": null, "index": 4, "child_indices": [0, 1, 2, 3], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"4": {"text": "This code file tests the functionality of the TokenTextSplitter class from the gpt_index.langchain_helpers module. It tests the split_text and truncate_text methods, as well as the ability to split a really long token and split while taking into account the chunk size used by an extra info string. The tests assert that the expected output is returned for each of the methods.", "doc_id": null, "embedding": null, "extra_info": null, "index": 4, "child_indices": [0, 1, 2, 3], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"c637335013c599b07de054fba07b47ecb86ad3e8": {"text": "\"\"\"Init params.\"\"\"\n", "doc_id": "c637335013c599b07de054fba07b47ecb86ad3e8", "embedding": null, "extra_info": {"file_path": "tests/langchain_helpers/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "ddb2540512e86c257e1680bbd058b930257b3b9e": {"text": "\"\"\"Test text splitter.\"\"\"\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\n\n\ndef test_split_token() -> None:\n    \"\"\"Test split normal token.\"\"\"\n    # tiktoken will say length is ~5k\n    token = \"foo bar\"\n    text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)\n    chunks = text_splitter.split_text(token)\n    assert chunks == [\"foo\", \"bar\"]\n\n    token = \"foo bar hello world\"\n    text_splitter = TokenTextSplitter(chunk_size=2, chunk_overlap=1)\n    chunks = text_splitter.split_text(token)\n    assert chunks == [\"foo bar\", \"bar hello\", \"hello world\"]\n\n\ndef test_truncate_token() -> None:\n    \"\"\"Test truncate normal token.\"\"\"\n    # tiktoken will say length is ~5k\n    token = \"foo bar\"\n    text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)\n    chunks = text_splitter.truncate_text(token)\n    assert chunks == \"foo\"\n\n\ndef test_split_long_token() -> None:\n    \"\"\"Test split a really long token.\"\"\"\n    # tiktoken will say length is ~5k\n    token = \"a\" * 100\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    text_splitter.split_text(token)\n\n    token = (\"a\" * 49) + \"\\n\" + (\"a\" * 50)\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    chunks = text_splitter.split_text(token)\n    assert len(chunks[0]) == 49\n    assert len(chunks[1]) == 50\n\n\ndef test_split_with_extra_info_str() -> None:\n    \"\"\"Test split while taking into account chunk size used by extra info str.\"\"\"\n    text = \" \".join([\"foo\"] * 20)\n    extra_info_str = \"test_extra_info_str\"\n\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    chunks = text_splitter.split_text(text)\n    assert len(chunks) == 1\n\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    chunks = text_splitter.split_text(text, extra_info_str=extra_info_str)\n    assert len(chunks) == 2\n", "doc_id": "ddb2540512e86c257e1680bbd058b930257b3b9e", "embedding": null, "extra_info": {"file_path": "tests/langchain_helpers/test_text_splitter.py", "file_name": "test_text_splitter.py"}, "__type__": "Document"}, "cf9ae30c-4bb8-4a82-83a6-46d4ceb5a4be": {"text": "\nThese documents are tests that check the functionality of the TokenTextSplitter class from the gpt_index.langchain_helpers module. The tests verify that the split_text and truncate_text methods return the expected results, as well as the ability to split a long token and split while considering the chunk size used by an extra info string.", "doc_id": "cf9ae30c-4bb8-4a82-83a6-46d4ceb5a4be", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Init params.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/langchain_helpers/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "c637335013c599b07de054fba07b47ecb86ad3e8", "node_info": null}, "1": {"text": "\"\"\"Test text splitter.\"\"\"\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\n\n\ndef test_split_token() -> None:\n    \"\"\"Test split normal token.\"\"\"\n    # tiktoken will say length is ~5k\n    token = \"foo bar\"\n    text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)\n    chunks = text_splitter.split_text(token)\n    assert chunks == [\"foo\", \"bar\"]\n\n    token = \"foo bar hello world\"\n    text_splitter = TokenTextSplitter(chunk_size=2, chunk_overlap=1)\n    chunks = text_splitter.split_text(token)\n    assert chunks == [\"foo bar\", \"bar hello\", \"hello world\"]\n\n\ndef test_truncate_token() -> None:\n    \"\"\"Test truncate normal token.\"\"\"\n    # tiktoken will say length is ~5k\n    token = \"foo bar\"\n    text_splitter =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/langchain_helpers/test_text_splitter.py", "file_name": "test_text_splitter.py"}, "index": 1, "child_indices": [], "ref_doc_id": "ddb2540512e86c257e1680bbd058b930257b3b9e", "node_info": null}, "2": {"text": " token = \"foo bar\"\n    text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)\n    chunks = text_splitter.truncate_text(token)\n    assert chunks == \"foo\"\n\n\ndef test_split_long_token() -> None:\n    \"\"\"Test split a really long token.\"\"\"\n    # tiktoken will say length is ~5k\n    token = \"a\" * 100\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    text_splitter.split_text(token)\n\n    token = (\"a\" * 49) + \"\\n\" + (\"a\" * 50)\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    chunks = text_splitter.split_text(token)\n    assert len(chunks[0]) == 49\n    assert len(chunks[1]) == 50\n\n\ndef test_split_with_extra_info_str() ->", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/langchain_helpers/test_text_splitter.py", "file_name": "test_text_splitter.py"}, "index": 2, "child_indices": [], "ref_doc_id": "ddb2540512e86c257e1680bbd058b930257b3b9e", "node_info": null}, "3": {"text": "test_split_with_extra_info_str() -> None:\n    \"\"\"Test split while taking into account chunk size used by extra info str.\"\"\"\n    text = \" \".join([\"foo\"] * 20)\n    extra_info_str = \"test_extra_info_str\"\n\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    chunks = text_splitter.split_text(text)\n    assert len(chunks) == 1\n\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    chunks = text_splitter.split_text(text, extra_info_str=extra_info_str)\n    assert len(chunks) == 2\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/langchain_helpers/test_text_splitter.py", "file_name": "test_text_splitter.py"}, "index": 3, "child_indices": [], "ref_doc_id": "ddb2540512e86c257e1680bbd058b930257b3b9e", "node_info": null}, "4": {"text": "This code file tests the functionality of the TokenTextSplitter class from the gpt_index.langchain_helpers module. It tests the split_text and truncate_text methods, as well as the ability to split a really long token and split while taking into account the chunk size used by an extra info string. The tests assert that the expected output is returned for each of the methods.", "doc_id": null, "embedding": null, "extra_info": null, "index": 4, "child_indices": [0, 1, 2, 3], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"4": {"text": "This code file tests the functionality of the TokenTextSplitter class from the gpt_index.langchain_helpers module. It tests the split_text and truncate_text methods, as well as the ability to split a really long token and split while taking into account the chunk size used by an extra info string. The tests assert that the expected output is returned for each of the methods.", "doc_id": null, "embedding": null, "extra_info": null, "index": 4, "child_indices": [0, 1, 2, 3], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}