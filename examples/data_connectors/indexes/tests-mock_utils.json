{"index_struct": {"text": "\nThe documents contain several mock functions and classes used to test the GPT-Index language chain. These include a shared decorator, a mock predict method of LLMPredictor, a mock LLMChain predict with a generic response, mock prompt utils, a mock token splitter by newline, a mock token splitter by newline with overlaps, and a mock keyword extract function. The keyword extract function does not filter stopwords.", "doc_id": "5e6911d7-f1d8-4bdd-a67a-739292cdda73", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "node_info": null}, "1": {"text": "\"\"\"Shared decorator.\"\"\"\nimport functools\nfrom typing import Any, Callable\nfrom unittest.mock import patch\n\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom tests.mock_utils.mock_predict import mock_llmpredictor_predict\nfrom tests.mock_utils.mock_text_splitter import (\n    mock_token_splitter_newline,\n    mock_token_splitter_newline_with_overlaps,\n)\n\n\ndef patch_common(f: Callable) -> Callable:\n    \"\"\"Create patch decorator with common mocks.\"\"\"\n\n    @patch.object(\n        TokenTextSplitter, \"split_text\", side_effect=mock_token_splitter_newline\n    )\n    @patch.object(\n        TokenTextSplitter,\n        \"split_text_with_overlaps\",\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_decorator.py", "file_name": "mock_decorator.py"}, "index": 1, "child_indices": [], "ref_doc_id": "ce69c63dfad837fbe436965bac2049338e5fd9e5", "node_info": null}, "2": {"text": "\"split_text_with_overlaps\",\n        side_effect=mock_token_splitter_newline_with_overlaps,\n    )\n    @patch.object(LLMPredictor, \"total_tokens_used\", return_value=0)\n    @patch.object(LLMPredictor, \"predict\", side_effect=mock_llmpredictor_predict)\n    @patch.object(LLMPredictor, \"__init__\", return_value=None)\n    @functools.wraps(f)\n    def functor(*args: Any, **kwargs: Any) -> Any:\n        return f(*args, **kwargs)\n\n    return functor\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_decorator.py", "file_name": "mock_decorator.py"}, "index": 2, "child_indices": [], "ref_doc_id": "ce69c63dfad837fbe436965bac2049338e5fd9e5", "node_info": null}, "3": {"text": "\"\"\"Mock predict.\"\"\"\n\nfrom typing import Any, Dict, Tuple\n\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.prompts.prompt_type import PromptType\nfrom gpt_index.token_counter.utils import mock_extract_keywords_response\n\n\ndef _mock_summary_predict(prompt_args: Dict) -> str:\n    \"\"\"Mock summary predict.\"\"\"\n    return prompt_args[\"context_str\"]\n\n\ndef _mock_insert_predict() -> str:\n    \"\"\"Mock insert predict.\n\n    Used in GPT tree index during insertion\n    to select the next node.\n\n    \"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_query_select() -> str:\n    \"\"\"Mock query predict.\n\n    Used in GPT tree index during query traversal\n    to select the next node.\n\n    \"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_answer(prompt_args: Dict) -> str:\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_predict.py", "file_name": "mock_predict.py"}, "index": 3, "child_indices": [], "ref_doc_id": "af0672c6bf667916d04434c0959d199c30af3bb8", "node_info": null}, "4": {"text": "Dict) -> str:\n    \"\"\"Mock answer.\"\"\"\n    return prompt_args[\"query_str\"] + \":\" + prompt_args[\"context_str\"]\n\n\ndef _mock_refine(prompt_args: Dict) -> str:\n    \"\"\"Mock refine.\"\"\"\n    return prompt_args[\"existing_answer\"]\n\n\ndef _mock_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"text\"])\n\n\ndef _mock_query_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock query keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"question\"])\n\n\ndef _mock_schema_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock schema extract.\"\"\"\n    return prompt_args[\"text\"]\n\n\ndef _mock_text_to_sql(prompt_args: Dict)", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_predict.py", "file_name": "mock_predict.py"}, "index": 4, "child_indices": [], "ref_doc_id": "af0672c6bf667916d04434c0959d199c30af3bb8", "node_info": null}, "5": {"text": "Dict) -> str:\n    \"\"\"Mock text to sql.\"\"\"\n    # assume it's a select query\n    tokens = prompt_args[\"query_str\"].split(\":\")\n    table_name = tokens[0]\n    subtokens = tokens[1].split(\",\")\n    return \"SELECT \" + \", \".join(subtokens) + f\" FROM {table_name}\"\n\n\ndef mock_llmpredictor_predict(prompt: Prompt, **prompt_args: Any) -> Tuple[str, str]:\n    \"\"\"Mock predict method of LLMPredictor.\n\n    Depending on the prompt, return response.\n\n    \"\"\"\n    formatted_prompt = prompt.format(**prompt_args)\n    full_prompt_args = prompt.get_full_format_args(prompt_args)\n    if prompt.prompt_type == PromptType.SUMMARY:\n        response = _mock_summary_predict(full_prompt_args)\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_predict.py", "file_name": "mock_predict.py"}, "index": 5, "child_indices": [], "ref_doc_id": "af0672c6bf667916d04434c0959d199c30af3bb8", "node_info": null}, "6": {"text": "   elif prompt.prompt_type == PromptType.TREE_INSERT:\n        response = _mock_insert_predict()\n    elif prompt.prompt_type == PromptType.TREE_SELECT:\n        response = _mock_query_select()\n    elif prompt.prompt_type == PromptType.REFINE:\n        response = _mock_refine(full_prompt_args)\n    elif prompt.prompt_type == PromptType.QUESTION_ANSWER:\n        response = _mock_answer(full_prompt_args)\n    elif prompt.prompt_type == PromptType.KEYWORD_EXTRACT:\n        response = _mock_keyword_extract(full_prompt_args)\n    elif prompt.prompt_type == PromptType.QUERY_KEYWORD_EXTRACT:\n        response =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_predict.py", "file_name": "mock_predict.py"}, "index": 6, "child_indices": [], "ref_doc_id": "af0672c6bf667916d04434c0959d199c30af3bb8", "node_info": null}, "7": {"text": "       response = _mock_query_keyword_extract(full_prompt_args)\n    elif prompt.prompt_type == PromptType.SCHEMA_EXTRACT:\n        response = _mock_schema_extract(full_prompt_args)\n    elif prompt.prompt_type == PromptType.TEXT_TO_SQL:\n        response = _mock_text_to_sql(full_prompt_args)\n    else:\n        raise ValueError(\"Invalid prompt to use with mocks.\")\n\n    return response, formatted_prompt\n\n\ndef mock_llmchain_predict(**full_prompt_args: Any) -> str:\n    \"\"\"Mock LLMChain predict with a generic response.\"\"\"\n    return \"generic response from LLMChain.predict()\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_predict.py", "file_name": "mock_predict.py"}, "index": 7, "child_indices": [], "ref_doc_id": "af0672c6bf667916d04434c0959d199c30af3bb8", "node_info": null}, "8": {"text": "\"\"\"Mock prompt utils.\"\"\"\n\nfrom gpt_index.prompts.prompts import (\n    KeywordExtractPrompt,\n    QueryKeywordExtractPrompt,\n    QuestionAnswerPrompt,\n    RefinePrompt,\n    SchemaExtractPrompt,\n    SummaryPrompt,\n    TableContextPrompt,\n    TextToSQLPrompt,\n    TreeInsertPrompt,\n    TreeSelectPrompt,\n)\n\nMOCK_SUMMARY_PROMPT_TMPL = \"{context_str}\\n\"\nMOCK_SUMMARY_PROMPT = SummaryPrompt(MOCK_SUMMARY_PROMPT_TMPL)\n\nMOCK_INSERT_PROMPT_TMPL = \"{num_chunks}\\n{context_list}{new_chunk_text}\\n\"\nMOCK_INSERT_PROMPT = TreeInsertPrompt(MOCK_INSERT_PROMPT_TMPL)\n\n# # single choice\nMOCK_QUERY_PROMPT_TMPL = \"{num_chunks}\\n\"", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_prompts.py", "file_name": "mock_prompts.py"}, "index": 8, "child_indices": [], "ref_doc_id": "f058467d8a4c77454a032b47a6ca7c62fb7aced6", "node_info": null}, "9": {"text": "= \"{num_chunks}\\n\" \"{context_list}\\n\" \"{query_str}'\\n\"\nMOCK_QUERY_PROMPT = TreeSelectPrompt(MOCK_QUERY_PROMPT_TMPL)\n\n\nMOCK_REFINE_PROMPT_TMPL = \"{query_str}\\n\" \"{existing_answer}\\n\" \"{context_msg}\\n\"\nMOCK_REFINE_PROMPT = RefinePrompt(MOCK_REFINE_PROMPT_TMPL)\n\n\nMOCK_TEXT_QA_PROMPT_TMPL = \"{context_str}\\n\" \"{query_str}\\n\"\nMOCK_TEXT_QA_PROMPT = QuestionAnswerPrompt(MOCK_TEXT_QA_PROMPT_TMPL)\n\n\nMOCK_KEYWORD_EXTRACT_PROMPT_TMPL = \"{max_keywords}\\n{text}\\n\"\nMOCK_KEYWORD_EXTRACT_PROMPT = KeywordExtractPrompt(MOCK_KEYWORD_EXTRACT_PROMPT_TMPL)\n\n# TODO: consolidate with keyword", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_prompts.py", "file_name": "mock_prompts.py"}, "index": 9, "child_indices": [], "ref_doc_id": "f058467d8a4c77454a032b47a6ca7c62fb7aced6", "node_info": null}, "10": {"text": "TODO: consolidate with keyword extract\nMOCK_QUERY_KEYWORD_EXTRACT_PROMPT_TMPL = \"{max_keywords}\\n{question}\\n\"\nMOCK_QUERY_KEYWORD_EXTRACT_PROMPT = QueryKeywordExtractPrompt(\n    MOCK_QUERY_KEYWORD_EXTRACT_PROMPT_TMPL\n)\n\n\nMOCK_SCHEMA_EXTRACT_PROMPT_TMPL = \"{text}\\n{schema}\"\nMOCK_SCHEMA_EXTRACT_PROMPT = SchemaExtractPrompt(MOCK_SCHEMA_EXTRACT_PROMPT_TMPL)\n\nMOCK_TEXT_TO_SQL_PROMPT_TMPL = \"{schema}\\n{query_str}\"\nMOCK_TEXT_TO_SQL_PROMPT = TextToSQLPrompt(MOCK_TEXT_TO_SQL_PROMPT_TMPL)\n\n\nMOCK_TABLE_CONTEXT_PROMPT_TMPL = \"{schema}\\n{context_str}\\n{query_str}\"\nMOCK_TABLE_CONTEXT_PROMPT", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_prompts.py", "file_name": "mock_prompts.py"}, "index": 10, "child_indices": [], "ref_doc_id": "f058467d8a4c77454a032b47a6ca7c62fb7aced6", "node_info": null}, "11": {"text": "= TableContextPrompt(MOCK_TABLE_CONTEXT_PROMPT_TMPL)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_prompts.py", "file_name": "mock_prompts.py"}, "index": 11, "child_indices": [], "ref_doc_id": "f058467d8a4c77454a032b47a6ca7c62fb7aced6", "node_info": null}, "12": {"text": "\"\"\"Mock text splitter.\"\"\"\n\nfrom typing import List, Optional\n\nfrom gpt_index.langchain_helpers.text_splitter import TextSplit\n\n\ndef mock_token_splitter_newline(\n    text: str, extra_info_str: Optional[str] = None\n) -> List[str]:\n    \"\"\"Mock token splitter by newline.\"\"\"\n    if text == \"\":\n        return []\n    return text.split(\"\\n\")\n\n\ndef mock_token_splitter_newline_with_overlaps(\n    text: str, extra_info_str: Optional[str]\n) -> List[TextSplit]:\n    \"\"\"Mock token splitter by newline.\"\"\"\n    if text == \"\":\n        return []\n    strings = text.split(\"\\n\")\n    return [TextSplit(string, 0) for string in strings]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_text_splitter.py", "file_name": "mock_text_splitter.py"}, "index": 12, "child_indices": [], "ref_doc_id": "5a3061faae831fc4d5601f0ac4e30c811c1efe28", "node_info": null}, "13": {"text": "\"\"\"Mock utils.\"\"\"\n\nfrom typing import List, Optional, Set\n\nfrom gpt_index.indices.keyword_table.utils import simple_extract_keywords\n\n\ndef mock_tokenizer(text: str) -> List[str]:\n    \"\"\"Mock tokenizer.\"\"\"\n    tokens = text.split(\" \")\n    result = []\n    for token in tokens:\n        if token.strip() == \"\":\n            continue\n        result.append(token.strip())\n    return result\n\n\ndef mock_extract_keywords(\n    text_chunk: str, max_keywords: Optional[int] = None, filter_stopwords: bool = True\n) -> Set[str]:\n    \"\"\"Extract keywords (mock).\n\n    Same as simple_extract_keywords but without filtering stopwords.\n\n    \"\"\"\n    return simple_extract_keywords(\n        text_chunk, max_keywords=max_keywords, filter_stopwords=False\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_utils.py", "file_name": "mock_utils.py"}, "index": 13, "child_indices": [], "ref_doc_id": "894c95fe14837faf82dcd09828b0859c16101e81", "node_info": null}, "14": {"text": "filter_stopwords=False\n    )\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_utils.py", "file_name": "mock_utils.py"}, "index": 14, "child_indices": [], "ref_doc_id": "894c95fe14837faf82dcd09828b0859c16101e81", "node_info": null}, "15": {"text": "This code file contains several mock functions and classes used to test the GPT-Index language chain. The file includes a patch decorator with common mocks, a mock predict method of LLMPredictor, a mock LLMChain predict with a generic response, and mock prompt utils. The mock predict method of LLMPredictor returns a response depending on the prompt, while the mock LLMChain predict returns a generic response. The mock prompt utils include several mock prompts such as SummaryPrompt, TreeInsertPrompt, TreeSelectPrompt, RefinePrompt, QuestionAnswerPrompt, KeywordExtractPrompt, QueryKeywordExtractPrompt, SchemaExtractPrompt, TextToSQLPrompt, and TableContextPrompt.", "doc_id": null, "embedding": null, "extra_info": null, "index": 15, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "16": {"text": "This code file contains two functions, mock_token_splitter_newline and mock_token_splitter_newline_with_overlaps, which are used to split text into tokens by newline. The mock_tokenizer function is used to tokenize a given string, and the mock_extract_keywords function is used to extract keywords from a text chunk without filtering stopwords. The code file also contains a variable, filter_stopwords, which is set to False.", "doc_id": null, "embedding": null, "extra_info": null, "index": 16, "child_indices": [12, 13, 14], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"15": {"text": "This code file contains several mock functions and classes used to test the GPT-Index language chain. The file includes a patch decorator with common mocks, a mock predict method of LLMPredictor, a mock LLMChain predict with a generic response, and mock prompt utils. The mock predict method of LLMPredictor returns a response depending on the prompt, while the mock LLMChain predict returns a generic response. The mock prompt utils include several mock prompts such as SummaryPrompt, TreeInsertPrompt, TreeSelectPrompt, RefinePrompt, QuestionAnswerPrompt, KeywordExtractPrompt, QueryKeywordExtractPrompt, SchemaExtractPrompt, TextToSQLPrompt, and TableContextPrompt.", "doc_id": null, "embedding": null, "extra_info": null, "index": 15, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "16": {"text": "This code file contains two functions, mock_token_splitter_newline and mock_token_splitter_newline_with_overlaps, which are used to split text into tokens by newline. The mock_tokenizer function is used to tokenize a given string, and the mock_extract_keywords function is used to extract keywords from a text chunk without filtering stopwords. The code file also contains a variable, filter_stopwords, which is set to False.", "doc_id": null, "embedding": null, "extra_info": null, "index": 16, "child_indices": [12, 13, 14], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"1d4640565ae2765d9ca96a509dc9809217f62f2f": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "embedding": null, "extra_info": {"file_path": "tests/mock_utils/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "ce69c63dfad837fbe436965bac2049338e5fd9e5": {"text": "\"\"\"Shared decorator.\"\"\"\nimport functools\nfrom typing import Any, Callable\nfrom unittest.mock import patch\n\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom tests.mock_utils.mock_predict import mock_llmpredictor_predict\nfrom tests.mock_utils.mock_text_splitter import (\n    mock_token_splitter_newline,\n    mock_token_splitter_newline_with_overlaps,\n)\n\n\ndef patch_common(f: Callable) -> Callable:\n    \"\"\"Create patch decorator with common mocks.\"\"\"\n\n    @patch.object(\n        TokenTextSplitter, \"split_text\", side_effect=mock_token_splitter_newline\n    )\n    @patch.object(\n        TokenTextSplitter,\n        \"split_text_with_overlaps\",\n        side_effect=mock_token_splitter_newline_with_overlaps,\n    )\n    @patch.object(LLMPredictor, \"total_tokens_used\", return_value=0)\n    @patch.object(LLMPredictor, \"predict\", side_effect=mock_llmpredictor_predict)\n    @patch.object(LLMPredictor, \"__init__\", return_value=None)\n    @functools.wraps(f)\n    def functor(*args: Any, **kwargs: Any) -> Any:\n        return f(*args, **kwargs)\n\n    return functor\n", "doc_id": "ce69c63dfad837fbe436965bac2049338e5fd9e5", "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_decorator.py", "file_name": "mock_decorator.py"}, "__type__": "Document"}, "af0672c6bf667916d04434c0959d199c30af3bb8": {"text": "\"\"\"Mock predict.\"\"\"\n\nfrom typing import Any, Dict, Tuple\n\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.prompts.prompt_type import PromptType\nfrom gpt_index.token_counter.utils import mock_extract_keywords_response\n\n\ndef _mock_summary_predict(prompt_args: Dict) -> str:\n    \"\"\"Mock summary predict.\"\"\"\n    return prompt_args[\"context_str\"]\n\n\ndef _mock_insert_predict() -> str:\n    \"\"\"Mock insert predict.\n\n    Used in GPT tree index during insertion\n    to select the next node.\n\n    \"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_query_select() -> str:\n    \"\"\"Mock query predict.\n\n    Used in GPT tree index during query traversal\n    to select the next node.\n\n    \"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_answer(prompt_args: Dict) -> str:\n    \"\"\"Mock answer.\"\"\"\n    return prompt_args[\"query_str\"] + \":\" + prompt_args[\"context_str\"]\n\n\ndef _mock_refine(prompt_args: Dict) -> str:\n    \"\"\"Mock refine.\"\"\"\n    return prompt_args[\"existing_answer\"]\n\n\ndef _mock_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"text\"])\n\n\ndef _mock_query_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock query keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"question\"])\n\n\ndef _mock_schema_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock schema extract.\"\"\"\n    return prompt_args[\"text\"]\n\n\ndef _mock_text_to_sql(prompt_args: Dict) -> str:\n    \"\"\"Mock text to sql.\"\"\"\n    # assume it's a select query\n    tokens = prompt_args[\"query_str\"].split(\":\")\n    table_name = tokens[0]\n    subtokens = tokens[1].split(\",\")\n    return \"SELECT \" + \", \".join(subtokens) + f\" FROM {table_name}\"\n\n\ndef mock_llmpredictor_predict(prompt: Prompt, **prompt_args: Any) -> Tuple[str, str]:\n    \"\"\"Mock predict method of LLMPredictor.\n\n    Depending on the prompt, return response.\n\n    \"\"\"\n    formatted_prompt = prompt.format(**prompt_args)\n    full_prompt_args = prompt.get_full_format_args(prompt_args)\n    if prompt.prompt_type == PromptType.SUMMARY:\n        response = _mock_summary_predict(full_prompt_args)\n    elif prompt.prompt_type == PromptType.TREE_INSERT:\n        response = _mock_insert_predict()\n    elif prompt.prompt_type == PromptType.TREE_SELECT:\n        response = _mock_query_select()\n    elif prompt.prompt_type == PromptType.REFINE:\n        response = _mock_refine(full_prompt_args)\n    elif prompt.prompt_type == PromptType.QUESTION_ANSWER:\n        response = _mock_answer(full_prompt_args)\n    elif prompt.prompt_type == PromptType.KEYWORD_EXTRACT:\n        response = _mock_keyword_extract(full_prompt_args)\n    elif prompt.prompt_type == PromptType.QUERY_KEYWORD_EXTRACT:\n        response = _mock_query_keyword_extract(full_prompt_args)\n    elif prompt.prompt_type == PromptType.SCHEMA_EXTRACT:\n        response = _mock_schema_extract(full_prompt_args)\n    elif prompt.prompt_type == PromptType.TEXT_TO_SQL:\n        response = _mock_text_to_sql(full_prompt_args)\n    else:\n        raise ValueError(\"Invalid prompt to use with mocks.\")\n\n    return response, formatted_prompt\n\n\ndef mock_llmchain_predict(**full_prompt_args: Any) -> str:\n    \"\"\"Mock LLMChain predict with a generic response.\"\"\"\n    return \"generic response from LLMChain.predict()\"\n", "doc_id": "af0672c6bf667916d04434c0959d199c30af3bb8", "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_predict.py", "file_name": "mock_predict.py"}, "__type__": "Document"}, "f058467d8a4c77454a032b47a6ca7c62fb7aced6": {"text": "\"\"\"Mock prompt utils.\"\"\"\n\nfrom gpt_index.prompts.prompts import (\n    KeywordExtractPrompt,\n    QueryKeywordExtractPrompt,\n    QuestionAnswerPrompt,\n    RefinePrompt,\n    SchemaExtractPrompt,\n    SummaryPrompt,\n    TableContextPrompt,\n    TextToSQLPrompt,\n    TreeInsertPrompt,\n    TreeSelectPrompt,\n)\n\nMOCK_SUMMARY_PROMPT_TMPL = \"{context_str}\\n\"\nMOCK_SUMMARY_PROMPT = SummaryPrompt(MOCK_SUMMARY_PROMPT_TMPL)\n\nMOCK_INSERT_PROMPT_TMPL = \"{num_chunks}\\n{context_list}{new_chunk_text}\\n\"\nMOCK_INSERT_PROMPT = TreeInsertPrompt(MOCK_INSERT_PROMPT_TMPL)\n\n# # single choice\nMOCK_QUERY_PROMPT_TMPL = \"{num_chunks}\\n\" \"{context_list}\\n\" \"{query_str}'\\n\"\nMOCK_QUERY_PROMPT = TreeSelectPrompt(MOCK_QUERY_PROMPT_TMPL)\n\n\nMOCK_REFINE_PROMPT_TMPL = \"{query_str}\\n\" \"{existing_answer}\\n\" \"{context_msg}\\n\"\nMOCK_REFINE_PROMPT = RefinePrompt(MOCK_REFINE_PROMPT_TMPL)\n\n\nMOCK_TEXT_QA_PROMPT_TMPL = \"{context_str}\\n\" \"{query_str}\\n\"\nMOCK_TEXT_QA_PROMPT = QuestionAnswerPrompt(MOCK_TEXT_QA_PROMPT_TMPL)\n\n\nMOCK_KEYWORD_EXTRACT_PROMPT_TMPL = \"{max_keywords}\\n{text}\\n\"\nMOCK_KEYWORD_EXTRACT_PROMPT = KeywordExtractPrompt(MOCK_KEYWORD_EXTRACT_PROMPT_TMPL)\n\n# TODO: consolidate with keyword extract\nMOCK_QUERY_KEYWORD_EXTRACT_PROMPT_TMPL = \"{max_keywords}\\n{question}\\n\"\nMOCK_QUERY_KEYWORD_EXTRACT_PROMPT = QueryKeywordExtractPrompt(\n    MOCK_QUERY_KEYWORD_EXTRACT_PROMPT_TMPL\n)\n\n\nMOCK_SCHEMA_EXTRACT_PROMPT_TMPL = \"{text}\\n{schema}\"\nMOCK_SCHEMA_EXTRACT_PROMPT = SchemaExtractPrompt(MOCK_SCHEMA_EXTRACT_PROMPT_TMPL)\n\nMOCK_TEXT_TO_SQL_PROMPT_TMPL = \"{schema}\\n{query_str}\"\nMOCK_TEXT_TO_SQL_PROMPT = TextToSQLPrompt(MOCK_TEXT_TO_SQL_PROMPT_TMPL)\n\n\nMOCK_TABLE_CONTEXT_PROMPT_TMPL = \"{schema}\\n{context_str}\\n{query_str}\"\nMOCK_TABLE_CONTEXT_PROMPT = TableContextPrompt(MOCK_TABLE_CONTEXT_PROMPT_TMPL)\n", "doc_id": "f058467d8a4c77454a032b47a6ca7c62fb7aced6", "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_prompts.py", "file_name": "mock_prompts.py"}, "__type__": "Document"}, "5a3061faae831fc4d5601f0ac4e30c811c1efe28": {"text": "\"\"\"Mock text splitter.\"\"\"\n\nfrom typing import List, Optional\n\nfrom gpt_index.langchain_helpers.text_splitter import TextSplit\n\n\ndef mock_token_splitter_newline(\n    text: str, extra_info_str: Optional[str] = None\n) -> List[str]:\n    \"\"\"Mock token splitter by newline.\"\"\"\n    if text == \"\":\n        return []\n    return text.split(\"\\n\")\n\n\ndef mock_token_splitter_newline_with_overlaps(\n    text: str, extra_info_str: Optional[str]\n) -> List[TextSplit]:\n    \"\"\"Mock token splitter by newline.\"\"\"\n    if text == \"\":\n        return []\n    strings = text.split(\"\\n\")\n    return [TextSplit(string, 0) for string in strings]\n", "doc_id": "5a3061faae831fc4d5601f0ac4e30c811c1efe28", "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_text_splitter.py", "file_name": "mock_text_splitter.py"}, "__type__": "Document"}, "894c95fe14837faf82dcd09828b0859c16101e81": {"text": "\"\"\"Mock utils.\"\"\"\n\nfrom typing import List, Optional, Set\n\nfrom gpt_index.indices.keyword_table.utils import simple_extract_keywords\n\n\ndef mock_tokenizer(text: str) -> List[str]:\n    \"\"\"Mock tokenizer.\"\"\"\n    tokens = text.split(\" \")\n    result = []\n    for token in tokens:\n        if token.strip() == \"\":\n            continue\n        result.append(token.strip())\n    return result\n\n\ndef mock_extract_keywords(\n    text_chunk: str, max_keywords: Optional[int] = None, filter_stopwords: bool = True\n) -> Set[str]:\n    \"\"\"Extract keywords (mock).\n\n    Same as simple_extract_keywords but without filtering stopwords.\n\n    \"\"\"\n    return simple_extract_keywords(\n        text_chunk, max_keywords=max_keywords, filter_stopwords=False\n    )\n", "doc_id": "894c95fe14837faf82dcd09828b0859c16101e81", "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_utils.py", "file_name": "mock_utils.py"}, "__type__": "Document"}, "5e6911d7-f1d8-4bdd-a67a-739292cdda73": {"text": "\nThe documents contain several mock functions and classes used to test the GPT-Index language chain. These include a shared decorator, a mock predict method of LLMPredictor, a mock LLMChain predict with a generic response, mock prompt utils, a mock token splitter by newline, a mock token splitter by newline with overlaps, and a mock keyword extract function. The keyword extract function does not filter stopwords.", "doc_id": "5e6911d7-f1d8-4bdd-a67a-739292cdda73", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "node_info": null}, "1": {"text": "\"\"\"Shared decorator.\"\"\"\nimport functools\nfrom typing import Any, Callable\nfrom unittest.mock import patch\n\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom tests.mock_utils.mock_predict import mock_llmpredictor_predict\nfrom tests.mock_utils.mock_text_splitter import (\n    mock_token_splitter_newline,\n    mock_token_splitter_newline_with_overlaps,\n)\n\n\ndef patch_common(f: Callable) -> Callable:\n    \"\"\"Create patch decorator with common mocks.\"\"\"\n\n    @patch.object(\n        TokenTextSplitter, \"split_text\", side_effect=mock_token_splitter_newline\n    )\n    @patch.object(\n        TokenTextSplitter,\n        \"split_text_with_overlaps\",\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_decorator.py", "file_name": "mock_decorator.py"}, "index": 1, "child_indices": [], "ref_doc_id": "ce69c63dfad837fbe436965bac2049338e5fd9e5", "node_info": null}, "2": {"text": "\"split_text_with_overlaps\",\n        side_effect=mock_token_splitter_newline_with_overlaps,\n    )\n    @patch.object(LLMPredictor, \"total_tokens_used\", return_value=0)\n    @patch.object(LLMPredictor, \"predict\", side_effect=mock_llmpredictor_predict)\n    @patch.object(LLMPredictor, \"__init__\", return_value=None)\n    @functools.wraps(f)\n    def functor(*args: Any, **kwargs: Any) -> Any:\n        return f(*args, **kwargs)\n\n    return functor\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_decorator.py", "file_name": "mock_decorator.py"}, "index": 2, "child_indices": [], "ref_doc_id": "ce69c63dfad837fbe436965bac2049338e5fd9e5", "node_info": null}, "3": {"text": "\"\"\"Mock predict.\"\"\"\n\nfrom typing import Any, Dict, Tuple\n\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.prompts.prompt_type import PromptType\nfrom gpt_index.token_counter.utils import mock_extract_keywords_response\n\n\ndef _mock_summary_predict(prompt_args: Dict) -> str:\n    \"\"\"Mock summary predict.\"\"\"\n    return prompt_args[\"context_str\"]\n\n\ndef _mock_insert_predict() -> str:\n    \"\"\"Mock insert predict.\n\n    Used in GPT tree index during insertion\n    to select the next node.\n\n    \"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_query_select() -> str:\n    \"\"\"Mock query predict.\n\n    Used in GPT tree index during query traversal\n    to select the next node.\n\n    \"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_answer(prompt_args: Dict) -> str:\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_predict.py", "file_name": "mock_predict.py"}, "index": 3, "child_indices": [], "ref_doc_id": "af0672c6bf667916d04434c0959d199c30af3bb8", "node_info": null}, "4": {"text": "Dict) -> str:\n    \"\"\"Mock answer.\"\"\"\n    return prompt_args[\"query_str\"] + \":\" + prompt_args[\"context_str\"]\n\n\ndef _mock_refine(prompt_args: Dict) -> str:\n    \"\"\"Mock refine.\"\"\"\n    return prompt_args[\"existing_answer\"]\n\n\ndef _mock_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"text\"])\n\n\ndef _mock_query_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock query keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"question\"])\n\n\ndef _mock_schema_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock schema extract.\"\"\"\n    return prompt_args[\"text\"]\n\n\ndef _mock_text_to_sql(prompt_args: Dict)", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_predict.py", "file_name": "mock_predict.py"}, "index": 4, "child_indices": [], "ref_doc_id": "af0672c6bf667916d04434c0959d199c30af3bb8", "node_info": null}, "5": {"text": "Dict) -> str:\n    \"\"\"Mock text to sql.\"\"\"\n    # assume it's a select query\n    tokens = prompt_args[\"query_str\"].split(\":\")\n    table_name = tokens[0]\n    subtokens = tokens[1].split(\",\")\n    return \"SELECT \" + \", \".join(subtokens) + f\" FROM {table_name}\"\n\n\ndef mock_llmpredictor_predict(prompt: Prompt, **prompt_args: Any) -> Tuple[str, str]:\n    \"\"\"Mock predict method of LLMPredictor.\n\n    Depending on the prompt, return response.\n\n    \"\"\"\n    formatted_prompt = prompt.format(**prompt_args)\n    full_prompt_args = prompt.get_full_format_args(prompt_args)\n    if prompt.prompt_type == PromptType.SUMMARY:\n        response = _mock_summary_predict(full_prompt_args)\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_predict.py", "file_name": "mock_predict.py"}, "index": 5, "child_indices": [], "ref_doc_id": "af0672c6bf667916d04434c0959d199c30af3bb8", "node_info": null}, "6": {"text": "   elif prompt.prompt_type == PromptType.TREE_INSERT:\n        response = _mock_insert_predict()\n    elif prompt.prompt_type == PromptType.TREE_SELECT:\n        response = _mock_query_select()\n    elif prompt.prompt_type == PromptType.REFINE:\n        response = _mock_refine(full_prompt_args)\n    elif prompt.prompt_type == PromptType.QUESTION_ANSWER:\n        response = _mock_answer(full_prompt_args)\n    elif prompt.prompt_type == PromptType.KEYWORD_EXTRACT:\n        response = _mock_keyword_extract(full_prompt_args)\n    elif prompt.prompt_type == PromptType.QUERY_KEYWORD_EXTRACT:\n        response =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_predict.py", "file_name": "mock_predict.py"}, "index": 6, "child_indices": [], "ref_doc_id": "af0672c6bf667916d04434c0959d199c30af3bb8", "node_info": null}, "7": {"text": "       response = _mock_query_keyword_extract(full_prompt_args)\n    elif prompt.prompt_type == PromptType.SCHEMA_EXTRACT:\n        response = _mock_schema_extract(full_prompt_args)\n    elif prompt.prompt_type == PromptType.TEXT_TO_SQL:\n        response = _mock_text_to_sql(full_prompt_args)\n    else:\n        raise ValueError(\"Invalid prompt to use with mocks.\")\n\n    return response, formatted_prompt\n\n\ndef mock_llmchain_predict(**full_prompt_args: Any) -> str:\n    \"\"\"Mock LLMChain predict with a generic response.\"\"\"\n    return \"generic response from LLMChain.predict()\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_predict.py", "file_name": "mock_predict.py"}, "index": 7, "child_indices": [], "ref_doc_id": "af0672c6bf667916d04434c0959d199c30af3bb8", "node_info": null}, "8": {"text": "\"\"\"Mock prompt utils.\"\"\"\n\nfrom gpt_index.prompts.prompts import (\n    KeywordExtractPrompt,\n    QueryKeywordExtractPrompt,\n    QuestionAnswerPrompt,\n    RefinePrompt,\n    SchemaExtractPrompt,\n    SummaryPrompt,\n    TableContextPrompt,\n    TextToSQLPrompt,\n    TreeInsertPrompt,\n    TreeSelectPrompt,\n)\n\nMOCK_SUMMARY_PROMPT_TMPL = \"{context_str}\\n\"\nMOCK_SUMMARY_PROMPT = SummaryPrompt(MOCK_SUMMARY_PROMPT_TMPL)\n\nMOCK_INSERT_PROMPT_TMPL = \"{num_chunks}\\n{context_list}{new_chunk_text}\\n\"\nMOCK_INSERT_PROMPT = TreeInsertPrompt(MOCK_INSERT_PROMPT_TMPL)\n\n# # single choice\nMOCK_QUERY_PROMPT_TMPL = \"{num_chunks}\\n\"", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_prompts.py", "file_name": "mock_prompts.py"}, "index": 8, "child_indices": [], "ref_doc_id": "f058467d8a4c77454a032b47a6ca7c62fb7aced6", "node_info": null}, "9": {"text": "= \"{num_chunks}\\n\" \"{context_list}\\n\" \"{query_str}'\\n\"\nMOCK_QUERY_PROMPT = TreeSelectPrompt(MOCK_QUERY_PROMPT_TMPL)\n\n\nMOCK_REFINE_PROMPT_TMPL = \"{query_str}\\n\" \"{existing_answer}\\n\" \"{context_msg}\\n\"\nMOCK_REFINE_PROMPT = RefinePrompt(MOCK_REFINE_PROMPT_TMPL)\n\n\nMOCK_TEXT_QA_PROMPT_TMPL = \"{context_str}\\n\" \"{query_str}\\n\"\nMOCK_TEXT_QA_PROMPT = QuestionAnswerPrompt(MOCK_TEXT_QA_PROMPT_TMPL)\n\n\nMOCK_KEYWORD_EXTRACT_PROMPT_TMPL = \"{max_keywords}\\n{text}\\n\"\nMOCK_KEYWORD_EXTRACT_PROMPT = KeywordExtractPrompt(MOCK_KEYWORD_EXTRACT_PROMPT_TMPL)\n\n# TODO: consolidate with keyword", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_prompts.py", "file_name": "mock_prompts.py"}, "index": 9, "child_indices": [], "ref_doc_id": "f058467d8a4c77454a032b47a6ca7c62fb7aced6", "node_info": null}, "10": {"text": "TODO: consolidate with keyword extract\nMOCK_QUERY_KEYWORD_EXTRACT_PROMPT_TMPL = \"{max_keywords}\\n{question}\\n\"\nMOCK_QUERY_KEYWORD_EXTRACT_PROMPT = QueryKeywordExtractPrompt(\n    MOCK_QUERY_KEYWORD_EXTRACT_PROMPT_TMPL\n)\n\n\nMOCK_SCHEMA_EXTRACT_PROMPT_TMPL = \"{text}\\n{schema}\"\nMOCK_SCHEMA_EXTRACT_PROMPT = SchemaExtractPrompt(MOCK_SCHEMA_EXTRACT_PROMPT_TMPL)\n\nMOCK_TEXT_TO_SQL_PROMPT_TMPL = \"{schema}\\n{query_str}\"\nMOCK_TEXT_TO_SQL_PROMPT = TextToSQLPrompt(MOCK_TEXT_TO_SQL_PROMPT_TMPL)\n\n\nMOCK_TABLE_CONTEXT_PROMPT_TMPL = \"{schema}\\n{context_str}\\n{query_str}\"\nMOCK_TABLE_CONTEXT_PROMPT", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_prompts.py", "file_name": "mock_prompts.py"}, "index": 10, "child_indices": [], "ref_doc_id": "f058467d8a4c77454a032b47a6ca7c62fb7aced6", "node_info": null}, "11": {"text": "= TableContextPrompt(MOCK_TABLE_CONTEXT_PROMPT_TMPL)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_prompts.py", "file_name": "mock_prompts.py"}, "index": 11, "child_indices": [], "ref_doc_id": "f058467d8a4c77454a032b47a6ca7c62fb7aced6", "node_info": null}, "12": {"text": "\"\"\"Mock text splitter.\"\"\"\n\nfrom typing import List, Optional\n\nfrom gpt_index.langchain_helpers.text_splitter import TextSplit\n\n\ndef mock_token_splitter_newline(\n    text: str, extra_info_str: Optional[str] = None\n) -> List[str]:\n    \"\"\"Mock token splitter by newline.\"\"\"\n    if text == \"\":\n        return []\n    return text.split(\"\\n\")\n\n\ndef mock_token_splitter_newline_with_overlaps(\n    text: str, extra_info_str: Optional[str]\n) -> List[TextSplit]:\n    \"\"\"Mock token splitter by newline.\"\"\"\n    if text == \"\":\n        return []\n    strings = text.split(\"\\n\")\n    return [TextSplit(string, 0) for string in strings]\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_text_splitter.py", "file_name": "mock_text_splitter.py"}, "index": 12, "child_indices": [], "ref_doc_id": "5a3061faae831fc4d5601f0ac4e30c811c1efe28", "node_info": null}, "13": {"text": "\"\"\"Mock utils.\"\"\"\n\nfrom typing import List, Optional, Set\n\nfrom gpt_index.indices.keyword_table.utils import simple_extract_keywords\n\n\ndef mock_tokenizer(text: str) -> List[str]:\n    \"\"\"Mock tokenizer.\"\"\"\n    tokens = text.split(\" \")\n    result = []\n    for token in tokens:\n        if token.strip() == \"\":\n            continue\n        result.append(token.strip())\n    return result\n\n\ndef mock_extract_keywords(\n    text_chunk: str, max_keywords: Optional[int] = None, filter_stopwords: bool = True\n) -> Set[str]:\n    \"\"\"Extract keywords (mock).\n\n    Same as simple_extract_keywords but without filtering stopwords.\n\n    \"\"\"\n    return simple_extract_keywords(\n        text_chunk, max_keywords=max_keywords, filter_stopwords=False\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_utils.py", "file_name": "mock_utils.py"}, "index": 13, "child_indices": [], "ref_doc_id": "894c95fe14837faf82dcd09828b0859c16101e81", "node_info": null}, "14": {"text": "filter_stopwords=False\n    )\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_utils.py", "file_name": "mock_utils.py"}, "index": 14, "child_indices": [], "ref_doc_id": "894c95fe14837faf82dcd09828b0859c16101e81", "node_info": null}, "15": {"text": "This code file contains several mock functions and classes used to test the GPT-Index language chain. The file includes a patch decorator with common mocks, a mock predict method of LLMPredictor, a mock LLMChain predict with a generic response, and mock prompt utils. The mock predict method of LLMPredictor returns a response depending on the prompt, while the mock LLMChain predict returns a generic response. The mock prompt utils include several mock prompts such as SummaryPrompt, TreeInsertPrompt, TreeSelectPrompt, RefinePrompt, QuestionAnswerPrompt, KeywordExtractPrompt, QueryKeywordExtractPrompt, SchemaExtractPrompt, TextToSQLPrompt, and TableContextPrompt.", "doc_id": null, "embedding": null, "extra_info": null, "index": 15, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "16": {"text": "This code file contains two functions, mock_token_splitter_newline and mock_token_splitter_newline_with_overlaps, which are used to split text into tokens by newline. The mock_tokenizer function is used to tokenize a given string, and the mock_extract_keywords function is used to extract keywords from a text chunk without filtering stopwords. The code file also contains a variable, filter_stopwords, which is set to False.", "doc_id": null, "embedding": null, "extra_info": null, "index": 16, "child_indices": [12, 13, 14], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"15": {"text": "This code file contains several mock functions and classes used to test the GPT-Index language chain. The file includes a patch decorator with common mocks, a mock predict method of LLMPredictor, a mock LLMChain predict with a generic response, and mock prompt utils. The mock predict method of LLMPredictor returns a response depending on the prompt, while the mock LLMChain predict returns a generic response. The mock prompt utils include several mock prompts such as SummaryPrompt, TreeInsertPrompt, TreeSelectPrompt, RefinePrompt, QuestionAnswerPrompt, KeywordExtractPrompt, QueryKeywordExtractPrompt, SchemaExtractPrompt, TextToSQLPrompt, and TableContextPrompt.", "doc_id": null, "embedding": null, "extra_info": null, "index": 15, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], "ref_doc_id": null, "node_info": null}, "16": {"text": "This code file contains two functions, mock_token_splitter_newline and mock_token_splitter_newline_with_overlaps, which are used to split text into tokens by newline. The mock_tokenizer function is used to tokenize a given string, and the mock_extract_keywords function is used to extract keywords from a text chunk without filtering stopwords. The code file also contains a variable, filter_stopwords, which is set to False.", "doc_id": null, "embedding": null, "extra_info": null, "index": 16, "child_indices": [12, 13, 14], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}