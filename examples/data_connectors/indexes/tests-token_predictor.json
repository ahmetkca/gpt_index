{"index_struct": {"text": "\nThe summaries of these documents are that they are used to test the token predictor, which uses various classes and functions to mock token counting. The purpose of the code is to assert that the token predictor runs correctly.", "doc_id": "d8a3b469-721e-4732-9703-de644405ed2d", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/token_predictor/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "node_info": null}, "1": {"text": "\"\"\"Test token predictor.\"\"\"\n\nfrom typing import Any\nfrom unittest.mock import patch\n\nfrom gpt_index.indices.keyword_table.base import GPTKeywordTableIndex\nfrom gpt_index.indices.list.base import GPTListIndex\nfrom gpt_index.indices.tree.base import GPTTreeIndex\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.readers.schema.base import Document\nfrom gpt_index.token_counter.mock_chain_wrapper import MockLLMPredictor\nfrom tests.mock_utils.mock_text_splitter import mock_token_splitter_newline\n\n\n@patch.object(TokenTextSplitter, \"split_text\", side_effect=mock_token_splitter_newline)\ndef test_token_predictor(mock_split: Any) -> None:\n    \"\"\"Test token predictor.\"\"\"\n    # here, just assert that token predictor runs (before checking behavior)\n    # TODO: mock token counting a bit more carefully\n    doc_text =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/token_predictor/test_base.py", "file_name": "test_base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "e0e254d6052ab57612b1bac55836ca45a71c201b", "node_info": null}, "2": {"text": "token counting a bit more carefully\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    document = Document(doc_text)\n    llm_predictor = MockLLMPredictor(max_tokens=256)\n\n    # test tree index\n    index = GPTTreeIndex([document], llm_predictor=llm_predictor)\n    index.query(\"What is?\", llm_predictor=llm_predictor)\n\n    # test keyword table index\n    index_keyword = GPTKeywordTableIndex([document], llm_predictor=llm_predictor)\n    index_keyword.query(\"What is?\", llm_predictor=llm_predictor)\n\n    # test list index\n    index_list = GPTListIndex([document],", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/token_predictor/test_base.py", "file_name": "test_base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "e0e254d6052ab57612b1bac55836ca45a71c201b", "node_info": null}, "3": {"text": "index\n    index_list = GPTListIndex([document], llm_predictor=llm_predictor)\n    index_list.query(\"What is?\", llm_predictor=llm_predictor)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/token_predictor/test_base.py", "file_name": "test_base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "e0e254d6052ab57612b1bac55836ca45a71c201b", "node_info": null}, "4": {"text": "This code file is a test for the token predictor. It uses the TokenTextSplitter, GPTKeywordTableIndex, GPTListIndex, and GPTTreeIndex classes to test the token predictor. It also uses the MockLLMPredictor and mock_token_splitter_newline functions to mock token counting. The purpose of the code is to assert that the token predictor runs correctly.", "doc_id": null, "embedding": null, "extra_info": null, "index": 4, "child_indices": [0, 1, 2, 3], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"4": {"text": "This code file is a test for the token predictor. It uses the TokenTextSplitter, GPTKeywordTableIndex, GPTListIndex, and GPTTreeIndex classes to test the token predictor. It also uses the MockLLMPredictor and mock_token_splitter_newline functions to mock token counting. The purpose of the code is to assert that the token predictor runs correctly.", "doc_id": null, "embedding": null, "extra_info": null, "index": 4, "child_indices": [0, 1, 2, 3], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"1d4640565ae2765d9ca96a509dc9809217f62f2f": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "embedding": null, "extra_info": {"file_path": "tests/token_predictor/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "e0e254d6052ab57612b1bac55836ca45a71c201b": {"text": "\"\"\"Test token predictor.\"\"\"\n\nfrom typing import Any\nfrom unittest.mock import patch\n\nfrom gpt_index.indices.keyword_table.base import GPTKeywordTableIndex\nfrom gpt_index.indices.list.base import GPTListIndex\nfrom gpt_index.indices.tree.base import GPTTreeIndex\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.readers.schema.base import Document\nfrom gpt_index.token_counter.mock_chain_wrapper import MockLLMPredictor\nfrom tests.mock_utils.mock_text_splitter import mock_token_splitter_newline\n\n\n@patch.object(TokenTextSplitter, \"split_text\", side_effect=mock_token_splitter_newline)\ndef test_token_predictor(mock_split: Any) -> None:\n    \"\"\"Test token predictor.\"\"\"\n    # here, just assert that token predictor runs (before checking behavior)\n    # TODO: mock token counting a bit more carefully\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    document = Document(doc_text)\n    llm_predictor = MockLLMPredictor(max_tokens=256)\n\n    # test tree index\n    index = GPTTreeIndex([document], llm_predictor=llm_predictor)\n    index.query(\"What is?\", llm_predictor=llm_predictor)\n\n    # test keyword table index\n    index_keyword = GPTKeywordTableIndex([document], llm_predictor=llm_predictor)\n    index_keyword.query(\"What is?\", llm_predictor=llm_predictor)\n\n    # test list index\n    index_list = GPTListIndex([document], llm_predictor=llm_predictor)\n    index_list.query(\"What is?\", llm_predictor=llm_predictor)\n", "doc_id": "e0e254d6052ab57612b1bac55836ca45a71c201b", "embedding": null, "extra_info": {"file_path": "tests/token_predictor/test_base.py", "file_name": "test_base.py"}, "__type__": "Document"}, "d8a3b469-721e-4732-9703-de644405ed2d": {"text": "\nThe summaries of these documents are that they are used to test the token predictor, which uses various classes and functions to mock token counting. The purpose of the code is to assert that the token predictor runs correctly.", "doc_id": "d8a3b469-721e-4732-9703-de644405ed2d", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/token_predictor/__init__.py", "file_name": "__init__.py"}, "index": 0, "child_indices": [], "ref_doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "node_info": null}, "1": {"text": "\"\"\"Test token predictor.\"\"\"\n\nfrom typing import Any\nfrom unittest.mock import patch\n\nfrom gpt_index.indices.keyword_table.base import GPTKeywordTableIndex\nfrom gpt_index.indices.list.base import GPTListIndex\nfrom gpt_index.indices.tree.base import GPTTreeIndex\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.readers.schema.base import Document\nfrom gpt_index.token_counter.mock_chain_wrapper import MockLLMPredictor\nfrom tests.mock_utils.mock_text_splitter import mock_token_splitter_newline\n\n\n@patch.object(TokenTextSplitter, \"split_text\", side_effect=mock_token_splitter_newline)\ndef test_token_predictor(mock_split: Any) -> None:\n    \"\"\"Test token predictor.\"\"\"\n    # here, just assert that token predictor runs (before checking behavior)\n    # TODO: mock token counting a bit more carefully\n    doc_text =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/token_predictor/test_base.py", "file_name": "test_base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "e0e254d6052ab57612b1bac55836ca45a71c201b", "node_info": null}, "2": {"text": "token counting a bit more carefully\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    document = Document(doc_text)\n    llm_predictor = MockLLMPredictor(max_tokens=256)\n\n    # test tree index\n    index = GPTTreeIndex([document], llm_predictor=llm_predictor)\n    index.query(\"What is?\", llm_predictor=llm_predictor)\n\n    # test keyword table index\n    index_keyword = GPTKeywordTableIndex([document], llm_predictor=llm_predictor)\n    index_keyword.query(\"What is?\", llm_predictor=llm_predictor)\n\n    # test list index\n    index_list = GPTListIndex([document],", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/token_predictor/test_base.py", "file_name": "test_base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "e0e254d6052ab57612b1bac55836ca45a71c201b", "node_info": null}, "3": {"text": "index\n    index_list = GPTListIndex([document], llm_predictor=llm_predictor)\n    index_list.query(\"What is?\", llm_predictor=llm_predictor)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/token_predictor/test_base.py", "file_name": "test_base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "e0e254d6052ab57612b1bac55836ca45a71c201b", "node_info": null}, "4": {"text": "This code file is a test for the token predictor. It uses the TokenTextSplitter, GPTKeywordTableIndex, GPTListIndex, and GPTTreeIndex classes to test the token predictor. It also uses the MockLLMPredictor and mock_token_splitter_newline functions to mock token counting. The purpose of the code is to assert that the token predictor runs correctly.", "doc_id": null, "embedding": null, "extra_info": null, "index": 4, "child_indices": [0, 1, 2, 3], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"4": {"text": "This code file is a test for the token predictor. It uses the TokenTextSplitter, GPTKeywordTableIndex, GPTListIndex, and GPTTreeIndex classes to test the token predictor. It also uses the MockLLMPredictor and mock_token_splitter_newline functions to mock token counting. The purpose of the code is to assert that the token predictor runs correctly.", "doc_id": null, "embedding": null, "extra_info": null, "index": 4, "child_indices": [0, 1, 2, 3], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}