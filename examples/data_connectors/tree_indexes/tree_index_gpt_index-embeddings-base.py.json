{"index_struct": {"text": "\nThe BaseEmbedding class is a base class for embeddings that provides methods for getting query and text embeddings, as well as a similarity method for calculating the similarity between two embeddings. It contains properties for tracking the total tokens used and the last token usage. The get_query_embedding and get_text_embedding methods use the tokenizer to tokenize the query or text and return the corresponding embedding. The similarity method uses the dot product or Euclidean distance to calculate the similarity between two embeddings. This code file provides a base class for embeddings that can be used to compare the similarity between two pieces of text. It uses a tokenizer to tokenize the query or text and then uses the dot product or Euclidean distance to calculate the similarity between the two embeddings.", "doc_id": "103a2dff-470d-4882-b233-f2a9f5a4e38a", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Base embeddings file.\"\"\"\n\nfrom abc import abstractmethod\nfrom enum import Enum\nfrom typing import Callable, List, Optional\n\nimport numpy as np\n\nfrom gpt_index.utils import globals_helper\n\n# TODO: change to numpy array\nEMB_TYPE = List\n\n\nclass SimilarityMode(str, Enum):\n    \"\"\"Modes for similarity/distance.\"\"\"\n\n    DEFAULT = \"cosine\"\n    DOT_PRODUCT = \"dot_product\"\n    EUCLIDEAN = \"euclidean\"\n\n\nclass BaseEmbedding:\n    \"\"\"Base class for embeddings.\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Init params.\"\"\"\n        self._total_tokens_used = 0\n        self._last_token_usage: Optional[int] = None\n        self._tokenizer: Callable = globals_helper.tokenizer\n\n    @abstractmethod\n    def _get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding.\"\"\"\n\n    def get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding.\"\"\"\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/base.py", "file_name": "base.py"}, "index": 0, "child_indices": [], "ref_doc_id": "6b44f11bc6046627318f35f08e5701d0cdce4208", "node_info": null}, "1": {"text": "     \"\"\"Get query embedding.\"\"\"\n        query_embedding = self._get_query_embedding(query)\n        query_tokens_count = len(self._tokenizer(query))\n        self._total_tokens_used += query_tokens_count\n        return query_embedding\n\n    @abstractmethod\n    def _get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get text embedding.\"\"\"\n\n    def get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get text embedding.\"\"\"\n        text_embedding = self._get_text_embedding(text)\n        text_tokens_count = len(self._tokenizer(text))\n        self._total_tokens_used += text_tokens_count\n        return text_embedding\n\n    def similarity(\n        self,\n        embedding1: EMB_TYPE,\n        embedding2: EMB_TYPE,\n        mode: SimilarityMode = SimilarityMode.DEFAULT,\n    ) ->", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/base.py", "file_name": "base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "6b44f11bc6046627318f35f08e5701d0cdce4208", "node_info": null}, "2": {"text": " mode: SimilarityMode = SimilarityMode.DEFAULT,\n    ) -> float:\n        \"\"\"Get embedding similarity.\"\"\"\n        if mode == SimilarityMode.EUCLIDEAN:\n            return float(np.linalg.norm(np.array(embedding1) - np.array(embedding2)))\n        elif mode == SimilarityMode.DOT_PRODUCT:\n            product = np.dot(embedding1, embedding2)\n            return product\n        else:\n            product = np.dot(embedding1, embedding2)\n            norm = np.linalg.norm(embedding1) * np.linalg.norm(embedding2)\n            return product / norm\n\n    @property\n    def total_tokens_used(self) -> int:\n        \"\"\"Get the total tokens used so far.\"\"\"\n        return self._total_tokens_used\n\n    @property\n    def last_token_usage(self) -> int:\n        \"\"\"Get the last token", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/base.py", "file_name": "base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "6b44f11bc6046627318f35f08e5701d0cdce4208", "node_info": null}, "3": {"text": "-> int:\n        \"\"\"Get the last token usage.\"\"\"\n        if self._last_token_usage is None:\n            return 0\n        return self._last_token_usage\n\n    @last_token_usage.setter\n    def last_token_usage(self, value: int) -> None:\n        \"\"\"Set the last token usage.\"\"\"\n        self._last_token_usage = value\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/base.py", "file_name": "base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "6b44f11bc6046627318f35f08e5701d0cdce4208", "node_info": null}, "4": {"text": "This code file contains the BaseEmbedding class, which is a base class for embeddings. It contains methods for getting query and text embeddings, as well as a similarity method for calculating the similarity between two embeddings. It also contains properties for tracking the total tokens used and the last token usage.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 4, "child_indices": [0, 1, 2, 3], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"4": {"text": "This code file contains the BaseEmbedding class, which is a base class for embeddings. It contains methods for getting query and text embeddings, as well as a similarity method for calculating the similarity between two embeddings. It also contains properties for tracking the total tokens used and the last token usage.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 4, "child_indices": [0, 1, 2, 3], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"6b44f11bc6046627318f35f08e5701d0cdce4208": {"text": "\"\"\"Base embeddings file.\"\"\"\n\nfrom abc import abstractmethod\nfrom enum import Enum\nfrom typing import Callable, List, Optional\n\nimport numpy as np\n\nfrom gpt_index.utils import globals_helper\n\n# TODO: change to numpy array\nEMB_TYPE = List\n\n\nclass SimilarityMode(str, Enum):\n    \"\"\"Modes for similarity/distance.\"\"\"\n\n    DEFAULT = \"cosine\"\n    DOT_PRODUCT = \"dot_product\"\n    EUCLIDEAN = \"euclidean\"\n\n\nclass BaseEmbedding:\n    \"\"\"Base class for embeddings.\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Init params.\"\"\"\n        self._total_tokens_used = 0\n        self._last_token_usage: Optional[int] = None\n        self._tokenizer: Callable = globals_helper.tokenizer\n\n    @abstractmethod\n    def _get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding.\"\"\"\n\n    def get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding.\"\"\"\n        query_embedding = self._get_query_embedding(query)\n        query_tokens_count = len(self._tokenizer(query))\n        self._total_tokens_used += query_tokens_count\n        return query_embedding\n\n    @abstractmethod\n    def _get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get text embedding.\"\"\"\n\n    def get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get text embedding.\"\"\"\n        text_embedding = self._get_text_embedding(text)\n        text_tokens_count = len(self._tokenizer(text))\n        self._total_tokens_used += text_tokens_count\n        return text_embedding\n\n    def similarity(\n        self,\n        embedding1: EMB_TYPE,\n        embedding2: EMB_TYPE,\n        mode: SimilarityMode = SimilarityMode.DEFAULT,\n    ) -> float:\n        \"\"\"Get embedding similarity.\"\"\"\n        if mode == SimilarityMode.EUCLIDEAN:\n            return float(np.linalg.norm(np.array(embedding1) - np.array(embedding2)))\n        elif mode == SimilarityMode.DOT_PRODUCT:\n            product = np.dot(embedding1, embedding2)\n            return product\n        else:\n            product = np.dot(embedding1, embedding2)\n            norm = np.linalg.norm(embedding1) * np.linalg.norm(embedding2)\n            return product / norm\n\n    @property\n    def total_tokens_used(self) -> int:\n        \"\"\"Get the total tokens used so far.\"\"\"\n        return self._total_tokens_used\n\n    @property\n    def last_token_usage(self) -> int:\n        \"\"\"Get the last token usage.\"\"\"\n        if self._last_token_usage is None:\n            return 0\n        return self._last_token_usage\n\n    @last_token_usage.setter\n    def last_token_usage(self, value: int) -> None:\n        \"\"\"Set the last token usage.\"\"\"\n        self._last_token_usage = value\n", "doc_id": "6b44f11bc6046627318f35f08e5701d0cdce4208", "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/base.py", "file_name": "base.py"}, "__type__": "Document"}, "103a2dff-470d-4882-b233-f2a9f5a4e38a": {"text": "\nThe BaseEmbedding class is a base class for embeddings that provides methods for getting query and text embeddings, as well as a similarity method for calculating the similarity between two embeddings. It contains properties for tracking the total tokens used and the last token usage. The get_query_embedding and get_text_embedding methods use the tokenizer to tokenize the query or text and return the corresponding embedding. The similarity method uses the dot product or Euclidean distance to calculate the similarity between two embeddings. This code file provides a base class for embeddings that can be used to compare the similarity between two pieces of text. It uses a tokenizer to tokenize the query or text and then uses the dot product or Euclidean distance to calculate the similarity between the two embeddings.", "doc_id": "103a2dff-470d-4882-b233-f2a9f5a4e38a", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Base embeddings file.\"\"\"\n\nfrom abc import abstractmethod\nfrom enum import Enum\nfrom typing import Callable, List, Optional\n\nimport numpy as np\n\nfrom gpt_index.utils import globals_helper\n\n# TODO: change to numpy array\nEMB_TYPE = List\n\n\nclass SimilarityMode(str, Enum):\n    \"\"\"Modes for similarity/distance.\"\"\"\n\n    DEFAULT = \"cosine\"\n    DOT_PRODUCT = \"dot_product\"\n    EUCLIDEAN = \"euclidean\"\n\n\nclass BaseEmbedding:\n    \"\"\"Base class for embeddings.\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Init params.\"\"\"\n        self._total_tokens_used = 0\n        self._last_token_usage: Optional[int] = None\n        self._tokenizer: Callable = globals_helper.tokenizer\n\n    @abstractmethod\n    def _get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding.\"\"\"\n\n    def get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding.\"\"\"\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/base.py", "file_name": "base.py"}, "index": 0, "child_indices": [], "ref_doc_id": "6b44f11bc6046627318f35f08e5701d0cdce4208", "node_info": null}, "1": {"text": "     \"\"\"Get query embedding.\"\"\"\n        query_embedding = self._get_query_embedding(query)\n        query_tokens_count = len(self._tokenizer(query))\n        self._total_tokens_used += query_tokens_count\n        return query_embedding\n\n    @abstractmethod\n    def _get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get text embedding.\"\"\"\n\n    def get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get text embedding.\"\"\"\n        text_embedding = self._get_text_embedding(text)\n        text_tokens_count = len(self._tokenizer(text))\n        self._total_tokens_used += text_tokens_count\n        return text_embedding\n\n    def similarity(\n        self,\n        embedding1: EMB_TYPE,\n        embedding2: EMB_TYPE,\n        mode: SimilarityMode = SimilarityMode.DEFAULT,\n    ) ->", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/base.py", "file_name": "base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "6b44f11bc6046627318f35f08e5701d0cdce4208", "node_info": null}, "2": {"text": " mode: SimilarityMode = SimilarityMode.DEFAULT,\n    ) -> float:\n        \"\"\"Get embedding similarity.\"\"\"\n        if mode == SimilarityMode.EUCLIDEAN:\n            return float(np.linalg.norm(np.array(embedding1) - np.array(embedding2)))\n        elif mode == SimilarityMode.DOT_PRODUCT:\n            product = np.dot(embedding1, embedding2)\n            return product\n        else:\n            product = np.dot(embedding1, embedding2)\n            norm = np.linalg.norm(embedding1) * np.linalg.norm(embedding2)\n            return product / norm\n\n    @property\n    def total_tokens_used(self) -> int:\n        \"\"\"Get the total tokens used so far.\"\"\"\n        return self._total_tokens_used\n\n    @property\n    def last_token_usage(self) -> int:\n        \"\"\"Get the last token", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/base.py", "file_name": "base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "6b44f11bc6046627318f35f08e5701d0cdce4208", "node_info": null}, "3": {"text": "-> int:\n        \"\"\"Get the last token usage.\"\"\"\n        if self._last_token_usage is None:\n            return 0\n        return self._last_token_usage\n\n    @last_token_usage.setter\n    def last_token_usage(self, value: int) -> None:\n        \"\"\"Set the last token usage.\"\"\"\n        self._last_token_usage = value\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/base.py", "file_name": "base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "6b44f11bc6046627318f35f08e5701d0cdce4208", "node_info": null}, "4": {"text": "This code file contains the BaseEmbedding class, which is a base class for embeddings. It contains methods for getting query and text embeddings, as well as a similarity method for calculating the similarity between two embeddings. It also contains properties for tracking the total tokens used and the last token usage.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 4, "child_indices": [0, 1, 2, 3], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"4": {"text": "This code file contains the BaseEmbedding class, which is a base class for embeddings. It contains methods for getting query and text embeddings, as well as a similarity method for calculating the similarity between two embeddings. It also contains properties for tracking the total tokens used and the last token usage.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 4, "child_indices": [0, 1, 2, 3], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}