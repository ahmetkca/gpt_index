{"index_struct": {"text": "\nThe BaseGPTIndex class is an abstract class used to create GPT indices. It provides methods for setting text, extra info, and doc_id for the index struct, as well as methods for building, inserting, deleting, and updating documents in the index. It also provides methods for querying the index, and for loading and saving the index from/to a string or disk. The index struct is composed of an index structure and a document store, and the code contains helper functions for getting nodes from documents, preprocessing queries, and validating documents. The index struct is built from documents using the build_index_from_documents method, and the index registry and docstore are updated with the index struct. The LLM predictor, embedding model, and prompt helper are used to help with queries. The purpose of the code is to provide a way to create and manage GPT indices, and to provide methods for loading and saving the index from/to a string or disk.", "doc_id": "7376fbf2-91d4-4c39-9edd-d1db98d1d271", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Base index classes.\"\"\"\nimport json\nimport logging\nfrom abc import abstractmethod\nfrom typing import (\n    Any,\n    Dict,\n    Generic,\n    List,\n    Optional,\n    Sequence,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom gpt_index.data_structs.data_structs import IndexStruct, Node\nfrom gpt_index.docstore import DOC_TYPE, DocumentStore\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.embeddings.openai import OpenAIEmbedding\nfrom gpt_index.indices.node_utils import get_nodes_from_document\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.query_runner import QueryRunner\nfrom gpt_index.indices.query.schema import QueryConfig, QueryMode\nfrom gpt_index.indices.registry import IndexRegistry\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.readers.schema.base import Document\nfrom gpt_index.response.schema import", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 0, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "1": {"text": "import Document\nfrom gpt_index.response.schema import Response\nfrom gpt_index.schema import BaseDocument\nfrom gpt_index.token_counter.token_counter import llm_token_counter\n\nIS = TypeVar(\"IS\", bound=IndexStruct)\n\n\nDOCUMENTS_INPUT = Union[BaseDocument, \"BaseGPTIndex\"]\n\n\nclass BaseGPTIndex(Generic[IS]):\n    \"\"\"Base GPT Index.\n\n    Args:\n        documents (Optional[Sequence[BaseDocument]]): List of documents to\n            build the index from.\n        llm_predictor (LLMPredictor): Optional LLMPredictor object. If not provided,\n            will use the default LLMPredictor (text-davinci-003)\n        prompt_helper (PromptHelper): Optional PromptHelper object. If not provided,\n            will use the default PromptHelper.\n        chunk_size_limit (Optional[int]): Optional chunk size limit. If not provided,\n            will use the default chunk size limit (4096 max input size).\n        include_extra_info (bool): Optional bool. If True, extra info (i.e. metadata)\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "2": {"text": "Optional bool. If True, extra info (i.e. metadata)\n            of each Document will be prepended to its text to help with queries.\n            Default is True.\n\n    \"\"\"\n\n    index_struct_cls: Type[IS]\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[IS] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        docstore: Optional[DocumentStore] = None,\n        index_registry: Optional[IndexRegistry] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        chunk_size_limit: Optional[int] = None,\n        include_extra_info: bool = True,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        if index_struct is None and documents is None:\n            raise", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "3": {"text": "None and documents is None:\n            raise ValueError(\"One of documents or index_struct must be provided.\")\n        if index_struct is not None and documents is not None:\n            raise ValueError(\"Only one of documents or index_struct can be provided.\")\n\n        self._llm_predictor = llm_predictor or LLMPredictor()\n        # NOTE: the embed_model isn't used in all indices\n        self._embed_model = embed_model or OpenAIEmbedding()\n        self._include_extra_info = include_extra_info\n\n        # TODO: move out of base if we need custom params per index\n        self._prompt_helper = prompt_helper or PromptHelper.from_llm_predictor(\n            self._llm_predictor, chunk_size_limit=chunk_size_limit\n        )\n\n        # build index struct in the init function\n        self._docstore = docstore or DocumentStore()\n        self._index_registry = index_registry or IndexRegistry()\n\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "4": {"text": "= index_registry or IndexRegistry()\n\n        if index_struct is not None:\n            if not isinstance(index_struct, self.index_struct_cls):\n                raise ValueError(\n                    f\"index_struct must be of type {self.index_struct_cls}\"\n                )\n            self._index_struct = index_struct\n        else:\n            documents = cast(Sequence[DOCUMENTS_INPUT], documents)\n            documents = self._process_documents(\n                documents, self._docstore, self._index_registry\n            )\n            self._validate_documents(documents)\n            # TODO: introduce document store outside __init__ function\n            self._index_struct = self.build_index_from_documents(documents)\n        # update index registry", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 4, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "5": {"text": "       # update index registry and docstore with index_struct\n        self._update_index_registry_and_docstore()\n\n    @property\n    def prompt_helper(self) -> PromptHelper:\n        \"\"\"Get the prompt helper corresponding to the index.\"\"\"\n        return self._prompt_helper\n\n    @property\n    def docstore(self) -> DocumentStore:\n        \"\"\"Get the docstore corresponding to the index.\"\"\"\n        return self._docstore\n\n    @property\n    def index_registry(self) -> IndexRegistry:\n        \"\"\"Get the index registry corresponding to the index.\"\"\"\n        return self._index_registry\n\n    @property\n    def llm_predictor(self) -> LLMPredictor:\n        \"\"\"Get the llm predictor.\"\"\"\n        return self._llm_predictor\n\n    @property\n    def embed_model(self) -> BaseEmbedding:\n        \"\"\"Get the llm predictor.\"\"\"\n        return self._embed_model\n\n    def", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 5, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "6": {"text": "       return self._embed_model\n\n    def _update_index_registry_and_docstore(self) -> None:\n        \"\"\"Update index registry and docstore.\"\"\"\n        # update index registry with current struct\n        cur_type = self.index_struct_cls.get_type()\n        self._index_registry.type_to_struct[cur_type] = self.index_struct_cls\n        self._index_registry.type_to_query[cur_type] = self.get_query_map()\n\n        # update docstore with current struct\n        self._docstore.add_documents([self.index_struct])\n\n    def _process_documents(\n        self,\n        documents: Sequence[DOCUMENTS_INPUT],\n        docstore: DocumentStore,\n        index_registry: IndexRegistry,\n    ) -> List[BaseDocument]:\n        \"\"\"Process documents.\"\"\"\n        results: List[DOC_TYPE] = []\n        for doc in documents:\n            if isinstance(doc,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 6, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "7": {"text": "           if isinstance(doc, BaseGPTIndex):\n                # if user passed in another index, we need to do the following:\n                # - update docstore with the docstore in the index\n                # - validate that the index is in the docstore\n                # - update the index registry\n\n                index_registry.update(doc.index_registry)\n                docstore.update_docstore(doc.docstore)\n                # assert that the doc exists within the docstore\n                sub_index_struct = doc.index_struct_with_text\n                if not docstore.document_exists(sub_index_struct.get_doc_id()):\n                    raise ValueError(\n                        \"The index struct of the sub-index must exist in the", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 7, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "8": {"text": "       \"The index struct of the sub-index must exist in the docstore. \"\n                        f\"Invalid doc ID: {sub_index_struct.get_doc_id()}\"\n                    )\n                results.append(sub_index_struct)\n            elif isinstance(doc, (Document, IndexStruct)):\n                results.append(doc)\n                # update docstore\n                docstore.add_documents([doc])\n            else:\n                raise ValueError(f\"Invalid document type: {type(doc)}.\")\n        return cast(List[BaseDocument], results)\n\n    def _validate_documents(self, documents: Sequence[BaseDocument]) -> None:\n        \"\"\"Validate documents.\"\"\"\n        for doc in documents:\n            if not isinstance(doc, BaseDocument):\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 8, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "9": {"text": "        if not isinstance(doc, BaseDocument):\n                raise ValueError(\"Documents must be of type BaseDocument.\")\n\n    @property\n    def index_struct(self) -> IS:\n        \"\"\"Get the index struct.\"\"\"\n        return self._index_struct\n\n    @property\n    def index_struct_with_text(self) -> IS:\n        \"\"\"Get the index struct with text.\n\n        If text not set, raise an error.\n        For use when composing indices with other indices.\n\n        \"\"\"\n        # make sure that we generate text for index struct\n        if self._index_struct.text is None:\n            # NOTE: set text to be empty string for now\n            raise ValueError(\n                \"Index must have text property set in order \"\n                \"to be composed with other indices. \"\n                \"In order to set text, please run `index.set_text()`.\"\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 9, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "10": {"text": "text, please run `index.set_text()`.\"\n            )\n        return self._index_struct\n\n    def set_text(self, text: str) -> None:\n        \"\"\"Set summary text for index struct.\n\n        This allows index_struct_with_text to be used to compose indices\n        with other indices.\n\n        \"\"\"\n        self._index_struct.text = text\n\n    def set_extra_info(self, extra_info: Dict[str, Any]) -> None:\n        \"\"\"Set extra info (metadata) for index struct.\n\n        If this index is used as a subindex for a parent index, the metadata\n        will be propagated to all nodes derived from this subindex, in the\n        parent index.\n\n        \"\"\"\n        self._index_struct.extra_info = extra_info\n\n    def set_doc_id(self, doc_id: str) -> None:\n        \"\"\"Set doc_id for index struct.\n\n        This is used to uniquely identify the index struct in the docstore.\n        If you wish to delete the", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 10, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "11": {"text": "in the docstore.\n        If you wish to delete the index struct, you can use this doc_id.\n\n        \"\"\"\n        old_doc_id = self._index_struct.get_doc_id()\n        self._index_struct.doc_id = doc_id\n        # Note: we also need to delete old doc_id, and update docstore\n        self._docstore.delete_document(old_doc_id)\n        self._docstore.add_documents([self._index_struct])\n\n    def get_doc_id(self) -> str:\n        \"\"\"Get doc_id for index struct.\n\n        If doc_id not set, raise an error.\n\n        \"\"\"\n        if self._index_struct.doc_id is None:\n            raise ValueError(\"Index must have doc_id property set.\")\n        return self._index_struct.doc_id\n\n    def _get_nodes_from_document(\n        self,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 11, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "12": {"text": "  text_splitter: TokenTextSplitter,\n        start_idx: int = 0,\n    ) -> List[Node]:\n        return get_nodes_from_document(\n            document=document,\n            text_splitter=text_splitter,\n            start_idx=start_idx,\n            include_extra_info=self._include_extra_info,\n        )\n\n    @abstractmethod\n    def _build_index_from_documents(self, documents: Sequence[BaseDocument]) -> IS:\n        \"\"\"Build the index from documents.\"\"\"\n\n    @llm_token_counter(\"build_index_from_documents\")\n    def build_index_from_documents(self, documents: Sequence[BaseDocument]) -> IS:\n        \"\"\"Build the index from documents.\"\"\"\n        return self._build_index_from_documents(documents)\n\n    @abstractmethod\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 12, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "13": {"text": "None:\n        \"\"\"Insert a document.\"\"\"\n\n    @llm_token_counter(\"insert\")\n    def insert(self, document: DOCUMENTS_INPUT, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\n\n        Args:\n            document (Union[BaseDocument, BaseGPTIndex]): document to insert\n\n        \"\"\"\n        processed_doc = self._process_documents(\n            [document], self._docstore, self._index_registry\n        )[0]\n        self._validate_documents([processed_doc])\n        self._insert(processed_doc, **insert_kwargs)\n\n    @abstractmethod\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n\n    def delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document from the index.\n\n        All nodes in the index related to the index will be deleted.\n\n        Args:\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 13, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "14": {"text": "       Args:\n            doc_id (str): document id\n            full_delete (bool): whether to delete the document from the docstore.\n                By default this is True.\n\n        \"\"\"\n        full_delete = delete_kwargs.pop(\"full_delete\", True)\n        logging.debug(f\"> Deleting document: {doc_id}\")\n        if full_delete:\n            self._docstore.delete_document(doc_id)\n        self._delete(doc_id, **delete_kwargs)\n\n    def update(self, document: DOCUMENTS_INPUT, **update_kwargs: Any) -> None:\n        \"\"\"Update a document.\n\n        This is equivalent to deleting the document and then inserting it again.\n\n        Args:\n            document (Union[BaseDocument, BaseGPTIndex]): document to update\n            insert_kwargs (Dict): kwargs to pass to insert\n            delete_kwargs (Dict): kwargs", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 14, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "15": {"text": "          delete_kwargs (Dict): kwargs to pass to delete\n\n        \"\"\"\n        self.delete(document.get_doc_id(), **update_kwargs.pop(\"delete_kwargs\", {}))\n        self.insert(document, **update_kwargs.pop(\"insert_kwargs\", {}))\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Dict) -> None:\n        \"\"\"Preprocess query.\n\n        This allows subclasses to pass in additional query kwargs\n        to query, for instance arguments that are shared between the\n        index and the query class. By default, this does nothing.\n        This also allows subclasses to do validation.\n\n        \"\"\"\n        pass\n\n    def query(\n        self,\n        query_str: str,\n        mode: str = QueryMode.DEFAULT,\n        **query_kwargs: Any,\n    ) -> Response:\n        \"\"\"Answer a query.\n\n        When `query` is called, we query the index with the given `mode` and\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 15, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "16": {"text": "When `query` is called, we query the index with the given `mode` and\n        `query_kwargs`. The `mode` determines the type of query to run, and\n        `query_kwargs` are parameters that are specific to the query type.\n\n        For a comprehensive documentation of available `mode` and `query_kwargs` to\n        query a given index, please visit :ref:`Ref-Query`.\n\n\n        \"\"\"\n        mode_enum = QueryMode(mode)\n        if mode_enum == QueryMode.RECURSIVE:\n            # TODO: deprecated, use ComposableGraph instead.\n            if \"query_configs\" not in query_kwargs:\n                raise ValueError(\"query_configs must be provided for recursive mode.\")\n            query_configs = query_kwargs[\"query_configs\"]\n            query_runner = QueryRunner(\n                self._llm_predictor,\n               ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 16, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "17": {"text": "               self._prompt_helper,\n                self._embed_model,\n                self._docstore,\n                self._index_registry,\n                query_configs=query_configs,\n                recursive=True,\n            )\n            return query_runner.query(query_str, self._index_struct)\n        else:\n            self._preprocess_query(mode_enum, query_kwargs)\n            # TODO: pass in query config directly\n            query_config = QueryConfig(\n                index_struct_type=self._index_struct.get_type(),\n                query_mode=mode_enum,\n                query_kwargs=query_kwargs,\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 17, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "18": {"text": "           )\n            query_runner = QueryRunner(\n                self._llm_predictor,\n                self._prompt_helper,\n                self._embed_model,\n                self._docstore,\n                self._index_registry,\n                query_configs=[query_config],\n                recursive=False,\n            )\n            return query_runner.query(query_str, self._index_struct)\n\n    @classmethod\n    @abstractmethod\n    def get_query_map(cls) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n\n    @classmethod\n    def load_from_string(cls, index_string: str, **kwargs: Any) -> \"BaseGPTIndex\":\n        \"\"\"Load index from string (in JSON-format).\n\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 18, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "19": {"text": "       \"\"\"Load index from string (in JSON-format).\n\n        This method loads the index from a JSON string. The index data\n        structure itself is preserved completely. If the index is defined over\n        subindices, those subindices will also be preserved (and subindices of\n        those subindices, etc.).\n\n        NOTE: load_from_string should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_string` and `load_from_string` on that instead.\n\n        Args:\n            index_string (str): The index string (in JSON-format).\n\n        Returns:\n            BaseGPTIndex: The loaded index.\n\n        \"\"\"\n        result_dict = json.loads(index_string)\n        index_struct = cls.index_struct_cls.from_dict(result_dict[\"index_struct\"])\n        type_to_struct = {index_struct.get_type(): type(index_struct)}\n        docstore", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 19, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "20": {"text": "type(index_struct)}\n        docstore = DocumentStore.load_from_dict(\n            result_dict[\"docstore\"],\n            type_to_struct=type_to_struct,\n        )\n        return cls(index_struct=index_struct, docstore=docstore, **kwargs)\n\n    @classmethod\n    def load_from_disk(cls, save_path: str, **kwargs: Any) -> \"BaseGPTIndex\":\n        \"\"\"Load index from disk.\n\n        This method loads the index from a JSON file stored on disk. The index data\n        structure itself is preserved completely. If the index is defined over\n        subindices, those subindices will also be preserved (and subindices of\n        those subindices, etc.).\n\n        NOTE: load_from_disk should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_disk` and `load_from_disk` on that instead.\n\n        Args:\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 20, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "21": {"text": "      Args:\n            save_path (str): The save_path of the file.\n\n        Returns:\n            BaseGPTIndex: The loaded index.\n\n        \"\"\"\n        with open(save_path, \"r\") as f:\n            file_contents = f.read()\n            return cls.load_from_string(file_contents, **kwargs)\n\n    def save_to_string(self, **save_kwargs: Any) -> str:\n        \"\"\"Save to string.\n\n        This method stores the index into a JSON string.\n\n        NOTE: save_to_string should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_string` and `load_from_string` on that instead.\n\n        Returns:\n            str: The JSON string of the index.\n\n        \"\"\"\n        if self.docstore.contains_index_struct(\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 21, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "22": {"text": "           exclude_ids=[self.index_struct.get_doc_id()]\n        ):\n            raise ValueError(\n                \"Cannot call `save_to_string` on index if index is composed on top of \"\n                \"other indices. Please define a `ComposableGraph` and use \"\n                \"`save_to_string` and `load_from_string` on that instead.\"\n            )\n        out_dict: Dict[str, dict] = {\n            \"index_struct\": self.index_struct.to_dict(),\n            \"docstore\": self.docstore.serialize_to_dict(),\n        }\n        return json.dumps(out_dict, **save_kwargs)\n\n    def save_to_disk(self, save_path: str, **save_kwargs: Any) -> None:\n        \"\"\"Save to file.\n\n        This method stores the index into a JSON file stored on disk.\n\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 22, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "23": {"text": "  This method stores the index into a JSON file stored on disk.\n\n        NOTE: save_to_disk should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_disk` and `load_from_disk` on that instead.\n\n        Args:\n            save_path (str): The save_path of the file.\n\n        \"\"\"\n        index_string = self.save_to_string(**save_kwargs)\n        with open(save_path, \"w\") as f:\n            f.write(index_string)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 23, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "24": {"text": "This code file contains the BaseGPTIndex class, which is used to build an index from a sequence of documents. It includes parameters such as documents, llm_predictor, embed_model, docstore, index_registry, prompt_helper, chunk_size_limit, and include_extra_info. It also includes methods to process documents, validate documents, update the index registry and docstore, and get the prompt helper, docstore, index registry, llm predictor, and embed model. The index struct is built in the init function and can be accessed with the index_struct property.", "doc_id": null, "embedding": null, "extra_info": null, "index": 24, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "ref_doc_id": null, "node_info": null}, "25": {"text": "This code file contains the BaseGPTIndex class, which is an abstract class for creating GPT indices. It provides methods for setting text, extra info, and doc_id for the index struct, as well as methods for building, inserting, deleting, and updating documents in the index. It also provides methods for querying the index, and for loading and saving the index from/to a string. The code also contains helper functions for getting nodes from documents, preprocessing queries, and validating documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 25, "child_indices": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], "ref_doc_id": null, "node_info": null}, "26": {"text": "This code file contains functions for loading and saving a BaseGPTIndex object from/to disk. The BaseGPTIndex object is composed of an index structure and a document store. The load_from_disk and save_to_disk functions are used to read and write the index structure and document store from/to a JSON file. The load_from_string and save_to_string functions are used to read and write the index structure and document store from/to a JSON string. The save_to_disk and save_to_string functions should not be used for indices composed on top of other indices, and instead a ComposableGraph should be used.", "doc_id": null, "embedding": null, "extra_info": null, "index": 26, "child_indices": [20, 21, 22, 23], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"24": {"text": "This code file contains the BaseGPTIndex class, which is used to build an index from a sequence of documents. It includes parameters such as documents, llm_predictor, embed_model, docstore, index_registry, prompt_helper, chunk_size_limit, and include_extra_info. It also includes methods to process documents, validate documents, update the index registry and docstore, and get the prompt helper, docstore, index registry, llm predictor, and embed model. The index struct is built in the init function and can be accessed with the index_struct property.", "doc_id": null, "embedding": null, "extra_info": null, "index": 24, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "ref_doc_id": null, "node_info": null}, "25": {"text": "This code file contains the BaseGPTIndex class, which is an abstract class for creating GPT indices. It provides methods for setting text, extra info, and doc_id for the index struct, as well as methods for building, inserting, deleting, and updating documents in the index. It also provides methods for querying the index, and for loading and saving the index from/to a string. The code also contains helper functions for getting nodes from documents, preprocessing queries, and validating documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 25, "child_indices": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], "ref_doc_id": null, "node_info": null}, "26": {"text": "This code file contains functions for loading and saving a BaseGPTIndex object from/to disk. The BaseGPTIndex object is composed of an index structure and a document store. The load_from_disk and save_to_disk functions are used to read and write the index structure and document store from/to a JSON file. The load_from_string and save_to_string functions are used to read and write the index structure and document store from/to a JSON string. The save_to_disk and save_to_string functions should not be used for indices composed on top of other indices, and instead a ComposableGraph should be used.", "doc_id": null, "embedding": null, "extra_info": null, "index": 26, "child_indices": [20, 21, 22, 23], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"2dec2853fc1bc65bc09aec5679d40f3cd702a74a": {"text": "\"\"\"Base index classes.\"\"\"\nimport json\nimport logging\nfrom abc import abstractmethod\nfrom typing import (\n    Any,\n    Dict,\n    Generic,\n    List,\n    Optional,\n    Sequence,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom gpt_index.data_structs.data_structs import IndexStruct, Node\nfrom gpt_index.docstore import DOC_TYPE, DocumentStore\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.embeddings.openai import OpenAIEmbedding\nfrom gpt_index.indices.node_utils import get_nodes_from_document\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.query_runner import QueryRunner\nfrom gpt_index.indices.query.schema import QueryConfig, QueryMode\nfrom gpt_index.indices.registry import IndexRegistry\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.readers.schema.base import Document\nfrom gpt_index.response.schema import Response\nfrom gpt_index.schema import BaseDocument\nfrom gpt_index.token_counter.token_counter import llm_token_counter\n\nIS = TypeVar(\"IS\", bound=IndexStruct)\n\n\nDOCUMENTS_INPUT = Union[BaseDocument, \"BaseGPTIndex\"]\n\n\nclass BaseGPTIndex(Generic[IS]):\n    \"\"\"Base GPT Index.\n\n    Args:\n        documents (Optional[Sequence[BaseDocument]]): List of documents to\n            build the index from.\n        llm_predictor (LLMPredictor): Optional LLMPredictor object. If not provided,\n            will use the default LLMPredictor (text-davinci-003)\n        prompt_helper (PromptHelper): Optional PromptHelper object. If not provided,\n            will use the default PromptHelper.\n        chunk_size_limit (Optional[int]): Optional chunk size limit. If not provided,\n            will use the default chunk size limit (4096 max input size).\n        include_extra_info (bool): Optional bool. If True, extra info (i.e. metadata)\n            of each Document will be prepended to its text to help with queries.\n            Default is True.\n\n    \"\"\"\n\n    index_struct_cls: Type[IS]\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[IS] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        docstore: Optional[DocumentStore] = None,\n        index_registry: Optional[IndexRegistry] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        chunk_size_limit: Optional[int] = None,\n        include_extra_info: bool = True,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        if index_struct is None and documents is None:\n            raise ValueError(\"One of documents or index_struct must be provided.\")\n        if index_struct is not None and documents is not None:\n            raise ValueError(\"Only one of documents or index_struct can be provided.\")\n\n        self._llm_predictor = llm_predictor or LLMPredictor()\n        # NOTE: the embed_model isn't used in all indices\n        self._embed_model = embed_model or OpenAIEmbedding()\n        self._include_extra_info = include_extra_info\n\n        # TODO: move out of base if we need custom params per index\n        self._prompt_helper = prompt_helper or PromptHelper.from_llm_predictor(\n            self._llm_predictor, chunk_size_limit=chunk_size_limit\n        )\n\n        # build index struct in the init function\n        self._docstore = docstore or DocumentStore()\n        self._index_registry = index_registry or IndexRegistry()\n\n        if index_struct is not None:\n            if not isinstance(index_struct, self.index_struct_cls):\n                raise ValueError(\n                    f\"index_struct must be of type {self.index_struct_cls}\"\n                )\n            self._index_struct = index_struct\n        else:\n            documents = cast(Sequence[DOCUMENTS_INPUT], documents)\n            documents = self._process_documents(\n                documents, self._docstore, self._index_registry\n            )\n            self._validate_documents(documents)\n            # TODO: introduce document store outside __init__ function\n            self._index_struct = self.build_index_from_documents(documents)\n        # update index registry and docstore with index_struct\n        self._update_index_registry_and_docstore()\n\n    @property\n    def prompt_helper(self) -> PromptHelper:\n        \"\"\"Get the prompt helper corresponding to the index.\"\"\"\n        return self._prompt_helper\n\n    @property\n    def docstore(self) -> DocumentStore:\n        \"\"\"Get the docstore corresponding to the index.\"\"\"\n        return self._docstore\n\n    @property\n    def index_registry(self) -> IndexRegistry:\n        \"\"\"Get the index registry corresponding to the index.\"\"\"\n        return self._index_registry\n\n    @property\n    def llm_predictor(self) -> LLMPredictor:\n        \"\"\"Get the llm predictor.\"\"\"\n        return self._llm_predictor\n\n    @property\n    def embed_model(self) -> BaseEmbedding:\n        \"\"\"Get the llm predictor.\"\"\"\n        return self._embed_model\n\n    def _update_index_registry_and_docstore(self) -> None:\n        \"\"\"Update index registry and docstore.\"\"\"\n        # update index registry with current struct\n        cur_type = self.index_struct_cls.get_type()\n        self._index_registry.type_to_struct[cur_type] = self.index_struct_cls\n        self._index_registry.type_to_query[cur_type] = self.get_query_map()\n\n        # update docstore with current struct\n        self._docstore.add_documents([self.index_struct])\n\n    def _process_documents(\n        self,\n        documents: Sequence[DOCUMENTS_INPUT],\n        docstore: DocumentStore,\n        index_registry: IndexRegistry,\n    ) -> List[BaseDocument]:\n        \"\"\"Process documents.\"\"\"\n        results: List[DOC_TYPE] = []\n        for doc in documents:\n            if isinstance(doc, BaseGPTIndex):\n                # if user passed in another index, we need to do the following:\n                # - update docstore with the docstore in the index\n                # - validate that the index is in the docstore\n                # - update the index registry\n\n                index_registry.update(doc.index_registry)\n                docstore.update_docstore(doc.docstore)\n                # assert that the doc exists within the docstore\n                sub_index_struct = doc.index_struct_with_text\n                if not docstore.document_exists(sub_index_struct.get_doc_id()):\n                    raise ValueError(\n                        \"The index struct of the sub-index must exist in the docstore. \"\n                        f\"Invalid doc ID: {sub_index_struct.get_doc_id()}\"\n                    )\n                results.append(sub_index_struct)\n            elif isinstance(doc, (Document, IndexStruct)):\n                results.append(doc)\n                # update docstore\n                docstore.add_documents([doc])\n            else:\n                raise ValueError(f\"Invalid document type: {type(doc)}.\")\n        return cast(List[BaseDocument], results)\n\n    def _validate_documents(self, documents: Sequence[BaseDocument]) -> None:\n        \"\"\"Validate documents.\"\"\"\n        for doc in documents:\n            if not isinstance(doc, BaseDocument):\n                raise ValueError(\"Documents must be of type BaseDocument.\")\n\n    @property\n    def index_struct(self) -> IS:\n        \"\"\"Get the index struct.\"\"\"\n        return self._index_struct\n\n    @property\n    def index_struct_with_text(self) -> IS:\n        \"\"\"Get the index struct with text.\n\n        If text not set, raise an error.\n        For use when composing indices with other indices.\n\n        \"\"\"\n        # make sure that we generate text for index struct\n        if self._index_struct.text is None:\n            # NOTE: set text to be empty string for now\n            raise ValueError(\n                \"Index must have text property set in order \"\n                \"to be composed with other indices. \"\n                \"In order to set text, please run `index.set_text()`.\"\n            )\n        return self._index_struct\n\n    def set_text(self, text: str) -> None:\n        \"\"\"Set summary text for index struct.\n\n        This allows index_struct_with_text to be used to compose indices\n        with other indices.\n\n        \"\"\"\n        self._index_struct.text = text\n\n    def set_extra_info(self, extra_info: Dict[str, Any]) -> None:\n        \"\"\"Set extra info (metadata) for index struct.\n\n        If this index is used as a subindex for a parent index, the metadata\n        will be propagated to all nodes derived from this subindex, in the\n        parent index.\n\n        \"\"\"\n        self._index_struct.extra_info = extra_info\n\n    def set_doc_id(self, doc_id: str) -> None:\n        \"\"\"Set doc_id for index struct.\n\n        This is used to uniquely identify the index struct in the docstore.\n        If you wish to delete the index struct, you can use this doc_id.\n\n        \"\"\"\n        old_doc_id = self._index_struct.get_doc_id()\n        self._index_struct.doc_id = doc_id\n        # Note: we also need to delete old doc_id, and update docstore\n        self._docstore.delete_document(old_doc_id)\n        self._docstore.add_documents([self._index_struct])\n\n    def get_doc_id(self) -> str:\n        \"\"\"Get doc_id for index struct.\n\n        If doc_id not set, raise an error.\n\n        \"\"\"\n        if self._index_struct.doc_id is None:\n            raise ValueError(\"Index must have doc_id property set.\")\n        return self._index_struct.doc_id\n\n    def _get_nodes_from_document(\n        self,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n        start_idx: int = 0,\n    ) -> List[Node]:\n        return get_nodes_from_document(\n            document=document,\n            text_splitter=text_splitter,\n            start_idx=start_idx,\n            include_extra_info=self._include_extra_info,\n        )\n\n    @abstractmethod\n    def _build_index_from_documents(self, documents: Sequence[BaseDocument]) -> IS:\n        \"\"\"Build the index from documents.\"\"\"\n\n    @llm_token_counter(\"build_index_from_documents\")\n    def build_index_from_documents(self, documents: Sequence[BaseDocument]) -> IS:\n        \"\"\"Build the index from documents.\"\"\"\n        return self._build_index_from_documents(documents)\n\n    @abstractmethod\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n\n    @llm_token_counter(\"insert\")\n    def insert(self, document: DOCUMENTS_INPUT, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\n\n        Args:\n            document (Union[BaseDocument, BaseGPTIndex]): document to insert\n\n        \"\"\"\n        processed_doc = self._process_documents(\n            [document], self._docstore, self._index_registry\n        )[0]\n        self._validate_documents([processed_doc])\n        self._insert(processed_doc, **insert_kwargs)\n\n    @abstractmethod\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n\n    def delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document from the index.\n\n        All nodes in the index related to the index will be deleted.\n\n        Args:\n            doc_id (str): document id\n            full_delete (bool): whether to delete the document from the docstore.\n                By default this is True.\n\n        \"\"\"\n        full_delete = delete_kwargs.pop(\"full_delete\", True)\n        logging.debug(f\"> Deleting document: {doc_id}\")\n        if full_delete:\n            self._docstore.delete_document(doc_id)\n        self._delete(doc_id, **delete_kwargs)\n\n    def update(self, document: DOCUMENTS_INPUT, **update_kwargs: Any) -> None:\n        \"\"\"Update a document.\n\n        This is equivalent to deleting the document and then inserting it again.\n\n        Args:\n            document (Union[BaseDocument, BaseGPTIndex]): document to update\n            insert_kwargs (Dict): kwargs to pass to insert\n            delete_kwargs (Dict): kwargs to pass to delete\n\n        \"\"\"\n        self.delete(document.get_doc_id(), **update_kwargs.pop(\"delete_kwargs\", {}))\n        self.insert(document, **update_kwargs.pop(\"insert_kwargs\", {}))\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Dict) -> None:\n        \"\"\"Preprocess query.\n\n        This allows subclasses to pass in additional query kwargs\n        to query, for instance arguments that are shared between the\n        index and the query class. By default, this does nothing.\n        This also allows subclasses to do validation.\n\n        \"\"\"\n        pass\n\n    def query(\n        self,\n        query_str: str,\n        mode: str = QueryMode.DEFAULT,\n        **query_kwargs: Any,\n    ) -> Response:\n        \"\"\"Answer a query.\n\n        When `query` is called, we query the index with the given `mode` and\n        `query_kwargs`. The `mode` determines the type of query to run, and\n        `query_kwargs` are parameters that are specific to the query type.\n\n        For a comprehensive documentation of available `mode` and `query_kwargs` to\n        query a given index, please visit :ref:`Ref-Query`.\n\n\n        \"\"\"\n        mode_enum = QueryMode(mode)\n        if mode_enum == QueryMode.RECURSIVE:\n            # TODO: deprecated, use ComposableGraph instead.\n            if \"query_configs\" not in query_kwargs:\n                raise ValueError(\"query_configs must be provided for recursive mode.\")\n            query_configs = query_kwargs[\"query_configs\"]\n            query_runner = QueryRunner(\n                self._llm_predictor,\n                self._prompt_helper,\n                self._embed_model,\n                self._docstore,\n                self._index_registry,\n                query_configs=query_configs,\n                recursive=True,\n            )\n            return query_runner.query(query_str, self._index_struct)\n        else:\n            self._preprocess_query(mode_enum, query_kwargs)\n            # TODO: pass in query config directly\n            query_config = QueryConfig(\n                index_struct_type=self._index_struct.get_type(),\n                query_mode=mode_enum,\n                query_kwargs=query_kwargs,\n            )\n            query_runner = QueryRunner(\n                self._llm_predictor,\n                self._prompt_helper,\n                self._embed_model,\n                self._docstore,\n                self._index_registry,\n                query_configs=[query_config],\n                recursive=False,\n            )\n            return query_runner.query(query_str, self._index_struct)\n\n    @classmethod\n    @abstractmethod\n    def get_query_map(cls) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n\n    @classmethod\n    def load_from_string(cls, index_string: str, **kwargs: Any) -> \"BaseGPTIndex\":\n        \"\"\"Load index from string (in JSON-format).\n\n        This method loads the index from a JSON string. The index data\n        structure itself is preserved completely. If the index is defined over\n        subindices, those subindices will also be preserved (and subindices of\n        those subindices, etc.).\n\n        NOTE: load_from_string should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_string` and `load_from_string` on that instead.\n\n        Args:\n            index_string (str): The index string (in JSON-format).\n\n        Returns:\n            BaseGPTIndex: The loaded index.\n\n        \"\"\"\n        result_dict = json.loads(index_string)\n        index_struct = cls.index_struct_cls.from_dict(result_dict[\"index_struct\"])\n        type_to_struct = {index_struct.get_type(): type(index_struct)}\n        docstore = DocumentStore.load_from_dict(\n            result_dict[\"docstore\"],\n            type_to_struct=type_to_struct,\n        )\n        return cls(index_struct=index_struct, docstore=docstore, **kwargs)\n\n    @classmethod\n    def load_from_disk(cls, save_path: str, **kwargs: Any) -> \"BaseGPTIndex\":\n        \"\"\"Load index from disk.\n\n        This method loads the index from a JSON file stored on disk. The index data\n        structure itself is preserved completely. If the index is defined over\n        subindices, those subindices will also be preserved (and subindices of\n        those subindices, etc.).\n\n        NOTE: load_from_disk should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_disk` and `load_from_disk` on that instead.\n\n        Args:\n            save_path (str): The save_path of the file.\n\n        Returns:\n            BaseGPTIndex: The loaded index.\n\n        \"\"\"\n        with open(save_path, \"r\") as f:\n            file_contents = f.read()\n            return cls.load_from_string(file_contents, **kwargs)\n\n    def save_to_string(self, **save_kwargs: Any) -> str:\n        \"\"\"Save to string.\n\n        This method stores the index into a JSON string.\n\n        NOTE: save_to_string should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_string` and `load_from_string` on that instead.\n\n        Returns:\n            str: The JSON string of the index.\n\n        \"\"\"\n        if self.docstore.contains_index_struct(\n            exclude_ids=[self.index_struct.get_doc_id()]\n        ):\n            raise ValueError(\n                \"Cannot call `save_to_string` on index if index is composed on top of \"\n                \"other indices. Please define a `ComposableGraph` and use \"\n                \"`save_to_string` and `load_from_string` on that instead.\"\n            )\n        out_dict: Dict[str, dict] = {\n            \"index_struct\": self.index_struct.to_dict(),\n            \"docstore\": self.docstore.serialize_to_dict(),\n        }\n        return json.dumps(out_dict, **save_kwargs)\n\n    def save_to_disk(self, save_path: str, **save_kwargs: Any) -> None:\n        \"\"\"Save to file.\n\n        This method stores the index into a JSON file stored on disk.\n\n        NOTE: save_to_disk should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_disk` and `load_from_disk` on that instead.\n\n        Args:\n            save_path (str): The save_path of the file.\n\n        \"\"\"\n        index_string = self.save_to_string(**save_kwargs)\n        with open(save_path, \"w\") as f:\n            f.write(index_string)\n", "doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "__type__": "Document"}, "7376fbf2-91d4-4c39-9edd-d1db98d1d271": {"text": "\nThe BaseGPTIndex class is an abstract class used to create GPT indices. It provides methods for setting text, extra info, and doc_id for the index struct, as well as methods for building, inserting, deleting, and updating documents in the index. It also provides methods for querying the index, and for loading and saving the index from/to a string or disk. The index struct is composed of an index structure and a document store, and the code contains helper functions for getting nodes from documents, preprocessing queries, and validating documents. The index struct is built from documents using the build_index_from_documents method, and the index registry and docstore are updated with the index struct. The LLM predictor, embedding model, and prompt helper are used to help with queries. The purpose of the code is to provide a way to create and manage GPT indices, and to provide methods for loading and saving the index from/to a string or disk.", "doc_id": "7376fbf2-91d4-4c39-9edd-d1db98d1d271", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Base index classes.\"\"\"\nimport json\nimport logging\nfrom abc import abstractmethod\nfrom typing import (\n    Any,\n    Dict,\n    Generic,\n    List,\n    Optional,\n    Sequence,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom gpt_index.data_structs.data_structs import IndexStruct, Node\nfrom gpt_index.docstore import DOC_TYPE, DocumentStore\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.embeddings.openai import OpenAIEmbedding\nfrom gpt_index.indices.node_utils import get_nodes_from_document\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.query_runner import QueryRunner\nfrom gpt_index.indices.query.schema import QueryConfig, QueryMode\nfrom gpt_index.indices.registry import IndexRegistry\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.readers.schema.base import Document\nfrom gpt_index.response.schema import", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 0, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "1": {"text": "import Document\nfrom gpt_index.response.schema import Response\nfrom gpt_index.schema import BaseDocument\nfrom gpt_index.token_counter.token_counter import llm_token_counter\n\nIS = TypeVar(\"IS\", bound=IndexStruct)\n\n\nDOCUMENTS_INPUT = Union[BaseDocument, \"BaseGPTIndex\"]\n\n\nclass BaseGPTIndex(Generic[IS]):\n    \"\"\"Base GPT Index.\n\n    Args:\n        documents (Optional[Sequence[BaseDocument]]): List of documents to\n            build the index from.\n        llm_predictor (LLMPredictor): Optional LLMPredictor object. If not provided,\n            will use the default LLMPredictor (text-davinci-003)\n        prompt_helper (PromptHelper): Optional PromptHelper object. If not provided,\n            will use the default PromptHelper.\n        chunk_size_limit (Optional[int]): Optional chunk size limit. If not provided,\n            will use the default chunk size limit (4096 max input size).\n        include_extra_info (bool): Optional bool. If True, extra info (i.e. metadata)\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "2": {"text": "Optional bool. If True, extra info (i.e. metadata)\n            of each Document will be prepended to its text to help with queries.\n            Default is True.\n\n    \"\"\"\n\n    index_struct_cls: Type[IS]\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[IS] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        docstore: Optional[DocumentStore] = None,\n        index_registry: Optional[IndexRegistry] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        chunk_size_limit: Optional[int] = None,\n        include_extra_info: bool = True,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        if index_struct is None and documents is None:\n            raise", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "3": {"text": "None and documents is None:\n            raise ValueError(\"One of documents or index_struct must be provided.\")\n        if index_struct is not None and documents is not None:\n            raise ValueError(\"Only one of documents or index_struct can be provided.\")\n\n        self._llm_predictor = llm_predictor or LLMPredictor()\n        # NOTE: the embed_model isn't used in all indices\n        self._embed_model = embed_model or OpenAIEmbedding()\n        self._include_extra_info = include_extra_info\n\n        # TODO: move out of base if we need custom params per index\n        self._prompt_helper = prompt_helper or PromptHelper.from_llm_predictor(\n            self._llm_predictor, chunk_size_limit=chunk_size_limit\n        )\n\n        # build index struct in the init function\n        self._docstore = docstore or DocumentStore()\n        self._index_registry = index_registry or IndexRegistry()\n\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "4": {"text": "= index_registry or IndexRegistry()\n\n        if index_struct is not None:\n            if not isinstance(index_struct, self.index_struct_cls):\n                raise ValueError(\n                    f\"index_struct must be of type {self.index_struct_cls}\"\n                )\n            self._index_struct = index_struct\n        else:\n            documents = cast(Sequence[DOCUMENTS_INPUT], documents)\n            documents = self._process_documents(\n                documents, self._docstore, self._index_registry\n            )\n            self._validate_documents(documents)\n            # TODO: introduce document store outside __init__ function\n            self._index_struct = self.build_index_from_documents(documents)\n        # update index registry", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 4, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "5": {"text": "       # update index registry and docstore with index_struct\n        self._update_index_registry_and_docstore()\n\n    @property\n    def prompt_helper(self) -> PromptHelper:\n        \"\"\"Get the prompt helper corresponding to the index.\"\"\"\n        return self._prompt_helper\n\n    @property\n    def docstore(self) -> DocumentStore:\n        \"\"\"Get the docstore corresponding to the index.\"\"\"\n        return self._docstore\n\n    @property\n    def index_registry(self) -> IndexRegistry:\n        \"\"\"Get the index registry corresponding to the index.\"\"\"\n        return self._index_registry\n\n    @property\n    def llm_predictor(self) -> LLMPredictor:\n        \"\"\"Get the llm predictor.\"\"\"\n        return self._llm_predictor\n\n    @property\n    def embed_model(self) -> BaseEmbedding:\n        \"\"\"Get the llm predictor.\"\"\"\n        return self._embed_model\n\n    def", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 5, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "6": {"text": "       return self._embed_model\n\n    def _update_index_registry_and_docstore(self) -> None:\n        \"\"\"Update index registry and docstore.\"\"\"\n        # update index registry with current struct\n        cur_type = self.index_struct_cls.get_type()\n        self._index_registry.type_to_struct[cur_type] = self.index_struct_cls\n        self._index_registry.type_to_query[cur_type] = self.get_query_map()\n\n        # update docstore with current struct\n        self._docstore.add_documents([self.index_struct])\n\n    def _process_documents(\n        self,\n        documents: Sequence[DOCUMENTS_INPUT],\n        docstore: DocumentStore,\n        index_registry: IndexRegistry,\n    ) -> List[BaseDocument]:\n        \"\"\"Process documents.\"\"\"\n        results: List[DOC_TYPE] = []\n        for doc in documents:\n            if isinstance(doc,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 6, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "7": {"text": "           if isinstance(doc, BaseGPTIndex):\n                # if user passed in another index, we need to do the following:\n                # - update docstore with the docstore in the index\n                # - validate that the index is in the docstore\n                # - update the index registry\n\n                index_registry.update(doc.index_registry)\n                docstore.update_docstore(doc.docstore)\n                # assert that the doc exists within the docstore\n                sub_index_struct = doc.index_struct_with_text\n                if not docstore.document_exists(sub_index_struct.get_doc_id()):\n                    raise ValueError(\n                        \"The index struct of the sub-index must exist in the", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 7, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "8": {"text": "       \"The index struct of the sub-index must exist in the docstore. \"\n                        f\"Invalid doc ID: {sub_index_struct.get_doc_id()}\"\n                    )\n                results.append(sub_index_struct)\n            elif isinstance(doc, (Document, IndexStruct)):\n                results.append(doc)\n                # update docstore\n                docstore.add_documents([doc])\n            else:\n                raise ValueError(f\"Invalid document type: {type(doc)}.\")\n        return cast(List[BaseDocument], results)\n\n    def _validate_documents(self, documents: Sequence[BaseDocument]) -> None:\n        \"\"\"Validate documents.\"\"\"\n        for doc in documents:\n            if not isinstance(doc, BaseDocument):\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 8, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "9": {"text": "        if not isinstance(doc, BaseDocument):\n                raise ValueError(\"Documents must be of type BaseDocument.\")\n\n    @property\n    def index_struct(self) -> IS:\n        \"\"\"Get the index struct.\"\"\"\n        return self._index_struct\n\n    @property\n    def index_struct_with_text(self) -> IS:\n        \"\"\"Get the index struct with text.\n\n        If text not set, raise an error.\n        For use when composing indices with other indices.\n\n        \"\"\"\n        # make sure that we generate text for index struct\n        if self._index_struct.text is None:\n            # NOTE: set text to be empty string for now\n            raise ValueError(\n                \"Index must have text property set in order \"\n                \"to be composed with other indices. \"\n                \"In order to set text, please run `index.set_text()`.\"\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 9, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "10": {"text": "text, please run `index.set_text()`.\"\n            )\n        return self._index_struct\n\n    def set_text(self, text: str) -> None:\n        \"\"\"Set summary text for index struct.\n\n        This allows index_struct_with_text to be used to compose indices\n        with other indices.\n\n        \"\"\"\n        self._index_struct.text = text\n\n    def set_extra_info(self, extra_info: Dict[str, Any]) -> None:\n        \"\"\"Set extra info (metadata) for index struct.\n\n        If this index is used as a subindex for a parent index, the metadata\n        will be propagated to all nodes derived from this subindex, in the\n        parent index.\n\n        \"\"\"\n        self._index_struct.extra_info = extra_info\n\n    def set_doc_id(self, doc_id: str) -> None:\n        \"\"\"Set doc_id for index struct.\n\n        This is used to uniquely identify the index struct in the docstore.\n        If you wish to delete the", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 10, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "11": {"text": "in the docstore.\n        If you wish to delete the index struct, you can use this doc_id.\n\n        \"\"\"\n        old_doc_id = self._index_struct.get_doc_id()\n        self._index_struct.doc_id = doc_id\n        # Note: we also need to delete old doc_id, and update docstore\n        self._docstore.delete_document(old_doc_id)\n        self._docstore.add_documents([self._index_struct])\n\n    def get_doc_id(self) -> str:\n        \"\"\"Get doc_id for index struct.\n\n        If doc_id not set, raise an error.\n\n        \"\"\"\n        if self._index_struct.doc_id is None:\n            raise ValueError(\"Index must have doc_id property set.\")\n        return self._index_struct.doc_id\n\n    def _get_nodes_from_document(\n        self,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 11, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "12": {"text": "  text_splitter: TokenTextSplitter,\n        start_idx: int = 0,\n    ) -> List[Node]:\n        return get_nodes_from_document(\n            document=document,\n            text_splitter=text_splitter,\n            start_idx=start_idx,\n            include_extra_info=self._include_extra_info,\n        )\n\n    @abstractmethod\n    def _build_index_from_documents(self, documents: Sequence[BaseDocument]) -> IS:\n        \"\"\"Build the index from documents.\"\"\"\n\n    @llm_token_counter(\"build_index_from_documents\")\n    def build_index_from_documents(self, documents: Sequence[BaseDocument]) -> IS:\n        \"\"\"Build the index from documents.\"\"\"\n        return self._build_index_from_documents(documents)\n\n    @abstractmethod\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 12, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "13": {"text": "None:\n        \"\"\"Insert a document.\"\"\"\n\n    @llm_token_counter(\"insert\")\n    def insert(self, document: DOCUMENTS_INPUT, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\n\n        Args:\n            document (Union[BaseDocument, BaseGPTIndex]): document to insert\n\n        \"\"\"\n        processed_doc = self._process_documents(\n            [document], self._docstore, self._index_registry\n        )[0]\n        self._validate_documents([processed_doc])\n        self._insert(processed_doc, **insert_kwargs)\n\n    @abstractmethod\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n\n    def delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document from the index.\n\n        All nodes in the index related to the index will be deleted.\n\n        Args:\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 13, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "14": {"text": "       Args:\n            doc_id (str): document id\n            full_delete (bool): whether to delete the document from the docstore.\n                By default this is True.\n\n        \"\"\"\n        full_delete = delete_kwargs.pop(\"full_delete\", True)\n        logging.debug(f\"> Deleting document: {doc_id}\")\n        if full_delete:\n            self._docstore.delete_document(doc_id)\n        self._delete(doc_id, **delete_kwargs)\n\n    def update(self, document: DOCUMENTS_INPUT, **update_kwargs: Any) -> None:\n        \"\"\"Update a document.\n\n        This is equivalent to deleting the document and then inserting it again.\n\n        Args:\n            document (Union[BaseDocument, BaseGPTIndex]): document to update\n            insert_kwargs (Dict): kwargs to pass to insert\n            delete_kwargs (Dict): kwargs", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 14, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "15": {"text": "          delete_kwargs (Dict): kwargs to pass to delete\n\n        \"\"\"\n        self.delete(document.get_doc_id(), **update_kwargs.pop(\"delete_kwargs\", {}))\n        self.insert(document, **update_kwargs.pop(\"insert_kwargs\", {}))\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Dict) -> None:\n        \"\"\"Preprocess query.\n\n        This allows subclasses to pass in additional query kwargs\n        to query, for instance arguments that are shared between the\n        index and the query class. By default, this does nothing.\n        This also allows subclasses to do validation.\n\n        \"\"\"\n        pass\n\n    def query(\n        self,\n        query_str: str,\n        mode: str = QueryMode.DEFAULT,\n        **query_kwargs: Any,\n    ) -> Response:\n        \"\"\"Answer a query.\n\n        When `query` is called, we query the index with the given `mode` and\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 15, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "16": {"text": "When `query` is called, we query the index with the given `mode` and\n        `query_kwargs`. The `mode` determines the type of query to run, and\n        `query_kwargs` are parameters that are specific to the query type.\n\n        For a comprehensive documentation of available `mode` and `query_kwargs` to\n        query a given index, please visit :ref:`Ref-Query`.\n\n\n        \"\"\"\n        mode_enum = QueryMode(mode)\n        if mode_enum == QueryMode.RECURSIVE:\n            # TODO: deprecated, use ComposableGraph instead.\n            if \"query_configs\" not in query_kwargs:\n                raise ValueError(\"query_configs must be provided for recursive mode.\")\n            query_configs = query_kwargs[\"query_configs\"]\n            query_runner = QueryRunner(\n                self._llm_predictor,\n               ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 16, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "17": {"text": "               self._prompt_helper,\n                self._embed_model,\n                self._docstore,\n                self._index_registry,\n                query_configs=query_configs,\n                recursive=True,\n            )\n            return query_runner.query(query_str, self._index_struct)\n        else:\n            self._preprocess_query(mode_enum, query_kwargs)\n            # TODO: pass in query config directly\n            query_config = QueryConfig(\n                index_struct_type=self._index_struct.get_type(),\n                query_mode=mode_enum,\n                query_kwargs=query_kwargs,\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 17, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "18": {"text": "           )\n            query_runner = QueryRunner(\n                self._llm_predictor,\n                self._prompt_helper,\n                self._embed_model,\n                self._docstore,\n                self._index_registry,\n                query_configs=[query_config],\n                recursive=False,\n            )\n            return query_runner.query(query_str, self._index_struct)\n\n    @classmethod\n    @abstractmethod\n    def get_query_map(cls) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n\n    @classmethod\n    def load_from_string(cls, index_string: str, **kwargs: Any) -> \"BaseGPTIndex\":\n        \"\"\"Load index from string (in JSON-format).\n\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 18, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "19": {"text": "       \"\"\"Load index from string (in JSON-format).\n\n        This method loads the index from a JSON string. The index data\n        structure itself is preserved completely. If the index is defined over\n        subindices, those subindices will also be preserved (and subindices of\n        those subindices, etc.).\n\n        NOTE: load_from_string should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_string` and `load_from_string` on that instead.\n\n        Args:\n            index_string (str): The index string (in JSON-format).\n\n        Returns:\n            BaseGPTIndex: The loaded index.\n\n        \"\"\"\n        result_dict = json.loads(index_string)\n        index_struct = cls.index_struct_cls.from_dict(result_dict[\"index_struct\"])\n        type_to_struct = {index_struct.get_type(): type(index_struct)}\n        docstore", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 19, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "20": {"text": "type(index_struct)}\n        docstore = DocumentStore.load_from_dict(\n            result_dict[\"docstore\"],\n            type_to_struct=type_to_struct,\n        )\n        return cls(index_struct=index_struct, docstore=docstore, **kwargs)\n\n    @classmethod\n    def load_from_disk(cls, save_path: str, **kwargs: Any) -> \"BaseGPTIndex\":\n        \"\"\"Load index from disk.\n\n        This method loads the index from a JSON file stored on disk. The index data\n        structure itself is preserved completely. If the index is defined over\n        subindices, those subindices will also be preserved (and subindices of\n        those subindices, etc.).\n\n        NOTE: load_from_disk should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_disk` and `load_from_disk` on that instead.\n\n        Args:\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 20, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "21": {"text": "      Args:\n            save_path (str): The save_path of the file.\n\n        Returns:\n            BaseGPTIndex: The loaded index.\n\n        \"\"\"\n        with open(save_path, \"r\") as f:\n            file_contents = f.read()\n            return cls.load_from_string(file_contents, **kwargs)\n\n    def save_to_string(self, **save_kwargs: Any) -> str:\n        \"\"\"Save to string.\n\n        This method stores the index into a JSON string.\n\n        NOTE: save_to_string should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_string` and `load_from_string` on that instead.\n\n        Returns:\n            str: The JSON string of the index.\n\n        \"\"\"\n        if self.docstore.contains_index_struct(\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 21, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "22": {"text": "           exclude_ids=[self.index_struct.get_doc_id()]\n        ):\n            raise ValueError(\n                \"Cannot call `save_to_string` on index if index is composed on top of \"\n                \"other indices. Please define a `ComposableGraph` and use \"\n                \"`save_to_string` and `load_from_string` on that instead.\"\n            )\n        out_dict: Dict[str, dict] = {\n            \"index_struct\": self.index_struct.to_dict(),\n            \"docstore\": self.docstore.serialize_to_dict(),\n        }\n        return json.dumps(out_dict, **save_kwargs)\n\n    def save_to_disk(self, save_path: str, **save_kwargs: Any) -> None:\n        \"\"\"Save to file.\n\n        This method stores the index into a JSON file stored on disk.\n\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 22, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "23": {"text": "  This method stores the index into a JSON file stored on disk.\n\n        NOTE: save_to_disk should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_disk` and `load_from_disk` on that instead.\n\n        Args:\n            save_path (str): The save_path of the file.\n\n        \"\"\"\n        index_string = self.save_to_string(**save_kwargs)\n        with open(save_path, \"w\") as f:\n            f.write(index_string)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "index": 23, "child_indices": [], "ref_doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "node_info": null}, "24": {"text": "This code file contains the BaseGPTIndex class, which is used to build an index from a sequence of documents. It includes parameters such as documents, llm_predictor, embed_model, docstore, index_registry, prompt_helper, chunk_size_limit, and include_extra_info. It also includes methods to process documents, validate documents, update the index registry and docstore, and get the prompt helper, docstore, index registry, llm predictor, and embed model. The index struct is built in the init function and can be accessed with the index_struct property.", "doc_id": null, "embedding": null, "extra_info": null, "index": 24, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "ref_doc_id": null, "node_info": null}, "25": {"text": "This code file contains the BaseGPTIndex class, which is an abstract class for creating GPT indices. It provides methods for setting text, extra info, and doc_id for the index struct, as well as methods for building, inserting, deleting, and updating documents in the index. It also provides methods for querying the index, and for loading and saving the index from/to a string. The code also contains helper functions for getting nodes from documents, preprocessing queries, and validating documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 25, "child_indices": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], "ref_doc_id": null, "node_info": null}, "26": {"text": "This code file contains functions for loading and saving a BaseGPTIndex object from/to disk. The BaseGPTIndex object is composed of an index structure and a document store. The load_from_disk and save_to_disk functions are used to read and write the index structure and document store from/to a JSON file. The load_from_string and save_to_string functions are used to read and write the index structure and document store from/to a JSON string. The save_to_disk and save_to_string functions should not be used for indices composed on top of other indices, and instead a ComposableGraph should be used.", "doc_id": null, "embedding": null, "extra_info": null, "index": 26, "child_indices": [20, 21, 22, 23], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"24": {"text": "This code file contains the BaseGPTIndex class, which is used to build an index from a sequence of documents. It includes parameters such as documents, llm_predictor, embed_model, docstore, index_registry, prompt_helper, chunk_size_limit, and include_extra_info. It also includes methods to process documents, validate documents, update the index registry and docstore, and get the prompt helper, docstore, index registry, llm predictor, and embed model. The index struct is built in the init function and can be accessed with the index_struct property.", "doc_id": null, "embedding": null, "extra_info": null, "index": 24, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "ref_doc_id": null, "node_info": null}, "25": {"text": "This code file contains the BaseGPTIndex class, which is an abstract class for creating GPT indices. It provides methods for setting text, extra info, and doc_id for the index struct, as well as methods for building, inserting, deleting, and updating documents in the index. It also provides methods for querying the index, and for loading and saving the index from/to a string. The code also contains helper functions for getting nodes from documents, preprocessing queries, and validating documents.", "doc_id": null, "embedding": null, "extra_info": null, "index": 25, "child_indices": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], "ref_doc_id": null, "node_info": null}, "26": {"text": "This code file contains functions for loading and saving a BaseGPTIndex object from/to disk. The BaseGPTIndex object is composed of an index structure and a document store. The load_from_disk and save_to_disk functions are used to read and write the index structure and document store from/to a JSON file. The load_from_string and save_to_string functions are used to read and write the index structure and document store from/to a JSON string. The save_to_disk and save_to_string functions should not be used for indices composed on top of other indices, and instead a ComposableGraph should be used.", "doc_id": null, "embedding": null, "extra_info": null, "index": 26, "child_indices": [20, 21, 22, 23], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}