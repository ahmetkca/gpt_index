{"index_struct": {"text": "\nThis code file contains functions for extracting keywords from text chunks and GPT-generated responses. The functions use two different algorithms: simple extraction and RAKE (Rapid Automatic Keyword Extraction). The simple extraction algorithm uses regular expressions to find words in the text chunk and filters out stopwords. The RAKE algorithm uses the NLTK library to extract keywords from the text chunk. The extracted keywords are stored in a set data structure. The function for extracting keywords from GPT-generated responses parses the response into individual words and then splits them into subwords, removing stopwords. The extracted keywords are also stored in a set data structure. The purpose of this code is to provide a way to quickly and accurately extract keywords from text chunks and GPT-generated responses.", "doc_id": "ff75641c-a447-4c63-820c-1b6dc6e9cbdf", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Utils for keyword table.\"\"\"\n\nimport re\nfrom typing import Optional, Set\n\nimport nltk\nimport pandas as pd\n\nfrom gpt_index.indices.utils import expand_tokens_with_subtokens\nfrom gpt_index.utils import globals_helper\n\n\ndef simple_extract_keywords(\n    text_chunk: str, max_keywords: Optional[int] = None, filter_stopwords: bool = True\n) -> Set[str]:\n    \"\"\"Extract keywords with simple algorithm.\"\"\"\n    tokens = [t.strip().lower() for t in re.findall(r\"\\w+\", text_chunk)]\n    if filter_stopwords:\n        tokens = [t for t in tokens if t not in globals_helper.stopwords]\n    value_counts = pd.Series(tokens).value_counts()\n    keywords = value_counts.index.tolist()[:max_keywords]\n    return set(keywords)\n\n\ndef rake_extract_keywords(\n    text_chunk: str,\n    max_keywords: Optional[int] = None,\n    expand_with_subtokens: bool = True,\n) -> Set[str]:\n    \"\"\"Extract keywords", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/utils.py", "file_name": "utils.py"}, "index": 0, "child_indices": [], "ref_doc_id": "4afd893ac94d8ef62581ff3fdcf0ca7800e773ac", "node_info": null}, "1": {"text": "bool = True,\n) -> Set[str]:\n    \"\"\"Extract keywords with RAKE.\"\"\"\n    nltk.download(\"punkt\")\n    try:\n        from rake_nltk import Rake\n    except ImportError:\n        raise ImportError(\"Please install rake_nltk: `pip install rake_nltk`\")\n\n    r = Rake()\n    r.extract_keywords_from_text(text_chunk)\n    keywords = r.get_ranked_phrases()[:max_keywords]\n    if expand_with_subtokens:\n        return set(expand_tokens_with_subtokens(keywords))\n    else:\n        return set(keywords)\n\n\ndef extract_keywords_given_response(\n    response: str, lowercase: bool = True, start_token: str = \"\"\n) -> Set[str]:\n    \"\"\"Extract keywords given the GPT-generated response.\n\n    Used by keyword table indices.\n    Parses <start_token>: <word1>, <word2>, ... into [word1, word2, ...]\n    Raises exception if response doesn't start with <start_token>\n    \"\"\"\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/utils.py", "file_name": "utils.py"}, "index": 1, "child_indices": [], "ref_doc_id": "4afd893ac94d8ef62581ff3fdcf0ca7800e773ac", "node_info": null}, "2": {"text": "if response doesn't start with <start_token>\n    \"\"\"\n    results = []\n    response = response.strip()  # Strip newlines from responses.\n\n    if response.startswith(start_token):\n        response = response[len(start_token) :]\n\n    keywords = response.split(\",\")\n    for k in keywords:\n        rk = k\n        if lowercase:\n            rk = rk.lower()\n        results.append(rk.strip())\n\n    # if keyword consists of multiple words, split into subwords\n    # (removing stopwords)\n    return expand_tokens_with_subtokens(set(results))\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/utils.py", "file_name": "utils.py"}, "index": 2, "child_indices": [], "ref_doc_id": "4afd893ac94d8ef62581ff3fdcf0ca7800e773ac", "node_info": null}, "3": {"text": "This code file contains functions for extracting keywords from text chunks. The functions use different algorithms, such as simple extraction and RAKE (Rapid Automatic Keyword Extraction). The code also includes a function for extracting keywords from GPT-generated responses. This function parses the response into individual words and then splits them into subwords, removing stopwords. The extracted keywords are stored in a set data structure.", "doc_id": null, "embedding": null, "extra_info": null, "index": 3, "child_indices": [0, 1, 2], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"3": {"text": "This code file contains functions for extracting keywords from text chunks. The functions use different algorithms, such as simple extraction and RAKE (Rapid Automatic Keyword Extraction). The code also includes a function for extracting keywords from GPT-generated responses. This function parses the response into individual words and then splits them into subwords, removing stopwords. The extracted keywords are stored in a set data structure.", "doc_id": null, "embedding": null, "extra_info": null, "index": 3, "child_indices": [0, 1, 2], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"4afd893ac94d8ef62581ff3fdcf0ca7800e773ac": {"text": "\"\"\"Utils for keyword table.\"\"\"\n\nimport re\nfrom typing import Optional, Set\n\nimport nltk\nimport pandas as pd\n\nfrom gpt_index.indices.utils import expand_tokens_with_subtokens\nfrom gpt_index.utils import globals_helper\n\n\ndef simple_extract_keywords(\n    text_chunk: str, max_keywords: Optional[int] = None, filter_stopwords: bool = True\n) -> Set[str]:\n    \"\"\"Extract keywords with simple algorithm.\"\"\"\n    tokens = [t.strip().lower() for t in re.findall(r\"\\w+\", text_chunk)]\n    if filter_stopwords:\n        tokens = [t for t in tokens if t not in globals_helper.stopwords]\n    value_counts = pd.Series(tokens).value_counts()\n    keywords = value_counts.index.tolist()[:max_keywords]\n    return set(keywords)\n\n\ndef rake_extract_keywords(\n    text_chunk: str,\n    max_keywords: Optional[int] = None,\n    expand_with_subtokens: bool = True,\n) -> Set[str]:\n    \"\"\"Extract keywords with RAKE.\"\"\"\n    nltk.download(\"punkt\")\n    try:\n        from rake_nltk import Rake\n    except ImportError:\n        raise ImportError(\"Please install rake_nltk: `pip install rake_nltk`\")\n\n    r = Rake()\n    r.extract_keywords_from_text(text_chunk)\n    keywords = r.get_ranked_phrases()[:max_keywords]\n    if expand_with_subtokens:\n        return set(expand_tokens_with_subtokens(keywords))\n    else:\n        return set(keywords)\n\n\ndef extract_keywords_given_response(\n    response: str, lowercase: bool = True, start_token: str = \"\"\n) -> Set[str]:\n    \"\"\"Extract keywords given the GPT-generated response.\n\n    Used by keyword table indices.\n    Parses <start_token>: <word1>, <word2>, ... into [word1, word2, ...]\n    Raises exception if response doesn't start with <start_token>\n    \"\"\"\n    results = []\n    response = response.strip()  # Strip newlines from responses.\n\n    if response.startswith(start_token):\n        response = response[len(start_token) :]\n\n    keywords = response.split(\",\")\n    for k in keywords:\n        rk = k\n        if lowercase:\n            rk = rk.lower()\n        results.append(rk.strip())\n\n    # if keyword consists of multiple words, split into subwords\n    # (removing stopwords)\n    return expand_tokens_with_subtokens(set(results))\n", "doc_id": "4afd893ac94d8ef62581ff3fdcf0ca7800e773ac", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/utils.py", "file_name": "utils.py"}, "__type__": "Document"}, "ff75641c-a447-4c63-820c-1b6dc6e9cbdf": {"text": "\nThis code file contains functions for extracting keywords from text chunks and GPT-generated responses. The functions use two different algorithms: simple extraction and RAKE (Rapid Automatic Keyword Extraction). The simple extraction algorithm uses regular expressions to find words in the text chunk and filters out stopwords. The RAKE algorithm uses the NLTK library to extract keywords from the text chunk. The extracted keywords are stored in a set data structure. The function for extracting keywords from GPT-generated responses parses the response into individual words and then splits them into subwords, removing stopwords. The extracted keywords are also stored in a set data structure. The purpose of this code is to provide a way to quickly and accurately extract keywords from text chunks and GPT-generated responses.", "doc_id": "ff75641c-a447-4c63-820c-1b6dc6e9cbdf", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Utils for keyword table.\"\"\"\n\nimport re\nfrom typing import Optional, Set\n\nimport nltk\nimport pandas as pd\n\nfrom gpt_index.indices.utils import expand_tokens_with_subtokens\nfrom gpt_index.utils import globals_helper\n\n\ndef simple_extract_keywords(\n    text_chunk: str, max_keywords: Optional[int] = None, filter_stopwords: bool = True\n) -> Set[str]:\n    \"\"\"Extract keywords with simple algorithm.\"\"\"\n    tokens = [t.strip().lower() for t in re.findall(r\"\\w+\", text_chunk)]\n    if filter_stopwords:\n        tokens = [t for t in tokens if t not in globals_helper.stopwords]\n    value_counts = pd.Series(tokens).value_counts()\n    keywords = value_counts.index.tolist()[:max_keywords]\n    return set(keywords)\n\n\ndef rake_extract_keywords(\n    text_chunk: str,\n    max_keywords: Optional[int] = None,\n    expand_with_subtokens: bool = True,\n) -> Set[str]:\n    \"\"\"Extract keywords", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/utils.py", "file_name": "utils.py"}, "index": 0, "child_indices": [], "ref_doc_id": "4afd893ac94d8ef62581ff3fdcf0ca7800e773ac", "node_info": null}, "1": {"text": "bool = True,\n) -> Set[str]:\n    \"\"\"Extract keywords with RAKE.\"\"\"\n    nltk.download(\"punkt\")\n    try:\n        from rake_nltk import Rake\n    except ImportError:\n        raise ImportError(\"Please install rake_nltk: `pip install rake_nltk`\")\n\n    r = Rake()\n    r.extract_keywords_from_text(text_chunk)\n    keywords = r.get_ranked_phrases()[:max_keywords]\n    if expand_with_subtokens:\n        return set(expand_tokens_with_subtokens(keywords))\n    else:\n        return set(keywords)\n\n\ndef extract_keywords_given_response(\n    response: str, lowercase: bool = True, start_token: str = \"\"\n) -> Set[str]:\n    \"\"\"Extract keywords given the GPT-generated response.\n\n    Used by keyword table indices.\n    Parses <start_token>: <word1>, <word2>, ... into [word1, word2, ...]\n    Raises exception if response doesn't start with <start_token>\n    \"\"\"\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/utils.py", "file_name": "utils.py"}, "index": 1, "child_indices": [], "ref_doc_id": "4afd893ac94d8ef62581ff3fdcf0ca7800e773ac", "node_info": null}, "2": {"text": "if response doesn't start with <start_token>\n    \"\"\"\n    results = []\n    response = response.strip()  # Strip newlines from responses.\n\n    if response.startswith(start_token):\n        response = response[len(start_token) :]\n\n    keywords = response.split(\",\")\n    for k in keywords:\n        rk = k\n        if lowercase:\n            rk = rk.lower()\n        results.append(rk.strip())\n\n    # if keyword consists of multiple words, split into subwords\n    # (removing stopwords)\n    return expand_tokens_with_subtokens(set(results))\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/utils.py", "file_name": "utils.py"}, "index": 2, "child_indices": [], "ref_doc_id": "4afd893ac94d8ef62581ff3fdcf0ca7800e773ac", "node_info": null}, "3": {"text": "This code file contains functions for extracting keywords from text chunks. The functions use different algorithms, such as simple extraction and RAKE (Rapid Automatic Keyword Extraction). The code also includes a function for extracting keywords from GPT-generated responses. This function parses the response into individual words and then splits them into subwords, removing stopwords. The extracted keywords are stored in a set data structure.", "doc_id": null, "embedding": null, "extra_info": null, "index": 3, "child_indices": [0, 1, 2], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"3": {"text": "This code file contains functions for extracting keywords from text chunks. The functions use different algorithms, such as simple extraction and RAKE (Rapid Automatic Keyword Extraction). The code also includes a function for extracting keywords from GPT-generated responses. This function parses the response into individual words and then splits them into subwords, removing stopwords. The extracted keywords are stored in a set data structure.", "doc_id": null, "embedding": null, "extra_info": null, "index": 3, "child_indices": [0, 1, 2], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}