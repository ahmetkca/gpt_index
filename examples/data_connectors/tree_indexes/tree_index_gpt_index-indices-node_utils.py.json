{"index_struct": {"text": "\nnode_utils.py is a file containing functions related to the creation of nodes for the GPT-Index. The main function, get_nodes_from_document, takes a BaseDocument object, a TokenTextSplitter object, and two optional parameters, start_idx and include_extra_info. It splits the text from the document into chunks with overlaps, and creates a Node object for each chunk. The Node object contains the text, index, reference document ID, embedding, extra info, and node info. The node info contains the start and end indices of the text chunk. The purpose of this code is to create a list of Node objects from a given document, which can then be used to index the document in the GPT-Index. The code uses algorithms to split the text into chunks, and data structures such as Node objects to store the information. The relationships between the functions, classes, and variables are used to create the Node objects and store the information in a way that can be used to index the document.", "doc_id": "966f5796-b050-469d-b4f4-20228d2c2e43", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"General node utils.\"\"\"\n\n\nimport logging\nfrom typing import List\n\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.indices.utils import truncate_text\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.schema import BaseDocument\n\n\ndef get_nodes_from_document(\n    document: BaseDocument,\n    text_splitter: TokenTextSplitter,\n    start_idx: int = 0,\n    include_extra_info: bool = True,\n) -> List[Node]:\n    \"\"\"Add document to index.\"\"\"\n    text_chunks_with_overlap = text_splitter.split_text_with_overlaps(\n        document.get_text(),\n        extra_info_str=document.extra_info_str if include_extra_info else None,\n    )\n    nodes = []\n    index_counter = 0\n    for i, text_split in enumerate(text_chunks_with_overlap):\n        text_chunk = text_split.text_chunk\n        logging.debug(f\"> Adding chunk: {truncate_text(text_chunk, 50)}\")\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/node_utils.py", "file_name": "node_utils.py"}, "index": 0, "child_indices": [], "ref_doc_id": "57e0df84f5302d7788c835809895fe5ae92d7f57", "node_info": null}, "1": {"text": "50)}\")\n        index_pos_info = {\n            # NOTE: start is inclusive, end is exclusive\n            \"start\": index_counter - text_split.num_char_overlap,\n            \"end\": index_counter - text_split.num_char_overlap + len(text_chunk),\n        }\n        index_counter += len(text_chunk) + 1\n        # if embedding specified in document, pass it to the Node\n        node = Node(\n            text=text_chunk,\n            index=start_idx + i,\n            ref_doc_id=document.get_doc_id(),\n            embedding=document.embedding,\n            extra_info=document.extra_info if include_extra_info else None,\n            node_info=index_pos_info,\n        )\n        nodes.append(node)\n    return nodes\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/node_utils.py", "file_name": "node_utils.py"}, "index": 1, "child_indices": [], "ref_doc_id": "57e0df84f5302d7788c835809895fe5ae92d7f57", "node_info": null}, "2": {"text": "node_utils.py is a file containing functions related to the creation of nodes for the GPT-Index. The main function, get_nodes_from_document, takes a BaseDocument object, a TokenTextSplitter object, and two optional parameters, start_idx and include_extra_info. It splits the text from the document into chunks with overlaps, and creates a Node object for each chunk. The Node object contains the text, index, reference document ID, embedding, extra info, and node info. The node info contains the start and end indices of the text chunk. The list of Node objects is then returned.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 2, "child_indices": [0, 1], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"2": {"text": "node_utils.py is a file containing functions related to the creation of nodes for the GPT-Index. The main function, get_nodes_from_document, takes a BaseDocument object, a TokenTextSplitter object, and two optional parameters, start_idx and include_extra_info. It splits the text from the document into chunks with overlaps, and creates a Node object for each chunk. The Node object contains the text, index, reference document ID, embedding, extra info, and node info. The node info contains the start and end indices of the text chunk. The list of Node objects is then returned.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 2, "child_indices": [0, 1], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"57e0df84f5302d7788c835809895fe5ae92d7f57": {"text": "\"\"\"General node utils.\"\"\"\n\n\nimport logging\nfrom typing import List\n\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.indices.utils import truncate_text\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.schema import BaseDocument\n\n\ndef get_nodes_from_document(\n    document: BaseDocument,\n    text_splitter: TokenTextSplitter,\n    start_idx: int = 0,\n    include_extra_info: bool = True,\n) -> List[Node]:\n    \"\"\"Add document to index.\"\"\"\n    text_chunks_with_overlap = text_splitter.split_text_with_overlaps(\n        document.get_text(),\n        extra_info_str=document.extra_info_str if include_extra_info else None,\n    )\n    nodes = []\n    index_counter = 0\n    for i, text_split in enumerate(text_chunks_with_overlap):\n        text_chunk = text_split.text_chunk\n        logging.debug(f\"> Adding chunk: {truncate_text(text_chunk, 50)}\")\n        index_pos_info = {\n            # NOTE: start is inclusive, end is exclusive\n            \"start\": index_counter - text_split.num_char_overlap,\n            \"end\": index_counter - text_split.num_char_overlap + len(text_chunk),\n        }\n        index_counter += len(text_chunk) + 1\n        # if embedding specified in document, pass it to the Node\n        node = Node(\n            text=text_chunk,\n            index=start_idx + i,\n            ref_doc_id=document.get_doc_id(),\n            embedding=document.embedding,\n            extra_info=document.extra_info if include_extra_info else None,\n            node_info=index_pos_info,\n        )\n        nodes.append(node)\n    return nodes\n", "doc_id": "57e0df84f5302d7788c835809895fe5ae92d7f57", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/node_utils.py", "file_name": "node_utils.py"}, "__type__": "Document"}, "966f5796-b050-469d-b4f4-20228d2c2e43": {"text": "\nnode_utils.py is a file containing functions related to the creation of nodes for the GPT-Index. The main function, get_nodes_from_document, takes a BaseDocument object, a TokenTextSplitter object, and two optional parameters, start_idx and include_extra_info. It splits the text from the document into chunks with overlaps, and creates a Node object for each chunk. The Node object contains the text, index, reference document ID, embedding, extra info, and node info. The node info contains the start and end indices of the text chunk. The purpose of this code is to create a list of Node objects from a given document, which can then be used to index the document in the GPT-Index. The code uses algorithms to split the text into chunks, and data structures such as Node objects to store the information. The relationships between the functions, classes, and variables are used to create the Node objects and store the information in a way that can be used to index the document.", "doc_id": "966f5796-b050-469d-b4f4-20228d2c2e43", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"General node utils.\"\"\"\n\n\nimport logging\nfrom typing import List\n\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.indices.utils import truncate_text\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.schema import BaseDocument\n\n\ndef get_nodes_from_document(\n    document: BaseDocument,\n    text_splitter: TokenTextSplitter,\n    start_idx: int = 0,\n    include_extra_info: bool = True,\n) -> List[Node]:\n    \"\"\"Add document to index.\"\"\"\n    text_chunks_with_overlap = text_splitter.split_text_with_overlaps(\n        document.get_text(),\n        extra_info_str=document.extra_info_str if include_extra_info else None,\n    )\n    nodes = []\n    index_counter = 0\n    for i, text_split in enumerate(text_chunks_with_overlap):\n        text_chunk = text_split.text_chunk\n        logging.debug(f\"> Adding chunk: {truncate_text(text_chunk, 50)}\")\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/node_utils.py", "file_name": "node_utils.py"}, "index": 0, "child_indices": [], "ref_doc_id": "57e0df84f5302d7788c835809895fe5ae92d7f57", "node_info": null}, "1": {"text": "50)}\")\n        index_pos_info = {\n            # NOTE: start is inclusive, end is exclusive\n            \"start\": index_counter - text_split.num_char_overlap,\n            \"end\": index_counter - text_split.num_char_overlap + len(text_chunk),\n        }\n        index_counter += len(text_chunk) + 1\n        # if embedding specified in document, pass it to the Node\n        node = Node(\n            text=text_chunk,\n            index=start_idx + i,\n            ref_doc_id=document.get_doc_id(),\n            embedding=document.embedding,\n            extra_info=document.extra_info if include_extra_info else None,\n            node_info=index_pos_info,\n        )\n        nodes.append(node)\n    return nodes\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/node_utils.py", "file_name": "node_utils.py"}, "index": 1, "child_indices": [], "ref_doc_id": "57e0df84f5302d7788c835809895fe5ae92d7f57", "node_info": null}, "2": {"text": "node_utils.py is a file containing functions related to the creation of nodes for the GPT-Index. The main function, get_nodes_from_document, takes a BaseDocument object, a TokenTextSplitter object, and two optional parameters, start_idx and include_extra_info. It splits the text from the document into chunks with overlaps, and creates a Node object for each chunk. The Node object contains the text, index, reference document ID, embedding, extra info, and node info. The node info contains the start and end indices of the text chunk. The list of Node objects is then returned.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 2, "child_indices": [0, 1], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"2": {"text": "node_utils.py is a file containing functions related to the creation of nodes for the GPT-Index. The main function, get_nodes_from_document, takes a BaseDocument object, a TokenTextSplitter object, and two optional parameters, start_idx and include_extra_info. It splits the text from the document into chunks with overlaps, and creates a Node object for each chunk. The Node object contains the text, index, reference document ID, embedding, extra info, and node info. The node info contains the start and end indices of the text chunk. The list of Node objects is then returned.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 2, "child_indices": [0, 1], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}