{"index_struct": {"text": "\nPromptHelper is a utility class that helps with token limitations when dealing with text. It has a constructor that takes in parameters such as max_input_size, num_output, max_chunk_overlap, embedding_limit, chunk_size_limit, tokenizer, and separator. It has methods to get the chunk size given a prompt, get the biggest prompt, get a text splitter given a prompt, get text from nodes, and get numbered text from nodes. The purpose of this code is to help with token limitations when dealing with text, by providing functions to format and split text into smaller chunks. The file prompt_helper.py contains functions to help with the formatting of prompts. The format_node_list function takes a list of nodes and formats them into a numbered list. The compact_text_chunks function takes a prompt and a list of text chunks and combines them into consolidated chunks that fit the max_input_size. It then uses the get_text_splitter_given_prompt function to split the combined text into smaller chunks. This code provides a way to format and split text into smaller chunks, allowing for easier manipulation of text with token limitations.", "doc_id": "de72f308-f402-4c39-befe-c2b9617b8a4b", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"General prompt helper that can help deal with token limitations.\n\nThe helper can split text. It can also concatenate text from Node\nstructs but keeping token limitations in mind.\n\n\"\"\"\n\nfrom typing import Callable, List, Optional\n\nfrom gpt_index.constants import MAX_CHUNK_OVERLAP\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.utils import globals_helper\n\n\nclass PromptHelper:\n    \"\"\"Prompt helper.\n\n    This utility helps us fill in the prompt, split the text,\n    and fill in context information according to necessary token limitations.\n\n    Args:\n        max_input_size (int): Maximum input size for the LLM.\n        num_output (int): Number of outputs for the LLM.\n        max_chunk_overlap (int): Maximum chunk overlap for the LLM.\n        embedding_limit (Optional[int]): Maximum number of embeddings to use.\n        chunk_size_limit", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 0, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "1": {"text": "embeddings to use.\n        chunk_size_limit (Optional[int]): Maximum chunk size to use.\n        tokenizer (Optional[Callable[[str], List]]): Tokenizer to use.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        max_input_size: int,\n        num_output: int,\n        max_chunk_overlap: int,\n        embedding_limit: Optional[int] = None,\n        chunk_size_limit: Optional[int] = None,\n        tokenizer: Optional[Callable[[str], List]] = None,\n        separator: str = \" \",\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self.max_input_size = max_input_size\n        self.num_output = num_output\n        self.max_chunk_overlap = max_chunk_overlap\n        self.embedding_limit = embedding_limit\n        self.chunk_size_limit = chunk_size_limit\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 1, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "2": {"text": "= chunk_size_limit\n        # TODO: make configurable\n        self._tokenizer = tokenizer or globals_helper.tokenizer\n        self._separator = separator\n        self.use_chunk_size_limit = chunk_size_limit is not None\n\n    @classmethod\n    def from_llm_predictor(\n        self,\n        llm_predictor: LLMPredictor,\n        max_chunk_overlap: Optional[int] = None,\n        embedding_limit: Optional[int] = None,\n        chunk_size_limit: Optional[int] = None,\n        tokenizer: Optional[Callable[[str], List]] = None,\n    ) -> \"PromptHelper\":\n        \"\"\"Create from llm predictor.\n\n        This will autofill values like max_input_size and num_output.\n\n        \"\"\"\n        llm_metadata = llm_predictor.get_llm_metadata()\n        max_chunk_overlap = max_chunk_overlap or", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 2, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "3": {"text": "    max_chunk_overlap = max_chunk_overlap or min(\n            MAX_CHUNK_OVERLAP, llm_metadata.max_input_size // 10\n        )\n        return self(\n            llm_metadata.max_input_size,\n            llm_metadata.num_output,\n            max_chunk_overlap,\n            embedding_limit=embedding_limit,\n            chunk_size_limit=chunk_size_limit,\n            tokenizer=tokenizer,\n        )\n\n    def get_chunk_size_given_prompt(\n        self, prompt_text: str, num_chunks: int, padding: Optional[int] = 1\n    ) -> int:\n        \"\"\"Get chunk size making sure we can also fit the prompt in.\n\n        Chunk size is computed based on a function of the total input size,\n        the prompt length, the number of outputs, and the number of", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 3, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "4": {"text": "     the prompt length, the number of outputs, and the number of chunks.\n\n        If padding is specified, then we subtract that from the chunk size.\n        By default we assume there is a padding of 1 (for the newline between chunks).\n\n        Limit by embedding_limit and chunk_size_limit if specified.\n\n        \"\"\"\n        prompt_tokens = self._tokenizer(prompt_text)\n        num_prompt_tokens = len(prompt_tokens)\n\n        # NOTE: if embedding limit is specified, then chunk_size must not be larger than\n        # embedding_limit\n        result = (\n            self.max_input_size - num_prompt_tokens - self.num_output\n        ) // num_chunks\n        if padding is not None:\n            result -= padding\n\n        if self.embedding_limit is not None:\n            result = min(result, self.embedding_limit)\n        if", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 4, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "5": {"text": "min(result, self.embedding_limit)\n        if self.chunk_size_limit is not None and self.use_chunk_size_limit:\n            result = min(result, self.chunk_size_limit)\n\n        return result\n\n    def _get_empty_prompt_txt(self, prompt: Prompt) -> str:\n        \"\"\"Get empty prompt text.\n\n        Substitute empty strings in parts of the prompt that have\n        not yet been filled out. Skip variables that have already\n        been partially formatted. This is used to compute the initial tokens.\n\n        \"\"\"\n        fmt_dict = {\n            v: \"\" for v in prompt.input_variables if v not in prompt.partial_dict\n        }\n        empty_prompt_txt = prompt.format(**fmt_dict)\n        return empty_prompt_txt\n\n    def get_biggest_prompt(self, prompts: List[Prompt]) -> Prompt:\n        \"\"\"Get biggest prompt.\n\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 5, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "6": {"text": "       \"\"\"Get biggest prompt.\n\n        Oftentimes we need to fetch the biggest prompt, in order to\n        be the most conservative about chunking text. This\n        is a helper utility for that.\n\n        \"\"\"\n        empty_prompt_txts = [self._get_empty_prompt_txt(prompt) for prompt in prompts]\n        empty_prompt_txt_lens = [len(txt) for txt in empty_prompt_txts]\n        biggest_prompt = prompts[\n            empty_prompt_txt_lens.index(max(empty_prompt_txt_lens))\n        ]\n        return biggest_prompt\n\n    def get_text_splitter_given_prompt(\n        self, prompt: Prompt, num_chunks: int, padding: Optional[int] = 1\n    ) -> TokenTextSplitter:\n        \"\"\"Get text splitter given initial prompt.\n\n        Allows us to get the text splitter which will split up text according\n        to the desired", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 6, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "7": {"text": "splitter which will split up text according\n        to the desired chunk size.\n\n        \"\"\"\n        # generate empty_prompt_txt to compute initial tokens\n        empty_prompt_txt = self._get_empty_prompt_txt(prompt)\n        chunk_size = self.get_chunk_size_given_prompt(\n            empty_prompt_txt, num_chunks, padding=padding\n        )\n        text_splitter = TokenTextSplitter(\n            separator=self._separator,\n            chunk_size=chunk_size,\n            chunk_overlap=self.max_chunk_overlap // num_chunks,\n            tokenizer=self._tokenizer,\n        )\n        return text_splitter\n\n    def get_text_from_nodes(\n        self, node_list: List[Node], prompt: Optional[Prompt] = None\n    ) -> str:\n        \"\"\"Get text", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 7, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "8": {"text": "None\n    ) -> str:\n        \"\"\"Get text from nodes. Used by tree-structured indices.\"\"\"\n        num_nodes = len(node_list)\n        text_splitter = None\n        if prompt is not None:\n            # add padding given the newline character\n            text_splitter = self.get_text_splitter_given_prompt(\n                prompt,\n                num_nodes,\n                padding=1,\n            )\n        results = []\n        for node in node_list:\n            text = (\n                text_splitter.truncate_text(node.get_text())\n                if text_splitter is not None\n                else node.get_text()\n            )\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 8, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "9": {"text": "           )\n            results.append(text)\n\n        return \"\\n\".join(results)\n\n    def get_numbered_text_from_nodes(\n        self, node_list: List[Node], prompt: Optional[Prompt] = None\n    ) -> str:\n        \"\"\"Get text from nodes in the format of a numbered list.\n\n        Used by tree-structured indices.\n\n        \"\"\"\n        num_nodes = len(node_list)\n        text_splitter = None\n        if prompt is not None:\n            # add padding given the number, and the newlines\n            text_splitter = self.get_text_splitter_given_prompt(\n                prompt,\n                num_nodes,\n                padding=5,\n            )\n        results = []\n        number = 1\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 9, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "10": {"text": "     results = []\n        number = 1\n        for node in node_list:\n            node_text = \" \".join(node.get_text().splitlines())\n            if text_splitter is not None:\n                node_text = text_splitter.truncate_text(node_text)\n            text = f\"({number}) {node_text}\"\n            results.append(text)\n            number += 1\n        return \"\\n\\n\".join(results)\n\n    def compact_text_chunks(self, prompt: Prompt, text_chunks: List[str]) -> List[str]:\n        \"\"\"Compact text chunks.\n\n        This will combine text chunks into consolidated chunks\n        that more fully \"pack\" the prompt template given the max_input_size.\n\n        \"\"\"\n        combined_str = \"\\n\\n\".join([c.strip() for c in text_chunks if c.strip()])\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 10, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "11": {"text": "for c in text_chunks if c.strip()])\n        # resplit based on self.max_chunk_overlap\n        text_splitter = self.get_text_splitter_given_prompt(prompt, 1, padding=1)\n        return text_splitter.split_text(combined_str)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 11, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "12": {"text": "PromptHelper is a utility class that helps with token limitations when dealing with text. It can split text, concatenate text from Node structs, and fill in context information according to necessary token limitations. It has methods to get the chunk size given a prompt, get the biggest prompt, get a text splitter given a prompt, get text from nodes, and get numbered text from nodes. It also has a constructor that takes in parameters such as max_input_size, num_output, max_chunk_overlap, embedding_limit, chunk_size_limit, tokenizer, and separator.", "doc_id": null, "embedding": null, "extra_info": null, "index": 12, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "ref_doc_id": null, "node_info": null}, "13": {"text": "prompt_helper.py is a file that contains functions to help with the formatting of prompts. The first function, format_node_list, takes a list of nodes and formats them into a numbered list. The second function, compact_text_chunks, takes a prompt and a list of text chunks and combines them into consolidated chunks that fit the max_input_size. It then uses the get_text_splitter_given_prompt function to split the combined text into smaller chunks.", "doc_id": null, "embedding": null, "extra_info": null, "index": 13, "child_indices": [10, 11], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"12": {"text": "PromptHelper is a utility class that helps with token limitations when dealing with text. It can split text, concatenate text from Node structs, and fill in context information according to necessary token limitations. It has methods to get the chunk size given a prompt, get the biggest prompt, get a text splitter given a prompt, get text from nodes, and get numbered text from nodes. It also has a constructor that takes in parameters such as max_input_size, num_output, max_chunk_overlap, embedding_limit, chunk_size_limit, tokenizer, and separator.", "doc_id": null, "embedding": null, "extra_info": null, "index": 12, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "ref_doc_id": null, "node_info": null}, "13": {"text": "prompt_helper.py is a file that contains functions to help with the formatting of prompts. The first function, format_node_list, takes a list of nodes and formats them into a numbered list. The second function, compact_text_chunks, takes a prompt and a list of text chunks and combines them into consolidated chunks that fit the max_input_size. It then uses the get_text_splitter_given_prompt function to split the combined text into smaller chunks.", "doc_id": null, "embedding": null, "extra_info": null, "index": 13, "child_indices": [10, 11], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01": {"text": "\"\"\"General prompt helper that can help deal with token limitations.\n\nThe helper can split text. It can also concatenate text from Node\nstructs but keeping token limitations in mind.\n\n\"\"\"\n\nfrom typing import Callable, List, Optional\n\nfrom gpt_index.constants import MAX_CHUNK_OVERLAP\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.utils import globals_helper\n\n\nclass PromptHelper:\n    \"\"\"Prompt helper.\n\n    This utility helps us fill in the prompt, split the text,\n    and fill in context information according to necessary token limitations.\n\n    Args:\n        max_input_size (int): Maximum input size for the LLM.\n        num_output (int): Number of outputs for the LLM.\n        max_chunk_overlap (int): Maximum chunk overlap for the LLM.\n        embedding_limit (Optional[int]): Maximum number of embeddings to use.\n        chunk_size_limit (Optional[int]): Maximum chunk size to use.\n        tokenizer (Optional[Callable[[str], List]]): Tokenizer to use.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        max_input_size: int,\n        num_output: int,\n        max_chunk_overlap: int,\n        embedding_limit: Optional[int] = None,\n        chunk_size_limit: Optional[int] = None,\n        tokenizer: Optional[Callable[[str], List]] = None,\n        separator: str = \" \",\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self.max_input_size = max_input_size\n        self.num_output = num_output\n        self.max_chunk_overlap = max_chunk_overlap\n        self.embedding_limit = embedding_limit\n        self.chunk_size_limit = chunk_size_limit\n        # TODO: make configurable\n        self._tokenizer = tokenizer or globals_helper.tokenizer\n        self._separator = separator\n        self.use_chunk_size_limit = chunk_size_limit is not None\n\n    @classmethod\n    def from_llm_predictor(\n        self,\n        llm_predictor: LLMPredictor,\n        max_chunk_overlap: Optional[int] = None,\n        embedding_limit: Optional[int] = None,\n        chunk_size_limit: Optional[int] = None,\n        tokenizer: Optional[Callable[[str], List]] = None,\n    ) -> \"PromptHelper\":\n        \"\"\"Create from llm predictor.\n\n        This will autofill values like max_input_size and num_output.\n\n        \"\"\"\n        llm_metadata = llm_predictor.get_llm_metadata()\n        max_chunk_overlap = max_chunk_overlap or min(\n            MAX_CHUNK_OVERLAP, llm_metadata.max_input_size // 10\n        )\n        return self(\n            llm_metadata.max_input_size,\n            llm_metadata.num_output,\n            max_chunk_overlap,\n            embedding_limit=embedding_limit,\n            chunk_size_limit=chunk_size_limit,\n            tokenizer=tokenizer,\n        )\n\n    def get_chunk_size_given_prompt(\n        self, prompt_text: str, num_chunks: int, padding: Optional[int] = 1\n    ) -> int:\n        \"\"\"Get chunk size making sure we can also fit the prompt in.\n\n        Chunk size is computed based on a function of the total input size,\n        the prompt length, the number of outputs, and the number of chunks.\n\n        If padding is specified, then we subtract that from the chunk size.\n        By default we assume there is a padding of 1 (for the newline between chunks).\n\n        Limit by embedding_limit and chunk_size_limit if specified.\n\n        \"\"\"\n        prompt_tokens = self._tokenizer(prompt_text)\n        num_prompt_tokens = len(prompt_tokens)\n\n        # NOTE: if embedding limit is specified, then chunk_size must not be larger than\n        # embedding_limit\n        result = (\n            self.max_input_size - num_prompt_tokens - self.num_output\n        ) // num_chunks\n        if padding is not None:\n            result -= padding\n\n        if self.embedding_limit is not None:\n            result = min(result, self.embedding_limit)\n        if self.chunk_size_limit is not None and self.use_chunk_size_limit:\n            result = min(result, self.chunk_size_limit)\n\n        return result\n\n    def _get_empty_prompt_txt(self, prompt: Prompt) -> str:\n        \"\"\"Get empty prompt text.\n\n        Substitute empty strings in parts of the prompt that have\n        not yet been filled out. Skip variables that have already\n        been partially formatted. This is used to compute the initial tokens.\n\n        \"\"\"\n        fmt_dict = {\n            v: \"\" for v in prompt.input_variables if v not in prompt.partial_dict\n        }\n        empty_prompt_txt = prompt.format(**fmt_dict)\n        return empty_prompt_txt\n\n    def get_biggest_prompt(self, prompts: List[Prompt]) -> Prompt:\n        \"\"\"Get biggest prompt.\n\n        Oftentimes we need to fetch the biggest prompt, in order to\n        be the most conservative about chunking text. This\n        is a helper utility for that.\n\n        \"\"\"\n        empty_prompt_txts = [self._get_empty_prompt_txt(prompt) for prompt in prompts]\n        empty_prompt_txt_lens = [len(txt) for txt in empty_prompt_txts]\n        biggest_prompt = prompts[\n            empty_prompt_txt_lens.index(max(empty_prompt_txt_lens))\n        ]\n        return biggest_prompt\n\n    def get_text_splitter_given_prompt(\n        self, prompt: Prompt, num_chunks: int, padding: Optional[int] = 1\n    ) -> TokenTextSplitter:\n        \"\"\"Get text splitter given initial prompt.\n\n        Allows us to get the text splitter which will split up text according\n        to the desired chunk size.\n\n        \"\"\"\n        # generate empty_prompt_txt to compute initial tokens\n        empty_prompt_txt = self._get_empty_prompt_txt(prompt)\n        chunk_size = self.get_chunk_size_given_prompt(\n            empty_prompt_txt, num_chunks, padding=padding\n        )\n        text_splitter = TokenTextSplitter(\n            separator=self._separator,\n            chunk_size=chunk_size,\n            chunk_overlap=self.max_chunk_overlap // num_chunks,\n            tokenizer=self._tokenizer,\n        )\n        return text_splitter\n\n    def get_text_from_nodes(\n        self, node_list: List[Node], prompt: Optional[Prompt] = None\n    ) -> str:\n        \"\"\"Get text from nodes. Used by tree-structured indices.\"\"\"\n        num_nodes = len(node_list)\n        text_splitter = None\n        if prompt is not None:\n            # add padding given the newline character\n            text_splitter = self.get_text_splitter_given_prompt(\n                prompt,\n                num_nodes,\n                padding=1,\n            )\n        results = []\n        for node in node_list:\n            text = (\n                text_splitter.truncate_text(node.get_text())\n                if text_splitter is not None\n                else node.get_text()\n            )\n            results.append(text)\n\n        return \"\\n\".join(results)\n\n    def get_numbered_text_from_nodes(\n        self, node_list: List[Node], prompt: Optional[Prompt] = None\n    ) -> str:\n        \"\"\"Get text from nodes in the format of a numbered list.\n\n        Used by tree-structured indices.\n\n        \"\"\"\n        num_nodes = len(node_list)\n        text_splitter = None\n        if prompt is not None:\n            # add padding given the number, and the newlines\n            text_splitter = self.get_text_splitter_given_prompt(\n                prompt,\n                num_nodes,\n                padding=5,\n            )\n        results = []\n        number = 1\n        for node in node_list:\n            node_text = \" \".join(node.get_text().splitlines())\n            if text_splitter is not None:\n                node_text = text_splitter.truncate_text(node_text)\n            text = f\"({number}) {node_text}\"\n            results.append(text)\n            number += 1\n        return \"\\n\\n\".join(results)\n\n    def compact_text_chunks(self, prompt: Prompt, text_chunks: List[str]) -> List[str]:\n        \"\"\"Compact text chunks.\n\n        This will combine text chunks into consolidated chunks\n        that more fully \"pack\" the prompt template given the max_input_size.\n\n        \"\"\"\n        combined_str = \"\\n\\n\".join([c.strip() for c in text_chunks if c.strip()])\n        # resplit based on self.max_chunk_overlap\n        text_splitter = self.get_text_splitter_given_prompt(prompt, 1, padding=1)\n        return text_splitter.split_text(combined_str)\n", "doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "__type__": "Document"}, "de72f308-f402-4c39-befe-c2b9617b8a4b": {"text": "\nPromptHelper is a utility class that helps with token limitations when dealing with text. It has a constructor that takes in parameters such as max_input_size, num_output, max_chunk_overlap, embedding_limit, chunk_size_limit, tokenizer, and separator. It has methods to get the chunk size given a prompt, get the biggest prompt, get a text splitter given a prompt, get text from nodes, and get numbered text from nodes. The purpose of this code is to help with token limitations when dealing with text, by providing functions to format and split text into smaller chunks. The file prompt_helper.py contains functions to help with the formatting of prompts. The format_node_list function takes a list of nodes and formats them into a numbered list. The compact_text_chunks function takes a prompt and a list of text chunks and combines them into consolidated chunks that fit the max_input_size. It then uses the get_text_splitter_given_prompt function to split the combined text into smaller chunks. This code provides a way to format and split text into smaller chunks, allowing for easier manipulation of text with token limitations.", "doc_id": "de72f308-f402-4c39-befe-c2b9617b8a4b", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"General prompt helper that can help deal with token limitations.\n\nThe helper can split text. It can also concatenate text from Node\nstructs but keeping token limitations in mind.\n\n\"\"\"\n\nfrom typing import Callable, List, Optional\n\nfrom gpt_index.constants import MAX_CHUNK_OVERLAP\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.utils import globals_helper\n\n\nclass PromptHelper:\n    \"\"\"Prompt helper.\n\n    This utility helps us fill in the prompt, split the text,\n    and fill in context information according to necessary token limitations.\n\n    Args:\n        max_input_size (int): Maximum input size for the LLM.\n        num_output (int): Number of outputs for the LLM.\n        max_chunk_overlap (int): Maximum chunk overlap for the LLM.\n        embedding_limit (Optional[int]): Maximum number of embeddings to use.\n        chunk_size_limit", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 0, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "1": {"text": "embeddings to use.\n        chunk_size_limit (Optional[int]): Maximum chunk size to use.\n        tokenizer (Optional[Callable[[str], List]]): Tokenizer to use.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        max_input_size: int,\n        num_output: int,\n        max_chunk_overlap: int,\n        embedding_limit: Optional[int] = None,\n        chunk_size_limit: Optional[int] = None,\n        tokenizer: Optional[Callable[[str], List]] = None,\n        separator: str = \" \",\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self.max_input_size = max_input_size\n        self.num_output = num_output\n        self.max_chunk_overlap = max_chunk_overlap\n        self.embedding_limit = embedding_limit\n        self.chunk_size_limit = chunk_size_limit\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 1, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "2": {"text": "= chunk_size_limit\n        # TODO: make configurable\n        self._tokenizer = tokenizer or globals_helper.tokenizer\n        self._separator = separator\n        self.use_chunk_size_limit = chunk_size_limit is not None\n\n    @classmethod\n    def from_llm_predictor(\n        self,\n        llm_predictor: LLMPredictor,\n        max_chunk_overlap: Optional[int] = None,\n        embedding_limit: Optional[int] = None,\n        chunk_size_limit: Optional[int] = None,\n        tokenizer: Optional[Callable[[str], List]] = None,\n    ) -> \"PromptHelper\":\n        \"\"\"Create from llm predictor.\n\n        This will autofill values like max_input_size and num_output.\n\n        \"\"\"\n        llm_metadata = llm_predictor.get_llm_metadata()\n        max_chunk_overlap = max_chunk_overlap or", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 2, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "3": {"text": "    max_chunk_overlap = max_chunk_overlap or min(\n            MAX_CHUNK_OVERLAP, llm_metadata.max_input_size // 10\n        )\n        return self(\n            llm_metadata.max_input_size,\n            llm_metadata.num_output,\n            max_chunk_overlap,\n            embedding_limit=embedding_limit,\n            chunk_size_limit=chunk_size_limit,\n            tokenizer=tokenizer,\n        )\n\n    def get_chunk_size_given_prompt(\n        self, prompt_text: str, num_chunks: int, padding: Optional[int] = 1\n    ) -> int:\n        \"\"\"Get chunk size making sure we can also fit the prompt in.\n\n        Chunk size is computed based on a function of the total input size,\n        the prompt length, the number of outputs, and the number of", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 3, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "4": {"text": "     the prompt length, the number of outputs, and the number of chunks.\n\n        If padding is specified, then we subtract that from the chunk size.\n        By default we assume there is a padding of 1 (for the newline between chunks).\n\n        Limit by embedding_limit and chunk_size_limit if specified.\n\n        \"\"\"\n        prompt_tokens = self._tokenizer(prompt_text)\n        num_prompt_tokens = len(prompt_tokens)\n\n        # NOTE: if embedding limit is specified, then chunk_size must not be larger than\n        # embedding_limit\n        result = (\n            self.max_input_size - num_prompt_tokens - self.num_output\n        ) // num_chunks\n        if padding is not None:\n            result -= padding\n\n        if self.embedding_limit is not None:\n            result = min(result, self.embedding_limit)\n        if", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 4, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "5": {"text": "min(result, self.embedding_limit)\n        if self.chunk_size_limit is not None and self.use_chunk_size_limit:\n            result = min(result, self.chunk_size_limit)\n\n        return result\n\n    def _get_empty_prompt_txt(self, prompt: Prompt) -> str:\n        \"\"\"Get empty prompt text.\n\n        Substitute empty strings in parts of the prompt that have\n        not yet been filled out. Skip variables that have already\n        been partially formatted. This is used to compute the initial tokens.\n\n        \"\"\"\n        fmt_dict = {\n            v: \"\" for v in prompt.input_variables if v not in prompt.partial_dict\n        }\n        empty_prompt_txt = prompt.format(**fmt_dict)\n        return empty_prompt_txt\n\n    def get_biggest_prompt(self, prompts: List[Prompt]) -> Prompt:\n        \"\"\"Get biggest prompt.\n\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 5, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "6": {"text": "       \"\"\"Get biggest prompt.\n\n        Oftentimes we need to fetch the biggest prompt, in order to\n        be the most conservative about chunking text. This\n        is a helper utility for that.\n\n        \"\"\"\n        empty_prompt_txts = [self._get_empty_prompt_txt(prompt) for prompt in prompts]\n        empty_prompt_txt_lens = [len(txt) for txt in empty_prompt_txts]\n        biggest_prompt = prompts[\n            empty_prompt_txt_lens.index(max(empty_prompt_txt_lens))\n        ]\n        return biggest_prompt\n\n    def get_text_splitter_given_prompt(\n        self, prompt: Prompt, num_chunks: int, padding: Optional[int] = 1\n    ) -> TokenTextSplitter:\n        \"\"\"Get text splitter given initial prompt.\n\n        Allows us to get the text splitter which will split up text according\n        to the desired", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 6, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "7": {"text": "splitter which will split up text according\n        to the desired chunk size.\n\n        \"\"\"\n        # generate empty_prompt_txt to compute initial tokens\n        empty_prompt_txt = self._get_empty_prompt_txt(prompt)\n        chunk_size = self.get_chunk_size_given_prompt(\n            empty_prompt_txt, num_chunks, padding=padding\n        )\n        text_splitter = TokenTextSplitter(\n            separator=self._separator,\n            chunk_size=chunk_size,\n            chunk_overlap=self.max_chunk_overlap // num_chunks,\n            tokenizer=self._tokenizer,\n        )\n        return text_splitter\n\n    def get_text_from_nodes(\n        self, node_list: List[Node], prompt: Optional[Prompt] = None\n    ) -> str:\n        \"\"\"Get text", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 7, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "8": {"text": "None\n    ) -> str:\n        \"\"\"Get text from nodes. Used by tree-structured indices.\"\"\"\n        num_nodes = len(node_list)\n        text_splitter = None\n        if prompt is not None:\n            # add padding given the newline character\n            text_splitter = self.get_text_splitter_given_prompt(\n                prompt,\n                num_nodes,\n                padding=1,\n            )\n        results = []\n        for node in node_list:\n            text = (\n                text_splitter.truncate_text(node.get_text())\n                if text_splitter is not None\n                else node.get_text()\n            )\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 8, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "9": {"text": "           )\n            results.append(text)\n\n        return \"\\n\".join(results)\n\n    def get_numbered_text_from_nodes(\n        self, node_list: List[Node], prompt: Optional[Prompt] = None\n    ) -> str:\n        \"\"\"Get text from nodes in the format of a numbered list.\n\n        Used by tree-structured indices.\n\n        \"\"\"\n        num_nodes = len(node_list)\n        text_splitter = None\n        if prompt is not None:\n            # add padding given the number, and the newlines\n            text_splitter = self.get_text_splitter_given_prompt(\n                prompt,\n                num_nodes,\n                padding=5,\n            )\n        results = []\n        number = 1\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 9, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "10": {"text": "     results = []\n        number = 1\n        for node in node_list:\n            node_text = \" \".join(node.get_text().splitlines())\n            if text_splitter is not None:\n                node_text = text_splitter.truncate_text(node_text)\n            text = f\"({number}) {node_text}\"\n            results.append(text)\n            number += 1\n        return \"\\n\\n\".join(results)\n\n    def compact_text_chunks(self, prompt: Prompt, text_chunks: List[str]) -> List[str]:\n        \"\"\"Compact text chunks.\n\n        This will combine text chunks into consolidated chunks\n        that more fully \"pack\" the prompt template given the max_input_size.\n\n        \"\"\"\n        combined_str = \"\\n\\n\".join([c.strip() for c in text_chunks if c.strip()])\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 10, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "11": {"text": "for c in text_chunks if c.strip()])\n        # resplit based on self.max_chunk_overlap\n        text_splitter = self.get_text_splitter_given_prompt(prompt, 1, padding=1)\n        return text_splitter.split_text(combined_str)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "index": 11, "child_indices": [], "ref_doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "node_info": null}, "12": {"text": "PromptHelper is a utility class that helps with token limitations when dealing with text. It can split text, concatenate text from Node structs, and fill in context information according to necessary token limitations. It has methods to get the chunk size given a prompt, get the biggest prompt, get a text splitter given a prompt, get text from nodes, and get numbered text from nodes. It also has a constructor that takes in parameters such as max_input_size, num_output, max_chunk_overlap, embedding_limit, chunk_size_limit, tokenizer, and separator.", "doc_id": null, "embedding": null, "extra_info": null, "index": 12, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "ref_doc_id": null, "node_info": null}, "13": {"text": "prompt_helper.py is a file that contains functions to help with the formatting of prompts. The first function, format_node_list, takes a list of nodes and formats them into a numbered list. The second function, compact_text_chunks, takes a prompt and a list of text chunks and combines them into consolidated chunks that fit the max_input_size. It then uses the get_text_splitter_given_prompt function to split the combined text into smaller chunks.", "doc_id": null, "embedding": null, "extra_info": null, "index": 13, "child_indices": [10, 11], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"12": {"text": "PromptHelper is a utility class that helps with token limitations when dealing with text. It can split text, concatenate text from Node structs, and fill in context information according to necessary token limitations. It has methods to get the chunk size given a prompt, get the biggest prompt, get a text splitter given a prompt, get text from nodes, and get numbered text from nodes. It also has a constructor that takes in parameters such as max_input_size, num_output, max_chunk_overlap, embedding_limit, chunk_size_limit, tokenizer, and separator.", "doc_id": null, "embedding": null, "extra_info": null, "index": 12, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "ref_doc_id": null, "node_info": null}, "13": {"text": "prompt_helper.py is a file that contains functions to help with the formatting of prompts. The first function, format_node_list, takes a list of nodes and formats them into a numbered list. The second function, compact_text_chunks, takes a prompt and a list of text chunks and combines them into consolidated chunks that fit the max_input_size. It then uses the get_text_splitter_given_prompt function to split the combined text into smaller chunks.", "doc_id": null, "embedding": null, "extra_info": null, "index": 13, "child_indices": [10, 11], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}