{"index_struct": {"text": "\nThe BaseGPTIndexQuery is a helper class used to query an index. It takes in parameters such as an IndexStruct, LLMPredictor, PromptHelper, Embedding Model, DocumentStore, QueryRunner, required and excluded keywords, ResponseMode, QuestionAnswerPrompt, RefinePrompt, and a similarity cutoff. It uses these parameters to filter out nodes that do not meet the criteria, and then uses the ResponseBuilder to generate a response. The code also uses a SimilarityTracker to track the distance from the query to the node, and an LLM token counter to track the query. The code contains functions that get nodes for a response, query a response, and get text from a node. It also contains an abstract method for getting nodes for a response. The code can also include a summary text if the include_summary flag is set to True. The purpose of the code is to provide a base for querying an index and generate a response based on the query string and nodes. It uses algorithms such as the LLM token counter and SimilarityTracker to track the query and the distance from the query to the node, and data structures such as the IndexStruct, PromptHelper, and DocumentStore to store and query the data.", "doc_id": "1d0e1c04-fa50-4c32-af9d-aefe933da9fe", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Base query classes.\"\"\"\n\nimport logging\nimport re\nfrom abc import abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Dict, Generic, List, Optional, Tuple, TypeVar, cast\n\nfrom gpt_index.data_structs.data_structs import IndexStruct, Node\nfrom gpt_index.docstore import DocumentStore\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.embeddings.openai import OpenAIEmbedding\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.response.builder import ResponseBuilder, ResponseMode, TextChunk\nfrom gpt_index.indices.utils import truncate_text\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.default_prompts import (\n    DEFAULT_REFINE_PROMPT,\n    DEFAULT_TEXT_QA_PROMPT,\n)\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\nfrom gpt_index.response.schema import Response\nfrom gpt_index.token_counter.token_counter import llm_token_counter\n\nIS = TypeVar(\"IS\",", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 0, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "1": {"text": "import llm_token_counter\n\nIS = TypeVar(\"IS\", bound=IndexStruct)\n\n\n@dataclass\nclass BaseQueryRunner:\n    \"\"\"Base query runner.\"\"\"\n\n    @abstractmethod\n    def query(self, query: str, index_struct: IndexStruct) -> Response:\n        \"\"\"Schedule a query.\"\"\"\n        raise NotImplementedError(\"Not implemented yet.\")\n\n\nclass BaseGPTIndexQuery(Generic[IS]):\n    \"\"\"Base GPT Index Query.\n\n    Helper class that is used to query an index. Can be called within `query`\n    method of a BaseGPTIndex object, or instantiated independently.\n\n    Args:\n        llm_predictor (LLMPredictor): Optional LLMPredictor object. If not provided,\n            will use the default LLMPredictor (text-davinci-003)\n        prompt_helper (PromptHelper): Optional PromptHelper object. If not provided,\n            will use the default PromptHelper.\n        required_keywords (List[str]): Optional list of keywords that must be present\n            in nodes. Can be used to query most indices (tree index is an", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "2": {"text": "   in nodes. Can be used to query most indices (tree index is an exception).\n        exclude_keywords (List[str]): Optional list of keywords that must not be\n            present in nodes. Can be used to query most indices (tree index is an\n            exception).\n        response_mode (ResponseMode): Optional ResponseMode. If not provided, will\n            use the default ResponseMode.\n        text_qa_template (QuestionAnswerPrompt): Optional QuestionAnswerPrompt object.\n            If not provided, will use the default QuestionAnswerPrompt.\n        refine_template (RefinePrompt): Optional RefinePrompt object. If not provided,\n            will use the default RefinePrompt.\n        include_summary (bool): Optional bool. If True, will also use the summary\n            text of the index when generating a response (the summary text can be set\n            through `index.set_text(\"<text>\")`).\n        similarity_cutoff (float): Optional float. If set, will filter out nodes with\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "3": {"text": "(float): Optional float. If set, will filter out nodes with\n            similarity below this cutoff threshold when computing the response\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IS,\n        # TODO: pass from superclass\n        llm_predictor: Optional[LLMPredictor] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        docstore: Optional[DocumentStore] = None,\n        query_runner: Optional[BaseQueryRunner] = None,\n        required_keywords: Optional[List[str]] = None,\n        exclude_keywords: Optional[List[str]] = None,\n        response_mode: ResponseMode = ResponseMode.DEFAULT,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        refine_template: Optional[RefinePrompt] = None,\n        include_summary: bool = False,\n        response_kwargs:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "4": {"text": "bool = False,\n        response_kwargs: Optional[Dict] = None,\n        similarity_cutoff: Optional[float] = None,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        if index_struct is None:\n            raise ValueError(\"index_struct must be provided.\")\n        self._validate_index_struct(index_struct)\n        self._index_struct = index_struct\n        self._llm_predictor = llm_predictor or LLMPredictor()\n        # NOTE: the embed_model isn't used in all indices\n        self._embed_model = embed_model or OpenAIEmbedding()\n        self._docstore = docstore\n        self._query_runner = query_runner\n        # TODO: make this a required param\n        if prompt_helper is None:\n            raise ValueError(\"prompt_helper must be provided.\")\n        self._prompt_helper = cast(PromptHelper, prompt_helper)\n\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 4, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "5": {"text": "= cast(PromptHelper, prompt_helper)\n\n        self._required_keywords = required_keywords\n        self._exclude_keywords = exclude_keywords\n        self._response_mode = ResponseMode(response_mode)\n\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        self.refine_template = refine_template or DEFAULT_REFINE_PROMPT\n        self._include_summary = include_summary\n\n        self._response_kwargs = response_kwargs or {}\n        self.response_builder = ResponseBuilder(\n            self._prompt_helper,\n            self._llm_predictor,\n            self.text_qa_template,\n            self.refine_template,\n        )\n\n        self.similarity_cutoff = similarity_cutoff\n\n    def _should_use_node(\n        self, node: Node, similarity_tracker: Optional[SimilarityTracker] = None\n    ) ->", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 5, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "6": {"text": "similarity_tracker: Optional[SimilarityTracker] = None\n    ) -> bool:\n        \"\"\"Run node through filters to determine if it should be used.\"\"\"\n        words = re.findall(r\"\\w+\", node.get_text())\n        if self._required_keywords is not None:\n            for w in self._required_keywords:\n                if w not in words:\n                    return False\n\n        if self._exclude_keywords is not None:\n            for w in self._exclude_keywords:\n                if w in words:\n                    return False\n\n        sim_cutoff_exists = (\n            similarity_tracker is not None and self.similarity_cutoff is not None\n        )\n\n        if sim_cutoff_exists:\n            similarity = cast(SimilarityTracker,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 6, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "7": {"text": "           similarity = cast(SimilarityTracker, similarity_tracker).find(node)\n            if similarity is None:\n                return False\n            if cast(float, similarity) < cast(float, self.similarity_cutoff):\n                return False\n\n        return True\n\n    def _get_text_from_node(\n        self,\n        query_str: str,\n        node: Node,\n        level: Optional[int] = None,\n    ) -> Tuple[TextChunk, Optional[Response]]:\n        \"\"\"Query a given node.\n\n        If node references a given document, then return the document.\n        If node references a given index, then query the index.\n\n        \"\"\"\n        level_str = \"\" if level is None else f\"[Level {level}]\"\n        fmt_text_chunk = truncate_text(node.get_text(), 50)\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 7, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "8": {"text": "50)\n        logging.debug(f\">{level_str} Searching in chunk: {fmt_text_chunk}\")\n\n        is_index_struct = False\n        # if self._query_runner is not None, assume we want to do a recursive\n        # query. In order to not perform a recursive query, make sure\n        # _query_runner is None.\n        if (\n            self._query_runner is not None\n            and node.ref_doc_id is not None\n            and self._docstore is not None\n        ):\n            doc = self._docstore.get_document(node.ref_doc_id, raise_error=True)\n            if isinstance(doc, IndexStruct):\n                is_index_struct = True\n\n        if is_index_struct:\n            query_runner = cast(BaseQueryRunner, self._query_runner)\n            response =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 8, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "9": {"text": "self._query_runner)\n            response = query_runner.query(query_str, cast(IndexStruct, doc))\n            return TextChunk(str(response), is_answer=True), response\n        else:\n            text = node.get_text()\n            return TextChunk(text), None\n\n    @property\n    def index_struct(self) -> IS:\n        \"\"\"Get the index struct.\"\"\"\n        return self._index_struct\n\n    def _validate_index_struct(self, index_struct: IS) -> None:\n        \"\"\"Validate the index struct.\"\"\"\n        pass\n\n    def _give_response_for_nodes(\n        self, query_str: str, text_chunks: List[TextChunk]\n    ) -> str:\n        \"\"\"Give response for nodes.\"\"\"\n        self.response_builder.reset()\n        for text in text_chunks:\n            self.response_builder.add_text_chunks([text])\n        response =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 9, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "10": {"text": "       response = self.response_builder.get_response(\n            query_str,\n            mode=self._response_mode,\n            **self._response_kwargs,\n        )\n\n        return response or \"\"\n\n    def get_nodes_and_similarities_for_response(\n        self, query_str: str\n    ) -> List[Tuple[Node, Optional[float]]]:\n        \"\"\"Get list of tuples of node and similarity for response.\n\n        First part of the tuple is the node.\n        Second part of tuple is the distance from query to the node.\n        If not applicable, it's None.\n        \"\"\"\n        similarity_tracker = SimilarityTracker()\n        nodes = self._get_nodes_for_response(\n            query_str, similarity_tracker=similarity_tracker\n        )\n        nodes = [\n            node for node in nodes if", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 10, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "11": {"text": "[\n            node for node in nodes if self._should_use_node(node, similarity_tracker)\n        ]\n\n        # TODO: create a `display` method to allow subclasses to print the Node\n        return similarity_tracker.get_zipped_nodes(nodes)\n\n    @abstractmethod\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n\n    def _query(self, query_str: str) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        # TODO: remove _query and just use query\n        tuples = self.get_nodes_and_similarities_for_response(query_str)\n        node_texts = []\n        for node, similarity in tuples:\n            text, response = self._get_text_from_node(query_str, node)\n        ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 11, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "12": {"text": "node)\n            self.response_builder.add_node(node, similarity=similarity)\n            if response is not None:\n                # these are source nodes from within this node (when it's an index)\n                for source_node in response.source_nodes:\n                    self.response_builder.add_source_node(source_node)\n            node_texts.append(text)\n\n        if self._response_mode != ResponseMode.NO_TEXT:\n            response_str = self._give_response_for_nodes(query_str, node_texts)\n        else:\n            response_str = None\n\n        return Response(response_str, source_nodes=self.response_builder.get_sources())\n\n    @llm_token_counter(\"query\")\n    def query(self, query_str: str) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        response = self._query(query_str)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 12, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "13": {"text": "       response = self._query(query_str)\n        # if include_summary is True, then include summary text in answer\n        # summary text is set through `set_text` on the underlying index.\n        # TODO: refactor response builder to be in the __init__\n        if self._response_mode != ResponseMode.NO_TEXT and self._include_summary:\n            response_builder = ResponseBuilder(\n                self._prompt_helper,\n                self._llm_predictor,\n                self.text_qa_template,\n                self.refine_template,\n                texts=[TextChunk(self._index_struct.get_text())],\n            )\n            # NOTE: use create and refine for now (default response mode)\n            response.response = response_builder.get_response(\n               ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 13, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "14": {"text": "               query_str,\n                mode=self._response_mode,\n                prev_response=response.response,\n            )\n\n        return response\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 14, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "15": {"text": "BaseGPTIndexQuery is a helper class used to query an index. It can be called within the query method of a BaseGPTIndex object, or instantiated independently. It takes in parameters such as an IndexStruct, LLMPredictor, PromptHelper, Embedding Model, DocumentStore, QueryRunner, required and excluded keywords, ResponseMode, QuestionAnswerPrompt, RefinePrompt, and a similarity cutoff. It uses these parameters to filter out nodes that do not meet the criteria, and then uses the ResponseBuilder to generate a response.", "doc_id": null, "embedding": null, "extra_info": null, "index": 15, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "ref_doc_id": null, "node_info": null}, "16": {"text": "This code file is part of a GPT index and is responsible for answering queries. It contains functions that get nodes for a response, query a response, and get text from a node. It also contains an abstract method for getting nodes for a response. The code uses a response builder to create a response from the query string and nodes, and it can also include a summary text if the include_summary flag is set to True. The code also uses a SimilarityTracker to track the distance from the query to the node. Finally, the code uses an LLM token counter to track the query.", "doc_id": null, "embedding": null, "extra_info": null, "index": 16, "child_indices": [10, 11, 12, 13, 14], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"15": {"text": "BaseGPTIndexQuery is a helper class used to query an index. It can be called within the query method of a BaseGPTIndex object, or instantiated independently. It takes in parameters such as an IndexStruct, LLMPredictor, PromptHelper, Embedding Model, DocumentStore, QueryRunner, required and excluded keywords, ResponseMode, QuestionAnswerPrompt, RefinePrompt, and a similarity cutoff. It uses these parameters to filter out nodes that do not meet the criteria, and then uses the ResponseBuilder to generate a response.", "doc_id": null, "embedding": null, "extra_info": null, "index": 15, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "ref_doc_id": null, "node_info": null}, "16": {"text": "This code file is part of a GPT index and is responsible for answering queries. It contains functions that get nodes for a response, query a response, and get text from a node. It also contains an abstract method for getting nodes for a response. The code uses a response builder to create a response from the query string and nodes, and it can also include a summary text if the include_summary flag is set to True. The code also uses a SimilarityTracker to track the distance from the query to the node. Finally, the code uses an LLM token counter to track the query.", "doc_id": null, "embedding": null, "extra_info": null, "index": 16, "child_indices": [10, 11, 12, 13, 14], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"642b4f3bbe0ec820cf0734cec152c1766a7d0725": {"text": "\"\"\"Base query classes.\"\"\"\n\nimport logging\nimport re\nfrom abc import abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Dict, Generic, List, Optional, Tuple, TypeVar, cast\n\nfrom gpt_index.data_structs.data_structs import IndexStruct, Node\nfrom gpt_index.docstore import DocumentStore\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.embeddings.openai import OpenAIEmbedding\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.response.builder import ResponseBuilder, ResponseMode, TextChunk\nfrom gpt_index.indices.utils import truncate_text\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.default_prompts import (\n    DEFAULT_REFINE_PROMPT,\n    DEFAULT_TEXT_QA_PROMPT,\n)\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\nfrom gpt_index.response.schema import Response\nfrom gpt_index.token_counter.token_counter import llm_token_counter\n\nIS = TypeVar(\"IS\", bound=IndexStruct)\n\n\n@dataclass\nclass BaseQueryRunner:\n    \"\"\"Base query runner.\"\"\"\n\n    @abstractmethod\n    def query(self, query: str, index_struct: IndexStruct) -> Response:\n        \"\"\"Schedule a query.\"\"\"\n        raise NotImplementedError(\"Not implemented yet.\")\n\n\nclass BaseGPTIndexQuery(Generic[IS]):\n    \"\"\"Base GPT Index Query.\n\n    Helper class that is used to query an index. Can be called within `query`\n    method of a BaseGPTIndex object, or instantiated independently.\n\n    Args:\n        llm_predictor (LLMPredictor): Optional LLMPredictor object. If not provided,\n            will use the default LLMPredictor (text-davinci-003)\n        prompt_helper (PromptHelper): Optional PromptHelper object. If not provided,\n            will use the default PromptHelper.\n        required_keywords (List[str]): Optional list of keywords that must be present\n            in nodes. Can be used to query most indices (tree index is an exception).\n        exclude_keywords (List[str]): Optional list of keywords that must not be\n            present in nodes. Can be used to query most indices (tree index is an\n            exception).\n        response_mode (ResponseMode): Optional ResponseMode. If not provided, will\n            use the default ResponseMode.\n        text_qa_template (QuestionAnswerPrompt): Optional QuestionAnswerPrompt object.\n            If not provided, will use the default QuestionAnswerPrompt.\n        refine_template (RefinePrompt): Optional RefinePrompt object. If not provided,\n            will use the default RefinePrompt.\n        include_summary (bool): Optional bool. If True, will also use the summary\n            text of the index when generating a response (the summary text can be set\n            through `index.set_text(\"<text>\")`).\n        similarity_cutoff (float): Optional float. If set, will filter out nodes with\n            similarity below this cutoff threshold when computing the response\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IS,\n        # TODO: pass from superclass\n        llm_predictor: Optional[LLMPredictor] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        docstore: Optional[DocumentStore] = None,\n        query_runner: Optional[BaseQueryRunner] = None,\n        required_keywords: Optional[List[str]] = None,\n        exclude_keywords: Optional[List[str]] = None,\n        response_mode: ResponseMode = ResponseMode.DEFAULT,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        refine_template: Optional[RefinePrompt] = None,\n        include_summary: bool = False,\n        response_kwargs: Optional[Dict] = None,\n        similarity_cutoff: Optional[float] = None,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        if index_struct is None:\n            raise ValueError(\"index_struct must be provided.\")\n        self._validate_index_struct(index_struct)\n        self._index_struct = index_struct\n        self._llm_predictor = llm_predictor or LLMPredictor()\n        # NOTE: the embed_model isn't used in all indices\n        self._embed_model = embed_model or OpenAIEmbedding()\n        self._docstore = docstore\n        self._query_runner = query_runner\n        # TODO: make this a required param\n        if prompt_helper is None:\n            raise ValueError(\"prompt_helper must be provided.\")\n        self._prompt_helper = cast(PromptHelper, prompt_helper)\n\n        self._required_keywords = required_keywords\n        self._exclude_keywords = exclude_keywords\n        self._response_mode = ResponseMode(response_mode)\n\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        self.refine_template = refine_template or DEFAULT_REFINE_PROMPT\n        self._include_summary = include_summary\n\n        self._response_kwargs = response_kwargs or {}\n        self.response_builder = ResponseBuilder(\n            self._prompt_helper,\n            self._llm_predictor,\n            self.text_qa_template,\n            self.refine_template,\n        )\n\n        self.similarity_cutoff = similarity_cutoff\n\n    def _should_use_node(\n        self, node: Node, similarity_tracker: Optional[SimilarityTracker] = None\n    ) -> bool:\n        \"\"\"Run node through filters to determine if it should be used.\"\"\"\n        words = re.findall(r\"\\w+\", node.get_text())\n        if self._required_keywords is not None:\n            for w in self._required_keywords:\n                if w not in words:\n                    return False\n\n        if self._exclude_keywords is not None:\n            for w in self._exclude_keywords:\n                if w in words:\n                    return False\n\n        sim_cutoff_exists = (\n            similarity_tracker is not None and self.similarity_cutoff is not None\n        )\n\n        if sim_cutoff_exists:\n            similarity = cast(SimilarityTracker, similarity_tracker).find(node)\n            if similarity is None:\n                return False\n            if cast(float, similarity) < cast(float, self.similarity_cutoff):\n                return False\n\n        return True\n\n    def _get_text_from_node(\n        self,\n        query_str: str,\n        node: Node,\n        level: Optional[int] = None,\n    ) -> Tuple[TextChunk, Optional[Response]]:\n        \"\"\"Query a given node.\n\n        If node references a given document, then return the document.\n        If node references a given index, then query the index.\n\n        \"\"\"\n        level_str = \"\" if level is None else f\"[Level {level}]\"\n        fmt_text_chunk = truncate_text(node.get_text(), 50)\n        logging.debug(f\">{level_str} Searching in chunk: {fmt_text_chunk}\")\n\n        is_index_struct = False\n        # if self._query_runner is not None, assume we want to do a recursive\n        # query. In order to not perform a recursive query, make sure\n        # _query_runner is None.\n        if (\n            self._query_runner is not None\n            and node.ref_doc_id is not None\n            and self._docstore is not None\n        ):\n            doc = self._docstore.get_document(node.ref_doc_id, raise_error=True)\n            if isinstance(doc, IndexStruct):\n                is_index_struct = True\n\n        if is_index_struct:\n            query_runner = cast(BaseQueryRunner, self._query_runner)\n            response = query_runner.query(query_str, cast(IndexStruct, doc))\n            return TextChunk(str(response), is_answer=True), response\n        else:\n            text = node.get_text()\n            return TextChunk(text), None\n\n    @property\n    def index_struct(self) -> IS:\n        \"\"\"Get the index struct.\"\"\"\n        return self._index_struct\n\n    def _validate_index_struct(self, index_struct: IS) -> None:\n        \"\"\"Validate the index struct.\"\"\"\n        pass\n\n    def _give_response_for_nodes(\n        self, query_str: str, text_chunks: List[TextChunk]\n    ) -> str:\n        \"\"\"Give response for nodes.\"\"\"\n        self.response_builder.reset()\n        for text in text_chunks:\n            self.response_builder.add_text_chunks([text])\n        response = self.response_builder.get_response(\n            query_str,\n            mode=self._response_mode,\n            **self._response_kwargs,\n        )\n\n        return response or \"\"\n\n    def get_nodes_and_similarities_for_response(\n        self, query_str: str\n    ) -> List[Tuple[Node, Optional[float]]]:\n        \"\"\"Get list of tuples of node and similarity for response.\n\n        First part of the tuple is the node.\n        Second part of tuple is the distance from query to the node.\n        If not applicable, it's None.\n        \"\"\"\n        similarity_tracker = SimilarityTracker()\n        nodes = self._get_nodes_for_response(\n            query_str, similarity_tracker=similarity_tracker\n        )\n        nodes = [\n            node for node in nodes if self._should_use_node(node, similarity_tracker)\n        ]\n\n        # TODO: create a `display` method to allow subclasses to print the Node\n        return similarity_tracker.get_zipped_nodes(nodes)\n\n    @abstractmethod\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n\n    def _query(self, query_str: str) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        # TODO: remove _query and just use query\n        tuples = self.get_nodes_and_similarities_for_response(query_str)\n        node_texts = []\n        for node, similarity in tuples:\n            text, response = self._get_text_from_node(query_str, node)\n            self.response_builder.add_node(node, similarity=similarity)\n            if response is not None:\n                # these are source nodes from within this node (when it's an index)\n                for source_node in response.source_nodes:\n                    self.response_builder.add_source_node(source_node)\n            node_texts.append(text)\n\n        if self._response_mode != ResponseMode.NO_TEXT:\n            response_str = self._give_response_for_nodes(query_str, node_texts)\n        else:\n            response_str = None\n\n        return Response(response_str, source_nodes=self.response_builder.get_sources())\n\n    @llm_token_counter(\"query\")\n    def query(self, query_str: str) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        response = self._query(query_str)\n        # if include_summary is True, then include summary text in answer\n        # summary text is set through `set_text` on the underlying index.\n        # TODO: refactor response builder to be in the __init__\n        if self._response_mode != ResponseMode.NO_TEXT and self._include_summary:\n            response_builder = ResponseBuilder(\n                self._prompt_helper,\n                self._llm_predictor,\n                self.text_qa_template,\n                self.refine_template,\n                texts=[TextChunk(self._index_struct.get_text())],\n            )\n            # NOTE: use create and refine for now (default response mode)\n            response.response = response_builder.get_response(\n                query_str,\n                mode=self._response_mode,\n                prev_response=response.response,\n            )\n\n        return response\n", "doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "__type__": "Document"}, "1d0e1c04-fa50-4c32-af9d-aefe933da9fe": {"text": "\nThe BaseGPTIndexQuery is a helper class used to query an index. It takes in parameters such as an IndexStruct, LLMPredictor, PromptHelper, Embedding Model, DocumentStore, QueryRunner, required and excluded keywords, ResponseMode, QuestionAnswerPrompt, RefinePrompt, and a similarity cutoff. It uses these parameters to filter out nodes that do not meet the criteria, and then uses the ResponseBuilder to generate a response. The code also uses a SimilarityTracker to track the distance from the query to the node, and an LLM token counter to track the query. The code contains functions that get nodes for a response, query a response, and get text from a node. It also contains an abstract method for getting nodes for a response. The code can also include a summary text if the include_summary flag is set to True. The purpose of the code is to provide a base for querying an index and generate a response based on the query string and nodes. It uses algorithms such as the LLM token counter and SimilarityTracker to track the query and the distance from the query to the node, and data structures such as the IndexStruct, PromptHelper, and DocumentStore to store and query the data.", "doc_id": "1d0e1c04-fa50-4c32-af9d-aefe933da9fe", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Base query classes.\"\"\"\n\nimport logging\nimport re\nfrom abc import abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Dict, Generic, List, Optional, Tuple, TypeVar, cast\n\nfrom gpt_index.data_structs.data_structs import IndexStruct, Node\nfrom gpt_index.docstore import DocumentStore\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.embeddings.openai import OpenAIEmbedding\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.response.builder import ResponseBuilder, ResponseMode, TextChunk\nfrom gpt_index.indices.utils import truncate_text\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.default_prompts import (\n    DEFAULT_REFINE_PROMPT,\n    DEFAULT_TEXT_QA_PROMPT,\n)\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\nfrom gpt_index.response.schema import Response\nfrom gpt_index.token_counter.token_counter import llm_token_counter\n\nIS = TypeVar(\"IS\",", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 0, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "1": {"text": "import llm_token_counter\n\nIS = TypeVar(\"IS\", bound=IndexStruct)\n\n\n@dataclass\nclass BaseQueryRunner:\n    \"\"\"Base query runner.\"\"\"\n\n    @abstractmethod\n    def query(self, query: str, index_struct: IndexStruct) -> Response:\n        \"\"\"Schedule a query.\"\"\"\n        raise NotImplementedError(\"Not implemented yet.\")\n\n\nclass BaseGPTIndexQuery(Generic[IS]):\n    \"\"\"Base GPT Index Query.\n\n    Helper class that is used to query an index. Can be called within `query`\n    method of a BaseGPTIndex object, or instantiated independently.\n\n    Args:\n        llm_predictor (LLMPredictor): Optional LLMPredictor object. If not provided,\n            will use the default LLMPredictor (text-davinci-003)\n        prompt_helper (PromptHelper): Optional PromptHelper object. If not provided,\n            will use the default PromptHelper.\n        required_keywords (List[str]): Optional list of keywords that must be present\n            in nodes. Can be used to query most indices (tree index is an", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "2": {"text": "   in nodes. Can be used to query most indices (tree index is an exception).\n        exclude_keywords (List[str]): Optional list of keywords that must not be\n            present in nodes. Can be used to query most indices (tree index is an\n            exception).\n        response_mode (ResponseMode): Optional ResponseMode. If not provided, will\n            use the default ResponseMode.\n        text_qa_template (QuestionAnswerPrompt): Optional QuestionAnswerPrompt object.\n            If not provided, will use the default QuestionAnswerPrompt.\n        refine_template (RefinePrompt): Optional RefinePrompt object. If not provided,\n            will use the default RefinePrompt.\n        include_summary (bool): Optional bool. If True, will also use the summary\n            text of the index when generating a response (the summary text can be set\n            through `index.set_text(\"<text>\")`).\n        similarity_cutoff (float): Optional float. If set, will filter out nodes with\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "3": {"text": "(float): Optional float. If set, will filter out nodes with\n            similarity below this cutoff threshold when computing the response\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IS,\n        # TODO: pass from superclass\n        llm_predictor: Optional[LLMPredictor] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        docstore: Optional[DocumentStore] = None,\n        query_runner: Optional[BaseQueryRunner] = None,\n        required_keywords: Optional[List[str]] = None,\n        exclude_keywords: Optional[List[str]] = None,\n        response_mode: ResponseMode = ResponseMode.DEFAULT,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        refine_template: Optional[RefinePrompt] = None,\n        include_summary: bool = False,\n        response_kwargs:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "4": {"text": "bool = False,\n        response_kwargs: Optional[Dict] = None,\n        similarity_cutoff: Optional[float] = None,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        if index_struct is None:\n            raise ValueError(\"index_struct must be provided.\")\n        self._validate_index_struct(index_struct)\n        self._index_struct = index_struct\n        self._llm_predictor = llm_predictor or LLMPredictor()\n        # NOTE: the embed_model isn't used in all indices\n        self._embed_model = embed_model or OpenAIEmbedding()\n        self._docstore = docstore\n        self._query_runner = query_runner\n        # TODO: make this a required param\n        if prompt_helper is None:\n            raise ValueError(\"prompt_helper must be provided.\")\n        self._prompt_helper = cast(PromptHelper, prompt_helper)\n\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 4, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "5": {"text": "= cast(PromptHelper, prompt_helper)\n\n        self._required_keywords = required_keywords\n        self._exclude_keywords = exclude_keywords\n        self._response_mode = ResponseMode(response_mode)\n\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        self.refine_template = refine_template or DEFAULT_REFINE_PROMPT\n        self._include_summary = include_summary\n\n        self._response_kwargs = response_kwargs or {}\n        self.response_builder = ResponseBuilder(\n            self._prompt_helper,\n            self._llm_predictor,\n            self.text_qa_template,\n            self.refine_template,\n        )\n\n        self.similarity_cutoff = similarity_cutoff\n\n    def _should_use_node(\n        self, node: Node, similarity_tracker: Optional[SimilarityTracker] = None\n    ) ->", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 5, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "6": {"text": "similarity_tracker: Optional[SimilarityTracker] = None\n    ) -> bool:\n        \"\"\"Run node through filters to determine if it should be used.\"\"\"\n        words = re.findall(r\"\\w+\", node.get_text())\n        if self._required_keywords is not None:\n            for w in self._required_keywords:\n                if w not in words:\n                    return False\n\n        if self._exclude_keywords is not None:\n            for w in self._exclude_keywords:\n                if w in words:\n                    return False\n\n        sim_cutoff_exists = (\n            similarity_tracker is not None and self.similarity_cutoff is not None\n        )\n\n        if sim_cutoff_exists:\n            similarity = cast(SimilarityTracker,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 6, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "7": {"text": "           similarity = cast(SimilarityTracker, similarity_tracker).find(node)\n            if similarity is None:\n                return False\n            if cast(float, similarity) < cast(float, self.similarity_cutoff):\n                return False\n\n        return True\n\n    def _get_text_from_node(\n        self,\n        query_str: str,\n        node: Node,\n        level: Optional[int] = None,\n    ) -> Tuple[TextChunk, Optional[Response]]:\n        \"\"\"Query a given node.\n\n        If node references a given document, then return the document.\n        If node references a given index, then query the index.\n\n        \"\"\"\n        level_str = \"\" if level is None else f\"[Level {level}]\"\n        fmt_text_chunk = truncate_text(node.get_text(), 50)\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 7, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "8": {"text": "50)\n        logging.debug(f\">{level_str} Searching in chunk: {fmt_text_chunk}\")\n\n        is_index_struct = False\n        # if self._query_runner is not None, assume we want to do a recursive\n        # query. In order to not perform a recursive query, make sure\n        # _query_runner is None.\n        if (\n            self._query_runner is not None\n            and node.ref_doc_id is not None\n            and self._docstore is not None\n        ):\n            doc = self._docstore.get_document(node.ref_doc_id, raise_error=True)\n            if isinstance(doc, IndexStruct):\n                is_index_struct = True\n\n        if is_index_struct:\n            query_runner = cast(BaseQueryRunner, self._query_runner)\n            response =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 8, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "9": {"text": "self._query_runner)\n            response = query_runner.query(query_str, cast(IndexStruct, doc))\n            return TextChunk(str(response), is_answer=True), response\n        else:\n            text = node.get_text()\n            return TextChunk(text), None\n\n    @property\n    def index_struct(self) -> IS:\n        \"\"\"Get the index struct.\"\"\"\n        return self._index_struct\n\n    def _validate_index_struct(self, index_struct: IS) -> None:\n        \"\"\"Validate the index struct.\"\"\"\n        pass\n\n    def _give_response_for_nodes(\n        self, query_str: str, text_chunks: List[TextChunk]\n    ) -> str:\n        \"\"\"Give response for nodes.\"\"\"\n        self.response_builder.reset()\n        for text in text_chunks:\n            self.response_builder.add_text_chunks([text])\n        response =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 9, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "10": {"text": "       response = self.response_builder.get_response(\n            query_str,\n            mode=self._response_mode,\n            **self._response_kwargs,\n        )\n\n        return response or \"\"\n\n    def get_nodes_and_similarities_for_response(\n        self, query_str: str\n    ) -> List[Tuple[Node, Optional[float]]]:\n        \"\"\"Get list of tuples of node and similarity for response.\n\n        First part of the tuple is the node.\n        Second part of tuple is the distance from query to the node.\n        If not applicable, it's None.\n        \"\"\"\n        similarity_tracker = SimilarityTracker()\n        nodes = self._get_nodes_for_response(\n            query_str, similarity_tracker=similarity_tracker\n        )\n        nodes = [\n            node for node in nodes if", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 10, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "11": {"text": "[\n            node for node in nodes if self._should_use_node(node, similarity_tracker)\n        ]\n\n        # TODO: create a `display` method to allow subclasses to print the Node\n        return similarity_tracker.get_zipped_nodes(nodes)\n\n    @abstractmethod\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n\n    def _query(self, query_str: str) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        # TODO: remove _query and just use query\n        tuples = self.get_nodes_and_similarities_for_response(query_str)\n        node_texts = []\n        for node, similarity in tuples:\n            text, response = self._get_text_from_node(query_str, node)\n        ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 11, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "12": {"text": "node)\n            self.response_builder.add_node(node, similarity=similarity)\n            if response is not None:\n                # these are source nodes from within this node (when it's an index)\n                for source_node in response.source_nodes:\n                    self.response_builder.add_source_node(source_node)\n            node_texts.append(text)\n\n        if self._response_mode != ResponseMode.NO_TEXT:\n            response_str = self._give_response_for_nodes(query_str, node_texts)\n        else:\n            response_str = None\n\n        return Response(response_str, source_nodes=self.response_builder.get_sources())\n\n    @llm_token_counter(\"query\")\n    def query(self, query_str: str) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        response = self._query(query_str)\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 12, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "13": {"text": "       response = self._query(query_str)\n        # if include_summary is True, then include summary text in answer\n        # summary text is set through `set_text` on the underlying index.\n        # TODO: refactor response builder to be in the __init__\n        if self._response_mode != ResponseMode.NO_TEXT and self._include_summary:\n            response_builder = ResponseBuilder(\n                self._prompt_helper,\n                self._llm_predictor,\n                self.text_qa_template,\n                self.refine_template,\n                texts=[TextChunk(self._index_struct.get_text())],\n            )\n            # NOTE: use create and refine for now (default response mode)\n            response.response = response_builder.get_response(\n               ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 13, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "14": {"text": "               query_str,\n                mode=self._response_mode,\n                prev_response=response.response,\n            )\n\n        return response\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "index": 14, "child_indices": [], "ref_doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "node_info": null}, "15": {"text": "BaseGPTIndexQuery is a helper class used to query an index. It can be called within the query method of a BaseGPTIndex object, or instantiated independently. It takes in parameters such as an IndexStruct, LLMPredictor, PromptHelper, Embedding Model, DocumentStore, QueryRunner, required and excluded keywords, ResponseMode, QuestionAnswerPrompt, RefinePrompt, and a similarity cutoff. It uses these parameters to filter out nodes that do not meet the criteria, and then uses the ResponseBuilder to generate a response.", "doc_id": null, "embedding": null, "extra_info": null, "index": 15, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "ref_doc_id": null, "node_info": null}, "16": {"text": "This code file is part of a GPT index and is responsible for answering queries. It contains functions that get nodes for a response, query a response, and get text from a node. It also contains an abstract method for getting nodes for a response. The code uses a response builder to create a response from the query string and nodes, and it can also include a summary text if the include_summary flag is set to True. The code also uses a SimilarityTracker to track the distance from the query to the node. Finally, the code uses an LLM token counter to track the query.", "doc_id": null, "embedding": null, "extra_info": null, "index": 16, "child_indices": [10, 11, 12, 13, 14], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"15": {"text": "BaseGPTIndexQuery is a helper class used to query an index. It can be called within the query method of a BaseGPTIndex object, or instantiated independently. It takes in parameters such as an IndexStruct, LLMPredictor, PromptHelper, Embedding Model, DocumentStore, QueryRunner, required and excluded keywords, ResponseMode, QuestionAnswerPrompt, RefinePrompt, and a similarity cutoff. It uses these parameters to filter out nodes that do not meet the criteria, and then uses the ResponseBuilder to generate a response.", "doc_id": null, "embedding": null, "extra_info": null, "index": 15, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "ref_doc_id": null, "node_info": null}, "16": {"text": "This code file is part of a GPT index and is responsible for answering queries. It contains functions that get nodes for a response, query a response, and get text from a node. It also contains an abstract method for getting nodes for a response. The code uses a response builder to create a response from the query string and nodes, and it can also include a summary text if the include_summary flag is set to True. The code also uses a SimilarityTracker to track the distance from the query to the node. Finally, the code uses an LLM token counter to track the query.", "doc_id": null, "embedding": null, "extra_info": null, "index": 16, "child_indices": [10, 11, 12, 13, 14], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}