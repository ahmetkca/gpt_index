{"index_struct": {"text": "\nThis code file implements a GPT Tree Index embedding query, which uses a BaseEmbedding model to calculate the embedding similarity between the query and the node text. It traverses the index graph using the Tree Select Query Prompt, Tree Select Query Prompt (Multiple), and a Refinement Prompt. The query will choose a certain number of child nodes to traverse for any given parent node, depending on the child_branch_factor parameter. The code also caches the query embedding and the node text embedding to improve performance. The purpose of this code is to allow users to query the index graph using the embedding similarity between the query and the node text, and to provide a more efficient way of traversing the index graph.", "doc_id": "b8b32418-81a4-4e41-8a89-fd88b2c6ae17", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Query Tree using embedding similarity between query and node text.\"\"\"\n\nimport logging\nfrom typing import Any, Dict, List, Optional, Tuple\n\nfrom gpt_index.data_structs.data_structs import IndexGraph, Node\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.tree.leaf_query import GPTTreeIndexLeafQuery\nfrom gpt_index.indices.utils import get_sorted_node_list\nfrom gpt_index.prompts.prompts import TreeSelectMultiplePrompt, TreeSelectPrompt\n\n\nclass GPTTreeIndexEmbeddingQuery(GPTTreeIndexLeafQuery):\n    \"\"\"\n    GPT Tree Index embedding query.\n\n    This class traverses the index graph using the embedding similarity between the\n    query and the node text.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"embedding\")\n\n    Args:\n        query_template (Optional[TreeSelectPrompt]): Tree Select Query Prompt\n            (see :ref:`Prompt-Templates`).\n        query_template_multiple (Optional[TreeSelectMultiplePrompt]): Tree Select\n            Query", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "index": 0, "child_indices": [], "ref_doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "node_info": null}, "1": {"text": "Tree Select\n            Query Prompt (Multiple)\n            (see :ref:`Prompt-Templates`).\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        child_branch_factor (int): Number of child nodes to consider at each level.\n            If child_branch_factor is 1, then the query will only choose one child node\n            to traverse for any given parent node.\n            If child_branch_factor is 2, then the query will choose two child nodes.\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "index": 1, "child_indices": [], "ref_doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "node_info": null}, "2": {"text": "      self,\n        index_struct: IndexGraph,\n        query_template: Optional[TreeSelectPrompt] = None,\n        query_template_multiple: Optional[TreeSelectMultiplePrompt] = None,\n        child_branch_factor: int = 1,\n        embed_model: Optional[BaseEmbedding] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            index_struct,\n            query_template=query_template,\n            query_template_multiple=query_template_multiple,\n            child_branch_factor=child_branch_factor,\n            embed_model=embed_model,\n            **kwargs,\n        )\n        self.child_branch_factor = child_branch_factor\n\n    def _query_level(\n        self,\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "index": 2, "child_indices": [], "ref_doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "node_info": null}, "3": {"text": "       self,\n        cur_nodes: Dict[int, Node],\n        query_str: str,\n        level: int = 0,\n    ) -> str:\n        cur_node_list = get_sorted_node_list(cur_nodes)\n\n        # Get the node with the highest similarity to the query\n        selected_node, selected_index = self._get_most_similar_node(\n            cur_node_list, query_str\n        )\n        logging.debug(\n            f\">[Level {level}] Node [{selected_index+1}] Summary text: \"\n            f\"{' '.join(selected_node.get_text().splitlines())}\"\n        )\n\n        # Get the response for the selected node\n        response = self._query_with_selected_node(selected_node, query_str, level=level)\n\n        return response\n\n    def _get_query_text_embedding_similarities(\n        self,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "index": 3, "child_indices": [], "ref_doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "node_info": null}, "4": {"text": "       self, query_str: str, nodes: List[Node]\n    ) -> List[float]:\n        \"\"\"\n        Get query text embedding similarity.\n\n        Cache the query embedding and the node text embedding.\n\n        \"\"\"\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n        similarities = []\n        for node in nodes:\n            if node.embedding is not None:\n                text_embedding = node.embedding\n            else:\n                text_embedding = self._embed_model.get_text_embedding(node.get_text())\n                node.embedding = text_embedding\n\n            similarity = self._embed_model.similarity(query_embedding, text_embedding)\n            similarities.append(similarity)\n        return similarities\n\n    def", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "index": 4, "child_indices": [], "ref_doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "node_info": null}, "5": {"text": "       return similarities\n\n    def _get_most_similar_node(\n        self, nodes: List[Node], query_str: str\n    ) -> Tuple[Node, int]:\n        \"\"\"Get the node with the highest similarity to the query.\"\"\"\n        similarities = self._get_query_text_embedding_similarities(query_str, nodes)\n\n        selected_index = similarities.index(max(similarities))\n\n        selected_node = nodes[similarities.index(max(similarities))]\n        return selected_node, selected_index\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "index": 5, "child_indices": [], "ref_doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "node_info": null}, "6": {"text": "This code file implements a GPT Tree Index embedding query, which traverses the index graph using the embedding similarity between the query and the node text. It uses a Tree Select Query Prompt, Tree Select Query Prompt (Multiple), and a Refinement Prompt to query the index graph. It also uses a BaseEmbedding model to calculate the embedding similarity between the query and the node text. The query will choose a certain number of child nodes to traverse for any given parent node, depending on the child_branch_factor parameter. The code also caches the query embedding and the node text embedding to improve performance.", "doc_id": null, "embedding": null, "extra_info": null, "index": 6, "child_indices": [0, 1, 2, 3, 4, 5], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"6": {"text": "This code file implements a GPT Tree Index embedding query, which traverses the index graph using the embedding similarity between the query and the node text. It uses a Tree Select Query Prompt, Tree Select Query Prompt (Multiple), and a Refinement Prompt to query the index graph. It also uses a BaseEmbedding model to calculate the embedding similarity between the query and the node text. The query will choose a certain number of child nodes to traverse for any given parent node, depending on the child_branch_factor parameter. The code also caches the query embedding and the node text embedding to improve performance.", "doc_id": null, "embedding": null, "extra_info": null, "index": 6, "child_indices": [0, 1, 2, 3, 4, 5], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"a5da8f1d8beb548e10422c7a5a2cf1c6328e9332": {"text": "\"\"\"Query Tree using embedding similarity between query and node text.\"\"\"\n\nimport logging\nfrom typing import Any, Dict, List, Optional, Tuple\n\nfrom gpt_index.data_structs.data_structs import IndexGraph, Node\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.tree.leaf_query import GPTTreeIndexLeafQuery\nfrom gpt_index.indices.utils import get_sorted_node_list\nfrom gpt_index.prompts.prompts import TreeSelectMultiplePrompt, TreeSelectPrompt\n\n\nclass GPTTreeIndexEmbeddingQuery(GPTTreeIndexLeafQuery):\n    \"\"\"\n    GPT Tree Index embedding query.\n\n    This class traverses the index graph using the embedding similarity between the\n    query and the node text.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"embedding\")\n\n    Args:\n        query_template (Optional[TreeSelectPrompt]): Tree Select Query Prompt\n            (see :ref:`Prompt-Templates`).\n        query_template_multiple (Optional[TreeSelectMultiplePrompt]): Tree Select\n            Query Prompt (Multiple)\n            (see :ref:`Prompt-Templates`).\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        child_branch_factor (int): Number of child nodes to consider at each level.\n            If child_branch_factor is 1, then the query will only choose one child node\n            to traverse for any given parent node.\n            If child_branch_factor is 2, then the query will choose two child nodes.\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexGraph,\n        query_template: Optional[TreeSelectPrompt] = None,\n        query_template_multiple: Optional[TreeSelectMultiplePrompt] = None,\n        child_branch_factor: int = 1,\n        embed_model: Optional[BaseEmbedding] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            index_struct,\n            query_template=query_template,\n            query_template_multiple=query_template_multiple,\n            child_branch_factor=child_branch_factor,\n            embed_model=embed_model,\n            **kwargs,\n        )\n        self.child_branch_factor = child_branch_factor\n\n    def _query_level(\n        self,\n        cur_nodes: Dict[int, Node],\n        query_str: str,\n        level: int = 0,\n    ) -> str:\n        cur_node_list = get_sorted_node_list(cur_nodes)\n\n        # Get the node with the highest similarity to the query\n        selected_node, selected_index = self._get_most_similar_node(\n            cur_node_list, query_str\n        )\n        logging.debug(\n            f\">[Level {level}] Node [{selected_index+1}] Summary text: \"\n            f\"{' '.join(selected_node.get_text().splitlines())}\"\n        )\n\n        # Get the response for the selected node\n        response = self._query_with_selected_node(selected_node, query_str, level=level)\n\n        return response\n\n    def _get_query_text_embedding_similarities(\n        self, query_str: str, nodes: List[Node]\n    ) -> List[float]:\n        \"\"\"\n        Get query text embedding similarity.\n\n        Cache the query embedding and the node text embedding.\n\n        \"\"\"\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n        similarities = []\n        for node in nodes:\n            if node.embedding is not None:\n                text_embedding = node.embedding\n            else:\n                text_embedding = self._embed_model.get_text_embedding(node.get_text())\n                node.embedding = text_embedding\n\n            similarity = self._embed_model.similarity(query_embedding, text_embedding)\n            similarities.append(similarity)\n        return similarities\n\n    def _get_most_similar_node(\n        self, nodes: List[Node], query_str: str\n    ) -> Tuple[Node, int]:\n        \"\"\"Get the node with the highest similarity to the query.\"\"\"\n        similarities = self._get_query_text_embedding_similarities(query_str, nodes)\n\n        selected_index = similarities.index(max(similarities))\n\n        selected_node = nodes[similarities.index(max(similarities))]\n        return selected_node, selected_index\n", "doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "__type__": "Document"}, "b8b32418-81a4-4e41-8a89-fd88b2c6ae17": {"text": "\nThis code file implements a GPT Tree Index embedding query, which uses a BaseEmbedding model to calculate the embedding similarity between the query and the node text. It traverses the index graph using the Tree Select Query Prompt, Tree Select Query Prompt (Multiple), and a Refinement Prompt. The query will choose a certain number of child nodes to traverse for any given parent node, depending on the child_branch_factor parameter. The code also caches the query embedding and the node text embedding to improve performance. The purpose of this code is to allow users to query the index graph using the embedding similarity between the query and the node text, and to provide a more efficient way of traversing the index graph.", "doc_id": "b8b32418-81a4-4e41-8a89-fd88b2c6ae17", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Query Tree using embedding similarity between query and node text.\"\"\"\n\nimport logging\nfrom typing import Any, Dict, List, Optional, Tuple\n\nfrom gpt_index.data_structs.data_structs import IndexGraph, Node\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.tree.leaf_query import GPTTreeIndexLeafQuery\nfrom gpt_index.indices.utils import get_sorted_node_list\nfrom gpt_index.prompts.prompts import TreeSelectMultiplePrompt, TreeSelectPrompt\n\n\nclass GPTTreeIndexEmbeddingQuery(GPTTreeIndexLeafQuery):\n    \"\"\"\n    GPT Tree Index embedding query.\n\n    This class traverses the index graph using the embedding similarity between the\n    query and the node text.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"embedding\")\n\n    Args:\n        query_template (Optional[TreeSelectPrompt]): Tree Select Query Prompt\n            (see :ref:`Prompt-Templates`).\n        query_template_multiple (Optional[TreeSelectMultiplePrompt]): Tree Select\n            Query", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "index": 0, "child_indices": [], "ref_doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "node_info": null}, "1": {"text": "Tree Select\n            Query Prompt (Multiple)\n            (see :ref:`Prompt-Templates`).\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        child_branch_factor (int): Number of child nodes to consider at each level.\n            If child_branch_factor is 1, then the query will only choose one child node\n            to traverse for any given parent node.\n            If child_branch_factor is 2, then the query will choose two child nodes.\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "index": 1, "child_indices": [], "ref_doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "node_info": null}, "2": {"text": "      self,\n        index_struct: IndexGraph,\n        query_template: Optional[TreeSelectPrompt] = None,\n        query_template_multiple: Optional[TreeSelectMultiplePrompt] = None,\n        child_branch_factor: int = 1,\n        embed_model: Optional[BaseEmbedding] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            index_struct,\n            query_template=query_template,\n            query_template_multiple=query_template_multiple,\n            child_branch_factor=child_branch_factor,\n            embed_model=embed_model,\n            **kwargs,\n        )\n        self.child_branch_factor = child_branch_factor\n\n    def _query_level(\n        self,\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "index": 2, "child_indices": [], "ref_doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "node_info": null}, "3": {"text": "       self,\n        cur_nodes: Dict[int, Node],\n        query_str: str,\n        level: int = 0,\n    ) -> str:\n        cur_node_list = get_sorted_node_list(cur_nodes)\n\n        # Get the node with the highest similarity to the query\n        selected_node, selected_index = self._get_most_similar_node(\n            cur_node_list, query_str\n        )\n        logging.debug(\n            f\">[Level {level}] Node [{selected_index+1}] Summary text: \"\n            f\"{' '.join(selected_node.get_text().splitlines())}\"\n        )\n\n        # Get the response for the selected node\n        response = self._query_with_selected_node(selected_node, query_str, level=level)\n\n        return response\n\n    def _get_query_text_embedding_similarities(\n        self,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "index": 3, "child_indices": [], "ref_doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "node_info": null}, "4": {"text": "       self, query_str: str, nodes: List[Node]\n    ) -> List[float]:\n        \"\"\"\n        Get query text embedding similarity.\n\n        Cache the query embedding and the node text embedding.\n\n        \"\"\"\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n        similarities = []\n        for node in nodes:\n            if node.embedding is not None:\n                text_embedding = node.embedding\n            else:\n                text_embedding = self._embed_model.get_text_embedding(node.get_text())\n                node.embedding = text_embedding\n\n            similarity = self._embed_model.similarity(query_embedding, text_embedding)\n            similarities.append(similarity)\n        return similarities\n\n    def", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "index": 4, "child_indices": [], "ref_doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "node_info": null}, "5": {"text": "       return similarities\n\n    def _get_most_similar_node(\n        self, nodes: List[Node], query_str: str\n    ) -> Tuple[Node, int]:\n        \"\"\"Get the node with the highest similarity to the query.\"\"\"\n        similarities = self._get_query_text_embedding_similarities(query_str, nodes)\n\n        selected_index = similarities.index(max(similarities))\n\n        selected_node = nodes[similarities.index(max(similarities))]\n        return selected_node, selected_index\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "index": 5, "child_indices": [], "ref_doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "node_info": null}, "6": {"text": "This code file implements a GPT Tree Index embedding query, which traverses the index graph using the embedding similarity between the query and the node text. It uses a Tree Select Query Prompt, Tree Select Query Prompt (Multiple), and a Refinement Prompt to query the index graph. It also uses a BaseEmbedding model to calculate the embedding similarity between the query and the node text. The query will choose a certain number of child nodes to traverse for any given parent node, depending on the child_branch_factor parameter. The code also caches the query embedding and the node text embedding to improve performance.", "doc_id": null, "embedding": null, "extra_info": null, "index": 6, "child_indices": [0, 1, 2, 3, 4, 5], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"6": {"text": "This code file implements a GPT Tree Index embedding query, which traverses the index graph using the embedding similarity between the query and the node text. It uses a Tree Select Query Prompt, Tree Select Query Prompt (Multiple), and a Refinement Prompt to query the index graph. It also uses a BaseEmbedding model to calculate the embedding similarity between the query and the node text. The query will choose a certain number of child nodes to traverse for any given parent node, depending on the child_branch_factor parameter. The code also caches the query embedding and the node text embedding to improve performance.", "doc_id": null, "embedding": null, "extra_info": null, "index": 6, "child_indices": [0, 1, 2, 3, 4, 5], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}