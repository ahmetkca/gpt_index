{"index_struct": {"text": "\nGPTFaissIndexQuery is a query class that uses an underlying Faiss index and an embedding model to retrieve the top-k nodes by embedding similarity to the query. It takes in a Faiss Index object, an Embedding model, and a number of similar nodes to retrieve as parameters. The query class uses the embedding model to get the query embedding, and uses the Faiss index to search for the top-k nodes. It also has a logging feature to print out the top-k nodes and their similarity scores. The purpose of the code is to provide a query for GPTFaissIndex that uses embedding similarity to retrieve the top-k nodes.", "doc_id": "dd9e0f21-b9bf-4838-bda3-7e771ee24c8a", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Default query for GPTFaissIndex.\"\"\"\n\nimport logging\nfrom typing import Any, List, Optional, cast\n\nimport numpy as np\n\nfrom gpt_index.data_structs.data_structs import IndexDict, Node\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.query.vector_store.base import BaseGPTVectorStoreIndexQuery\nfrom gpt_index.indices.utils import truncate_text\n\n\nclass GPTFaissIndexQuery(BaseGPTVectorStoreIndexQuery[IndexDict]):\n    \"\"\"GPTFaissIndex query.\n\n    An embedding-based query for GPTFaissIndex, which queries\n    an underlying Faiss index to retrieve top-k nodes by\n    embedding similarity to the query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 0, "child_indices": [], "ref_doc_id": "1ea526708a54837eb2d2594ce803150877deeec3", "node_info": null}, "1": {"text": "(Optional[RefinePrompt]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        faiss_index (faiss.Index): A Faiss Index object (required)\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        similarity_top_k (int): Number of similar nodes to retrieve.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexDict,\n        faiss_index: Optional[Any] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        similarity_top_k: Optional[int] = 1,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            index_struct=index_struct,\n            embed_model=embed_model,\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 1, "child_indices": [], "ref_doc_id": "1ea526708a54837eb2d2594ce803150877deeec3", "node_info": null}, "2": {"text": "embed_model=embed_model,\n            similarity_top_k=similarity_top_k,\n            **kwargs,\n        )\n        if faiss_index is None:\n            raise ValueError(\"faiss_index cannot be None.\")\n        # NOTE: cast to Any for now\n        self._faiss_index = cast(Any, faiss_index)\n        self._faiss_index = faiss_index\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n        query_embedding_np = np.array(query_embedding, dtype=\"float32\")[np.newaxis, :]\n        dists, indices = self._faiss_index.search(\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 2, "child_indices": [], "ref_doc_id": "1ea526708a54837eb2d2594ce803150877deeec3", "node_info": null}, "3": {"text": "indices = self._faiss_index.search(\n            query_embedding_np, self.similarity_top_k\n        )\n        dists = [d[0] for d in dists]\n        # if empty, then return an empty response\n        if len(indices) == 0:\n            return []\n\n        # returned dimension is 1 x k\n        node_idxs = list([str(i) for i in indices[0]])\n        top_k_nodes = self._index_struct.get_nodes(node_idxs)\n\n        if similarity_tracker is not None:\n            for node, similarity in zip(top_k_nodes, dists):\n                similarity_tracker.add(node, similarity)\n\n        if logging.getLogger(__name__).getEffectiveLevel() == logging.DEBUG:\n            fmt_txts = []\n            for node_idx, node_similarity, node in", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 3, "child_indices": [], "ref_doc_id": "1ea526708a54837eb2d2594ce803150877deeec3", "node_info": null}, "4": {"text": "       for node_idx, node_similarity, node in zip(node_idxs, dists, top_k_nodes):\n                fmt_txt = f\"> [Node {node_idx}] [Similarity score: \\\n                    {float(node_similarity):.6}] {truncate_text(node.get_text(), 100)}\"\n                fmt_txts.append(fmt_txt)\n            top_k_node_text = \"\\n\".join(fmt_txts)\n            logging.debug(f\"> Top {len(top_k_nodes)} nodes:\\n{top_k_node_text}\")\n\n        return top_k_nodes\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 4, "child_indices": [], "ref_doc_id": "1ea526708a54837eb2d2594ce803150877deeec3", "node_info": null}, "5": {"text": "GPTFaissIndexQuery is a query class for GPTFaissIndex, which uses an underlying Faiss index to retrieve top-k nodes by embedding similarity to the query. It takes in a Faiss Index object, an Embedding model, and a number of similar nodes to retrieve as parameters. It then uses the embedding model to get the query embedding, and uses the Faiss index to search for the top-k nodes. It also has a logging feature to print out the top-k nodes and their similarity scores.", "doc_id": null, "embedding": null, "extra_info": null, "index": 5, "child_indices": [0, 1, 2, 3, 4], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"5": {"text": "GPTFaissIndexQuery is a query class for GPTFaissIndex, which uses an underlying Faiss index to retrieve top-k nodes by embedding similarity to the query. It takes in a Faiss Index object, an Embedding model, and a number of similar nodes to retrieve as parameters. It then uses the embedding model to get the query embedding, and uses the Faiss index to search for the top-k nodes. It also has a logging feature to print out the top-k nodes and their similarity scores.", "doc_id": null, "embedding": null, "extra_info": null, "index": 5, "child_indices": [0, 1, 2, 3, 4], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"1ea526708a54837eb2d2594ce803150877deeec3": {"text": "\"\"\"Default query for GPTFaissIndex.\"\"\"\n\nimport logging\nfrom typing import Any, List, Optional, cast\n\nimport numpy as np\n\nfrom gpt_index.data_structs.data_structs import IndexDict, Node\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.query.vector_store.base import BaseGPTVectorStoreIndexQuery\nfrom gpt_index.indices.utils import truncate_text\n\n\nclass GPTFaissIndexQuery(BaseGPTVectorStoreIndexQuery[IndexDict]):\n    \"\"\"GPTFaissIndex query.\n\n    An embedding-based query for GPTFaissIndex, which queries\n    an underlying Faiss index to retrieve top-k nodes by\n    embedding similarity to the query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        faiss_index (faiss.Index): A Faiss Index object (required)\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        similarity_top_k (int): Number of similar nodes to retrieve.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexDict,\n        faiss_index: Optional[Any] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        similarity_top_k: Optional[int] = 1,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            index_struct=index_struct,\n            embed_model=embed_model,\n            similarity_top_k=similarity_top_k,\n            **kwargs,\n        )\n        if faiss_index is None:\n            raise ValueError(\"faiss_index cannot be None.\")\n        # NOTE: cast to Any for now\n        self._faiss_index = cast(Any, faiss_index)\n        self._faiss_index = faiss_index\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n        query_embedding_np = np.array(query_embedding, dtype=\"float32\")[np.newaxis, :]\n        dists, indices = self._faiss_index.search(\n            query_embedding_np, self.similarity_top_k\n        )\n        dists = [d[0] for d in dists]\n        # if empty, then return an empty response\n        if len(indices) == 0:\n            return []\n\n        # returned dimension is 1 x k\n        node_idxs = list([str(i) for i in indices[0]])\n        top_k_nodes = self._index_struct.get_nodes(node_idxs)\n\n        if similarity_tracker is not None:\n            for node, similarity in zip(top_k_nodes, dists):\n                similarity_tracker.add(node, similarity)\n\n        if logging.getLogger(__name__).getEffectiveLevel() == logging.DEBUG:\n            fmt_txts = []\n            for node_idx, node_similarity, node in zip(node_idxs, dists, top_k_nodes):\n                fmt_txt = f\"> [Node {node_idx}] [Similarity score: \\\n                    {float(node_similarity):.6}] {truncate_text(node.get_text(), 100)}\"\n                fmt_txts.append(fmt_txt)\n            top_k_node_text = \"\\n\".join(fmt_txts)\n            logging.debug(f\"> Top {len(top_k_nodes)} nodes:\\n{top_k_node_text}\")\n\n        return top_k_nodes\n", "doc_id": "1ea526708a54837eb2d2594ce803150877deeec3", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/faiss.py", "file_name": "faiss.py"}, "__type__": "Document"}, "dd9e0f21-b9bf-4838-bda3-7e771ee24c8a": {"text": "\nGPTFaissIndexQuery is a query class that uses an underlying Faiss index and an embedding model to retrieve the top-k nodes by embedding similarity to the query. It takes in a Faiss Index object, an Embedding model, and a number of similar nodes to retrieve as parameters. The query class uses the embedding model to get the query embedding, and uses the Faiss index to search for the top-k nodes. It also has a logging feature to print out the top-k nodes and their similarity scores. The purpose of the code is to provide a query for GPTFaissIndex that uses embedding similarity to retrieve the top-k nodes.", "doc_id": "dd9e0f21-b9bf-4838-bda3-7e771ee24c8a", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Default query for GPTFaissIndex.\"\"\"\n\nimport logging\nfrom typing import Any, List, Optional, cast\n\nimport numpy as np\n\nfrom gpt_index.data_structs.data_structs import IndexDict, Node\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.query.vector_store.base import BaseGPTVectorStoreIndexQuery\nfrom gpt_index.indices.utils import truncate_text\n\n\nclass GPTFaissIndexQuery(BaseGPTVectorStoreIndexQuery[IndexDict]):\n    \"\"\"GPTFaissIndex query.\n\n    An embedding-based query for GPTFaissIndex, which queries\n    an underlying Faiss index to retrieve top-k nodes by\n    embedding similarity to the query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 0, "child_indices": [], "ref_doc_id": "1ea526708a54837eb2d2594ce803150877deeec3", "node_info": null}, "1": {"text": "(Optional[RefinePrompt]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        faiss_index (faiss.Index): A Faiss Index object (required)\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        similarity_top_k (int): Number of similar nodes to retrieve.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexDict,\n        faiss_index: Optional[Any] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        similarity_top_k: Optional[int] = 1,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            index_struct=index_struct,\n            embed_model=embed_model,\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 1, "child_indices": [], "ref_doc_id": "1ea526708a54837eb2d2594ce803150877deeec3", "node_info": null}, "2": {"text": "embed_model=embed_model,\n            similarity_top_k=similarity_top_k,\n            **kwargs,\n        )\n        if faiss_index is None:\n            raise ValueError(\"faiss_index cannot be None.\")\n        # NOTE: cast to Any for now\n        self._faiss_index = cast(Any, faiss_index)\n        self._faiss_index = faiss_index\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n        query_embedding_np = np.array(query_embedding, dtype=\"float32\")[np.newaxis, :]\n        dists, indices = self._faiss_index.search(\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 2, "child_indices": [], "ref_doc_id": "1ea526708a54837eb2d2594ce803150877deeec3", "node_info": null}, "3": {"text": "indices = self._faiss_index.search(\n            query_embedding_np, self.similarity_top_k\n        )\n        dists = [d[0] for d in dists]\n        # if empty, then return an empty response\n        if len(indices) == 0:\n            return []\n\n        # returned dimension is 1 x k\n        node_idxs = list([str(i) for i in indices[0]])\n        top_k_nodes = self._index_struct.get_nodes(node_idxs)\n\n        if similarity_tracker is not None:\n            for node, similarity in zip(top_k_nodes, dists):\n                similarity_tracker.add(node, similarity)\n\n        if logging.getLogger(__name__).getEffectiveLevel() == logging.DEBUG:\n            fmt_txts = []\n            for node_idx, node_similarity, node in", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 3, "child_indices": [], "ref_doc_id": "1ea526708a54837eb2d2594ce803150877deeec3", "node_info": null}, "4": {"text": "       for node_idx, node_similarity, node in zip(node_idxs, dists, top_k_nodes):\n                fmt_txt = f\"> [Node {node_idx}] [Similarity score: \\\n                    {float(node_similarity):.6}] {truncate_text(node.get_text(), 100)}\"\n                fmt_txts.append(fmt_txt)\n            top_k_node_text = \"\\n\".join(fmt_txts)\n            logging.debug(f\"> Top {len(top_k_nodes)} nodes:\\n{top_k_node_text}\")\n\n        return top_k_nodes\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/faiss.py", "file_name": "faiss.py"}, "index": 4, "child_indices": [], "ref_doc_id": "1ea526708a54837eb2d2594ce803150877deeec3", "node_info": null}, "5": {"text": "GPTFaissIndexQuery is a query class for GPTFaissIndex, which uses an underlying Faiss index to retrieve top-k nodes by embedding similarity to the query. It takes in a Faiss Index object, an Embedding model, and a number of similar nodes to retrieve as parameters. It then uses the embedding model to get the query embedding, and uses the Faiss index to search for the top-k nodes. It also has a logging feature to print out the top-k nodes and their similarity scores.", "doc_id": null, "embedding": null, "extra_info": null, "index": 5, "child_indices": [0, 1, 2, 3, 4], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"5": {"text": "GPTFaissIndexQuery is a query class for GPTFaissIndex, which uses an underlying Faiss index to retrieve top-k nodes by embedding similarity to the query. It takes in a Faiss Index object, an Embedding model, and a number of similar nodes to retrieve as parameters. It then uses the embedding model to get the query embedding, and uses the Faiss index to search for the top-k nodes. It also has a logging feature to print out the top-k nodes and their similarity scores.", "doc_id": null, "embedding": null, "extra_info": null, "index": 5, "child_indices": [0, 1, 2, 3, 4], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}