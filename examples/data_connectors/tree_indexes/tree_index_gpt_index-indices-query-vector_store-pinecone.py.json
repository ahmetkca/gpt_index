{"index_struct": {"text": "\nGPTPineconeIndexQuery is a class that provides an efficient way to query an underlying Pinecone index and retrieve the top-k nodes by embedding similarity to the query. It takes in a text_qa_template, refine_template, pinecone_index, embed_model, similarity_top_k, and pinecone_kwargs as parameters. The embed_model is used to get a query embedding from the query string, and the pinecone_index is used to query the embedding and retrieve the top-k nodes. The class also has a logging feature to print out the top-k nodes and their similarity scores. The purpose of this code is to provide an efficient way to query an underlying Pinecone index and retrieve the top-k nodes by embedding similarity to the query.", "doc_id": "ede3d1ec-03ef-4f34-8e12-ab9829ab5deb", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Pinecone vector store index query.\"\"\"\nimport logging\nfrom typing import Any, Dict, List, Optional, cast\n\nfrom gpt_index.data_structs.data_structs import IndexDict, Node\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.query.vector_store.base import BaseGPTVectorStoreIndexQuery\nfrom gpt_index.indices.utils import truncate_text\n\n\nclass GPTPineconeIndexQuery(BaseGPTVectorStoreIndexQuery[IndexDict]):\n    \"\"\"GPTPineconeIndex query.\n\n    An embedding-based query for GPTPineconeIndex, which queries\n    an undelrying Pinecone index to retrieve top-k nodes by\n    embedding similarity to the query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 0, "child_indices": [], "ref_doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "node_info": null}, "1": {"text": "Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        pinecone_index (pinecone.Index): A Pinecone Index object (required)\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        similarity_top_k (int): Number of similar nodes to retrieve.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexDict,\n        pinecone_index: Optional[Any] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        similarity_top_k: Optional[int] = 1,\n        pinecone_kwargs: Optional[Dict] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            index_struct=index_struct,\n            embed_model=embed_model,\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 1, "child_indices": [], "ref_doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "node_info": null}, "2": {"text": "          embed_model=embed_model,\n            similarity_top_k=similarity_top_k,\n            **kwargs,\n        )\n        if pinecone_index is None:\n            raise ValueError(\"pinecone_index cannot be None.\")\n        # NOTE: cast to Any for now\n        self._pinecone_index = cast(Any, pinecone_index)\n        self._pinecone_index = pinecone_index\n\n        self._pinecone_kwargs = pinecone_kwargs or {}\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n\n        response = self._pinecone_index.query(\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 2, "child_indices": [], "ref_doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "node_info": null}, "3": {"text": "           query_embedding,\n            top_k=self.similarity_top_k,\n            include_values=True,\n            include_metadata=True,\n            **self._pinecone_kwargs,\n        )\n\n        top_k_nodes = []\n        top_k_ids = []\n        top_k_scores = []\n        for match in response.matches:\n            text = match.metadata[\"text\"]\n            node = Node(text=text, extra_info=match.metadata)\n            top_k_ids.append(match.id)\n            top_k_nodes.append(node)\n            top_k_scores.append(match.score)\n            if similarity_tracker is not None:\n                similarity_tracker.add(node, match.score)\n\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 3, "child_indices": [], "ref_doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "node_info": null}, "4": {"text": "    similarity_tracker.add(node, match.score)\n\n        if logging.getLogger(__name__).getEffectiveLevel() == logging.DEBUG:\n            fmt_txts = []\n            for node_idx, node_similarity, node in zip(\n                top_k_ids, top_k_scores, top_k_nodes\n            ):\n                fmt_txt = f\"> [Node {node_idx}] [Similarity score: \\\n                    {node_similarity:.6}] {truncate_text(node.get_text(), 100)}\"\n                fmt_txts.append(fmt_txt)\n            top_k_node_text = \"\\n\".join(fmt_txts)\n            logging.debug(f\"> Top {len(top_k_nodes)} nodes:\\n{top_k_node_text}\")\n\n        return", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 4, "child_indices": [], "ref_doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "node_info": null}, "5": {"text": "       return top_k_nodes\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 5, "child_indices": [], "ref_doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "node_info": null}, "6": {"text": "GPTPineconeIndexQuery is a class that provides an embedding-based query for GPTPineconeIndex, which queries an underlying Pinecone index to retrieve top-k nodes by embedding similarity to the query. It takes in a text_qa_template, refine_template, pinecone_index, embed_model, similarity_top_k, and pinecone_kwargs as parameters. It then uses the embed_model to get a query embedding from the query string, and uses the pinecone_index to query the embedding and retrieve the top-k nodes. It also has a logging feature to print out the top-k nodes and their similarity scores.", "doc_id": null, "embedding": null, "extra_info": null, "index": 6, "child_indices": [0, 1, 2, 3, 4, 5], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"6": {"text": "GPTPineconeIndexQuery is a class that provides an embedding-based query for GPTPineconeIndex, which queries an underlying Pinecone index to retrieve top-k nodes by embedding similarity to the query. It takes in a text_qa_template, refine_template, pinecone_index, embed_model, similarity_top_k, and pinecone_kwargs as parameters. It then uses the embed_model to get a query embedding from the query string, and uses the pinecone_index to query the embedding and retrieve the top-k nodes. It also has a logging feature to print out the top-k nodes and their similarity scores.", "doc_id": null, "embedding": null, "extra_info": null, "index": 6, "child_indices": [0, 1, 2, 3, 4, 5], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"0260639a9de8f896a7e32e927fa8f257c442953b": {"text": "\"\"\"Pinecone vector store index query.\"\"\"\nimport logging\nfrom typing import Any, Dict, List, Optional, cast\n\nfrom gpt_index.data_structs.data_structs import IndexDict, Node\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.query.vector_store.base import BaseGPTVectorStoreIndexQuery\nfrom gpt_index.indices.utils import truncate_text\n\n\nclass GPTPineconeIndexQuery(BaseGPTVectorStoreIndexQuery[IndexDict]):\n    \"\"\"GPTPineconeIndex query.\n\n    An embedding-based query for GPTPineconeIndex, which queries\n    an undelrying Pinecone index to retrieve top-k nodes by\n    embedding similarity to the query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        pinecone_index (pinecone.Index): A Pinecone Index object (required)\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        similarity_top_k (int): Number of similar nodes to retrieve.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexDict,\n        pinecone_index: Optional[Any] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        similarity_top_k: Optional[int] = 1,\n        pinecone_kwargs: Optional[Dict] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            index_struct=index_struct,\n            embed_model=embed_model,\n            similarity_top_k=similarity_top_k,\n            **kwargs,\n        )\n        if pinecone_index is None:\n            raise ValueError(\"pinecone_index cannot be None.\")\n        # NOTE: cast to Any for now\n        self._pinecone_index = cast(Any, pinecone_index)\n        self._pinecone_index = pinecone_index\n\n        self._pinecone_kwargs = pinecone_kwargs or {}\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n\n        response = self._pinecone_index.query(\n            query_embedding,\n            top_k=self.similarity_top_k,\n            include_values=True,\n            include_metadata=True,\n            **self._pinecone_kwargs,\n        )\n\n        top_k_nodes = []\n        top_k_ids = []\n        top_k_scores = []\n        for match in response.matches:\n            text = match.metadata[\"text\"]\n            node = Node(text=text, extra_info=match.metadata)\n            top_k_ids.append(match.id)\n            top_k_nodes.append(node)\n            top_k_scores.append(match.score)\n            if similarity_tracker is not None:\n                similarity_tracker.add(node, match.score)\n\n        if logging.getLogger(__name__).getEffectiveLevel() == logging.DEBUG:\n            fmt_txts = []\n            for node_idx, node_similarity, node in zip(\n                top_k_ids, top_k_scores, top_k_nodes\n            ):\n                fmt_txt = f\"> [Node {node_idx}] [Similarity score: \\\n                    {node_similarity:.6}] {truncate_text(node.get_text(), 100)}\"\n                fmt_txts.append(fmt_txt)\n            top_k_node_text = \"\\n\".join(fmt_txts)\n            logging.debug(f\"> Top {len(top_k_nodes)} nodes:\\n{top_k_node_text}\")\n\n        return top_k_nodes\n", "doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "__type__": "Document"}, "ede3d1ec-03ef-4f34-8e12-ab9829ab5deb": {"text": "\nGPTPineconeIndexQuery is a class that provides an efficient way to query an underlying Pinecone index and retrieve the top-k nodes by embedding similarity to the query. It takes in a text_qa_template, refine_template, pinecone_index, embed_model, similarity_top_k, and pinecone_kwargs as parameters. The embed_model is used to get a query embedding from the query string, and the pinecone_index is used to query the embedding and retrieve the top-k nodes. The class also has a logging feature to print out the top-k nodes and their similarity scores. The purpose of this code is to provide an efficient way to query an underlying Pinecone index and retrieve the top-k nodes by embedding similarity to the query.", "doc_id": "ede3d1ec-03ef-4f34-8e12-ab9829ab5deb", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Pinecone vector store index query.\"\"\"\nimport logging\nfrom typing import Any, Dict, List, Optional, cast\n\nfrom gpt_index.data_structs.data_structs import IndexDict, Node\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.query.vector_store.base import BaseGPTVectorStoreIndexQuery\nfrom gpt_index.indices.utils import truncate_text\n\n\nclass GPTPineconeIndexQuery(BaseGPTVectorStoreIndexQuery[IndexDict]):\n    \"\"\"GPTPineconeIndex query.\n\n    An embedding-based query for GPTPineconeIndex, which queries\n    an undelrying Pinecone index to retrieve top-k nodes by\n    embedding similarity to the query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 0, "child_indices": [], "ref_doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "node_info": null}, "1": {"text": "Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        pinecone_index (pinecone.Index): A Pinecone Index object (required)\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        similarity_top_k (int): Number of similar nodes to retrieve.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexDict,\n        pinecone_index: Optional[Any] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        similarity_top_k: Optional[int] = 1,\n        pinecone_kwargs: Optional[Dict] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            index_struct=index_struct,\n            embed_model=embed_model,\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 1, "child_indices": [], "ref_doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "node_info": null}, "2": {"text": "          embed_model=embed_model,\n            similarity_top_k=similarity_top_k,\n            **kwargs,\n        )\n        if pinecone_index is None:\n            raise ValueError(\"pinecone_index cannot be None.\")\n        # NOTE: cast to Any for now\n        self._pinecone_index = cast(Any, pinecone_index)\n        self._pinecone_index = pinecone_index\n\n        self._pinecone_kwargs = pinecone_kwargs or {}\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n\n        response = self._pinecone_index.query(\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 2, "child_indices": [], "ref_doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "node_info": null}, "3": {"text": "           query_embedding,\n            top_k=self.similarity_top_k,\n            include_values=True,\n            include_metadata=True,\n            **self._pinecone_kwargs,\n        )\n\n        top_k_nodes = []\n        top_k_ids = []\n        top_k_scores = []\n        for match in response.matches:\n            text = match.metadata[\"text\"]\n            node = Node(text=text, extra_info=match.metadata)\n            top_k_ids.append(match.id)\n            top_k_nodes.append(node)\n            top_k_scores.append(match.score)\n            if similarity_tracker is not None:\n                similarity_tracker.add(node, match.score)\n\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 3, "child_indices": [], "ref_doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "node_info": null}, "4": {"text": "    similarity_tracker.add(node, match.score)\n\n        if logging.getLogger(__name__).getEffectiveLevel() == logging.DEBUG:\n            fmt_txts = []\n            for node_idx, node_similarity, node in zip(\n                top_k_ids, top_k_scores, top_k_nodes\n            ):\n                fmt_txt = f\"> [Node {node_idx}] [Similarity score: \\\n                    {node_similarity:.6}] {truncate_text(node.get_text(), 100)}\"\n                fmt_txts.append(fmt_txt)\n            top_k_node_text = \"\\n\".join(fmt_txts)\n            logging.debug(f\"> Top {len(top_k_nodes)} nodes:\\n{top_k_node_text}\")\n\n        return", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 4, "child_indices": [], "ref_doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "node_info": null}, "5": {"text": "       return top_k_nodes\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 5, "child_indices": [], "ref_doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "node_info": null}, "6": {"text": "GPTPineconeIndexQuery is a class that provides an embedding-based query for GPTPineconeIndex, which queries an underlying Pinecone index to retrieve top-k nodes by embedding similarity to the query. It takes in a text_qa_template, refine_template, pinecone_index, embed_model, similarity_top_k, and pinecone_kwargs as parameters. It then uses the embed_model to get a query embedding from the query string, and uses the pinecone_index to query the embedding and retrieve the top-k nodes. It also has a logging feature to print out the top-k nodes and their similarity scores.", "doc_id": null, "embedding": null, "extra_info": null, "index": 6, "child_indices": [0, 1, 2, 3, 4, 5], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"6": {"text": "GPTPineconeIndexQuery is a class that provides an embedding-based query for GPTPineconeIndex, which queries an underlying Pinecone index to retrieve top-k nodes by embedding similarity to the query. It takes in a text_qa_template, refine_template, pinecone_index, embed_model, similarity_top_k, and pinecone_kwargs as parameters. It then uses the embed_model to get a query embedding from the query string, and uses the pinecone_index to query the embedding and retrieve the top-k nodes. It also has a logging feature to print out the top-k nodes and their similarity scores.", "doc_id": null, "embedding": null, "extra_info": null, "index": 6, "child_indices": [0, 1, 2, 3, 4, 5], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}