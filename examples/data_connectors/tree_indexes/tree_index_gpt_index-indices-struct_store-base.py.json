{"index_struct": {"text": "\nBaseGPTStructStoreIndex is a code file that provides methods for extracting relevant information from unstructured text and inserting documents into an index. It uses a SchemaExtractPrompt and a LLMPredictor to extract the relevant information, and validates the fields with a col_types_map. The index is built by splitting the text into chunks and extracting the relevant information from each chunk. The extracted information is then validated and inserted into the index. The code file also provides a default output parser for parsing the output of schema extraction. The purpose of the code is to provide a base class for GPT Struct Store Index, which allows for efficient storage and retrieval of documents.", "doc_id": "b6b9f3ae-253a-4fa9-82cd-792504ef0e3e", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Struct store.\"\"\"\n\nimport logging\nimport re\nfrom abc import abstractmethod\nfrom typing import Any, Callable, Dict, Generic, Optional, Sequence, TypeVar\n\nfrom gpt_index.data_structs.table import BaseStructTable, StructDatapoint\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.utils import truncate_text\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_SCHEMA_EXTRACT_PROMPT\nfrom gpt_index.prompts.prompts import SchemaExtractPrompt\nfrom gpt_index.schema import BaseDocument\n\nBST = TypeVar(\"BST\", bound=BaseStructTable)\n\n\ndef default_output_parser(output: str) -> Optional[Dict[str, Any]]:\n    \"\"\"Parse output of schema extraction.\n\n    Attempt to parse the following format from the default prompt:\n    field1: <value>, field2: <value>, ...\n\n    \"\"\"\n    tups = output.split(\"\\n\")\n\n    fields = {}\n    for tup in tups:\n        if", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 0, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "1": {"text": "   for tup in tups:\n        if \":\" in tup:\n            tokens = tup.split(\":\")\n            field = re.sub(r\"\\W+\", \"\", tokens[0])\n            value = re.sub(r\"\\W+\", \"\", tokens[1])\n            fields[field] = value\n    return fields\n\n\nOUTPUT_PARSER_TYPE = Callable[[str], Optional[Dict[str, Any]]]\n\n\nclass BaseGPTStructStoreIndex(BaseGPTIndex[BST], Generic[BST]):\n    \"\"\"Base GPT Struct Store Index.\"\"\"\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[BST] = None,\n        schema_extract_prompt: Optional[SchemaExtractPrompt] = None,\n        output_parser: Optional[OUTPUT_PARSER_TYPE] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "2": {"text": "Optional[LLMPredictor] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self.schema_extract_prompt = (\n            schema_extract_prompt or DEFAULT_SCHEMA_EXTRACT_PROMPT\n        )\n        self.output_parser = output_parser or default_output_parser\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            **kwargs,\n        )\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.schema_extract_prompt, 1\n        )\n\n    @abstractmethod\n    def _insert_datapoint(self, datapoint: StructDatapoint) -> None:\n        \"\"\"Insert", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "3": {"text": "StructDatapoint) -> None:\n        \"\"\"Insert datapoint into index.\"\"\"\n\n    @abstractmethod\n    def _get_col_types_map(self) -> Dict[str, type]:\n        \"\"\"Get col types map for schema.\"\"\"\n\n    def _clean_and_validate_fields(self, fields: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate fields with col_types_map.\"\"\"\n        new_fields = {}\n        col_types_map = self._get_col_types_map()\n        for field, value in fields.items():\n            clean_value = value\n            if field not in col_types_map:\n                continue\n            # if expected type is int or float, try to convert value to int or float\n            expected_type = col_types_map[field]\n            if expected_type == int:\n                try:\n                ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "4": {"text": " try:\n                    clean_value = int(value)\n                except ValueError:\n                    continue\n            elif expected_type == float:\n                try:\n                    clean_value = float(value)\n                except ValueError:\n                    continue\n            else:\n                if len(value) == 0:\n                    continue\n                if not isinstance(value, col_types_map[field]):\n                    continue\n            new_fields[field] = clean_value\n        return new_fields\n\n    @abstractmethod\n    def _get_schema_text(self)", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 4, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "5": {"text": " @abstractmethod\n    def _get_schema_text(self) -> str:\n        \"\"\"Get schema text for extracting relevant info from unstructured text.\"\"\"\n\n    def _add_document_to_index(\n        self,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        text_chunks = text_splitter.split_text(document.get_text())\n        fields = {}\n        for i, text_chunk in enumerate(text_chunks):\n            fmt_text_chunk = truncate_text(text_chunk, 50)\n            logging.info(f\"> Adding chunk {i}: {fmt_text_chunk}\")\n            # if embedding specified in document, pass it to the Node\n            schema_text = self._get_schema_text()\n            response_str, _ = self._llm_predictor.predict(\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 5, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "6": {"text": "= self._llm_predictor.predict(\n                self.schema_extract_prompt,\n                text=text_chunk,\n                schema=schema_text,\n            )\n            cur_fields = self.output_parser(response_str)\n            if cur_fields is None:\n                continue\n            # validate fields with col_types_map\n            new_cur_fields = self._clean_and_validate_fields(cur_fields)\n            fields.update(new_cur_fields)\n\n        struct_datapoint = StructDatapoint(fields)\n        if struct_datapoint is not None:\n            self._insert_datapoint(struct_datapoint)\n            logging.debug(f\"> Added datapoint: {fields}\")\n\n    def", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 6, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "7": {"text": "Added datapoint: {fields}\")\n\n    def _build_index_from_documents(self, documents: Sequence[BaseDocument]) -> BST:\n        \"\"\"Build index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.schema_extract_prompt, 1\n        )\n        index_struct = self.index_struct_cls()\n        for d in documents:\n            self._add_document_to_index(d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(document, self._text_splitter)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        raise NotImplementedError(\"Delete not implemented for Struct Store Index.\")\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 7, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "8": {"text": "BaseGPTStructStoreIndex is a base class for GPT Struct Store Index. It provides methods for inserting documents into the index, validating fields, and building the index from documents. It also provides a default output parser for parsing the output of schema extraction. The class uses a SchemaExtractPrompt and a LLMPredictor to extract relevant information from unstructured text.", "doc_id": null, "embedding": null, "extra_info": null, "index": 8, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"8": {"text": "BaseGPTStructStoreIndex is a base class for GPT Struct Store Index. It provides methods for inserting documents into the index, validating fields, and building the index from documents. It also provides a default output parser for parsing the output of schema extraction. The class uses a SchemaExtractPrompt and a LLMPredictor to extract relevant information from unstructured text.", "doc_id": null, "embedding": null, "extra_info": null, "index": 8, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"ed3819ef94005896d68caab2ee933ca64cb2f62c": {"text": "\"\"\"Struct store.\"\"\"\n\nimport logging\nimport re\nfrom abc import abstractmethod\nfrom typing import Any, Callable, Dict, Generic, Optional, Sequence, TypeVar\n\nfrom gpt_index.data_structs.table import BaseStructTable, StructDatapoint\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.utils import truncate_text\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_SCHEMA_EXTRACT_PROMPT\nfrom gpt_index.prompts.prompts import SchemaExtractPrompt\nfrom gpt_index.schema import BaseDocument\n\nBST = TypeVar(\"BST\", bound=BaseStructTable)\n\n\ndef default_output_parser(output: str) -> Optional[Dict[str, Any]]:\n    \"\"\"Parse output of schema extraction.\n\n    Attempt to parse the following format from the default prompt:\n    field1: <value>, field2: <value>, ...\n\n    \"\"\"\n    tups = output.split(\"\\n\")\n\n    fields = {}\n    for tup in tups:\n        if \":\" in tup:\n            tokens = tup.split(\":\")\n            field = re.sub(r\"\\W+\", \"\", tokens[0])\n            value = re.sub(r\"\\W+\", \"\", tokens[1])\n            fields[field] = value\n    return fields\n\n\nOUTPUT_PARSER_TYPE = Callable[[str], Optional[Dict[str, Any]]]\n\n\nclass BaseGPTStructStoreIndex(BaseGPTIndex[BST], Generic[BST]):\n    \"\"\"Base GPT Struct Store Index.\"\"\"\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[BST] = None,\n        schema_extract_prompt: Optional[SchemaExtractPrompt] = None,\n        output_parser: Optional[OUTPUT_PARSER_TYPE] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self.schema_extract_prompt = (\n            schema_extract_prompt or DEFAULT_SCHEMA_EXTRACT_PROMPT\n        )\n        self.output_parser = output_parser or default_output_parser\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            **kwargs,\n        )\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.schema_extract_prompt, 1\n        )\n\n    @abstractmethod\n    def _insert_datapoint(self, datapoint: StructDatapoint) -> None:\n        \"\"\"Insert datapoint into index.\"\"\"\n\n    @abstractmethod\n    def _get_col_types_map(self) -> Dict[str, type]:\n        \"\"\"Get col types map for schema.\"\"\"\n\n    def _clean_and_validate_fields(self, fields: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate fields with col_types_map.\"\"\"\n        new_fields = {}\n        col_types_map = self._get_col_types_map()\n        for field, value in fields.items():\n            clean_value = value\n            if field not in col_types_map:\n                continue\n            # if expected type is int or float, try to convert value to int or float\n            expected_type = col_types_map[field]\n            if expected_type == int:\n                try:\n                    clean_value = int(value)\n                except ValueError:\n                    continue\n            elif expected_type == float:\n                try:\n                    clean_value = float(value)\n                except ValueError:\n                    continue\n            else:\n                if len(value) == 0:\n                    continue\n                if not isinstance(value, col_types_map[field]):\n                    continue\n            new_fields[field] = clean_value\n        return new_fields\n\n    @abstractmethod\n    def _get_schema_text(self) -> str:\n        \"\"\"Get schema text for extracting relevant info from unstructured text.\"\"\"\n\n    def _add_document_to_index(\n        self,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        text_chunks = text_splitter.split_text(document.get_text())\n        fields = {}\n        for i, text_chunk in enumerate(text_chunks):\n            fmt_text_chunk = truncate_text(text_chunk, 50)\n            logging.info(f\"> Adding chunk {i}: {fmt_text_chunk}\")\n            # if embedding specified in document, pass it to the Node\n            schema_text = self._get_schema_text()\n            response_str, _ = self._llm_predictor.predict(\n                self.schema_extract_prompt,\n                text=text_chunk,\n                schema=schema_text,\n            )\n            cur_fields = self.output_parser(response_str)\n            if cur_fields is None:\n                continue\n            # validate fields with col_types_map\n            new_cur_fields = self._clean_and_validate_fields(cur_fields)\n            fields.update(new_cur_fields)\n\n        struct_datapoint = StructDatapoint(fields)\n        if struct_datapoint is not None:\n            self._insert_datapoint(struct_datapoint)\n            logging.debug(f\"> Added datapoint: {fields}\")\n\n    def _build_index_from_documents(self, documents: Sequence[BaseDocument]) -> BST:\n        \"\"\"Build index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.schema_extract_prompt, 1\n        )\n        index_struct = self.index_struct_cls()\n        for d in documents:\n            self._add_document_to_index(d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(document, self._text_splitter)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        raise NotImplementedError(\"Delete not implemented for Struct Store Index.\")\n", "doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "__type__": "Document"}, "b6b9f3ae-253a-4fa9-82cd-792504ef0e3e": {"text": "\nBaseGPTStructStoreIndex is a code file that provides methods for extracting relevant information from unstructured text and inserting documents into an index. It uses a SchemaExtractPrompt and a LLMPredictor to extract the relevant information, and validates the fields with a col_types_map. The index is built by splitting the text into chunks and extracting the relevant information from each chunk. The extracted information is then validated and inserted into the index. The code file also provides a default output parser for parsing the output of schema extraction. The purpose of the code is to provide a base class for GPT Struct Store Index, which allows for efficient storage and retrieval of documents.", "doc_id": "b6b9f3ae-253a-4fa9-82cd-792504ef0e3e", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Struct store.\"\"\"\n\nimport logging\nimport re\nfrom abc import abstractmethod\nfrom typing import Any, Callable, Dict, Generic, Optional, Sequence, TypeVar\n\nfrom gpt_index.data_structs.table import BaseStructTable, StructDatapoint\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.utils import truncate_text\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_SCHEMA_EXTRACT_PROMPT\nfrom gpt_index.prompts.prompts import SchemaExtractPrompt\nfrom gpt_index.schema import BaseDocument\n\nBST = TypeVar(\"BST\", bound=BaseStructTable)\n\n\ndef default_output_parser(output: str) -> Optional[Dict[str, Any]]:\n    \"\"\"Parse output of schema extraction.\n\n    Attempt to parse the following format from the default prompt:\n    field1: <value>, field2: <value>, ...\n\n    \"\"\"\n    tups = output.split(\"\\n\")\n\n    fields = {}\n    for tup in tups:\n        if", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 0, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "1": {"text": "   for tup in tups:\n        if \":\" in tup:\n            tokens = tup.split(\":\")\n            field = re.sub(r\"\\W+\", \"\", tokens[0])\n            value = re.sub(r\"\\W+\", \"\", tokens[1])\n            fields[field] = value\n    return fields\n\n\nOUTPUT_PARSER_TYPE = Callable[[str], Optional[Dict[str, Any]]]\n\n\nclass BaseGPTStructStoreIndex(BaseGPTIndex[BST], Generic[BST]):\n    \"\"\"Base GPT Struct Store Index.\"\"\"\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[BST] = None,\n        schema_extract_prompt: Optional[SchemaExtractPrompt] = None,\n        output_parser: Optional[OUTPUT_PARSER_TYPE] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 1, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "2": {"text": "Optional[LLMPredictor] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self.schema_extract_prompt = (\n            schema_extract_prompt or DEFAULT_SCHEMA_EXTRACT_PROMPT\n        )\n        self.output_parser = output_parser or default_output_parser\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            **kwargs,\n        )\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.schema_extract_prompt, 1\n        )\n\n    @abstractmethod\n    def _insert_datapoint(self, datapoint: StructDatapoint) -> None:\n        \"\"\"Insert", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 2, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "3": {"text": "StructDatapoint) -> None:\n        \"\"\"Insert datapoint into index.\"\"\"\n\n    @abstractmethod\n    def _get_col_types_map(self) -> Dict[str, type]:\n        \"\"\"Get col types map for schema.\"\"\"\n\n    def _clean_and_validate_fields(self, fields: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate fields with col_types_map.\"\"\"\n        new_fields = {}\n        col_types_map = self._get_col_types_map()\n        for field, value in fields.items():\n            clean_value = value\n            if field not in col_types_map:\n                continue\n            # if expected type is int or float, try to convert value to int or float\n            expected_type = col_types_map[field]\n            if expected_type == int:\n                try:\n                ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 3, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "4": {"text": " try:\n                    clean_value = int(value)\n                except ValueError:\n                    continue\n            elif expected_type == float:\n                try:\n                    clean_value = float(value)\n                except ValueError:\n                    continue\n            else:\n                if len(value) == 0:\n                    continue\n                if not isinstance(value, col_types_map[field]):\n                    continue\n            new_fields[field] = clean_value\n        return new_fields\n\n    @abstractmethod\n    def _get_schema_text(self)", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 4, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "5": {"text": " @abstractmethod\n    def _get_schema_text(self) -> str:\n        \"\"\"Get schema text for extracting relevant info from unstructured text.\"\"\"\n\n    def _add_document_to_index(\n        self,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        text_chunks = text_splitter.split_text(document.get_text())\n        fields = {}\n        for i, text_chunk in enumerate(text_chunks):\n            fmt_text_chunk = truncate_text(text_chunk, 50)\n            logging.info(f\"> Adding chunk {i}: {fmt_text_chunk}\")\n            # if embedding specified in document, pass it to the Node\n            schema_text = self._get_schema_text()\n            response_str, _ = self._llm_predictor.predict(\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 5, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "6": {"text": "= self._llm_predictor.predict(\n                self.schema_extract_prompt,\n                text=text_chunk,\n                schema=schema_text,\n            )\n            cur_fields = self.output_parser(response_str)\n            if cur_fields is None:\n                continue\n            # validate fields with col_types_map\n            new_cur_fields = self._clean_and_validate_fields(cur_fields)\n            fields.update(new_cur_fields)\n\n        struct_datapoint = StructDatapoint(fields)\n        if struct_datapoint is not None:\n            self._insert_datapoint(struct_datapoint)\n            logging.debug(f\"> Added datapoint: {fields}\")\n\n    def", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 6, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "7": {"text": "Added datapoint: {fields}\")\n\n    def _build_index_from_documents(self, documents: Sequence[BaseDocument]) -> BST:\n        \"\"\"Build index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.schema_extract_prompt, 1\n        )\n        index_struct = self.index_struct_cls()\n        for d in documents:\n            self._add_document_to_index(d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(document, self._text_splitter)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        raise NotImplementedError(\"Delete not implemented for Struct Store Index.\")\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "index": 7, "child_indices": [], "ref_doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "node_info": null}, "8": {"text": "BaseGPTStructStoreIndex is a base class for GPT Struct Store Index. It provides methods for inserting documents into the index, validating fields, and building the index from documents. It also provides a default output parser for parsing the output of schema extraction. The class uses a SchemaExtractPrompt and a LLMPredictor to extract relevant information from unstructured text.", "doc_id": null, "embedding": null, "extra_info": null, "index": 8, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"8": {"text": "BaseGPTStructStoreIndex is a base class for GPT Struct Store Index. It provides methods for inserting documents into the index, validating fields, and building the index from documents. It also provides a default output parser for parsing the output of schema extraction. The class uses a SchemaExtractPrompt and a LLMPredictor to extract relevant information from unstructured text.", "doc_id": null, "embedding": null, "extra_info": null, "index": 8, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}