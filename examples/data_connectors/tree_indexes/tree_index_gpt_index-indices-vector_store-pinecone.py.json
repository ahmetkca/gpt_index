{"index_struct": {"text": "\nThe GPTPineconeIndex code file provides a data structure that uses an existing vector store to store nodes keyed by embeddings. It allows for efficient retrieval of documents based on their embeddings. The code initializes parameters, gets query map, adds documents to the index, builds the index from documents, inserts documents, and preprocesses queries. During index construction, documents are chunked up, converted to nodes with text, and encoded in document embeddings stored within Pinecone. During query time, the index uses Pinecone to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes. The purpose of the code is to create an index that can quickly and accurately retrieve documents based on their embeddings.", "doc_id": "9f865f7a-c52a-4d90-be14-fe3a56d80ed8", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Pinecone Vector store index.\n\nAn index that that is built on top of an existing vector store.\n\n\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence, Type, cast\n\nfrom gpt_index.data_structs.data_structs import PineconeIndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.vector_store.pinecone import GPTPineconeIndexQuery\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.schema import BaseDocument\nfrom gpt_index.utils import get_new_id\n\n\nclass GPTPineconeIndex(BaseGPTIndex[PineconeIndexStruct]):\n    \"\"\"GPT Pinecone Index.\n\n    The GPTPineconeIndex is a data structure where nodes are keyed by\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 0, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "1": {"text": "is a data structure where nodes are keyed by\n    embeddings, and those embeddings are stored within a Pinecone index.\n    During index construction, the document texts are chunked up,\n    converted to nodes with text; they are then encoded in\n    document embeddings stored within Pinecone.\n\n    During query time, the index uses Pinecone to query for the top\n    k most similar nodes, and synthesizes an answer from the\n    retrieved nodes.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        chunk_size_limit (int): Maximum number of tokens per chunk. NOTE:\n            in Pinecone the default is 2048 due to metadata size restrictions.\n    \"\"\"\n\n    index_struct_cls = PineconeIndexStruct\n\n    def __init__(\n        self,\n        documents:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 1, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "2": {"text": "       self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[PineconeIndexStruct] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        pinecone_index: Optional[Any] = None,\n        chunk_size_limit: int = 2048,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        import_err_msg = (\n            \"`pinecone` package not found, please run `pip install pinecone-client`\"\n        )\n        try:\n            import pinecone  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n        self._pinecone_index =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 2, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "3": {"text": "       self._pinecone_index = cast(pinecone.Index, pinecone_index)\n\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            embed_model=embed_model,\n            chunk_size_limit=chunk_size_limit,\n            **kwargs,\n        )\n        # NOTE: when building the vector store index, text_qa_template is not partially\n        # formatted because we don't know the query ahead of time.\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n\n    @classmethod\n    def get_query_map(self) ->", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 3, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "4": {"text": "   @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTPineconeIndexQuery,\n            QueryMode.EMBEDDING: GPTPineconeIndexQuery,\n        }\n\n    def _add_document_to_index(\n        self,\n        index_struct: PineconeIndexStruct,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        nodes = self._get_nodes_from_document(document, text_splitter)\n        for n in nodes:\n            if n.embedding is None:\n                text_embedding = self._embed_model.get_text_embedding(n.get_text())\n            else:\n            ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 4, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "5": {"text": "     else:\n                text_embedding = n.embedding\n\n            while True:\n                new_id = get_new_id(set())\n                result = self._pinecone_index.fetch([new_id])\n                if len(result[\"vectors\"]) == 0:\n                    break\n\n            metadata = {\n                \"text\": n.get_text(),\n                \"doc_id\": document.get_doc_id(),\n            }\n\n            self._pinecone_index.upsert([(new_id, text_embedding, metadata)])\n\n    def _build_index_from_documents(\n        self, documents: Sequence[BaseDocument]\n    ) -> PineconeIndexStruct:\n        \"\"\"Build index from documents.\"\"\"\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 5, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "6": {"text": "    \"\"\"Build index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n        index_struct = self.index_struct_cls()\n        for d in documents:\n            self._add_document_to_index(index_struct, d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(self._index_struct, document, self._text_splitter)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        # delete by filtering on the doc_id metadata\n        self._pinecone_index.delete(filter={\"doc_id\": {\"$eq\": doc_id}})\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Any)", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 6, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "7": {"text": "_preprocess_query(self, mode: QueryMode, query_kwargs: Any) -> None:\n        \"\"\"Query mode to class.\"\"\"\n        super()._preprocess_query(mode, query_kwargs)\n        # pass along pinecone client and info\n        query_kwargs[\"pinecone_index\"] = self._pinecone_index\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 7, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "8": {"text": "The GPTPineconeIndex is a data structure that uses an existing vector store to store nodes keyed by embeddings. During index construction, documents are chunked up, converted to nodes with text, and encoded in document embeddings stored within Pinecone. During query time, the index uses Pinecone to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes. The GPTPineconeIndex class initializes parameters, gets query map, adds documents to the index, builds the index from documents, inserts documents, and preprocesses queries.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 8, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"8": {"text": "The GPTPineconeIndex is a data structure that uses an existing vector store to store nodes keyed by embeddings. During index construction, documents are chunked up, converted to nodes with text, and encoded in document embeddings stored within Pinecone. During query time, the index uses Pinecone to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes. The GPTPineconeIndex class initializes parameters, gets query map, adds documents to the index, builds the index from documents, inserts documents, and preprocesses queries.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 8, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"b51addbca1a5a149e748be38f9cfe1d28e5577c3": {"text": "\"\"\"Pinecone Vector store index.\n\nAn index that that is built on top of an existing vector store.\n\n\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence, Type, cast\n\nfrom gpt_index.data_structs.data_structs import PineconeIndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.vector_store.pinecone import GPTPineconeIndexQuery\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.schema import BaseDocument\nfrom gpt_index.utils import get_new_id\n\n\nclass GPTPineconeIndex(BaseGPTIndex[PineconeIndexStruct]):\n    \"\"\"GPT Pinecone Index.\n\n    The GPTPineconeIndex is a data structure where nodes are keyed by\n    embeddings, and those embeddings are stored within a Pinecone index.\n    During index construction, the document texts are chunked up,\n    converted to nodes with text; they are then encoded in\n    document embeddings stored within Pinecone.\n\n    During query time, the index uses Pinecone to query for the top\n    k most similar nodes, and synthesizes an answer from the\n    retrieved nodes.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        chunk_size_limit (int): Maximum number of tokens per chunk. NOTE:\n            in Pinecone the default is 2048 due to metadata size restrictions.\n    \"\"\"\n\n    index_struct_cls = PineconeIndexStruct\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[PineconeIndexStruct] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        pinecone_index: Optional[Any] = None,\n        chunk_size_limit: int = 2048,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        import_err_msg = (\n            \"`pinecone` package not found, please run `pip install pinecone-client`\"\n        )\n        try:\n            import pinecone  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n        self._pinecone_index = cast(pinecone.Index, pinecone_index)\n\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            embed_model=embed_model,\n            chunk_size_limit=chunk_size_limit,\n            **kwargs,\n        )\n        # NOTE: when building the vector store index, text_qa_template is not partially\n        # formatted because we don't know the query ahead of time.\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTPineconeIndexQuery,\n            QueryMode.EMBEDDING: GPTPineconeIndexQuery,\n        }\n\n    def _add_document_to_index(\n        self,\n        index_struct: PineconeIndexStruct,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        nodes = self._get_nodes_from_document(document, text_splitter)\n        for n in nodes:\n            if n.embedding is None:\n                text_embedding = self._embed_model.get_text_embedding(n.get_text())\n            else:\n                text_embedding = n.embedding\n\n            while True:\n                new_id = get_new_id(set())\n                result = self._pinecone_index.fetch([new_id])\n                if len(result[\"vectors\"]) == 0:\n                    break\n\n            metadata = {\n                \"text\": n.get_text(),\n                \"doc_id\": document.get_doc_id(),\n            }\n\n            self._pinecone_index.upsert([(new_id, text_embedding, metadata)])\n\n    def _build_index_from_documents(\n        self, documents: Sequence[BaseDocument]\n    ) -> PineconeIndexStruct:\n        \"\"\"Build index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n        index_struct = self.index_struct_cls()\n        for d in documents:\n            self._add_document_to_index(index_struct, d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(self._index_struct, document, self._text_splitter)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        # delete by filtering on the doc_id metadata\n        self._pinecone_index.delete(filter={\"doc_id\": {\"$eq\": doc_id}})\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Any) -> None:\n        \"\"\"Query mode to class.\"\"\"\n        super()._preprocess_query(mode, query_kwargs)\n        # pass along pinecone client and info\n        query_kwargs[\"pinecone_index\"] = self._pinecone_index\n", "doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "__type__": "Document"}, "9f865f7a-c52a-4d90-be14-fe3a56d80ed8": {"text": "\nThe GPTPineconeIndex code file provides a data structure that uses an existing vector store to store nodes keyed by embeddings. It allows for efficient retrieval of documents based on their embeddings. The code initializes parameters, gets query map, adds documents to the index, builds the index from documents, inserts documents, and preprocesses queries. During index construction, documents are chunked up, converted to nodes with text, and encoded in document embeddings stored within Pinecone. During query time, the index uses Pinecone to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes. The purpose of the code is to create an index that can quickly and accurately retrieve documents based on their embeddings.", "doc_id": "9f865f7a-c52a-4d90-be14-fe3a56d80ed8", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Pinecone Vector store index.\n\nAn index that that is built on top of an existing vector store.\n\n\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence, Type, cast\n\nfrom gpt_index.data_structs.data_structs import PineconeIndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.vector_store.pinecone import GPTPineconeIndexQuery\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.schema import BaseDocument\nfrom gpt_index.utils import get_new_id\n\n\nclass GPTPineconeIndex(BaseGPTIndex[PineconeIndexStruct]):\n    \"\"\"GPT Pinecone Index.\n\n    The GPTPineconeIndex is a data structure where nodes are keyed by\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 0, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "1": {"text": "is a data structure where nodes are keyed by\n    embeddings, and those embeddings are stored within a Pinecone index.\n    During index construction, the document texts are chunked up,\n    converted to nodes with text; they are then encoded in\n    document embeddings stored within Pinecone.\n\n    During query time, the index uses Pinecone to query for the top\n    k most similar nodes, and synthesizes an answer from the\n    retrieved nodes.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        chunk_size_limit (int): Maximum number of tokens per chunk. NOTE:\n            in Pinecone the default is 2048 due to metadata size restrictions.\n    \"\"\"\n\n    index_struct_cls = PineconeIndexStruct\n\n    def __init__(\n        self,\n        documents:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 1, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "2": {"text": "       self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[PineconeIndexStruct] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        pinecone_index: Optional[Any] = None,\n        chunk_size_limit: int = 2048,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        import_err_msg = (\n            \"`pinecone` package not found, please run `pip install pinecone-client`\"\n        )\n        try:\n            import pinecone  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n        self._pinecone_index =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 2, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "3": {"text": "       self._pinecone_index = cast(pinecone.Index, pinecone_index)\n\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            embed_model=embed_model,\n            chunk_size_limit=chunk_size_limit,\n            **kwargs,\n        )\n        # NOTE: when building the vector store index, text_qa_template is not partially\n        # formatted because we don't know the query ahead of time.\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n\n    @classmethod\n    def get_query_map(self) ->", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 3, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "4": {"text": "   @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTPineconeIndexQuery,\n            QueryMode.EMBEDDING: GPTPineconeIndexQuery,\n        }\n\n    def _add_document_to_index(\n        self,\n        index_struct: PineconeIndexStruct,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        nodes = self._get_nodes_from_document(document, text_splitter)\n        for n in nodes:\n            if n.embedding is None:\n                text_embedding = self._embed_model.get_text_embedding(n.get_text())\n            else:\n            ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 4, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "5": {"text": "     else:\n                text_embedding = n.embedding\n\n            while True:\n                new_id = get_new_id(set())\n                result = self._pinecone_index.fetch([new_id])\n                if len(result[\"vectors\"]) == 0:\n                    break\n\n            metadata = {\n                \"text\": n.get_text(),\n                \"doc_id\": document.get_doc_id(),\n            }\n\n            self._pinecone_index.upsert([(new_id, text_embedding, metadata)])\n\n    def _build_index_from_documents(\n        self, documents: Sequence[BaseDocument]\n    ) -> PineconeIndexStruct:\n        \"\"\"Build index from documents.\"\"\"\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 5, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "6": {"text": "    \"\"\"Build index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n        index_struct = self.index_struct_cls()\n        for d in documents:\n            self._add_document_to_index(index_struct, d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(self._index_struct, document, self._text_splitter)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        # delete by filtering on the doc_id metadata\n        self._pinecone_index.delete(filter={\"doc_id\": {\"$eq\": doc_id}})\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Any)", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 6, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "7": {"text": "_preprocess_query(self, mode: QueryMode, query_kwargs: Any) -> None:\n        \"\"\"Query mode to class.\"\"\"\n        super()._preprocess_query(mode, query_kwargs)\n        # pass along pinecone client and info\n        query_kwargs[\"pinecone_index\"] = self._pinecone_index\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "index": 7, "child_indices": [], "ref_doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "node_info": null}, "8": {"text": "The GPTPineconeIndex is a data structure that uses an existing vector store to store nodes keyed by embeddings. During index construction, documents are chunked up, converted to nodes with text, and encoded in document embeddings stored within Pinecone. During query time, the index uses Pinecone to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes. The GPTPineconeIndex class initializes parameters, gets query map, adds documents to the index, builds the index from documents, inserts documents, and preprocesses queries.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 8, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"8": {"text": "The GPTPineconeIndex is a data structure that uses an existing vector store to store nodes keyed by embeddings. During index construction, documents are chunked up, converted to nodes with text, and encoded in document embeddings stored within Pinecone. During query time, the index uses Pinecone to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes. The GPTPineconeIndex class initializes parameters, gets query map, adds documents to the index, builds the index from documents, inserts documents, and preprocesses queries.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 8, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}