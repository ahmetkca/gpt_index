{"index_struct": {"text": "\nThe GPTWeaviateIndex is a data structure that uses an existing vector store to store nodes keyed by embeddings. It is used to store nodes and query for the most similar nodes, and synthesize an answer from the retrieved nodes. The code file contains components such as a Question-Answer Prompt, an Embedding model, a text splitter, and a LLM Predictor for query preprocessing. The Weaviate package is used to create a schema and store the nodes. During index construction, the document texts are chunked up, converted to nodes with text, and encoded in document embeddings stored within Weaviate. During query time, the index uses Weaviate to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes. The purpose of the code is to provide a data structure that can store nodes and query for the most similar nodes, and synthesize an answer from the retrieved nodes.", "doc_id": "cb8721c1-caa8-44dc-878c-d0ba05a0965a", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Weaviate Vector store index.\n\nAn index that that is built on top of an existing vector store.\n\n\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence, Type, cast\n\nfrom gpt_index.data_structs.data_structs import WeaviateIndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.vector_store.weaviate import GPTWeaviateIndexQuery\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.readers.weaviate.data_structs import WeaviateNode\nfrom gpt_index.readers.weaviate.utils import get_default_class_prefix\nfrom gpt_index.schema import BaseDocument\n\n\nclass GPTWeaviateIndex(BaseGPTIndex[WeaviateIndexStruct]):\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 0, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "1": {"text": "   \"\"\"GPT Weaviate Index.\n\n    The GPTWeaviateIndex is a data structure where nodes are keyed by\n    embeddings, and those embeddings are stored within a Weaviate index.\n    During index construction, the document texts are chunked up,\n    converted to nodes with text; they are then encoded in\n    document embeddings stored within Weaviate.\n\n    During query time, the index uses Weaviate to query for the top\n    k most similar nodes, and synthesizes an answer from the\n    retrieved nodes.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n    \"\"\"\n\n    index_struct_cls = WeaviateIndexStruct\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 1, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "2": {"text": "= None,\n        index_struct: Optional[WeaviateIndexStruct] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        weaviate_client: Optional[Any] = None,\n        class_prefix: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        import_err_msg = (\n            \"`weaviate` package not found, please run `pip install weaviate-client`\"\n        )\n        try:\n            import weaviate  # noqa: F401\n            from weaviate import Client  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n\n        self.client = cast(Client,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 2, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "3": {"text": "       self.client = cast(Client, weaviate_client)\n        if index_struct is not None:\n            if class_prefix is not None:\n                raise ValueError(\n                    \"class_prefix must be None when index_struct is not None.\"\n                )\n            self.class_prefix = index_struct.get_class_prefix()\n        else:\n            self.class_prefix = class_prefix or get_default_class_prefix()\n        # try to create schema\n        WeaviateNode.create_schema(self.client, self.class_prefix)\n\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 3, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "4": {"text": "index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            embed_model=embed_model,\n            **kwargs,\n        )\n        # NOTE: when building the vector store index, text_qa_template is not partially\n        # formatted because we don't know the query ahead of time.\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTWeaviateIndexQuery,\n            QueryMode.EMBEDDING: GPTWeaviateIndexQuery,\n        }\n\n    def _add_document_to_index(\n        self,\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 4, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "5": {"text": "       self,\n        index_struct: WeaviateIndexStruct,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        nodes = self._get_nodes_from_document(document, text_splitter)\n        for n in nodes:\n            if n.embedding is None:\n                n.embedding = self._embed_model.get_text_embedding(n.get_text())\n            WeaviateNode.from_gpt_index(self.client, n, index_struct.get_class_prefix())\n\n    def _build_index_from_documents(\n        self, documents: Sequence[BaseDocument]\n    ) -> WeaviateIndexStruct:\n        \"\"\"Build index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 5, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "6": {"text": "       self.text_qa_template, 1\n        )\n        index_struct = self.index_struct_cls(class_prefix=self.class_prefix)\n        for d in documents:\n            self._add_document_to_index(index_struct, d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(self._index_struct, document, self._text_splitter)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        WeaviateNode.delete_document(self.client, doc_id, self.class_prefix)\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Any) -> None:\n        \"\"\"Query mode to class.\"\"\"\n        super()._preprocess_query(mode, query_kwargs)\n        # pass along weaviate client and info\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 6, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "7": {"text": "       # pass along weaviate client and info\n        query_kwargs[\"weaviate_client\"] = self.client\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 7, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "8": {"text": "The GPTWeaviateIndex is a data structure that uses an existing vector store to store nodes keyed by embeddings. During index construction, the document texts are chunked up, converted to nodes with text, and encoded in document embeddings stored within Weaviate. During query time, the index uses Weaviate to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes. The GPTWeaviateIndex also has a Question-Answer Prompt and an Embedding model for embedding similarity. The Weaviate package is used to create a schema and store the nodes. The GPTWeaviateIndex also has a text splitter and a LLM Predictor for query preprocessing.\n", "doc_id": null, "embedding": null, "extra_info": null, "index": 8, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"8": {"text": "The GPTWeaviateIndex is a data structure that uses an existing vector store to store nodes keyed by embeddings. During index construction, the document texts are chunked up, converted to nodes with text, and encoded in document embeddings stored within Weaviate. During query time, the index uses Weaviate to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes. The GPTWeaviateIndex also has a Question-Answer Prompt and an Embedding model for embedding similarity. The Weaviate package is used to create a schema and store the nodes. The GPTWeaviateIndex also has a text splitter and a LLM Predictor for query preprocessing.\n", "doc_id": null, "embedding": null, "extra_info": null, "index": 8, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"9c650f2a0631e9cc1398df985038f660d8b10030": {"text": "\"\"\"Weaviate Vector store index.\n\nAn index that that is built on top of an existing vector store.\n\n\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence, Type, cast\n\nfrom gpt_index.data_structs.data_structs import WeaviateIndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.vector_store.weaviate import GPTWeaviateIndexQuery\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.readers.weaviate.data_structs import WeaviateNode\nfrom gpt_index.readers.weaviate.utils import get_default_class_prefix\nfrom gpt_index.schema import BaseDocument\n\n\nclass GPTWeaviateIndex(BaseGPTIndex[WeaviateIndexStruct]):\n    \"\"\"GPT Weaviate Index.\n\n    The GPTWeaviateIndex is a data structure where nodes are keyed by\n    embeddings, and those embeddings are stored within a Weaviate index.\n    During index construction, the document texts are chunked up,\n    converted to nodes with text; they are then encoded in\n    document embeddings stored within Weaviate.\n\n    During query time, the index uses Weaviate to query for the top\n    k most similar nodes, and synthesizes an answer from the\n    retrieved nodes.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n    \"\"\"\n\n    index_struct_cls = WeaviateIndexStruct\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[WeaviateIndexStruct] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        weaviate_client: Optional[Any] = None,\n        class_prefix: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        import_err_msg = (\n            \"`weaviate` package not found, please run `pip install weaviate-client`\"\n        )\n        try:\n            import weaviate  # noqa: F401\n            from weaviate import Client  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n\n        self.client = cast(Client, weaviate_client)\n        if index_struct is not None:\n            if class_prefix is not None:\n                raise ValueError(\n                    \"class_prefix must be None when index_struct is not None.\"\n                )\n            self.class_prefix = index_struct.get_class_prefix()\n        else:\n            self.class_prefix = class_prefix or get_default_class_prefix()\n        # try to create schema\n        WeaviateNode.create_schema(self.client, self.class_prefix)\n\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            embed_model=embed_model,\n            **kwargs,\n        )\n        # NOTE: when building the vector store index, text_qa_template is not partially\n        # formatted because we don't know the query ahead of time.\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTWeaviateIndexQuery,\n            QueryMode.EMBEDDING: GPTWeaviateIndexQuery,\n        }\n\n    def _add_document_to_index(\n        self,\n        index_struct: WeaviateIndexStruct,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        nodes = self._get_nodes_from_document(document, text_splitter)\n        for n in nodes:\n            if n.embedding is None:\n                n.embedding = self._embed_model.get_text_embedding(n.get_text())\n            WeaviateNode.from_gpt_index(self.client, n, index_struct.get_class_prefix())\n\n    def _build_index_from_documents(\n        self, documents: Sequence[BaseDocument]\n    ) -> WeaviateIndexStruct:\n        \"\"\"Build index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n        index_struct = self.index_struct_cls(class_prefix=self.class_prefix)\n        for d in documents:\n            self._add_document_to_index(index_struct, d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(self._index_struct, document, self._text_splitter)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        WeaviateNode.delete_document(self.client, doc_id, self.class_prefix)\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Any) -> None:\n        \"\"\"Query mode to class.\"\"\"\n        super()._preprocess_query(mode, query_kwargs)\n        # pass along weaviate client and info\n        query_kwargs[\"weaviate_client\"] = self.client\n", "doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "__type__": "Document"}, "cb8721c1-caa8-44dc-878c-d0ba05a0965a": {"text": "\nThe GPTWeaviateIndex is a data structure that uses an existing vector store to store nodes keyed by embeddings. It is used to store nodes and query for the most similar nodes, and synthesize an answer from the retrieved nodes. The code file contains components such as a Question-Answer Prompt, an Embedding model, a text splitter, and a LLM Predictor for query preprocessing. The Weaviate package is used to create a schema and store the nodes. During index construction, the document texts are chunked up, converted to nodes with text, and encoded in document embeddings stored within Weaviate. During query time, the index uses Weaviate to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes. The purpose of the code is to provide a data structure that can store nodes and query for the most similar nodes, and synthesize an answer from the retrieved nodes.", "doc_id": "cb8721c1-caa8-44dc-878c-d0ba05a0965a", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Weaviate Vector store index.\n\nAn index that that is built on top of an existing vector store.\n\n\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence, Type, cast\n\nfrom gpt_index.data_structs.data_structs import WeaviateIndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.vector_store.weaviate import GPTWeaviateIndexQuery\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.readers.weaviate.data_structs import WeaviateNode\nfrom gpt_index.readers.weaviate.utils import get_default_class_prefix\nfrom gpt_index.schema import BaseDocument\n\n\nclass GPTWeaviateIndex(BaseGPTIndex[WeaviateIndexStruct]):\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 0, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "1": {"text": "   \"\"\"GPT Weaviate Index.\n\n    The GPTWeaviateIndex is a data structure where nodes are keyed by\n    embeddings, and those embeddings are stored within a Weaviate index.\n    During index construction, the document texts are chunked up,\n    converted to nodes with text; they are then encoded in\n    document embeddings stored within Weaviate.\n\n    During query time, the index uses Weaviate to query for the top\n    k most similar nodes, and synthesizes an answer from the\n    retrieved nodes.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n    \"\"\"\n\n    index_struct_cls = WeaviateIndexStruct\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct:", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 1, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "2": {"text": "= None,\n        index_struct: Optional[WeaviateIndexStruct] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        weaviate_client: Optional[Any] = None,\n        class_prefix: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        import_err_msg = (\n            \"`weaviate` package not found, please run `pip install weaviate-client`\"\n        )\n        try:\n            import weaviate  # noqa: F401\n            from weaviate import Client  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n\n        self.client = cast(Client,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 2, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "3": {"text": "       self.client = cast(Client, weaviate_client)\n        if index_struct is not None:\n            if class_prefix is not None:\n                raise ValueError(\n                    \"class_prefix must be None when index_struct is not None.\"\n                )\n            self.class_prefix = index_struct.get_class_prefix()\n        else:\n            self.class_prefix = class_prefix or get_default_class_prefix()\n        # try to create schema\n        WeaviateNode.create_schema(self.client, self.class_prefix)\n\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 3, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "4": {"text": "index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            embed_model=embed_model,\n            **kwargs,\n        )\n        # NOTE: when building the vector store index, text_qa_template is not partially\n        # formatted because we don't know the query ahead of time.\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTWeaviateIndexQuery,\n            QueryMode.EMBEDDING: GPTWeaviateIndexQuery,\n        }\n\n    def _add_document_to_index(\n        self,\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 4, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "5": {"text": "       self,\n        index_struct: WeaviateIndexStruct,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        nodes = self._get_nodes_from_document(document, text_splitter)\n        for n in nodes:\n            if n.embedding is None:\n                n.embedding = self._embed_model.get_text_embedding(n.get_text())\n            WeaviateNode.from_gpt_index(self.client, n, index_struct.get_class_prefix())\n\n    def _build_index_from_documents(\n        self, documents: Sequence[BaseDocument]\n    ) -> WeaviateIndexStruct:\n        \"\"\"Build index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 5, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "6": {"text": "       self.text_qa_template, 1\n        )\n        index_struct = self.index_struct_cls(class_prefix=self.class_prefix)\n        for d in documents:\n            self._add_document_to_index(index_struct, d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(self._index_struct, document, self._text_splitter)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        WeaviateNode.delete_document(self.client, doc_id, self.class_prefix)\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Any) -> None:\n        \"\"\"Query mode to class.\"\"\"\n        super()._preprocess_query(mode, query_kwargs)\n        # pass along weaviate client and info\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 6, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "7": {"text": "       # pass along weaviate client and info\n        query_kwargs[\"weaviate_client\"] = self.client\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "index": 7, "child_indices": [], "ref_doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "node_info": null}, "8": {"text": "The GPTWeaviateIndex is a data structure that uses an existing vector store to store nodes keyed by embeddings. During index construction, the document texts are chunked up, converted to nodes with text, and encoded in document embeddings stored within Weaviate. During query time, the index uses Weaviate to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes. The GPTWeaviateIndex also has a Question-Answer Prompt and an Embedding model for embedding similarity. The Weaviate package is used to create a schema and store the nodes. The GPTWeaviateIndex also has a text splitter and a LLM Predictor for query preprocessing.\n", "doc_id": null, "embedding": null, "extra_info": null, "index": 8, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"8": {"text": "The GPTWeaviateIndex is a data structure that uses an existing vector store to store nodes keyed by embeddings. During index construction, the document texts are chunked up, converted to nodes with text, and encoded in document embeddings stored within Weaviate. During query time, the index uses Weaviate to query for the top k most similar nodes, and synthesizes an answer from the retrieved nodes. The GPTWeaviateIndex also has a Question-Answer Prompt and an Embedding model for embedding similarity. The Weaviate package is used to create a schema and store the nodes. The GPTWeaviateIndex also has a text splitter and a LLM Predictor for query preprocessing.\n", "doc_id": null, "embedding": null, "extra_info": null, "index": 8, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}