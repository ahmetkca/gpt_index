{"index_struct": {"text": "\nThis code file provides a wrapper class around an LLM chain from Langchain, allowing for easy access to the LLM's metadata, predictions, and token counts. The LLMMetadata class is used to extract metadata from the LLM, while the LLMPredictor class wraps around the LLMChain and provides a predict function to get the answer to a query. The predict function also has a retry_on_throttling parameter to retry on rate limit errors. The purpose of this code is to provide an easy way to use an LLM chain from Langchain for predictions.", "doc_id": "88e0742d-f5d6-44ab-b90a-b09de79e6f78", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Wrapper functions around an LLM chain.\"\"\"\n\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Any, Optional, Tuple\n\nimport openai\nfrom langchain import Cohere, LLMChain, OpenAI\nfrom langchain.llms import AI21\nfrom langchain.llms.base import BaseLLM\n\nfrom gpt_index.constants import MAX_CHUNK_SIZE, NUM_OUTPUTS\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.utils import (\n    ErrorToRetry,\n    globals_helper,\n    retry_on_exceptions_with_backoff,\n)\n\n\n@dataclass\nclass LLMMetadata:\n    \"\"\"LLM metadata.\n\n    We extract this metadata to help with our prompts.\n\n    \"\"\"\n\n    max_input_size: int = MAX_CHUNK_SIZE\n    num_output: int = NUM_OUTPUTS\n\n\ndef _get_llm_metadata(llm: BaseLLM) -> LLMMetadata:\n    \"\"\"Get LLM metadata from llm.\"\"\"\n    if not isinstance(llm, BaseLLM):\n        raise ValueError(\"llm must be an instance of langchain.llms.base.LLM\")\n    if isinstance(llm,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 0, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "1": {"text": "   if isinstance(llm, OpenAI):\n        return LLMMetadata(\n            max_input_size=llm.modelname_to_contextsize(llm.model_name),\n            num_output=llm.max_tokens,\n        )\n    elif isinstance(llm, Cohere):\n        # TODO: figure out max input size for cohere\n        return LLMMetadata(num_output=llm.max_tokens)\n    elif isinstance(llm, AI21):\n        # TODO: figure out max input size for AI21\n        return LLMMetadata(num_output=llm.maxTokens)\n    else:\n        return LLMMetadata()\n\n\nclass LLMPredictor:\n    \"\"\"LLM predictor class.\n\n    Wrapper around an LLMChain from Langchain.\n\n    Args:\n        llm (Optional[langchain.llms.base.LLM]): LLM from Langchain to use\n            for predictions. Defaults to OpenAI's text-davinci-003", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 1, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "2": {"text": "   for predictions. Defaults to OpenAI's text-davinci-003 model.\n            Please see `Langchain's LLM Page\n            <https://langchain.readthedocs.io/en/latest/modules/llms.html>`_\n            for more details.\n\n        retry_on_throttling (bool): Whether to retry on rate limit errors.\n            Defaults to true.\n\n    \"\"\"\n\n    def __init__(\n        self, llm: Optional[BaseLLM] = None, retry_on_throttling: bool = True\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._llm = llm or OpenAI(temperature=0, model_name=\"text-davinci-003\")\n        self.retry_on_throttling = retry_on_throttling\n        self._total_tokens_used = 0\n        self.flag = True\n        self._last_token_usage: Optional[int] = None\n\n    def get_llm_metadata(self)", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 2, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "3": {"text": "Optional[int] = None\n\n    def get_llm_metadata(self) -> LLMMetadata:\n        \"\"\"Get LLM metadata.\"\"\"\n        # TODO: refactor mocks in unit tests, this is a stopgap solution\n        if hasattr(self, \"_llm\"):\n            return _get_llm_metadata(self._llm)\n        else:\n            return LLMMetadata()\n\n    def _predict(self, prompt: Prompt, **prompt_args: Any) -> str:\n        \"\"\"Inner predict function.\n\n        If retry_on_throttling is true, we will retry on rate limit errors.\n\n        \"\"\"\n        llm_chain = LLMChain(prompt=prompt.get_langchain_prompt(), llm=self._llm)\n\n        # Note: we don't pass formatted_prompt to llm_chain.predict because\n        # langchain does the same formatting under the hood\n        full_prompt_args = prompt.get_full_format_args(prompt_args)\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 3, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "4": {"text": "= prompt.get_full_format_args(prompt_args)\n        if self.retry_on_throttling:\n            llm_prediction = retry_on_exceptions_with_backoff(\n                lambda: llm_chain.predict(**full_prompt_args),\n                [\n                    ErrorToRetry(openai.error.RateLimitError),\n                    ErrorToRetry(openai.error.ServiceUnavailableError),\n                    ErrorToRetry(openai.error.TryAgain),\n                    ErrorToRetry(\n                        openai.error.APIConnectionError, lambda e: e.should_retry\n                    ),\n                ],\n            )\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 4, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "5": {"text": "   ],\n            )\n        else:\n            llm_prediction = llm_chain.predict(**full_prompt_args)\n        return llm_prediction\n\n    def predict(self, prompt: Prompt, **prompt_args: Any) -> Tuple[str, str]:\n        \"\"\"Predict the answer to a query.\n\n        Args:\n            prompt (Prompt): Prompt to use for prediction.\n\n        Returns:\n            Tuple[str, str]: Tuple of the predicted answer and the formatted prompt.\n\n        \"\"\"\n        formatted_prompt = prompt.format(**prompt_args)\n        llm_prediction = self._predict(prompt, **prompt_args)\n        logging.debug(llm_prediction)\n\n        # We assume that the value of formatted_prompt is exactly the thing\n        # eventually sent to OpenAI, or whatever LLM downstream\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 5, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "6": {"text": "sent to OpenAI, or whatever LLM downstream\n        prompt_tokens_count = self._count_tokens(formatted_prompt)\n        prediction_tokens_count = self._count_tokens(llm_prediction)\n        self._total_tokens_used += prompt_tokens_count + prediction_tokens_count\n        return llm_prediction, formatted_prompt\n\n    @property\n    def total_tokens_used(self) -> int:\n        \"\"\"Get the total tokens used so far.\"\"\"\n        return self._total_tokens_used\n\n    def _count_tokens(self, text: str) -> int:\n        tokens = globals_helper.tokenizer(text)\n        return len(tokens)\n\n    @property\n    def last_token_usage(self) -> int:\n        \"\"\"Get the last token usage.\"\"\"\n        if self._last_token_usage is None:\n            return 0\n        return self._last_token_usage\n\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 6, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "7": {"text": "       return self._last_token_usage\n\n    @last_token_usage.setter\n    def last_token_usage(self, value: int) -> None:\n        \"\"\"Set the last token usage.\"\"\"\n        self._last_token_usage = value\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 7, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "8": {"text": "This code file contains a wrapper class around an LLM chain from Langchain. It provides functions to get LLM metadata, predict answers to queries, and count tokens used. It also has a retry_on_throttling parameter to retry on rate limit errors. The purpose of this code is to provide an easy way to use an LLM chain from Langchain for predictions.", "doc_id": null, "embedding": null, "extra_info": null, "index": 8, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"8": {"text": "This code file contains a wrapper class around an LLM chain from Langchain. It provides functions to get LLM metadata, predict answers to queries, and count tokens used. It also has a retry_on_throttling parameter to retry on rate limit errors. The purpose of this code is to provide an easy way to use an LLM chain from Langchain for predictions.", "doc_id": null, "embedding": null, "extra_info": null, "index": 8, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"10540e3570e6348e14f5264d0904fd9dc4488e93": {"text": "\"\"\"Wrapper functions around an LLM chain.\"\"\"\n\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Any, Optional, Tuple\n\nimport openai\nfrom langchain import Cohere, LLMChain, OpenAI\nfrom langchain.llms import AI21\nfrom langchain.llms.base import BaseLLM\n\nfrom gpt_index.constants import MAX_CHUNK_SIZE, NUM_OUTPUTS\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.utils import (\n    ErrorToRetry,\n    globals_helper,\n    retry_on_exceptions_with_backoff,\n)\n\n\n@dataclass\nclass LLMMetadata:\n    \"\"\"LLM metadata.\n\n    We extract this metadata to help with our prompts.\n\n    \"\"\"\n\n    max_input_size: int = MAX_CHUNK_SIZE\n    num_output: int = NUM_OUTPUTS\n\n\ndef _get_llm_metadata(llm: BaseLLM) -> LLMMetadata:\n    \"\"\"Get LLM metadata from llm.\"\"\"\n    if not isinstance(llm, BaseLLM):\n        raise ValueError(\"llm must be an instance of langchain.llms.base.LLM\")\n    if isinstance(llm, OpenAI):\n        return LLMMetadata(\n            max_input_size=llm.modelname_to_contextsize(llm.model_name),\n            num_output=llm.max_tokens,\n        )\n    elif isinstance(llm, Cohere):\n        # TODO: figure out max input size for cohere\n        return LLMMetadata(num_output=llm.max_tokens)\n    elif isinstance(llm, AI21):\n        # TODO: figure out max input size for AI21\n        return LLMMetadata(num_output=llm.maxTokens)\n    else:\n        return LLMMetadata()\n\n\nclass LLMPredictor:\n    \"\"\"LLM predictor class.\n\n    Wrapper around an LLMChain from Langchain.\n\n    Args:\n        llm (Optional[langchain.llms.base.LLM]): LLM from Langchain to use\n            for predictions. Defaults to OpenAI's text-davinci-003 model.\n            Please see `Langchain's LLM Page\n            <https://langchain.readthedocs.io/en/latest/modules/llms.html>`_\n            for more details.\n\n        retry_on_throttling (bool): Whether to retry on rate limit errors.\n            Defaults to true.\n\n    \"\"\"\n\n    def __init__(\n        self, llm: Optional[BaseLLM] = None, retry_on_throttling: bool = True\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._llm = llm or OpenAI(temperature=0, model_name=\"text-davinci-003\")\n        self.retry_on_throttling = retry_on_throttling\n        self._total_tokens_used = 0\n        self.flag = True\n        self._last_token_usage: Optional[int] = None\n\n    def get_llm_metadata(self) -> LLMMetadata:\n        \"\"\"Get LLM metadata.\"\"\"\n        # TODO: refactor mocks in unit tests, this is a stopgap solution\n        if hasattr(self, \"_llm\"):\n            return _get_llm_metadata(self._llm)\n        else:\n            return LLMMetadata()\n\n    def _predict(self, prompt: Prompt, **prompt_args: Any) -> str:\n        \"\"\"Inner predict function.\n\n        If retry_on_throttling is true, we will retry on rate limit errors.\n\n        \"\"\"\n        llm_chain = LLMChain(prompt=prompt.get_langchain_prompt(), llm=self._llm)\n\n        # Note: we don't pass formatted_prompt to llm_chain.predict because\n        # langchain does the same formatting under the hood\n        full_prompt_args = prompt.get_full_format_args(prompt_args)\n        if self.retry_on_throttling:\n            llm_prediction = retry_on_exceptions_with_backoff(\n                lambda: llm_chain.predict(**full_prompt_args),\n                [\n                    ErrorToRetry(openai.error.RateLimitError),\n                    ErrorToRetry(openai.error.ServiceUnavailableError),\n                    ErrorToRetry(openai.error.TryAgain),\n                    ErrorToRetry(\n                        openai.error.APIConnectionError, lambda e: e.should_retry\n                    ),\n                ],\n            )\n        else:\n            llm_prediction = llm_chain.predict(**full_prompt_args)\n        return llm_prediction\n\n    def predict(self, prompt: Prompt, **prompt_args: Any) -> Tuple[str, str]:\n        \"\"\"Predict the answer to a query.\n\n        Args:\n            prompt (Prompt): Prompt to use for prediction.\n\n        Returns:\n            Tuple[str, str]: Tuple of the predicted answer and the formatted prompt.\n\n        \"\"\"\n        formatted_prompt = prompt.format(**prompt_args)\n        llm_prediction = self._predict(prompt, **prompt_args)\n        logging.debug(llm_prediction)\n\n        # We assume that the value of formatted_prompt is exactly the thing\n        # eventually sent to OpenAI, or whatever LLM downstream\n        prompt_tokens_count = self._count_tokens(formatted_prompt)\n        prediction_tokens_count = self._count_tokens(llm_prediction)\n        self._total_tokens_used += prompt_tokens_count + prediction_tokens_count\n        return llm_prediction, formatted_prompt\n\n    @property\n    def total_tokens_used(self) -> int:\n        \"\"\"Get the total tokens used so far.\"\"\"\n        return self._total_tokens_used\n\n    def _count_tokens(self, text: str) -> int:\n        tokens = globals_helper.tokenizer(text)\n        return len(tokens)\n\n    @property\n    def last_token_usage(self) -> int:\n        \"\"\"Get the last token usage.\"\"\"\n        if self._last_token_usage is None:\n            return 0\n        return self._last_token_usage\n\n    @last_token_usage.setter\n    def last_token_usage(self, value: int) -> None:\n        \"\"\"Set the last token usage.\"\"\"\n        self._last_token_usage = value\n", "doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "__type__": "Document"}, "88e0742d-f5d6-44ab-b90a-b09de79e6f78": {"text": "\nThis code file provides a wrapper class around an LLM chain from Langchain, allowing for easy access to the LLM's metadata, predictions, and token counts. The LLMMetadata class is used to extract metadata from the LLM, while the LLMPredictor class wraps around the LLMChain and provides a predict function to get the answer to a query. The predict function also has a retry_on_throttling parameter to retry on rate limit errors. The purpose of this code is to provide an easy way to use an LLM chain from Langchain for predictions.", "doc_id": "88e0742d-f5d6-44ab-b90a-b09de79e6f78", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Wrapper functions around an LLM chain.\"\"\"\n\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Any, Optional, Tuple\n\nimport openai\nfrom langchain import Cohere, LLMChain, OpenAI\nfrom langchain.llms import AI21\nfrom langchain.llms.base import BaseLLM\n\nfrom gpt_index.constants import MAX_CHUNK_SIZE, NUM_OUTPUTS\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.utils import (\n    ErrorToRetry,\n    globals_helper,\n    retry_on_exceptions_with_backoff,\n)\n\n\n@dataclass\nclass LLMMetadata:\n    \"\"\"LLM metadata.\n\n    We extract this metadata to help with our prompts.\n\n    \"\"\"\n\n    max_input_size: int = MAX_CHUNK_SIZE\n    num_output: int = NUM_OUTPUTS\n\n\ndef _get_llm_metadata(llm: BaseLLM) -> LLMMetadata:\n    \"\"\"Get LLM metadata from llm.\"\"\"\n    if not isinstance(llm, BaseLLM):\n        raise ValueError(\"llm must be an instance of langchain.llms.base.LLM\")\n    if isinstance(llm,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 0, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "1": {"text": "   if isinstance(llm, OpenAI):\n        return LLMMetadata(\n            max_input_size=llm.modelname_to_contextsize(llm.model_name),\n            num_output=llm.max_tokens,\n        )\n    elif isinstance(llm, Cohere):\n        # TODO: figure out max input size for cohere\n        return LLMMetadata(num_output=llm.max_tokens)\n    elif isinstance(llm, AI21):\n        # TODO: figure out max input size for AI21\n        return LLMMetadata(num_output=llm.maxTokens)\n    else:\n        return LLMMetadata()\n\n\nclass LLMPredictor:\n    \"\"\"LLM predictor class.\n\n    Wrapper around an LLMChain from Langchain.\n\n    Args:\n        llm (Optional[langchain.llms.base.LLM]): LLM from Langchain to use\n            for predictions. Defaults to OpenAI's text-davinci-003", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 1, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "2": {"text": "   for predictions. Defaults to OpenAI's text-davinci-003 model.\n            Please see `Langchain's LLM Page\n            <https://langchain.readthedocs.io/en/latest/modules/llms.html>`_\n            for more details.\n\n        retry_on_throttling (bool): Whether to retry on rate limit errors.\n            Defaults to true.\n\n    \"\"\"\n\n    def __init__(\n        self, llm: Optional[BaseLLM] = None, retry_on_throttling: bool = True\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._llm = llm or OpenAI(temperature=0, model_name=\"text-davinci-003\")\n        self.retry_on_throttling = retry_on_throttling\n        self._total_tokens_used = 0\n        self.flag = True\n        self._last_token_usage: Optional[int] = None\n\n    def get_llm_metadata(self)", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 2, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "3": {"text": "Optional[int] = None\n\n    def get_llm_metadata(self) -> LLMMetadata:\n        \"\"\"Get LLM metadata.\"\"\"\n        # TODO: refactor mocks in unit tests, this is a stopgap solution\n        if hasattr(self, \"_llm\"):\n            return _get_llm_metadata(self._llm)\n        else:\n            return LLMMetadata()\n\n    def _predict(self, prompt: Prompt, **prompt_args: Any) -> str:\n        \"\"\"Inner predict function.\n\n        If retry_on_throttling is true, we will retry on rate limit errors.\n\n        \"\"\"\n        llm_chain = LLMChain(prompt=prompt.get_langchain_prompt(), llm=self._llm)\n\n        # Note: we don't pass formatted_prompt to llm_chain.predict because\n        # langchain does the same formatting under the hood\n        full_prompt_args = prompt.get_full_format_args(prompt_args)\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 3, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "4": {"text": "= prompt.get_full_format_args(prompt_args)\n        if self.retry_on_throttling:\n            llm_prediction = retry_on_exceptions_with_backoff(\n                lambda: llm_chain.predict(**full_prompt_args),\n                [\n                    ErrorToRetry(openai.error.RateLimitError),\n                    ErrorToRetry(openai.error.ServiceUnavailableError),\n                    ErrorToRetry(openai.error.TryAgain),\n                    ErrorToRetry(\n                        openai.error.APIConnectionError, lambda e: e.should_retry\n                    ),\n                ],\n            )\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 4, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "5": {"text": "   ],\n            )\n        else:\n            llm_prediction = llm_chain.predict(**full_prompt_args)\n        return llm_prediction\n\n    def predict(self, prompt: Prompt, **prompt_args: Any) -> Tuple[str, str]:\n        \"\"\"Predict the answer to a query.\n\n        Args:\n            prompt (Prompt): Prompt to use for prediction.\n\n        Returns:\n            Tuple[str, str]: Tuple of the predicted answer and the formatted prompt.\n\n        \"\"\"\n        formatted_prompt = prompt.format(**prompt_args)\n        llm_prediction = self._predict(prompt, **prompt_args)\n        logging.debug(llm_prediction)\n\n        # We assume that the value of formatted_prompt is exactly the thing\n        # eventually sent to OpenAI, or whatever LLM downstream\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 5, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "6": {"text": "sent to OpenAI, or whatever LLM downstream\n        prompt_tokens_count = self._count_tokens(formatted_prompt)\n        prediction_tokens_count = self._count_tokens(llm_prediction)\n        self._total_tokens_used += prompt_tokens_count + prediction_tokens_count\n        return llm_prediction, formatted_prompt\n\n    @property\n    def total_tokens_used(self) -> int:\n        \"\"\"Get the total tokens used so far.\"\"\"\n        return self._total_tokens_used\n\n    def _count_tokens(self, text: str) -> int:\n        tokens = globals_helper.tokenizer(text)\n        return len(tokens)\n\n    @property\n    def last_token_usage(self) -> int:\n        \"\"\"Get the last token usage.\"\"\"\n        if self._last_token_usage is None:\n            return 0\n        return self._last_token_usage\n\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 6, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "7": {"text": "       return self._last_token_usage\n\n    @last_token_usage.setter\n    def last_token_usage(self, value: int) -> None:\n        \"\"\"Set the last token usage.\"\"\"\n        self._last_token_usage = value\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "index": 7, "child_indices": [], "ref_doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "node_info": null}, "8": {"text": "This code file contains a wrapper class around an LLM chain from Langchain. It provides functions to get LLM metadata, predict answers to queries, and count tokens used. It also has a retry_on_throttling parameter to retry on rate limit errors. The purpose of this code is to provide an easy way to use an LLM chain from Langchain for predictions.", "doc_id": null, "embedding": null, "extra_info": null, "index": 8, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"8": {"text": "This code file contains a wrapper class around an LLM chain from Langchain. It provides functions to get LLM metadata, predict answers to queries, and count tokens used. It also has a retry_on_throttling parameter to retry on rate limit errors. The purpose of this code is to provide an easy way to use an LLM chain from Langchain for predictions.", "doc_id": null, "embedding": null, "extra_info": null, "index": 8, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}