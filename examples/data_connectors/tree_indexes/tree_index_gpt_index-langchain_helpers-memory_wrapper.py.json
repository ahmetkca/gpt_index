{"index_struct": {"text": "\nThis code file contains a class called GPTIndexMemory which provides methods to store and access data from the GPT Index in a Langchain memory. It has properties to set the human and AI prefixes, memory key, GPT Index instance, and query keyword arguments. The class has methods to get the prompt input key, load memory variables, save context, and clear memory contents. The save context method stores the input and output from the chain in the GPT Index, while the load memory variables method queries the GPT Index for the response based on the text input from the chain. The purpose of this code is to provide an efficient way to store and access data from the GPT Index in a Langchain memory.", "doc_id": "f952ff23-534a-4c2e-9bfc-2242fbedca5e", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Langchain memory wrapper (for GPT Index).\"\"\"\n\nfrom typing import Any, Dict, List, Optional\n\nfrom langchain.chains.base import Memory\nfrom pydantic import Field\n\nfrom gpt_index.indices.base import BaseGPTIndex\nfrom gpt_index.readers.schema.base import Document\n\n\ndef get_prompt_input_key(inputs: Dict[str, Any], memory_variables: List[str]) -> str:\n    \"\"\"Get prompt input key.\n\n    Copied over from langchain.\n\n    \"\"\"\n    # \"stop\" is a special key that can be passed as input but is not used to\n    # format the prompt.\n    prompt_input_keys = list(set(inputs).difference(memory_variables + [\"stop\"]))\n    if len(prompt_input_keys) != 1:\n        raise ValueError(f\"One input key expected got {prompt_input_keys}\")\n    return prompt_input_keys[0]\n\n\nclass GPTIndexMemory(Memory):\n    \"\"\"Langchain memory wrapper (for GPT Index).\n\n    Args:\n        human_prefix (str): Prefix for human input. Defaults to \"Human\".\n        ai_prefix (str): Prefix for AI output. Defaults to", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/memory_wrapper.py", "file_name": "memory_wrapper.py"}, "index": 0, "child_indices": [], "ref_doc_id": "6af9d1a0133bbdf496857f225254cdf042edd0b3", "node_info": null}, "1": {"text": "     ai_prefix (str): Prefix for AI output. Defaults to \"AI\".\n        memory_key (str): Key for memory. Defaults to \"history\".\n        index (BaseGPTIndex): GPT Index instance.\n        query_kwargs (Dict[str, Any]): Keyword arguments for GPT Index query.\n        input_key (Optional[str]): Input key. Defaults to None.\n        output_key (Optional[str]): Output key. Defaults to None.\n\n    \"\"\"\n\n    human_prefix: str = \"Human\"\n    ai_prefix: str = \"AI\"\n    memory_key: str = \"history\"\n    index: BaseGPTIndex\n    query_kwargs: Dict = Field(default_factory=dict)\n    output_key: Optional[str] = None\n    input_key: Optional[str] = None\n\n    @property\n    def memory_variables(self) -> List[str]:\n        \"\"\"Return memory variables.\"\"\"\n        return [self.memory_key]\n\n    def _get_prompt_input_key(self, inputs: Dict[str, Any]) -> str:\n        if", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/memory_wrapper.py", "file_name": "memory_wrapper.py"}, "index": 1, "child_indices": [], "ref_doc_id": "6af9d1a0133bbdf496857f225254cdf042edd0b3", "node_info": null}, "2": {"text": "Dict[str, Any]) -> str:\n        if self.input_key is None:\n            prompt_input_key = get_prompt_input_key(inputs, self.memory_variables)\n        else:\n            prompt_input_key = self.input_key\n        return prompt_input_key\n\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n        \"\"\"Return key-value pairs given the text input to the chain.\"\"\"\n        prompt_input_key = self._get_prompt_input_key(inputs)\n        query_str = inputs[prompt_input_key]\n\n        # TODO: wrap in prompt\n        # TODO: add option to return the raw text\n        # NOTE: currently it's a hack\n        response = self.index.query(query_str, **self.query_kwargs)\n        return {self.memory_key: str(response)}\n\n    def save_context(self, inputs: Dict[str, Any],", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/memory_wrapper.py", "file_name": "memory_wrapper.py"}, "index": 2, "child_indices": [], "ref_doc_id": "6af9d1a0133bbdf496857f225254cdf042edd0b3", "node_info": null}, "3": {"text": "   def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\n        \"\"\"Save the context of this model run to memory.\"\"\"\n        prompt_input_key = self._get_prompt_input_key(inputs)\n        if self.output_key is None:\n            if len(outputs) != 1:\n                raise ValueError(f\"One output key expected, got {outputs.keys()}\")\n            output_key = list(outputs.keys())[0]\n        else:\n            output_key = self.output_key\n        human = f\"{self.human_prefix}: \" + inputs[prompt_input_key]\n        ai = f\"{self.ai_prefix}: \" + outputs[output_key]\n        doc_text = \"\\n\".join([human, ai])\n        doc = Document(text=doc_text)\n        self.index.insert(doc)\n\n    def clear(self) -> None:\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/memory_wrapper.py", "file_name": "memory_wrapper.py"}, "index": 3, "child_indices": [], "ref_doc_id": "6af9d1a0133bbdf496857f225254cdf042edd0b3", "node_info": null}, "4": {"text": "   def clear(self) -> None:\n        \"\"\"Clear memory contents.\"\"\"\n        pass\n\n    def __repr__(self) -> str:\n        \"\"\"Return representation.\"\"\"\n        return \"GPTIndexMemory()\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/memory_wrapper.py", "file_name": "memory_wrapper.py"}, "index": 4, "child_indices": [], "ref_doc_id": "6af9d1a0133bbdf496857f225254cdf042edd0b3", "node_info": null}, "5": {"text": "This code file contains a class called GPTIndexMemory which is a wrapper for the Langchain memory. It provides methods to get the prompt input key, load memory variables, save context, and clear memory contents. It also has properties to set the human and AI prefixes, memory key, GPT Index instance, and query keyword arguments. The purpose of this code is to provide a way to store and access data from the GPT Index in a Langchain memory.", "doc_id": null, "embedding": null, "extra_info": null, "index": 5, "child_indices": [0, 1, 2, 3, 4], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"5": {"text": "This code file contains a class called GPTIndexMemory which is a wrapper for the Langchain memory. It provides methods to get the prompt input key, load memory variables, save context, and clear memory contents. It also has properties to set the human and AI prefixes, memory key, GPT Index instance, and query keyword arguments. The purpose of this code is to provide a way to store and access data from the GPT Index in a Langchain memory.", "doc_id": null, "embedding": null, "extra_info": null, "index": 5, "child_indices": [0, 1, 2, 3, 4], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"6af9d1a0133bbdf496857f225254cdf042edd0b3": {"text": "\"\"\"Langchain memory wrapper (for GPT Index).\"\"\"\n\nfrom typing import Any, Dict, List, Optional\n\nfrom langchain.chains.base import Memory\nfrom pydantic import Field\n\nfrom gpt_index.indices.base import BaseGPTIndex\nfrom gpt_index.readers.schema.base import Document\n\n\ndef get_prompt_input_key(inputs: Dict[str, Any], memory_variables: List[str]) -> str:\n    \"\"\"Get prompt input key.\n\n    Copied over from langchain.\n\n    \"\"\"\n    # \"stop\" is a special key that can be passed as input but is not used to\n    # format the prompt.\n    prompt_input_keys = list(set(inputs).difference(memory_variables + [\"stop\"]))\n    if len(prompt_input_keys) != 1:\n        raise ValueError(f\"One input key expected got {prompt_input_keys}\")\n    return prompt_input_keys[0]\n\n\nclass GPTIndexMemory(Memory):\n    \"\"\"Langchain memory wrapper (for GPT Index).\n\n    Args:\n        human_prefix (str): Prefix for human input. Defaults to \"Human\".\n        ai_prefix (str): Prefix for AI output. Defaults to \"AI\".\n        memory_key (str): Key for memory. Defaults to \"history\".\n        index (BaseGPTIndex): GPT Index instance.\n        query_kwargs (Dict[str, Any]): Keyword arguments for GPT Index query.\n        input_key (Optional[str]): Input key. Defaults to None.\n        output_key (Optional[str]): Output key. Defaults to None.\n\n    \"\"\"\n\n    human_prefix: str = \"Human\"\n    ai_prefix: str = \"AI\"\n    memory_key: str = \"history\"\n    index: BaseGPTIndex\n    query_kwargs: Dict = Field(default_factory=dict)\n    output_key: Optional[str] = None\n    input_key: Optional[str] = None\n\n    @property\n    def memory_variables(self) -> List[str]:\n        \"\"\"Return memory variables.\"\"\"\n        return [self.memory_key]\n\n    def _get_prompt_input_key(self, inputs: Dict[str, Any]) -> str:\n        if self.input_key is None:\n            prompt_input_key = get_prompt_input_key(inputs, self.memory_variables)\n        else:\n            prompt_input_key = self.input_key\n        return prompt_input_key\n\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n        \"\"\"Return key-value pairs given the text input to the chain.\"\"\"\n        prompt_input_key = self._get_prompt_input_key(inputs)\n        query_str = inputs[prompt_input_key]\n\n        # TODO: wrap in prompt\n        # TODO: add option to return the raw text\n        # NOTE: currently it's a hack\n        response = self.index.query(query_str, **self.query_kwargs)\n        return {self.memory_key: str(response)}\n\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\n        \"\"\"Save the context of this model run to memory.\"\"\"\n        prompt_input_key = self._get_prompt_input_key(inputs)\n        if self.output_key is None:\n            if len(outputs) != 1:\n                raise ValueError(f\"One output key expected, got {outputs.keys()}\")\n            output_key = list(outputs.keys())[0]\n        else:\n            output_key = self.output_key\n        human = f\"{self.human_prefix}: \" + inputs[prompt_input_key]\n        ai = f\"{self.ai_prefix}: \" + outputs[output_key]\n        doc_text = \"\\n\".join([human, ai])\n        doc = Document(text=doc_text)\n        self.index.insert(doc)\n\n    def clear(self) -> None:\n        \"\"\"Clear memory contents.\"\"\"\n        pass\n\n    def __repr__(self) -> str:\n        \"\"\"Return representation.\"\"\"\n        return \"GPTIndexMemory()\"\n", "doc_id": "6af9d1a0133bbdf496857f225254cdf042edd0b3", "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/memory_wrapper.py", "file_name": "memory_wrapper.py"}, "__type__": "Document"}, "f952ff23-534a-4c2e-9bfc-2242fbedca5e": {"text": "\nThis code file contains a class called GPTIndexMemory which provides methods to store and access data from the GPT Index in a Langchain memory. It has properties to set the human and AI prefixes, memory key, GPT Index instance, and query keyword arguments. The class has methods to get the prompt input key, load memory variables, save context, and clear memory contents. The save context method stores the input and output from the chain in the GPT Index, while the load memory variables method queries the GPT Index for the response based on the text input from the chain. The purpose of this code is to provide an efficient way to store and access data from the GPT Index in a Langchain memory.", "doc_id": "f952ff23-534a-4c2e-9bfc-2242fbedca5e", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Langchain memory wrapper (for GPT Index).\"\"\"\n\nfrom typing import Any, Dict, List, Optional\n\nfrom langchain.chains.base import Memory\nfrom pydantic import Field\n\nfrom gpt_index.indices.base import BaseGPTIndex\nfrom gpt_index.readers.schema.base import Document\n\n\ndef get_prompt_input_key(inputs: Dict[str, Any], memory_variables: List[str]) -> str:\n    \"\"\"Get prompt input key.\n\n    Copied over from langchain.\n\n    \"\"\"\n    # \"stop\" is a special key that can be passed as input but is not used to\n    # format the prompt.\n    prompt_input_keys = list(set(inputs).difference(memory_variables + [\"stop\"]))\n    if len(prompt_input_keys) != 1:\n        raise ValueError(f\"One input key expected got {prompt_input_keys}\")\n    return prompt_input_keys[0]\n\n\nclass GPTIndexMemory(Memory):\n    \"\"\"Langchain memory wrapper (for GPT Index).\n\n    Args:\n        human_prefix (str): Prefix for human input. Defaults to \"Human\".\n        ai_prefix (str): Prefix for AI output. Defaults to", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/memory_wrapper.py", "file_name": "memory_wrapper.py"}, "index": 0, "child_indices": [], "ref_doc_id": "6af9d1a0133bbdf496857f225254cdf042edd0b3", "node_info": null}, "1": {"text": "     ai_prefix (str): Prefix for AI output. Defaults to \"AI\".\n        memory_key (str): Key for memory. Defaults to \"history\".\n        index (BaseGPTIndex): GPT Index instance.\n        query_kwargs (Dict[str, Any]): Keyword arguments for GPT Index query.\n        input_key (Optional[str]): Input key. Defaults to None.\n        output_key (Optional[str]): Output key. Defaults to None.\n\n    \"\"\"\n\n    human_prefix: str = \"Human\"\n    ai_prefix: str = \"AI\"\n    memory_key: str = \"history\"\n    index: BaseGPTIndex\n    query_kwargs: Dict = Field(default_factory=dict)\n    output_key: Optional[str] = None\n    input_key: Optional[str] = None\n\n    @property\n    def memory_variables(self) -> List[str]:\n        \"\"\"Return memory variables.\"\"\"\n        return [self.memory_key]\n\n    def _get_prompt_input_key(self, inputs: Dict[str, Any]) -> str:\n        if", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/memory_wrapper.py", "file_name": "memory_wrapper.py"}, "index": 1, "child_indices": [], "ref_doc_id": "6af9d1a0133bbdf496857f225254cdf042edd0b3", "node_info": null}, "2": {"text": "Dict[str, Any]) -> str:\n        if self.input_key is None:\n            prompt_input_key = get_prompt_input_key(inputs, self.memory_variables)\n        else:\n            prompt_input_key = self.input_key\n        return prompt_input_key\n\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n        \"\"\"Return key-value pairs given the text input to the chain.\"\"\"\n        prompt_input_key = self._get_prompt_input_key(inputs)\n        query_str = inputs[prompt_input_key]\n\n        # TODO: wrap in prompt\n        # TODO: add option to return the raw text\n        # NOTE: currently it's a hack\n        response = self.index.query(query_str, **self.query_kwargs)\n        return {self.memory_key: str(response)}\n\n    def save_context(self, inputs: Dict[str, Any],", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/memory_wrapper.py", "file_name": "memory_wrapper.py"}, "index": 2, "child_indices": [], "ref_doc_id": "6af9d1a0133bbdf496857f225254cdf042edd0b3", "node_info": null}, "3": {"text": "   def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\n        \"\"\"Save the context of this model run to memory.\"\"\"\n        prompt_input_key = self._get_prompt_input_key(inputs)\n        if self.output_key is None:\n            if len(outputs) != 1:\n                raise ValueError(f\"One output key expected, got {outputs.keys()}\")\n            output_key = list(outputs.keys())[0]\n        else:\n            output_key = self.output_key\n        human = f\"{self.human_prefix}: \" + inputs[prompt_input_key]\n        ai = f\"{self.ai_prefix}: \" + outputs[output_key]\n        doc_text = \"\\n\".join([human, ai])\n        doc = Document(text=doc_text)\n        self.index.insert(doc)\n\n    def clear(self) -> None:\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/memory_wrapper.py", "file_name": "memory_wrapper.py"}, "index": 3, "child_indices": [], "ref_doc_id": "6af9d1a0133bbdf496857f225254cdf042edd0b3", "node_info": null}, "4": {"text": "   def clear(self) -> None:\n        \"\"\"Clear memory contents.\"\"\"\n        pass\n\n    def __repr__(self) -> str:\n        \"\"\"Return representation.\"\"\"\n        return \"GPTIndexMemory()\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/memory_wrapper.py", "file_name": "memory_wrapper.py"}, "index": 4, "child_indices": [], "ref_doc_id": "6af9d1a0133bbdf496857f225254cdf042edd0b3", "node_info": null}, "5": {"text": "This code file contains a class called GPTIndexMemory which is a wrapper for the Langchain memory. It provides methods to get the prompt input key, load memory variables, save context, and clear memory contents. It also has properties to set the human and AI prefixes, memory key, GPT Index instance, and query keyword arguments. The purpose of this code is to provide a way to store and access data from the GPT Index in a Langchain memory.", "doc_id": null, "embedding": null, "extra_info": null, "index": 5, "child_indices": [0, 1, 2, 3, 4], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"5": {"text": "This code file contains a class called GPTIndexMemory which is a wrapper for the Langchain memory. It provides methods to get the prompt input key, load memory variables, save context, and clear memory contents. It also has properties to set the human and AI prefixes, memory key, GPT Index instance, and query keyword arguments. The purpose of this code is to provide a way to store and access data from the GPT Index in a Langchain memory.", "doc_id": null, "embedding": null, "extra_info": null, "index": 5, "child_indices": [0, 1, 2, 3, 4], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}