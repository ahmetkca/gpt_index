{"index_struct": {"text": "\nThe text_splitter.py file contains functions to split text into chunks of a specified size. It uses a tokenizer to count the number of tokens in each chunk and a separator to join the chunks together. The split_text and split_text_with_overlaps functions are used to split the text into chunks and return them as a list of TextSplit objects. The _reduce_chunk_size and _process_splits functions are used to reduce the chunk size and process the splits, respectively. The truncate_text function is used to truncate the text in order to fit the underlying chunk size. The code is designed to make it easier to split large amounts of text into smaller chunks for easier processing. The TextSplitter and TokenTextSplitter classes are implementations of the TextSplitter class that look at word tokens when splitting the text. The parameters for the separator, chunk size, chunk overlap, tokenizer, and backup separators are used to customize the splitting process.", "doc_id": "53b91517-3e8b-4706-86f9-a33bd245f3ad", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Text splitter implementations.\"\"\"\nfrom dataclasses import dataclass\nfrom typing import Callable, List, Optional\n\nfrom langchain.text_splitter import TextSplitter\n\nfrom gpt_index.utils import globals_helper\n\n\n@dataclass\nclass TextSplit:\n    \"\"\"Text split with overlap.\n\n    Attributes:\n        text_chunk: The text string.\n        num_char_overlap: The number of overlapping characters with the previous chunk.\n    \"\"\"\n\n    text_chunk: str\n    num_char_overlap: int\n\n\nclass TokenTextSplitter(TextSplitter):\n    \"\"\"Implementation of splitting text that looks at word tokens.\"\"\"\n\n    def __init__(\n        self,\n        separator: str = \" \",\n        chunk_size: int = 4000,\n        chunk_overlap: int = 200,\n        tokenizer: Optional[Callable] = None,\n        backup_separators: Optional[List[str]] = [\"\\n\"],\n    ):\n        \"\"\"Initialize with parameters.\"\"\"\n        if chunk_overlap >", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 0, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "1": {"text": "with parameters.\"\"\"\n        if chunk_overlap > chunk_size:\n            raise ValueError(\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\n                f\"({chunk_size}), should be smaller.\"\n            )\n        self._separator = separator\n        self._chunk_size = chunk_size\n        self._chunk_overlap = chunk_overlap\n        self.tokenizer = tokenizer or globals_helper.tokenizer\n        self._backup_separators = backup_separators\n\n    def _reduce_chunk_size(\n        self, start_idx: int, cur_idx: int, splits: List[str]\n    ) -> int:\n        \"\"\"Reduce the chunk size by reducing cur_idx.\n\n        Return the new cur_idx.\n\n        \"\"\"\n        current_doc_total = len(\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 1, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "2": {"text": "\"\"\"\n        current_doc_total = len(\n            self.tokenizer(self._separator.join(splits[start_idx:cur_idx]))\n        )\n        while current_doc_total > self._chunk_size:\n            percent_to_reduce = (\n                current_doc_total - self._chunk_size\n            ) / current_doc_total\n            num_to_reduce = int(percent_to_reduce * (cur_idx - start_idx)) + 1\n            cur_idx -= num_to_reduce\n            current_doc_total = len(\n                self.tokenizer(self._separator.join(splits[start_idx:cur_idx]))\n            )\n        return cur_idx\n\n    def _process_splits(self, splits: List[str], chunk_size: int) -> List[str]:\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 2, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "3": {"text": "chunk_size: int) -> List[str]:\n        \"\"\"Process splits.\n\n        Specifically search for tokens that are too large for chunk size,\n        and see if we can separate those tokens more\n        (via backup separators if specified, or force chunking).\n\n        \"\"\"\n        new_splits = []\n        for split in splits:\n            num_cur_tokens = len(self.tokenizer(split))\n            if num_cur_tokens <= chunk_size:\n                new_splits.append(split)\n            else:\n                cur_splits = []\n                if self._backup_separators:\n                    for sep in self._backup_separators:\n                        if sep in split:\n          ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 3, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "4": {"text": "   if sep in split:\n                            cur_splits = split.split(sep)\n                            break\n                else:\n                    cur_splits = [split]\n\n                cur_splits2 = []\n                for cur_split in cur_splits:\n                    num_cur_tokens = len(self.tokenizer(cur_split))\n                    if num_cur_tokens <= chunk_size:\n                        cur_splits2.extend([cur_split])\n                    else:\n                        cur_split_chunks = [\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 4, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "5": {"text": "       cur_split_chunks = [\n                            cur_split[i : i + chunk_size]\n                            for i in range(0, len(cur_split), chunk_size)\n                        ]\n                        cur_splits2.extend(cur_split_chunks)\n\n                new_splits.extend(cur_splits2)\n        return new_splits\n\n    def split_text(self, text: str, extra_info_str: Optional[str] = None) -> List[str]:\n        \"\"\"Split incoming text and return chunks.\"\"\"\n        text_slits = self.split_text_with_overlaps(text, extra_info_str=extra_info_str)\n        return [text_split.text_chunk for text_split in text_slits]\n\n    def", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 5, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "6": {"text": "for text_split in text_slits]\n\n    def split_text_with_overlaps(\n        self, text: str, extra_info_str: Optional[str] = None\n    ) -> List[TextSplit]:\n        \"\"\"Split incoming text and return chunks with overlap size.\"\"\"\n        if text == \"\":\n            return []\n\n        # NOTE: Consider extra info str that will be added to the chunk at query time\n        #       This reduces the effective chunk size that we can have\n        if extra_info_str is not None:\n            # NOTE: extra 2 newline chars for formatting when prepending in query\n            num_extra_tokens = len(self.tokenizer(f\"{extra_info_str}\\n\\n\")) + 1\n            effective_chunk_size = self._chunk_size - num_extra_tokens\n\n            if effective_chunk_size <= 0:\n                raise ValueError(\n            ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 6, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "7": {"text": "   raise ValueError(\n                    \"Effective chunk size is non positive after considering extra_info\"\n                )\n        else:\n            effective_chunk_size = self._chunk_size\n\n        # First we naively split the large input into a bunch of smaller ones.\n        splits = text.split(self._separator)\n        splits = self._process_splits(splits, effective_chunk_size)\n        # We now want to combine these smaller pieces into medium size\n        # chunks to send to the LLM.\n        docs = []\n\n        start_idx = 0\n        cur_idx = 0\n        cur_total = 0\n        prev_idx = 0  # store the previous end index\n        while cur_idx < len(splits):\n            cur_token = splits[cur_idx]\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 7, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "8": {"text": "splits[cur_idx]\n            num_cur_tokens = max(len(self.tokenizer(cur_token)), 1)\n            if num_cur_tokens > effective_chunk_size:\n                raise ValueError(\n                    \"A single term is larger than the allowed chunk size.\\n\"\n                    f\"Term size: {num_cur_tokens}\\n\"\n                    f\"Chunk size: {self._chunk_size}\"\n                    f\"Effective chunk size: {effective_chunk_size}\"\n                )\n            # If adding token to current_doc would exceed the chunk size:\n            # 1. First verify with tokenizer that current_doc\n            # 1. Update the docs list\n            if cur_total +", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 8, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "9": {"text": "the docs list\n            if cur_total + num_cur_tokens > effective_chunk_size:\n                # NOTE: since we use a proxy for counting tokens, we want to\n                # run tokenizer across all of current_doc first. If\n                # the chunk is too big, then we will reduce text in pieces\n                cur_idx = self._reduce_chunk_size(start_idx, cur_idx, splits)\n                overlap = 0\n                # after first round, check if last chunk ended after this chunk begins\n                if prev_idx > 0 and prev_idx > start_idx:\n                    overlap = sum([len(splits[i]) for i in range(start_idx, prev_idx)])\n                docs.append(\n            ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 9, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "10": {"text": "   docs.append(\n                    TextSplit(self._separator.join(splits[start_idx:cur_idx]), overlap)\n                )\n                prev_idx = cur_idx\n                # 2. Shrink the current_doc (from the front) until it is gets smaller\n                # than the overlap size\n                # NOTE: because counting tokens individually is an imperfect\n                # proxy (but much faster proxy) for the total number of tokens consumed,\n                # we need to enforce that start_idx <= cur_idx, otherwise\n                # start_idx has a chance of going out of bounds.\n                while cur_total > self._chunk_overlap and start_idx < cur_idx:\n               ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 10, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "11": {"text": "                   cur_num_tokens = max(len(self.tokenizer(splits[start_idx])), 1)\n                    cur_total -= cur_num_tokens\n                    start_idx += 1\n            # Build up the current_doc with term d, and update the total counter with\n            # the number of the number of tokens in d, wrt self.tokenizer\n\n            # we reassign cur_token and num_cur_tokens, because cur_idx\n            # may have changed\n            cur_token = splits[cur_idx]\n            num_cur_tokens = max(len(self.tokenizer(cur_token)), 1)\n\n            cur_total += num_cur_tokens\n            cur_idx += 1\n        overlap = 0\n        # after first round, check if last", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 11, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "12": {"text": "overlap = 0\n        # after first round, check if last chunk ended after this chunk begins\n        if prev_idx > start_idx:\n            overlap = sum([len(splits[i]) for i in range(start_idx, prev_idx)]) + len(\n                range(start_idx, prev_idx)\n            )\n        docs.append(TextSplit(self._separator.join(splits[start_idx:cur_idx]), overlap))\n        return docs\n\n    def truncate_text(self, text: str) -> str:\n        \"\"\"Truncate text in order to fit the underlying chunk size.\"\"\"\n        if text == \"\":\n            return \"\"\n        # First we naively split the large input into a bunch of smaller ones.\n        splits = text.split(self._separator)\n        splits = self._process_splits(splits, self._chunk_size)\n\n        start_idx = 0\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 12, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "13": {"text": "       start_idx = 0\n        cur_idx = 0\n        cur_total = 0\n        while cur_idx < len(splits):\n            cur_token = splits[cur_idx]\n            num_cur_tokens = max(len(self.tokenizer(cur_token)), 1)\n            if cur_total + num_cur_tokens > self._chunk_size:\n                cur_idx = self._reduce_chunk_size(start_idx, cur_idx, splits)\n                break\n            cur_total += num_cur_tokens\n            cur_idx += 1\n        return self._separator.join(splits[start_idx:cur_idx])\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 13, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "14": {"text": "This code file contains the implementation of a text splitter that splits text into chunks of a specified size. It uses a tokenizer to count the number of tokens in each chunk and can also take into account an extra info string that will be added to the chunk at query time. It also has the ability to split tokens that are too large for the chunk size, either by using backup separators or by forcing chunking. The split_text and split_text_with_overlaps functions are used to split the text into chunks and return them as a list of TextSplit objects, which contain the text chunk and the number of overlapping characters with the previous chunk.", "doc_id": null, "embedding": null, "extra_info": null, "index": 14, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "ref_doc_id": null, "node_info": null}, "15": {"text": "text_splitter.py is a Python file that contains functions to split text into chunks of a specified size. It uses the tokenizer function to count the number of tokens in each chunk and the _separator to join the chunks together. The _process_splits function is used to split the text into smaller chunks, while the _reduce_chunk_size function is used to shrink the current chunk until it is smaller than the overlap size. The truncate_text function is used to truncate the text in order to fit the underlying chunk size.", "doc_id": null, "embedding": null, "extra_info": null, "index": 15, "child_indices": [10, 11, 12, 13], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"14": {"text": "This code file contains the implementation of a text splitter that splits text into chunks of a specified size. It uses a tokenizer to count the number of tokens in each chunk and can also take into account an extra info string that will be added to the chunk at query time. It also has the ability to split tokens that are too large for the chunk size, either by using backup separators or by forcing chunking. The split_text and split_text_with_overlaps functions are used to split the text into chunks and return them as a list of TextSplit objects, which contain the text chunk and the number of overlapping characters with the previous chunk.", "doc_id": null, "embedding": null, "extra_info": null, "index": 14, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "ref_doc_id": null, "node_info": null}, "15": {"text": "text_splitter.py is a Python file that contains functions to split text into chunks of a specified size. It uses the tokenizer function to count the number of tokens in each chunk and the _separator to join the chunks together. The _process_splits function is used to split the text into smaller chunks, while the _reduce_chunk_size function is used to shrink the current chunk until it is smaller than the overlap size. The truncate_text function is used to truncate the text in order to fit the underlying chunk size.", "doc_id": null, "embedding": null, "extra_info": null, "index": 15, "child_indices": [10, 11, 12, 13], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"b329d2f933f606b0fe34b6e40cf993b5ebe75d02": {"text": "\"\"\"Text splitter implementations.\"\"\"\nfrom dataclasses import dataclass\nfrom typing import Callable, List, Optional\n\nfrom langchain.text_splitter import TextSplitter\n\nfrom gpt_index.utils import globals_helper\n\n\n@dataclass\nclass TextSplit:\n    \"\"\"Text split with overlap.\n\n    Attributes:\n        text_chunk: The text string.\n        num_char_overlap: The number of overlapping characters with the previous chunk.\n    \"\"\"\n\n    text_chunk: str\n    num_char_overlap: int\n\n\nclass TokenTextSplitter(TextSplitter):\n    \"\"\"Implementation of splitting text that looks at word tokens.\"\"\"\n\n    def __init__(\n        self,\n        separator: str = \" \",\n        chunk_size: int = 4000,\n        chunk_overlap: int = 200,\n        tokenizer: Optional[Callable] = None,\n        backup_separators: Optional[List[str]] = [\"\\n\"],\n    ):\n        \"\"\"Initialize with parameters.\"\"\"\n        if chunk_overlap > chunk_size:\n            raise ValueError(\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\n                f\"({chunk_size}), should be smaller.\"\n            )\n        self._separator = separator\n        self._chunk_size = chunk_size\n        self._chunk_overlap = chunk_overlap\n        self.tokenizer = tokenizer or globals_helper.tokenizer\n        self._backup_separators = backup_separators\n\n    def _reduce_chunk_size(\n        self, start_idx: int, cur_idx: int, splits: List[str]\n    ) -> int:\n        \"\"\"Reduce the chunk size by reducing cur_idx.\n\n        Return the new cur_idx.\n\n        \"\"\"\n        current_doc_total = len(\n            self.tokenizer(self._separator.join(splits[start_idx:cur_idx]))\n        )\n        while current_doc_total > self._chunk_size:\n            percent_to_reduce = (\n                current_doc_total - self._chunk_size\n            ) / current_doc_total\n            num_to_reduce = int(percent_to_reduce * (cur_idx - start_idx)) + 1\n            cur_idx -= num_to_reduce\n            current_doc_total = len(\n                self.tokenizer(self._separator.join(splits[start_idx:cur_idx]))\n            )\n        return cur_idx\n\n    def _process_splits(self, splits: List[str], chunk_size: int) -> List[str]:\n        \"\"\"Process splits.\n\n        Specifically search for tokens that are too large for chunk size,\n        and see if we can separate those tokens more\n        (via backup separators if specified, or force chunking).\n\n        \"\"\"\n        new_splits = []\n        for split in splits:\n            num_cur_tokens = len(self.tokenizer(split))\n            if num_cur_tokens <= chunk_size:\n                new_splits.append(split)\n            else:\n                cur_splits = []\n                if self._backup_separators:\n                    for sep in self._backup_separators:\n                        if sep in split:\n                            cur_splits = split.split(sep)\n                            break\n                else:\n                    cur_splits = [split]\n\n                cur_splits2 = []\n                for cur_split in cur_splits:\n                    num_cur_tokens = len(self.tokenizer(cur_split))\n                    if num_cur_tokens <= chunk_size:\n                        cur_splits2.extend([cur_split])\n                    else:\n                        cur_split_chunks = [\n                            cur_split[i : i + chunk_size]\n                            for i in range(0, len(cur_split), chunk_size)\n                        ]\n                        cur_splits2.extend(cur_split_chunks)\n\n                new_splits.extend(cur_splits2)\n        return new_splits\n\n    def split_text(self, text: str, extra_info_str: Optional[str] = None) -> List[str]:\n        \"\"\"Split incoming text and return chunks.\"\"\"\n        text_slits = self.split_text_with_overlaps(text, extra_info_str=extra_info_str)\n        return [text_split.text_chunk for text_split in text_slits]\n\n    def split_text_with_overlaps(\n        self, text: str, extra_info_str: Optional[str] = None\n    ) -> List[TextSplit]:\n        \"\"\"Split incoming text and return chunks with overlap size.\"\"\"\n        if text == \"\":\n            return []\n\n        # NOTE: Consider extra info str that will be added to the chunk at query time\n        #       This reduces the effective chunk size that we can have\n        if extra_info_str is not None:\n            # NOTE: extra 2 newline chars for formatting when prepending in query\n            num_extra_tokens = len(self.tokenizer(f\"{extra_info_str}\\n\\n\")) + 1\n            effective_chunk_size = self._chunk_size - num_extra_tokens\n\n            if effective_chunk_size <= 0:\n                raise ValueError(\n                    \"Effective chunk size is non positive after considering extra_info\"\n                )\n        else:\n            effective_chunk_size = self._chunk_size\n\n        # First we naively split the large input into a bunch of smaller ones.\n        splits = text.split(self._separator)\n        splits = self._process_splits(splits, effective_chunk_size)\n        # We now want to combine these smaller pieces into medium size\n        # chunks to send to the LLM.\n        docs = []\n\n        start_idx = 0\n        cur_idx = 0\n        cur_total = 0\n        prev_idx = 0  # store the previous end index\n        while cur_idx < len(splits):\n            cur_token = splits[cur_idx]\n            num_cur_tokens = max(len(self.tokenizer(cur_token)), 1)\n            if num_cur_tokens > effective_chunk_size:\n                raise ValueError(\n                    \"A single term is larger than the allowed chunk size.\\n\"\n                    f\"Term size: {num_cur_tokens}\\n\"\n                    f\"Chunk size: {self._chunk_size}\"\n                    f\"Effective chunk size: {effective_chunk_size}\"\n                )\n            # If adding token to current_doc would exceed the chunk size:\n            # 1. First verify with tokenizer that current_doc\n            # 1. Update the docs list\n            if cur_total + num_cur_tokens > effective_chunk_size:\n                # NOTE: since we use a proxy for counting tokens, we want to\n                # run tokenizer across all of current_doc first. If\n                # the chunk is too big, then we will reduce text in pieces\n                cur_idx = self._reduce_chunk_size(start_idx, cur_idx, splits)\n                overlap = 0\n                # after first round, check if last chunk ended after this chunk begins\n                if prev_idx > 0 and prev_idx > start_idx:\n                    overlap = sum([len(splits[i]) for i in range(start_idx, prev_idx)])\n                docs.append(\n                    TextSplit(self._separator.join(splits[start_idx:cur_idx]), overlap)\n                )\n                prev_idx = cur_idx\n                # 2. Shrink the current_doc (from the front) until it is gets smaller\n                # than the overlap size\n                # NOTE: because counting tokens individually is an imperfect\n                # proxy (but much faster proxy) for the total number of tokens consumed,\n                # we need to enforce that start_idx <= cur_idx, otherwise\n                # start_idx has a chance of going out of bounds.\n                while cur_total > self._chunk_overlap and start_idx < cur_idx:\n                    cur_num_tokens = max(len(self.tokenizer(splits[start_idx])), 1)\n                    cur_total -= cur_num_tokens\n                    start_idx += 1\n            # Build up the current_doc with term d, and update the total counter with\n            # the number of the number of tokens in d, wrt self.tokenizer\n\n            # we reassign cur_token and num_cur_tokens, because cur_idx\n            # may have changed\n            cur_token = splits[cur_idx]\n            num_cur_tokens = max(len(self.tokenizer(cur_token)), 1)\n\n            cur_total += num_cur_tokens\n            cur_idx += 1\n        overlap = 0\n        # after first round, check if last chunk ended after this chunk begins\n        if prev_idx > start_idx:\n            overlap = sum([len(splits[i]) for i in range(start_idx, prev_idx)]) + len(\n                range(start_idx, prev_idx)\n            )\n        docs.append(TextSplit(self._separator.join(splits[start_idx:cur_idx]), overlap))\n        return docs\n\n    def truncate_text(self, text: str) -> str:\n        \"\"\"Truncate text in order to fit the underlying chunk size.\"\"\"\n        if text == \"\":\n            return \"\"\n        # First we naively split the large input into a bunch of smaller ones.\n        splits = text.split(self._separator)\n        splits = self._process_splits(splits, self._chunk_size)\n\n        start_idx = 0\n        cur_idx = 0\n        cur_total = 0\n        while cur_idx < len(splits):\n            cur_token = splits[cur_idx]\n            num_cur_tokens = max(len(self.tokenizer(cur_token)), 1)\n            if cur_total + num_cur_tokens > self._chunk_size:\n                cur_idx = self._reduce_chunk_size(start_idx, cur_idx, splits)\n                break\n            cur_total += num_cur_tokens\n            cur_idx += 1\n        return self._separator.join(splits[start_idx:cur_idx])\n", "doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "__type__": "Document"}, "53b91517-3e8b-4706-86f9-a33bd245f3ad": {"text": "\nThe text_splitter.py file contains functions to split text into chunks of a specified size. It uses a tokenizer to count the number of tokens in each chunk and a separator to join the chunks together. The split_text and split_text_with_overlaps functions are used to split the text into chunks and return them as a list of TextSplit objects. The _reduce_chunk_size and _process_splits functions are used to reduce the chunk size and process the splits, respectively. The truncate_text function is used to truncate the text in order to fit the underlying chunk size. The code is designed to make it easier to split large amounts of text into smaller chunks for easier processing. The TextSplitter and TokenTextSplitter classes are implementations of the TextSplitter class that look at word tokens when splitting the text. The parameters for the separator, chunk size, chunk overlap, tokenizer, and backup separators are used to customize the splitting process.", "doc_id": "53b91517-3e8b-4706-86f9-a33bd245f3ad", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Text splitter implementations.\"\"\"\nfrom dataclasses import dataclass\nfrom typing import Callable, List, Optional\n\nfrom langchain.text_splitter import TextSplitter\n\nfrom gpt_index.utils import globals_helper\n\n\n@dataclass\nclass TextSplit:\n    \"\"\"Text split with overlap.\n\n    Attributes:\n        text_chunk: The text string.\n        num_char_overlap: The number of overlapping characters with the previous chunk.\n    \"\"\"\n\n    text_chunk: str\n    num_char_overlap: int\n\n\nclass TokenTextSplitter(TextSplitter):\n    \"\"\"Implementation of splitting text that looks at word tokens.\"\"\"\n\n    def __init__(\n        self,\n        separator: str = \" \",\n        chunk_size: int = 4000,\n        chunk_overlap: int = 200,\n        tokenizer: Optional[Callable] = None,\n        backup_separators: Optional[List[str]] = [\"\\n\"],\n    ):\n        \"\"\"Initialize with parameters.\"\"\"\n        if chunk_overlap >", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 0, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "1": {"text": "with parameters.\"\"\"\n        if chunk_overlap > chunk_size:\n            raise ValueError(\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\n                f\"({chunk_size}), should be smaller.\"\n            )\n        self._separator = separator\n        self._chunk_size = chunk_size\n        self._chunk_overlap = chunk_overlap\n        self.tokenizer = tokenizer or globals_helper.tokenizer\n        self._backup_separators = backup_separators\n\n    def _reduce_chunk_size(\n        self, start_idx: int, cur_idx: int, splits: List[str]\n    ) -> int:\n        \"\"\"Reduce the chunk size by reducing cur_idx.\n\n        Return the new cur_idx.\n\n        \"\"\"\n        current_doc_total = len(\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 1, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "2": {"text": "\"\"\"\n        current_doc_total = len(\n            self.tokenizer(self._separator.join(splits[start_idx:cur_idx]))\n        )\n        while current_doc_total > self._chunk_size:\n            percent_to_reduce = (\n                current_doc_total - self._chunk_size\n            ) / current_doc_total\n            num_to_reduce = int(percent_to_reduce * (cur_idx - start_idx)) + 1\n            cur_idx -= num_to_reduce\n            current_doc_total = len(\n                self.tokenizer(self._separator.join(splits[start_idx:cur_idx]))\n            )\n        return cur_idx\n\n    def _process_splits(self, splits: List[str], chunk_size: int) -> List[str]:\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 2, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "3": {"text": "chunk_size: int) -> List[str]:\n        \"\"\"Process splits.\n\n        Specifically search for tokens that are too large for chunk size,\n        and see if we can separate those tokens more\n        (via backup separators if specified, or force chunking).\n\n        \"\"\"\n        new_splits = []\n        for split in splits:\n            num_cur_tokens = len(self.tokenizer(split))\n            if num_cur_tokens <= chunk_size:\n                new_splits.append(split)\n            else:\n                cur_splits = []\n                if self._backup_separators:\n                    for sep in self._backup_separators:\n                        if sep in split:\n          ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 3, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "4": {"text": "   if sep in split:\n                            cur_splits = split.split(sep)\n                            break\n                else:\n                    cur_splits = [split]\n\n                cur_splits2 = []\n                for cur_split in cur_splits:\n                    num_cur_tokens = len(self.tokenizer(cur_split))\n                    if num_cur_tokens <= chunk_size:\n                        cur_splits2.extend([cur_split])\n                    else:\n                        cur_split_chunks = [\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 4, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "5": {"text": "       cur_split_chunks = [\n                            cur_split[i : i + chunk_size]\n                            for i in range(0, len(cur_split), chunk_size)\n                        ]\n                        cur_splits2.extend(cur_split_chunks)\n\n                new_splits.extend(cur_splits2)\n        return new_splits\n\n    def split_text(self, text: str, extra_info_str: Optional[str] = None) -> List[str]:\n        \"\"\"Split incoming text and return chunks.\"\"\"\n        text_slits = self.split_text_with_overlaps(text, extra_info_str=extra_info_str)\n        return [text_split.text_chunk for text_split in text_slits]\n\n    def", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 5, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "6": {"text": "for text_split in text_slits]\n\n    def split_text_with_overlaps(\n        self, text: str, extra_info_str: Optional[str] = None\n    ) -> List[TextSplit]:\n        \"\"\"Split incoming text and return chunks with overlap size.\"\"\"\n        if text == \"\":\n            return []\n\n        # NOTE: Consider extra info str that will be added to the chunk at query time\n        #       This reduces the effective chunk size that we can have\n        if extra_info_str is not None:\n            # NOTE: extra 2 newline chars for formatting when prepending in query\n            num_extra_tokens = len(self.tokenizer(f\"{extra_info_str}\\n\\n\")) + 1\n            effective_chunk_size = self._chunk_size - num_extra_tokens\n\n            if effective_chunk_size <= 0:\n                raise ValueError(\n            ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 6, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "7": {"text": "   raise ValueError(\n                    \"Effective chunk size is non positive after considering extra_info\"\n                )\n        else:\n            effective_chunk_size = self._chunk_size\n\n        # First we naively split the large input into a bunch of smaller ones.\n        splits = text.split(self._separator)\n        splits = self._process_splits(splits, effective_chunk_size)\n        # We now want to combine these smaller pieces into medium size\n        # chunks to send to the LLM.\n        docs = []\n\n        start_idx = 0\n        cur_idx = 0\n        cur_total = 0\n        prev_idx = 0  # store the previous end index\n        while cur_idx < len(splits):\n            cur_token = splits[cur_idx]\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 7, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "8": {"text": "splits[cur_idx]\n            num_cur_tokens = max(len(self.tokenizer(cur_token)), 1)\n            if num_cur_tokens > effective_chunk_size:\n                raise ValueError(\n                    \"A single term is larger than the allowed chunk size.\\n\"\n                    f\"Term size: {num_cur_tokens}\\n\"\n                    f\"Chunk size: {self._chunk_size}\"\n                    f\"Effective chunk size: {effective_chunk_size}\"\n                )\n            # If adding token to current_doc would exceed the chunk size:\n            # 1. First verify with tokenizer that current_doc\n            # 1. Update the docs list\n            if cur_total +", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 8, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "9": {"text": "the docs list\n            if cur_total + num_cur_tokens > effective_chunk_size:\n                # NOTE: since we use a proxy for counting tokens, we want to\n                # run tokenizer across all of current_doc first. If\n                # the chunk is too big, then we will reduce text in pieces\n                cur_idx = self._reduce_chunk_size(start_idx, cur_idx, splits)\n                overlap = 0\n                # after first round, check if last chunk ended after this chunk begins\n                if prev_idx > 0 and prev_idx > start_idx:\n                    overlap = sum([len(splits[i]) for i in range(start_idx, prev_idx)])\n                docs.append(\n            ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 9, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "10": {"text": "   docs.append(\n                    TextSplit(self._separator.join(splits[start_idx:cur_idx]), overlap)\n                )\n                prev_idx = cur_idx\n                # 2. Shrink the current_doc (from the front) until it is gets smaller\n                # than the overlap size\n                # NOTE: because counting tokens individually is an imperfect\n                # proxy (but much faster proxy) for the total number of tokens consumed,\n                # we need to enforce that start_idx <= cur_idx, otherwise\n                # start_idx has a chance of going out of bounds.\n                while cur_total > self._chunk_overlap and start_idx < cur_idx:\n               ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 10, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "11": {"text": "                   cur_num_tokens = max(len(self.tokenizer(splits[start_idx])), 1)\n                    cur_total -= cur_num_tokens\n                    start_idx += 1\n            # Build up the current_doc with term d, and update the total counter with\n            # the number of the number of tokens in d, wrt self.tokenizer\n\n            # we reassign cur_token and num_cur_tokens, because cur_idx\n            # may have changed\n            cur_token = splits[cur_idx]\n            num_cur_tokens = max(len(self.tokenizer(cur_token)), 1)\n\n            cur_total += num_cur_tokens\n            cur_idx += 1\n        overlap = 0\n        # after first round, check if last", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 11, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "12": {"text": "overlap = 0\n        # after first round, check if last chunk ended after this chunk begins\n        if prev_idx > start_idx:\n            overlap = sum([len(splits[i]) for i in range(start_idx, prev_idx)]) + len(\n                range(start_idx, prev_idx)\n            )\n        docs.append(TextSplit(self._separator.join(splits[start_idx:cur_idx]), overlap))\n        return docs\n\n    def truncate_text(self, text: str) -> str:\n        \"\"\"Truncate text in order to fit the underlying chunk size.\"\"\"\n        if text == \"\":\n            return \"\"\n        # First we naively split the large input into a bunch of smaller ones.\n        splits = text.split(self._separator)\n        splits = self._process_splits(splits, self._chunk_size)\n\n        start_idx = 0\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 12, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "13": {"text": "       start_idx = 0\n        cur_idx = 0\n        cur_total = 0\n        while cur_idx < len(splits):\n            cur_token = splits[cur_idx]\n            num_cur_tokens = max(len(self.tokenizer(cur_token)), 1)\n            if cur_total + num_cur_tokens > self._chunk_size:\n                cur_idx = self._reduce_chunk_size(start_idx, cur_idx, splits)\n                break\n            cur_total += num_cur_tokens\n            cur_idx += 1\n        return self._separator.join(splits[start_idx:cur_idx])\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "index": 13, "child_indices": [], "ref_doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "node_info": null}, "14": {"text": "This code file contains the implementation of a text splitter that splits text into chunks of a specified size. It uses a tokenizer to count the number of tokens in each chunk and can also take into account an extra info string that will be added to the chunk at query time. It also has the ability to split tokens that are too large for the chunk size, either by using backup separators or by forcing chunking. The split_text and split_text_with_overlaps functions are used to split the text into chunks and return them as a list of TextSplit objects, which contain the text chunk and the number of overlapping characters with the previous chunk.", "doc_id": null, "embedding": null, "extra_info": null, "index": 14, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "ref_doc_id": null, "node_info": null}, "15": {"text": "text_splitter.py is a Python file that contains functions to split text into chunks of a specified size. It uses the tokenizer function to count the number of tokens in each chunk and the _separator to join the chunks together. The _process_splits function is used to split the text into smaller chunks, while the _reduce_chunk_size function is used to shrink the current chunk until it is smaller than the overlap size. The truncate_text function is used to truncate the text in order to fit the underlying chunk size.", "doc_id": null, "embedding": null, "extra_info": null, "index": 15, "child_indices": [10, 11, 12, 13], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"14": {"text": "This code file contains the implementation of a text splitter that splits text into chunks of a specified size. It uses a tokenizer to count the number of tokens in each chunk and can also take into account an extra info string that will be added to the chunk at query time. It also has the ability to split tokens that are too large for the chunk size, either by using backup separators or by forcing chunking. The split_text and split_text_with_overlaps functions are used to split the text into chunks and return them as a list of TextSplit objects, which contain the text chunk and the number of overlapping characters with the previous chunk.", "doc_id": null, "embedding": null, "extra_info": null, "index": 14, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "ref_doc_id": null, "node_info": null}, "15": {"text": "text_splitter.py is a Python file that contains functions to split text into chunks of a specified size. It uses the tokenizer function to count the number of tokens in each chunk and the _separator to join the chunks together. The _process_splits function is used to split the text into smaller chunks, while the _reduce_chunk_size function is used to shrink the current chunk until it is smaller than the overlap size. The truncate_text function is used to truncate the text in order to fit the underlying chunk size.", "doc_id": null, "embedding": null, "extra_info": null, "index": 15, "child_indices": [10, 11, 12, 13], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}