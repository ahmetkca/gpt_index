{"index_struct": {"text": "\nThe web.py file contains code for four web page readers: SimpleWebPageReader, TrafilaturaWebReader, BeautifulSoupWebReader, and RssReader. The SimpleWebPageReader class uses the RequestsWrapper utility to scrape web pages and convert HTML to text if specified. The TrafilaturaWebReader class uses the trafilatura package to extract text from web pages and throws an error if the page cannot be parsed. The BeautifulSoupWebReader class uses the bs4, requests, and urllib packages to scrape web pages and extract text from them. The RssReader class uses the feedparser package to read content from an RSS feed and optionally convert HTML to text using the html2text package. The code also contains a main function which calls the SimpleWebPageReader class and prints the results. The purpose of this code is to provide a way to scrape web pages and extract text from them, as well as to read content from an RSS feed and convert HTML to text if specified.", "doc_id": "4cced4a2-ebaa-47a9-9741-7403b306e07c", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Web scraper.\"\"\"\nimport logging\nfrom typing import Any, Callable, Dict, List, Optional, Tuple\n\nfrom langchain.utilities import RequestsWrapper\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass SimpleWebPageReader(BaseReader):\n    \"\"\"Simple web page reader.\n\n    Reads pages from the web.\n\n    Args:\n        html_to_text (bool): Whether to convert HTML to text.\n            Requires `html2text` package.\n\n    \"\"\"\n\n    def __init__(self, html_to_text: bool = False) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            import html2text  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`html2text` package not found, please run `pip install html2text`\"\n            )\n        self._html_to_text = html_to_text\n\n    def load_data(self, urls: List[str]) ->", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 0, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "1": {"text": "   def load_data(self, urls: List[str]) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            urls (List[str]): List of URLs to scrape.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        if not isinstance(urls, list):\n            raise ValueError(\"urls must be a list of strings.\")\n        requests = RequestsWrapper()\n        documents = []\n        for url in urls:\n            response = requests.run(url)\n            if self._html_to_text:\n                import html2text\n\n                response = html2text.html2text(response)\n\n            documents.append(Document(response))\n\n        return documents\n\n\nclass TrafilaturaWebReader(BaseReader):\n    \"\"\"Trafilatura web page reader.\n\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 1, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "2": {"text": "   \"\"\"Trafilatura web page reader.\n\n    Reads pages from the web.\n    Requires the `trafilatura` package.\n\n    \"\"\"\n\n    def __init__(self, error_on_missing: bool = False) -> None:\n        \"\"\"Initialize with parameters.\n\n        Args:\n            error_on_missing (bool): Throw an error when data cannot be parsed\n        \"\"\"\n        self.error_on_missing = error_on_missing\n        try:\n            import trafilatura  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`trafilatura` package not found, please run `pip install trafilatura`\"\n            )\n\n    def load_data(self, urls: List[str]) -> List[Document]:\n        \"\"\"Load data from the urls.\n\n        Args:\n            urls (List[str]): List of URLs to scrape.\n\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 2, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "3": {"text": "List of URLs to scrape.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        import trafilatura\n\n        if not isinstance(urls, list):\n            raise ValueError(\"urls must be a list of strings.\")\n        documents = []\n        for url in urls:\n            downloaded = trafilatura.fetch_url(url)\n            if not downloaded:\n                if self.error_on_missing:\n                    raise ValueError(f\"Trafilatura fails to get string from url: {url}\")\n                continue\n            response = trafilatura.extract(downloaded)\n            if not response:\n                if self.error_on_missing:\n                    raise", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 3, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "4": {"text": "                   raise ValueError(f\"Trafilatura fails to parse page: {url}\")\n                continue\n            documents.append(Document(response))\n\n        return documents\n\n\ndef _substack_reader(soup: Any) -> Tuple[str, Dict[str, Any]]:\n    \"\"\"Extract text from Substack blog post.\"\"\"\n    extra_info = {\n        \"Title of this Substack post\": soup.select_one(\"h1.post-title\").getText(),\n        \"Subtitle\": soup.select_one(\"h3.subtitle\").getText(),\n        \"Author\": soup.select_one(\"span.byline-names\").getText(),\n    }\n    text = soup.select_one(\"div.available-content\").getText()\n    return text, extra_info\n\n\nDEFAULT_WEBSITE_EXTRACTOR: Dict[str, Callable[[Any], Tuple[str, Dict[str, Any]]]] = {\n    \"substack.com\": _substack_reader,\n}\n\n\nclass BeautifulSoupWebReader(BaseReader):\n    \"\"\"BeautifulSoup web page reader.\n\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 4, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "5": {"text": "   \"\"\"BeautifulSoup web page reader.\n\n    Reads pages from the web.\n    Requires the `bs4` and `urllib` packages.\n\n    Args:\n        file_extractor (Optional[Dict[str, Callable]]): A mapping of website\n            hostname (e.g. google.com) to a function that specifies how to\n            extract text from the BeautifulSoup obj. See DEFAULT_WEBSITE_EXTRACTOR.\n    \"\"\"\n\n    def __init__(\n        self,\n        website_extractor: Optional[Dict[str, Callable]] = None,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            from urllib.parse import urlparse  # noqa: F401\n\n            import requests  # noqa: F401\n            from bs4 import BeautifulSoup  # noqa: F401\n        except ImportError:\n            raise ValueError(\n               ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 5, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "6": {"text": "raise ValueError(\n                \"`bs4`, `requests`, and `urllib` must be installed to scrape websites.\"\n                \"Please run `pip install bs4 requests urllib`.\"\n            )\n\n        self.website_extractor = website_extractor or DEFAULT_WEBSITE_EXTRACTOR\n\n    def load_data(\n        self, urls: List[str], custom_hostname: Optional[str] = None\n    ) -> List[Document]:\n        \"\"\"Load data from the urls.\n\n        Args:\n            urls (List[str]): List of URLs to scrape.\n            custom_hostname (Optional[str]): Force a certain hostname in the case\n                a website is displayed under custom URLs (e.g. Substack blogs)\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        from urllib.parse import urlparse\n\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 6, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "7": {"text": "     from urllib.parse import urlparse\n\n        import requests\n        from bs4 import BeautifulSoup\n\n        documents = []\n        for url in urls:\n            try:\n                page = requests.get(url)\n            except Exception:\n                raise ValueError(f\"One of the inputs is not a valid url: {url}\")\n\n            hostname = custom_hostname or urlparse(url).hostname or \"\"\n\n            soup = BeautifulSoup(page.content, \"html.parser\")\n\n            data = \"\"\n            extra_info = {\"URL\": url}\n            if hostname in self.website_extractor:\n                data, metadata = self.website_extractor[hostname](soup)\n                extra_info.update(metadata)\n            else:\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 7, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "8": {"text": "           else:\n                data = soup.getText()\n\n            documents.append(Document(data, extra_info=extra_info))\n\n        return documents\n\n\nclass RssReader(BaseReader):\n    \"\"\"RSS reader.\n\n    Reads content from an RSS feed.\n\n    \"\"\"\n\n    def __init__(self, html_to_text: bool = False) -> None:\n        \"\"\"Initialize with parameters.\n\n        Args:\n            html_to_text (bool): Whether to convert HTML to text.\n                Requires `html2text` package.\n\n        \"\"\"\n        try:\n            import feedparser  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`feedparser` package not found, please run `pip install feedparser`\"\n            )\n\n        if html_to_text:\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 8, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "9": {"text": "       if html_to_text:\n            try:\n                import html2text  # noqa: F401\n            except ImportError:\n                raise ValueError(\n                    \"`html2text` package not found, please run `pip install html2text`\"\n                )\n        self._html_to_text = html_to_text\n\n    def load_data(self, urls: List[str]) -> List[Document]:\n        \"\"\"Load data from RSS feeds.\n\n        Args:\n            urls (List[str]): List of RSS URLs to load.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        import feedparser\n\n        if not isinstance(urls, list):\n            raise ValueError(\"urls must be a list of strings.\")\n\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 9, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "10": {"text": "ValueError(\"urls must be a list of strings.\")\n\n        documents = []\n\n        for url in urls:\n            parsed = feedparser.parse(url)\n            for entry in parsed.entries:\n                if entry.content:\n                    data = entry.content[0].value\n                else:\n                    data = entry.description or entry.summary\n\n                if self._html_to_text:\n                    import html2text\n\n                    data = html2text.html2text(data)\n\n                extra_info = {\"title\": entry.title, \"link\": entry.link}\n                documents.append(Document(data, extra_info=extra_info))\n\n        return documents\n\n\nif __name__ == \"__main__\":\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 10, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "11": {"text": "    return documents\n\n\nif __name__ == \"__main__\":\n    reader = SimpleWebPageReader()\n    logging.info(reader.load_data([\"http://www.google.com\"]))\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 11, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "12": {"text": "This code file contains two classes, SimpleWebPageReader and TrafilaturaWebReader, which are used to scrape web pages. The SimpleWebPageReader class uses the RequestsWrapper utility to scrape web pages and convert HTML to text if specified. The TrafilaturaWebReader class uses the trafilatura package to scrape web pages and extract text from them. The BeautifulSoupWebReader class uses the bs4 and urllib packages to scrape web pages and extract text from them. The RssReader class uses the feedparser package to scrape RSS feeds and convert HTML to text if specified.", "doc_id": null, "embedding": null, "extra_info": null, "index": 12, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "ref_doc_id": null, "node_info": null}, "13": {"text": "This code file, web.py, is part of the gpt_index/readers directory. It contains a class, SimpleWebPageReader, which is used to read webpages from a list of URLs. The class contains a method, load_data, which takes a list of strings as input and parses the webpages using the feedparser library. It then extracts the content, title, and link from each entry and stores them in a Document object. The Document object is then appended to a list of documents which is returned. The code also contains an if statement which checks if the html_to_text flag is set, and if so, it uses the html2text library to convert the data to text. Finally, the code contains a main function which calls the SimpleWebPageReader class and prints the result.", "doc_id": null, "embedding": null, "extra_info": null, "index": 13, "child_indices": [10, 11], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"12": {"text": "This code file contains two classes, SimpleWebPageReader and TrafilaturaWebReader, which are used to scrape web pages. The SimpleWebPageReader class uses the RequestsWrapper utility to scrape web pages and convert HTML to text if specified. The TrafilaturaWebReader class uses the trafilatura package to scrape web pages and extract text from them. The BeautifulSoupWebReader class uses the bs4 and urllib packages to scrape web pages and extract text from them. The RssReader class uses the feedparser package to scrape RSS feeds and convert HTML to text if specified.", "doc_id": null, "embedding": null, "extra_info": null, "index": 12, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "ref_doc_id": null, "node_info": null}, "13": {"text": "This code file, web.py, is part of the gpt_index/readers directory. It contains a class, SimpleWebPageReader, which is used to read webpages from a list of URLs. The class contains a method, load_data, which takes a list of strings as input and parses the webpages using the feedparser library. It then extracts the content, title, and link from each entry and stores them in a Document object. The Document object is then appended to a list of documents which is returned. The code also contains an if statement which checks if the html_to_text flag is set, and if so, it uses the html2text library to convert the data to text. Finally, the code contains a main function which calls the SimpleWebPageReader class and prints the result.", "doc_id": null, "embedding": null, "extra_info": null, "index": 13, "child_indices": [10, 11], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"eac2fa9f945363412e1401c12d4b4c1fb14376b1": {"text": "\"\"\"Web scraper.\"\"\"\nimport logging\nfrom typing import Any, Callable, Dict, List, Optional, Tuple\n\nfrom langchain.utilities import RequestsWrapper\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass SimpleWebPageReader(BaseReader):\n    \"\"\"Simple web page reader.\n\n    Reads pages from the web.\n\n    Args:\n        html_to_text (bool): Whether to convert HTML to text.\n            Requires `html2text` package.\n\n    \"\"\"\n\n    def __init__(self, html_to_text: bool = False) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            import html2text  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`html2text` package not found, please run `pip install html2text`\"\n            )\n        self._html_to_text = html_to_text\n\n    def load_data(self, urls: List[str]) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            urls (List[str]): List of URLs to scrape.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        if not isinstance(urls, list):\n            raise ValueError(\"urls must be a list of strings.\")\n        requests = RequestsWrapper()\n        documents = []\n        for url in urls:\n            response = requests.run(url)\n            if self._html_to_text:\n                import html2text\n\n                response = html2text.html2text(response)\n\n            documents.append(Document(response))\n\n        return documents\n\n\nclass TrafilaturaWebReader(BaseReader):\n    \"\"\"Trafilatura web page reader.\n\n    Reads pages from the web.\n    Requires the `trafilatura` package.\n\n    \"\"\"\n\n    def __init__(self, error_on_missing: bool = False) -> None:\n        \"\"\"Initialize with parameters.\n\n        Args:\n            error_on_missing (bool): Throw an error when data cannot be parsed\n        \"\"\"\n        self.error_on_missing = error_on_missing\n        try:\n            import trafilatura  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`trafilatura` package not found, please run `pip install trafilatura`\"\n            )\n\n    def load_data(self, urls: List[str]) -> List[Document]:\n        \"\"\"Load data from the urls.\n\n        Args:\n            urls (List[str]): List of URLs to scrape.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        import trafilatura\n\n        if not isinstance(urls, list):\n            raise ValueError(\"urls must be a list of strings.\")\n        documents = []\n        for url in urls:\n            downloaded = trafilatura.fetch_url(url)\n            if not downloaded:\n                if self.error_on_missing:\n                    raise ValueError(f\"Trafilatura fails to get string from url: {url}\")\n                continue\n            response = trafilatura.extract(downloaded)\n            if not response:\n                if self.error_on_missing:\n                    raise ValueError(f\"Trafilatura fails to parse page: {url}\")\n                continue\n            documents.append(Document(response))\n\n        return documents\n\n\ndef _substack_reader(soup: Any) -> Tuple[str, Dict[str, Any]]:\n    \"\"\"Extract text from Substack blog post.\"\"\"\n    extra_info = {\n        \"Title of this Substack post\": soup.select_one(\"h1.post-title\").getText(),\n        \"Subtitle\": soup.select_one(\"h3.subtitle\").getText(),\n        \"Author\": soup.select_one(\"span.byline-names\").getText(),\n    }\n    text = soup.select_one(\"div.available-content\").getText()\n    return text, extra_info\n\n\nDEFAULT_WEBSITE_EXTRACTOR: Dict[str, Callable[[Any], Tuple[str, Dict[str, Any]]]] = {\n    \"substack.com\": _substack_reader,\n}\n\n\nclass BeautifulSoupWebReader(BaseReader):\n    \"\"\"BeautifulSoup web page reader.\n\n    Reads pages from the web.\n    Requires the `bs4` and `urllib` packages.\n\n    Args:\n        file_extractor (Optional[Dict[str, Callable]]): A mapping of website\n            hostname (e.g. google.com) to a function that specifies how to\n            extract text from the BeautifulSoup obj. See DEFAULT_WEBSITE_EXTRACTOR.\n    \"\"\"\n\n    def __init__(\n        self,\n        website_extractor: Optional[Dict[str, Callable]] = None,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            from urllib.parse import urlparse  # noqa: F401\n\n            import requests  # noqa: F401\n            from bs4 import BeautifulSoup  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`bs4`, `requests`, and `urllib` must be installed to scrape websites.\"\n                \"Please run `pip install bs4 requests urllib`.\"\n            )\n\n        self.website_extractor = website_extractor or DEFAULT_WEBSITE_EXTRACTOR\n\n    def load_data(\n        self, urls: List[str], custom_hostname: Optional[str] = None\n    ) -> List[Document]:\n        \"\"\"Load data from the urls.\n\n        Args:\n            urls (List[str]): List of URLs to scrape.\n            custom_hostname (Optional[str]): Force a certain hostname in the case\n                a website is displayed under custom URLs (e.g. Substack blogs)\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        from urllib.parse import urlparse\n\n        import requests\n        from bs4 import BeautifulSoup\n\n        documents = []\n        for url in urls:\n            try:\n                page = requests.get(url)\n            except Exception:\n                raise ValueError(f\"One of the inputs is not a valid url: {url}\")\n\n            hostname = custom_hostname or urlparse(url).hostname or \"\"\n\n            soup = BeautifulSoup(page.content, \"html.parser\")\n\n            data = \"\"\n            extra_info = {\"URL\": url}\n            if hostname in self.website_extractor:\n                data, metadata = self.website_extractor[hostname](soup)\n                extra_info.update(metadata)\n            else:\n                data = soup.getText()\n\n            documents.append(Document(data, extra_info=extra_info))\n\n        return documents\n\n\nclass RssReader(BaseReader):\n    \"\"\"RSS reader.\n\n    Reads content from an RSS feed.\n\n    \"\"\"\n\n    def __init__(self, html_to_text: bool = False) -> None:\n        \"\"\"Initialize with parameters.\n\n        Args:\n            html_to_text (bool): Whether to convert HTML to text.\n                Requires `html2text` package.\n\n        \"\"\"\n        try:\n            import feedparser  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`feedparser` package not found, please run `pip install feedparser`\"\n            )\n\n        if html_to_text:\n            try:\n                import html2text  # noqa: F401\n            except ImportError:\n                raise ValueError(\n                    \"`html2text` package not found, please run `pip install html2text`\"\n                )\n        self._html_to_text = html_to_text\n\n    def load_data(self, urls: List[str]) -> List[Document]:\n        \"\"\"Load data from RSS feeds.\n\n        Args:\n            urls (List[str]): List of RSS URLs to load.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        import feedparser\n\n        if not isinstance(urls, list):\n            raise ValueError(\"urls must be a list of strings.\")\n\n        documents = []\n\n        for url in urls:\n            parsed = feedparser.parse(url)\n            for entry in parsed.entries:\n                if entry.content:\n                    data = entry.content[0].value\n                else:\n                    data = entry.description or entry.summary\n\n                if self._html_to_text:\n                    import html2text\n\n                    data = html2text.html2text(data)\n\n                extra_info = {\"title\": entry.title, \"link\": entry.link}\n                documents.append(Document(data, extra_info=extra_info))\n\n        return documents\n\n\nif __name__ == \"__main__\":\n    reader = SimpleWebPageReader()\n    logging.info(reader.load_data([\"http://www.google.com\"]))\n", "doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "__type__": "Document"}, "4cced4a2-ebaa-47a9-9741-7403b306e07c": {"text": "\nThe web.py file contains code for four web page readers: SimpleWebPageReader, TrafilaturaWebReader, BeautifulSoupWebReader, and RssReader. The SimpleWebPageReader class uses the RequestsWrapper utility to scrape web pages and convert HTML to text if specified. The TrafilaturaWebReader class uses the trafilatura package to extract text from web pages and throws an error if the page cannot be parsed. The BeautifulSoupWebReader class uses the bs4, requests, and urllib packages to scrape web pages and extract text from them. The RssReader class uses the feedparser package to read content from an RSS feed and optionally convert HTML to text using the html2text package. The code also contains a main function which calls the SimpleWebPageReader class and prints the results. The purpose of this code is to provide a way to scrape web pages and extract text from them, as well as to read content from an RSS feed and convert HTML to text if specified.", "doc_id": "4cced4a2-ebaa-47a9-9741-7403b306e07c", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Web scraper.\"\"\"\nimport logging\nfrom typing import Any, Callable, Dict, List, Optional, Tuple\n\nfrom langchain.utilities import RequestsWrapper\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass SimpleWebPageReader(BaseReader):\n    \"\"\"Simple web page reader.\n\n    Reads pages from the web.\n\n    Args:\n        html_to_text (bool): Whether to convert HTML to text.\n            Requires `html2text` package.\n\n    \"\"\"\n\n    def __init__(self, html_to_text: bool = False) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            import html2text  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`html2text` package not found, please run `pip install html2text`\"\n            )\n        self._html_to_text = html_to_text\n\n    def load_data(self, urls: List[str]) ->", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 0, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "1": {"text": "   def load_data(self, urls: List[str]) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            urls (List[str]): List of URLs to scrape.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        if not isinstance(urls, list):\n            raise ValueError(\"urls must be a list of strings.\")\n        requests = RequestsWrapper()\n        documents = []\n        for url in urls:\n            response = requests.run(url)\n            if self._html_to_text:\n                import html2text\n\n                response = html2text.html2text(response)\n\n            documents.append(Document(response))\n\n        return documents\n\n\nclass TrafilaturaWebReader(BaseReader):\n    \"\"\"Trafilatura web page reader.\n\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 1, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "2": {"text": "   \"\"\"Trafilatura web page reader.\n\n    Reads pages from the web.\n    Requires the `trafilatura` package.\n\n    \"\"\"\n\n    def __init__(self, error_on_missing: bool = False) -> None:\n        \"\"\"Initialize with parameters.\n\n        Args:\n            error_on_missing (bool): Throw an error when data cannot be parsed\n        \"\"\"\n        self.error_on_missing = error_on_missing\n        try:\n            import trafilatura  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`trafilatura` package not found, please run `pip install trafilatura`\"\n            )\n\n    def load_data(self, urls: List[str]) -> List[Document]:\n        \"\"\"Load data from the urls.\n\n        Args:\n            urls (List[str]): List of URLs to scrape.\n\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 2, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "3": {"text": "List of URLs to scrape.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        import trafilatura\n\n        if not isinstance(urls, list):\n            raise ValueError(\"urls must be a list of strings.\")\n        documents = []\n        for url in urls:\n            downloaded = trafilatura.fetch_url(url)\n            if not downloaded:\n                if self.error_on_missing:\n                    raise ValueError(f\"Trafilatura fails to get string from url: {url}\")\n                continue\n            response = trafilatura.extract(downloaded)\n            if not response:\n                if self.error_on_missing:\n                    raise", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 3, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "4": {"text": "                   raise ValueError(f\"Trafilatura fails to parse page: {url}\")\n                continue\n            documents.append(Document(response))\n\n        return documents\n\n\ndef _substack_reader(soup: Any) -> Tuple[str, Dict[str, Any]]:\n    \"\"\"Extract text from Substack blog post.\"\"\"\n    extra_info = {\n        \"Title of this Substack post\": soup.select_one(\"h1.post-title\").getText(),\n        \"Subtitle\": soup.select_one(\"h3.subtitle\").getText(),\n        \"Author\": soup.select_one(\"span.byline-names\").getText(),\n    }\n    text = soup.select_one(\"div.available-content\").getText()\n    return text, extra_info\n\n\nDEFAULT_WEBSITE_EXTRACTOR: Dict[str, Callable[[Any], Tuple[str, Dict[str, Any]]]] = {\n    \"substack.com\": _substack_reader,\n}\n\n\nclass BeautifulSoupWebReader(BaseReader):\n    \"\"\"BeautifulSoup web page reader.\n\n  ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 4, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "5": {"text": "   \"\"\"BeautifulSoup web page reader.\n\n    Reads pages from the web.\n    Requires the `bs4` and `urllib` packages.\n\n    Args:\n        file_extractor (Optional[Dict[str, Callable]]): A mapping of website\n            hostname (e.g. google.com) to a function that specifies how to\n            extract text from the BeautifulSoup obj. See DEFAULT_WEBSITE_EXTRACTOR.\n    \"\"\"\n\n    def __init__(\n        self,\n        website_extractor: Optional[Dict[str, Callable]] = None,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            from urllib.parse import urlparse  # noqa: F401\n\n            import requests  # noqa: F401\n            from bs4 import BeautifulSoup  # noqa: F401\n        except ImportError:\n            raise ValueError(\n               ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 5, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "6": {"text": "raise ValueError(\n                \"`bs4`, `requests`, and `urllib` must be installed to scrape websites.\"\n                \"Please run `pip install bs4 requests urllib`.\"\n            )\n\n        self.website_extractor = website_extractor or DEFAULT_WEBSITE_EXTRACTOR\n\n    def load_data(\n        self, urls: List[str], custom_hostname: Optional[str] = None\n    ) -> List[Document]:\n        \"\"\"Load data from the urls.\n\n        Args:\n            urls (List[str]): List of URLs to scrape.\n            custom_hostname (Optional[str]): Force a certain hostname in the case\n                a website is displayed under custom URLs (e.g. Substack blogs)\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        from urllib.parse import urlparse\n\n     ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 6, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "7": {"text": "     from urllib.parse import urlparse\n\n        import requests\n        from bs4 import BeautifulSoup\n\n        documents = []\n        for url in urls:\n            try:\n                page = requests.get(url)\n            except Exception:\n                raise ValueError(f\"One of the inputs is not a valid url: {url}\")\n\n            hostname = custom_hostname or urlparse(url).hostname or \"\"\n\n            soup = BeautifulSoup(page.content, \"html.parser\")\n\n            data = \"\"\n            extra_info = {\"URL\": url}\n            if hostname in self.website_extractor:\n                data, metadata = self.website_extractor[hostname](soup)\n                extra_info.update(metadata)\n            else:\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 7, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "8": {"text": "           else:\n                data = soup.getText()\n\n            documents.append(Document(data, extra_info=extra_info))\n\n        return documents\n\n\nclass RssReader(BaseReader):\n    \"\"\"RSS reader.\n\n    Reads content from an RSS feed.\n\n    \"\"\"\n\n    def __init__(self, html_to_text: bool = False) -> None:\n        \"\"\"Initialize with parameters.\n\n        Args:\n            html_to_text (bool): Whether to convert HTML to text.\n                Requires `html2text` package.\n\n        \"\"\"\n        try:\n            import feedparser  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`feedparser` package not found, please run `pip install feedparser`\"\n            )\n\n        if html_to_text:\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 8, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "9": {"text": "       if html_to_text:\n            try:\n                import html2text  # noqa: F401\n            except ImportError:\n                raise ValueError(\n                    \"`html2text` package not found, please run `pip install html2text`\"\n                )\n        self._html_to_text = html_to_text\n\n    def load_data(self, urls: List[str]) -> List[Document]:\n        \"\"\"Load data from RSS feeds.\n\n        Args:\n            urls (List[str]): List of RSS URLs to load.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        import feedparser\n\n        if not isinstance(urls, list):\n            raise ValueError(\"urls must be a list of strings.\")\n\n       ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 9, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "10": {"text": "ValueError(\"urls must be a list of strings.\")\n\n        documents = []\n\n        for url in urls:\n            parsed = feedparser.parse(url)\n            for entry in parsed.entries:\n                if entry.content:\n                    data = entry.content[0].value\n                else:\n                    data = entry.description or entry.summary\n\n                if self._html_to_text:\n                    import html2text\n\n                    data = html2text.html2text(data)\n\n                extra_info = {\"title\": entry.title, \"link\": entry.link}\n                documents.append(Document(data, extra_info=extra_info))\n\n        return documents\n\n\nif __name__ == \"__main__\":\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 10, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "11": {"text": "    return documents\n\n\nif __name__ == \"__main__\":\n    reader = SimpleWebPageReader()\n    logging.info(reader.load_data([\"http://www.google.com\"]))\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "index": 11, "child_indices": [], "ref_doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "node_info": null}, "12": {"text": "This code file contains two classes, SimpleWebPageReader and TrafilaturaWebReader, which are used to scrape web pages. The SimpleWebPageReader class uses the RequestsWrapper utility to scrape web pages and convert HTML to text if specified. The TrafilaturaWebReader class uses the trafilatura package to scrape web pages and extract text from them. The BeautifulSoupWebReader class uses the bs4 and urllib packages to scrape web pages and extract text from them. The RssReader class uses the feedparser package to scrape RSS feeds and convert HTML to text if specified.", "doc_id": null, "embedding": null, "extra_info": null, "index": 12, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "ref_doc_id": null, "node_info": null}, "13": {"text": "This code file, web.py, is part of the gpt_index/readers directory. It contains a class, SimpleWebPageReader, which is used to read webpages from a list of URLs. The class contains a method, load_data, which takes a list of strings as input and parses the webpages using the feedparser library. It then extracts the content, title, and link from each entry and stores them in a Document object. The Document object is then appended to a list of documents which is returned. The code also contains an if statement which checks if the html_to_text flag is set, and if so, it uses the html2text library to convert the data to text. Finally, the code contains a main function which calls the SimpleWebPageReader class and prints the result.", "doc_id": null, "embedding": null, "extra_info": null, "index": 13, "child_indices": [10, 11], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"12": {"text": "This code file contains two classes, SimpleWebPageReader and TrafilaturaWebReader, which are used to scrape web pages. The SimpleWebPageReader class uses the RequestsWrapper utility to scrape web pages and convert HTML to text if specified. The TrafilaturaWebReader class uses the trafilatura package to scrape web pages and extract text from them. The BeautifulSoupWebReader class uses the bs4 and urllib packages to scrape web pages and extract text from them. The RssReader class uses the feedparser package to scrape RSS feeds and convert HTML to text if specified.", "doc_id": null, "embedding": null, "extra_info": null, "index": 12, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "ref_doc_id": null, "node_info": null}, "13": {"text": "This code file, web.py, is part of the gpt_index/readers directory. It contains a class, SimpleWebPageReader, which is used to read webpages from a list of URLs. The class contains a method, load_data, which takes a list of strings as input and parses the webpages using the feedparser library. It then extracts the content, title, and link from each entry and stores them in a Document object. The Document object is then appended to a list of documents which is returned. The code also contains an if statement which checks if the html_to_text flag is set, and if so, it uses the html2text library to convert the data to text. Finally, the code contains a main function which calls the SimpleWebPageReader class and prints the result.", "doc_id": null, "embedding": null, "extra_info": null, "index": 13, "child_indices": [10, 11], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}