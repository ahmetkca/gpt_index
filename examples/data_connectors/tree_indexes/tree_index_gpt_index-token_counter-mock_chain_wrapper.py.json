{"index_struct": {"text": "\nMock chain wrapper is a Python file that provides mock implementations of GPT-Index algorithms and data structures for testing purposes. It contains functions for predicting summaries, inserting data, selecting data, refining answers, answering questions, and extracting keywords. It also contains a MockLLMPredictor class which provides a mock implementation of the LLM Predictor. The MockLLMPredictor class initializes parameters such as the maximum number of tokens, the total tokens used, and the last token usage. The _predict function is used to determine the appropriate response based on the prompt type. The purpose of this code is to provide a testing environment for GPT-Index algorithms and data structures, allowing developers to test their code without having to use the actual GPT-Index.", "doc_id": "54e41680-0585-40e4-b904-6c4bf91a9eb4", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Mock chain wrapper.\"\"\"\n\nfrom typing import Any, Dict\n\nfrom gpt_index.constants import NUM_OUTPUTS\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.prompts.prompt_type import PromptType\nfrom gpt_index.token_counter.utils import mock_extract_keywords_response\nfrom gpt_index.utils import globals_helper\n\n# TODO: consolidate with unit tests in tests/mock_utils/mock_predict.py\n\n\ndef _mock_summary_predict(max_tokens: int, prompt_args: Dict) -> str:\n    \"\"\"Mock summary predict.\"\"\"\n    # tokens in response shouldn't be larger than tokens in `context_str`\n    num_text_tokens = len(globals_helper.tokenizer(prompt_args[\"context_str\"]))\n    token_limit = min(num_text_tokens, max_tokens)\n    return \" \".join([\"summary\"] * token_limit)\n\n\ndef _mock_insert_predict() -> str:\n    \"\"\"Mock insert predict.\"\"\"\n    return \"ANSWER: 1\"\n\n\ndef", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "index": 0, "child_indices": [], "ref_doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "node_info": null}, "1": {"text": "insert predict.\"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_query_select() -> str:\n    \"\"\"Mock query select.\"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_query_select_multiple(num_chunks: int) -> str:\n    \"\"\"Mock query select.\"\"\"\n    nums_str = \", \".join([str(i) for i in range(num_chunks)])\n    return f\"ANSWER: {nums_str}\"\n\n\ndef _mock_answer(max_tokens: int, prompt_args: Dict) -> str:\n    \"\"\"Mock answer.\"\"\"\n    # tokens in response shouldn't be larger than tokens in `text`\n    num_ctx_tokens = len(globals_helper.tokenizer(prompt_args[\"context_str\"]))\n    token_limit = min(num_ctx_tokens, max_tokens)\n    return \" \".join([\"answer\"] * token_limit)\n\n\ndef _mock_refine(max_tokens: int, prompt: Prompt, prompt_args: Dict) -> str:\n    \"\"\"Mock refine.\"\"\"\n    # tokens in response shouldn't be larger than tokens in\n    #", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "index": 1, "child_indices": [], "ref_doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "node_info": null}, "2": {"text": "in response shouldn't be larger than tokens in\n    # `existing_answer` + `context_msg`\n    # NOTE: if existing_answer is not in prompt_args, we need to get it from the prompt\n    if \"existing_answer\" not in prompt_args:\n        existing_answer = prompt.partial_dict[\"existing_answer\"]\n    else:\n        existing_answer = prompt_args[\"existing_answer\"]\n    num_ctx_tokens = len(globals_helper.tokenizer(prompt_args[\"context_msg\"]))\n    num_exist_tokens = len(globals_helper.tokenizer(existing_answer))\n    token_limit = min(num_ctx_tokens + num_exist_tokens, max_tokens)\n    return \" \".join([\"answer\"] * token_limit)\n\n\ndef _mock_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"text\"])\n\n\ndef _mock_query_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock query keyword extract.\"\"\"\n    return", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "index": 2, "child_indices": [], "ref_doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "node_info": null}, "3": {"text": "   \"\"\"Mock query keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"question\"])\n\n\nclass MockLLMPredictor(LLMPredictor):\n    \"\"\"Mock LLM Predictor.\"\"\"\n\n    def __init__(self, max_tokens: int = NUM_OUTPUTS) -> None:\n        \"\"\"Initialize params.\"\"\"\n        # NOTE: don't call super, we don't want to instantiate LLM\n        self.max_tokens = max_tokens\n        self._total_tokens_used = 0\n        self.flag = True\n        self._last_token_usage = None\n\n    def _predict(self, prompt: Prompt, **prompt_args: Any) -> str:\n        \"\"\"Mock predict.\"\"\"\n        prompt_str = prompt.prompt_type\n        if prompt_str == PromptType.SUMMARY:\n            return _mock_summary_predict(self.max_tokens, prompt_args)\n        elif prompt_str ==", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "index": 3, "child_indices": [], "ref_doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "node_info": null}, "4": {"text": "prompt_args)\n        elif prompt_str == PromptType.TREE_INSERT:\n            return _mock_insert_predict()\n        elif prompt_str == PromptType.TREE_SELECT:\n            return _mock_query_select()\n        elif prompt_str == PromptType.TREE_SELECT_MULTIPLE:\n            return _mock_query_select_multiple(prompt_args[\"num_chunks\"])\n        elif prompt_str == PromptType.REFINE:\n            return _mock_refine(self.max_tokens, prompt, prompt_args)\n        elif prompt_str == PromptType.QUESTION_ANSWER:\n            return _mock_answer(self.max_tokens, prompt_args)\n        elif prompt_str == PromptType.KEYWORD_EXTRACT:\n            return _mock_keyword_extract(prompt_args)\n        elif", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "index": 4, "child_indices": [], "ref_doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "node_info": null}, "5": {"text": "       elif prompt_str == PromptType.QUERY_KEYWORD_EXTRACT:\n            return _mock_query_keyword_extract(prompt_args)\n        else:\n            raise ValueError(\"Invalid prompt type.\")\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "index": 5, "child_indices": [], "ref_doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "node_info": null}, "6": {"text": "Mock chain wrapper is a Python file that provides mock implementations of various GPT-Index algorithms and data structures. It contains functions for predicting summaries, inserting data, selecting data, refining answers, answering questions, and extracting keywords. It also contains a MockLLMPredictor class which provides a mock implementation of the LLM Predictor. The purpose of this code is to provide a mock implementation of GPT-Index algorithms and data structures for testing purposes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 6, "child_indices": [0, 1, 2, 3, 4, 5], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"6": {"text": "Mock chain wrapper is a Python file that provides mock implementations of various GPT-Index algorithms and data structures. It contains functions for predicting summaries, inserting data, selecting data, refining answers, answering questions, and extracting keywords. It also contains a MockLLMPredictor class which provides a mock implementation of the LLM Predictor. The purpose of this code is to provide a mock implementation of GPT-Index algorithms and data structures for testing purposes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 6, "child_indices": [0, 1, 2, 3, 4, 5], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d": {"text": "\"\"\"Mock chain wrapper.\"\"\"\n\nfrom typing import Any, Dict\n\nfrom gpt_index.constants import NUM_OUTPUTS\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.prompts.prompt_type import PromptType\nfrom gpt_index.token_counter.utils import mock_extract_keywords_response\nfrom gpt_index.utils import globals_helper\n\n# TODO: consolidate with unit tests in tests/mock_utils/mock_predict.py\n\n\ndef _mock_summary_predict(max_tokens: int, prompt_args: Dict) -> str:\n    \"\"\"Mock summary predict.\"\"\"\n    # tokens in response shouldn't be larger than tokens in `context_str`\n    num_text_tokens = len(globals_helper.tokenizer(prompt_args[\"context_str\"]))\n    token_limit = min(num_text_tokens, max_tokens)\n    return \" \".join([\"summary\"] * token_limit)\n\n\ndef _mock_insert_predict() -> str:\n    \"\"\"Mock insert predict.\"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_query_select() -> str:\n    \"\"\"Mock query select.\"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_query_select_multiple(num_chunks: int) -> str:\n    \"\"\"Mock query select.\"\"\"\n    nums_str = \", \".join([str(i) for i in range(num_chunks)])\n    return f\"ANSWER: {nums_str}\"\n\n\ndef _mock_answer(max_tokens: int, prompt_args: Dict) -> str:\n    \"\"\"Mock answer.\"\"\"\n    # tokens in response shouldn't be larger than tokens in `text`\n    num_ctx_tokens = len(globals_helper.tokenizer(prompt_args[\"context_str\"]))\n    token_limit = min(num_ctx_tokens, max_tokens)\n    return \" \".join([\"answer\"] * token_limit)\n\n\ndef _mock_refine(max_tokens: int, prompt: Prompt, prompt_args: Dict) -> str:\n    \"\"\"Mock refine.\"\"\"\n    # tokens in response shouldn't be larger than tokens in\n    # `existing_answer` + `context_msg`\n    # NOTE: if existing_answer is not in prompt_args, we need to get it from the prompt\n    if \"existing_answer\" not in prompt_args:\n        existing_answer = prompt.partial_dict[\"existing_answer\"]\n    else:\n        existing_answer = prompt_args[\"existing_answer\"]\n    num_ctx_tokens = len(globals_helper.tokenizer(prompt_args[\"context_msg\"]))\n    num_exist_tokens = len(globals_helper.tokenizer(existing_answer))\n    token_limit = min(num_ctx_tokens + num_exist_tokens, max_tokens)\n    return \" \".join([\"answer\"] * token_limit)\n\n\ndef _mock_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"text\"])\n\n\ndef _mock_query_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock query keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"question\"])\n\n\nclass MockLLMPredictor(LLMPredictor):\n    \"\"\"Mock LLM Predictor.\"\"\"\n\n    def __init__(self, max_tokens: int = NUM_OUTPUTS) -> None:\n        \"\"\"Initialize params.\"\"\"\n        # NOTE: don't call super, we don't want to instantiate LLM\n        self.max_tokens = max_tokens\n        self._total_tokens_used = 0\n        self.flag = True\n        self._last_token_usage = None\n\n    def _predict(self, prompt: Prompt, **prompt_args: Any) -> str:\n        \"\"\"Mock predict.\"\"\"\n        prompt_str = prompt.prompt_type\n        if prompt_str == PromptType.SUMMARY:\n            return _mock_summary_predict(self.max_tokens, prompt_args)\n        elif prompt_str == PromptType.TREE_INSERT:\n            return _mock_insert_predict()\n        elif prompt_str == PromptType.TREE_SELECT:\n            return _mock_query_select()\n        elif prompt_str == PromptType.TREE_SELECT_MULTIPLE:\n            return _mock_query_select_multiple(prompt_args[\"num_chunks\"])\n        elif prompt_str == PromptType.REFINE:\n            return _mock_refine(self.max_tokens, prompt, prompt_args)\n        elif prompt_str == PromptType.QUESTION_ANSWER:\n            return _mock_answer(self.max_tokens, prompt_args)\n        elif prompt_str == PromptType.KEYWORD_EXTRACT:\n            return _mock_keyword_extract(prompt_args)\n        elif prompt_str == PromptType.QUERY_KEYWORD_EXTRACT:\n            return _mock_query_keyword_extract(prompt_args)\n        else:\n            raise ValueError(\"Invalid prompt type.\")\n", "doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "__type__": "Document"}, "54e41680-0585-40e4-b904-6c4bf91a9eb4": {"text": "\nMock chain wrapper is a Python file that provides mock implementations of GPT-Index algorithms and data structures for testing purposes. It contains functions for predicting summaries, inserting data, selecting data, refining answers, answering questions, and extracting keywords. It also contains a MockLLMPredictor class which provides a mock implementation of the LLM Predictor. The MockLLMPredictor class initializes parameters such as the maximum number of tokens, the total tokens used, and the last token usage. The _predict function is used to determine the appropriate response based on the prompt type. The purpose of this code is to provide a testing environment for GPT-Index algorithms and data structures, allowing developers to test their code without having to use the actual GPT-Index.", "doc_id": "54e41680-0585-40e4-b904-6c4bf91a9eb4", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Mock chain wrapper.\"\"\"\n\nfrom typing import Any, Dict\n\nfrom gpt_index.constants import NUM_OUTPUTS\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.prompts.prompt_type import PromptType\nfrom gpt_index.token_counter.utils import mock_extract_keywords_response\nfrom gpt_index.utils import globals_helper\n\n# TODO: consolidate with unit tests in tests/mock_utils/mock_predict.py\n\n\ndef _mock_summary_predict(max_tokens: int, prompt_args: Dict) -> str:\n    \"\"\"Mock summary predict.\"\"\"\n    # tokens in response shouldn't be larger than tokens in `context_str`\n    num_text_tokens = len(globals_helper.tokenizer(prompt_args[\"context_str\"]))\n    token_limit = min(num_text_tokens, max_tokens)\n    return \" \".join([\"summary\"] * token_limit)\n\n\ndef _mock_insert_predict() -> str:\n    \"\"\"Mock insert predict.\"\"\"\n    return \"ANSWER: 1\"\n\n\ndef", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "index": 0, "child_indices": [], "ref_doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "node_info": null}, "1": {"text": "insert predict.\"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_query_select() -> str:\n    \"\"\"Mock query select.\"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_query_select_multiple(num_chunks: int) -> str:\n    \"\"\"Mock query select.\"\"\"\n    nums_str = \", \".join([str(i) for i in range(num_chunks)])\n    return f\"ANSWER: {nums_str}\"\n\n\ndef _mock_answer(max_tokens: int, prompt_args: Dict) -> str:\n    \"\"\"Mock answer.\"\"\"\n    # tokens in response shouldn't be larger than tokens in `text`\n    num_ctx_tokens = len(globals_helper.tokenizer(prompt_args[\"context_str\"]))\n    token_limit = min(num_ctx_tokens, max_tokens)\n    return \" \".join([\"answer\"] * token_limit)\n\n\ndef _mock_refine(max_tokens: int, prompt: Prompt, prompt_args: Dict) -> str:\n    \"\"\"Mock refine.\"\"\"\n    # tokens in response shouldn't be larger than tokens in\n    #", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "index": 1, "child_indices": [], "ref_doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "node_info": null}, "2": {"text": "in response shouldn't be larger than tokens in\n    # `existing_answer` + `context_msg`\n    # NOTE: if existing_answer is not in prompt_args, we need to get it from the prompt\n    if \"existing_answer\" not in prompt_args:\n        existing_answer = prompt.partial_dict[\"existing_answer\"]\n    else:\n        existing_answer = prompt_args[\"existing_answer\"]\n    num_ctx_tokens = len(globals_helper.tokenizer(prompt_args[\"context_msg\"]))\n    num_exist_tokens = len(globals_helper.tokenizer(existing_answer))\n    token_limit = min(num_ctx_tokens + num_exist_tokens, max_tokens)\n    return \" \".join([\"answer\"] * token_limit)\n\n\ndef _mock_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"text\"])\n\n\ndef _mock_query_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock query keyword extract.\"\"\"\n    return", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "index": 2, "child_indices": [], "ref_doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "node_info": null}, "3": {"text": "   \"\"\"Mock query keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"question\"])\n\n\nclass MockLLMPredictor(LLMPredictor):\n    \"\"\"Mock LLM Predictor.\"\"\"\n\n    def __init__(self, max_tokens: int = NUM_OUTPUTS) -> None:\n        \"\"\"Initialize params.\"\"\"\n        # NOTE: don't call super, we don't want to instantiate LLM\n        self.max_tokens = max_tokens\n        self._total_tokens_used = 0\n        self.flag = True\n        self._last_token_usage = None\n\n    def _predict(self, prompt: Prompt, **prompt_args: Any) -> str:\n        \"\"\"Mock predict.\"\"\"\n        prompt_str = prompt.prompt_type\n        if prompt_str == PromptType.SUMMARY:\n            return _mock_summary_predict(self.max_tokens, prompt_args)\n        elif prompt_str ==", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "index": 3, "child_indices": [], "ref_doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "node_info": null}, "4": {"text": "prompt_args)\n        elif prompt_str == PromptType.TREE_INSERT:\n            return _mock_insert_predict()\n        elif prompt_str == PromptType.TREE_SELECT:\n            return _mock_query_select()\n        elif prompt_str == PromptType.TREE_SELECT_MULTIPLE:\n            return _mock_query_select_multiple(prompt_args[\"num_chunks\"])\n        elif prompt_str == PromptType.REFINE:\n            return _mock_refine(self.max_tokens, prompt, prompt_args)\n        elif prompt_str == PromptType.QUESTION_ANSWER:\n            return _mock_answer(self.max_tokens, prompt_args)\n        elif prompt_str == PromptType.KEYWORD_EXTRACT:\n            return _mock_keyword_extract(prompt_args)\n        elif", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "index": 4, "child_indices": [], "ref_doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "node_info": null}, "5": {"text": "       elif prompt_str == PromptType.QUERY_KEYWORD_EXTRACT:\n            return _mock_query_keyword_extract(prompt_args)\n        else:\n            raise ValueError(\"Invalid prompt type.\")\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "index": 5, "child_indices": [], "ref_doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "node_info": null}, "6": {"text": "Mock chain wrapper is a Python file that provides mock implementations of various GPT-Index algorithms and data structures. It contains functions for predicting summaries, inserting data, selecting data, refining answers, answering questions, and extracting keywords. It also contains a MockLLMPredictor class which provides a mock implementation of the LLM Predictor. The purpose of this code is to provide a mock implementation of GPT-Index algorithms and data structures for testing purposes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 6, "child_indices": [0, 1, 2, 3, 4, 5], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"6": {"text": "Mock chain wrapper is a Python file that provides mock implementations of various GPT-Index algorithms and data structures. It contains functions for predicting summaries, inserting data, selecting data, refining answers, answering questions, and extracting keywords. It also contains a MockLLMPredictor class which provides a mock implementation of the LLM Predictor. The purpose of this code is to provide a mock implementation of GPT-Index algorithms and data structures for testing purposes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 6, "child_indices": [0, 1, 2, 3, 4, 5], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}