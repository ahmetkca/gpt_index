{"index_struct": {"text": "\nThe token_counter.py file contains a function called llm_token_counter which is used as a decorator for methods in index/query classes that make calls to LLMs. This function checks for the presence of the _llm_predictor and _embed_model attributes in the class instance and then calculates the total token usage by subtracting the start token count from the end token count. It then prints the output in the form of total LLM token usage and total embedding token usage. Additionally, it sets the last token usage attribute for the LLM predictor and embedding model. The purpose of this code is to provide a way to track the total number of tokens used by the LLM and embedding models, allowing for better understanding of the usage of these models.", "doc_id": "dc85c9ac-075a-4d1c-8cf8-700ea7aeb473", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Token counter function.\"\"\"\n\nimport logging\nfrom typing import Any, Callable, cast\n\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\n\n\ndef llm_token_counter(method_name_str: str) -> Callable:\n    \"\"\"\n    Use this as a decorator for methods in index/query classes that make calls to LLMs.\n\n    At the moment, this decorator can only be used on class instance methods with a\n    `_llm_predictor` attribute.\n\n    Do not use this on abstract methods.\n\n    For example, consider the class below:\n        .. code-block:: python\n            class GPTTreeIndexBuilder:\n            ...\n            @llm_token_counter(\"build_from_text\")\n            def build_from_text(self, documents: Sequence[BaseDocument]) -> IndexGraph:\n                ...\n\n    If you run `build_from_text()`, it will print the output in the form below:\n\n    ```\n    [build_from_text] Total token", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/token_counter.py", "file_name": "token_counter.py"}, "index": 0, "child_indices": [], "ref_doc_id": "ee4b9ac3b34abb65842a80d4ae2390733c1542c1", "node_info": null}, "1": {"text": "   ```\n    [build_from_text] Total token usage: <some-number> tokens\n    ```\n    \"\"\"\n\n    def wrap(f: Callable) -> Callable:\n        def wrapped_llm_predict(_self: Any, *args: Any, **kwargs: Any) -> Any:\n            llm_predictor = getattr(_self, \"_llm_predictor\", None)\n            if llm_predictor is None:\n                raise ValueError(\n                    \"Cannot use llm_token_counter on an instance \"\n                    \"without a _llm_predictor attribute.\"\n                )\n            llm_predictor = cast(LLMPredictor, llm_predictor)\n\n            embed_model = getattr(_self, \"_embed_model\", None)\n            if embed_model is None:\n              ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/token_counter.py", "file_name": "token_counter.py"}, "index": 1, "child_indices": [], "ref_doc_id": "ee4b9ac3b34abb65842a80d4ae2390733c1542c1", "node_info": null}, "2": {"text": "is None:\n                raise ValueError(\n                    \"Cannot use llm_token_counter on an instance \"\n                    \"without a _embed_model attribute.\"\n                )\n            embed_model = cast(BaseEmbedding, embed_model)\n\n            start_token_ct = llm_predictor.total_tokens_used\n            start_embed_token_ct = embed_model.total_tokens_used\n\n            f_return_val = f(_self, *args, **kwargs)\n\n            net_tokens = llm_predictor.total_tokens_used - start_token_ct\n            llm_predictor.last_token_usage = net_tokens\n            net_embed_tokens = embed_model.total_tokens_used - start_embed_token_ct\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/token_counter.py", "file_name": "token_counter.py"}, "index": 2, "child_indices": [], "ref_doc_id": "ee4b9ac3b34abb65842a80d4ae2390733c1542c1", "node_info": null}, "3": {"text": "- start_embed_token_ct\n            embed_model.last_token_usage = net_embed_tokens\n\n            # print outputs\n            logging.info(\n                f\"> [{method_name_str}] Total LLM token usage: {net_tokens} tokens\"\n            )\n            logging.info(\n                f\"> [{method_name_str}] Total embedding token usage: \"\n                f\"{net_embed_tokens} tokens\"\n            )\n\n            return f_return_val\n\n        return wrapped_llm_predict\n\n    return wrap\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/token_counter.py", "file_name": "token_counter.py"}, "index": 3, "child_indices": [], "ref_doc_id": "ee4b9ac3b34abb65842a80d4ae2390733c1542c1", "node_info": null}, "4": {"text": "This code file contains a function called llm_token_counter which is used as a decorator for methods in index/query classes that make calls to LLMs. It is used to count the total number of tokens used by the LLM and embedding models. It prints the output in the form of total LLM token usage and total embedding token usage. It also sets the last token usage attribute for the LLM predictor and embedding model.", "doc_id": null, "embedding": null, "extra_info": null, "index": 4, "child_indices": [0, 1, 2, 3], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"4": {"text": "This code file contains a function called llm_token_counter which is used as a decorator for methods in index/query classes that make calls to LLMs. It is used to count the total number of tokens used by the LLM and embedding models. It prints the output in the form of total LLM token usage and total embedding token usage. It also sets the last token usage attribute for the LLM predictor and embedding model.", "doc_id": null, "embedding": null, "extra_info": null, "index": 4, "child_indices": [0, 1, 2, 3], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"ee4b9ac3b34abb65842a80d4ae2390733c1542c1": {"text": "\"\"\"Token counter function.\"\"\"\n\nimport logging\nfrom typing import Any, Callable, cast\n\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\n\n\ndef llm_token_counter(method_name_str: str) -> Callable:\n    \"\"\"\n    Use this as a decorator for methods in index/query classes that make calls to LLMs.\n\n    At the moment, this decorator can only be used on class instance methods with a\n    `_llm_predictor` attribute.\n\n    Do not use this on abstract methods.\n\n    For example, consider the class below:\n        .. code-block:: python\n            class GPTTreeIndexBuilder:\n            ...\n            @llm_token_counter(\"build_from_text\")\n            def build_from_text(self, documents: Sequence[BaseDocument]) -> IndexGraph:\n                ...\n\n    If you run `build_from_text()`, it will print the output in the form below:\n\n    ```\n    [build_from_text] Total token usage: <some-number> tokens\n    ```\n    \"\"\"\n\n    def wrap(f: Callable) -> Callable:\n        def wrapped_llm_predict(_self: Any, *args: Any, **kwargs: Any) -> Any:\n            llm_predictor = getattr(_self, \"_llm_predictor\", None)\n            if llm_predictor is None:\n                raise ValueError(\n                    \"Cannot use llm_token_counter on an instance \"\n                    \"without a _llm_predictor attribute.\"\n                )\n            llm_predictor = cast(LLMPredictor, llm_predictor)\n\n            embed_model = getattr(_self, \"_embed_model\", None)\n            if embed_model is None:\n                raise ValueError(\n                    \"Cannot use llm_token_counter on an instance \"\n                    \"without a _embed_model attribute.\"\n                )\n            embed_model = cast(BaseEmbedding, embed_model)\n\n            start_token_ct = llm_predictor.total_tokens_used\n            start_embed_token_ct = embed_model.total_tokens_used\n\n            f_return_val = f(_self, *args, **kwargs)\n\n            net_tokens = llm_predictor.total_tokens_used - start_token_ct\n            llm_predictor.last_token_usage = net_tokens\n            net_embed_tokens = embed_model.total_tokens_used - start_embed_token_ct\n            embed_model.last_token_usage = net_embed_tokens\n\n            # print outputs\n            logging.info(\n                f\"> [{method_name_str}] Total LLM token usage: {net_tokens} tokens\"\n            )\n            logging.info(\n                f\"> [{method_name_str}] Total embedding token usage: \"\n                f\"{net_embed_tokens} tokens\"\n            )\n\n            return f_return_val\n\n        return wrapped_llm_predict\n\n    return wrap\n", "doc_id": "ee4b9ac3b34abb65842a80d4ae2390733c1542c1", "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/token_counter.py", "file_name": "token_counter.py"}, "__type__": "Document"}, "dc85c9ac-075a-4d1c-8cf8-700ea7aeb473": {"text": "\nThe token_counter.py file contains a function called llm_token_counter which is used as a decorator for methods in index/query classes that make calls to LLMs. This function checks for the presence of the _llm_predictor and _embed_model attributes in the class instance and then calculates the total token usage by subtracting the start token count from the end token count. It then prints the output in the form of total LLM token usage and total embedding token usage. Additionally, it sets the last token usage attribute for the LLM predictor and embedding model. The purpose of this code is to provide a way to track the total number of tokens used by the LLM and embedding models, allowing for better understanding of the usage of these models.", "doc_id": "dc85c9ac-075a-4d1c-8cf8-700ea7aeb473", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Token counter function.\"\"\"\n\nimport logging\nfrom typing import Any, Callable, cast\n\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\n\n\ndef llm_token_counter(method_name_str: str) -> Callable:\n    \"\"\"\n    Use this as a decorator for methods in index/query classes that make calls to LLMs.\n\n    At the moment, this decorator can only be used on class instance methods with a\n    `_llm_predictor` attribute.\n\n    Do not use this on abstract methods.\n\n    For example, consider the class below:\n        .. code-block:: python\n            class GPTTreeIndexBuilder:\n            ...\n            @llm_token_counter(\"build_from_text\")\n            def build_from_text(self, documents: Sequence[BaseDocument]) -> IndexGraph:\n                ...\n\n    If you run `build_from_text()`, it will print the output in the form below:\n\n    ```\n    [build_from_text] Total token", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/token_counter.py", "file_name": "token_counter.py"}, "index": 0, "child_indices": [], "ref_doc_id": "ee4b9ac3b34abb65842a80d4ae2390733c1542c1", "node_info": null}, "1": {"text": "   ```\n    [build_from_text] Total token usage: <some-number> tokens\n    ```\n    \"\"\"\n\n    def wrap(f: Callable) -> Callable:\n        def wrapped_llm_predict(_self: Any, *args: Any, **kwargs: Any) -> Any:\n            llm_predictor = getattr(_self, \"_llm_predictor\", None)\n            if llm_predictor is None:\n                raise ValueError(\n                    \"Cannot use llm_token_counter on an instance \"\n                    \"without a _llm_predictor attribute.\"\n                )\n            llm_predictor = cast(LLMPredictor, llm_predictor)\n\n            embed_model = getattr(_self, \"_embed_model\", None)\n            if embed_model is None:\n              ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/token_counter.py", "file_name": "token_counter.py"}, "index": 1, "child_indices": [], "ref_doc_id": "ee4b9ac3b34abb65842a80d4ae2390733c1542c1", "node_info": null}, "2": {"text": "is None:\n                raise ValueError(\n                    \"Cannot use llm_token_counter on an instance \"\n                    \"without a _embed_model attribute.\"\n                )\n            embed_model = cast(BaseEmbedding, embed_model)\n\n            start_token_ct = llm_predictor.total_tokens_used\n            start_embed_token_ct = embed_model.total_tokens_used\n\n            f_return_val = f(_self, *args, **kwargs)\n\n            net_tokens = llm_predictor.total_tokens_used - start_token_ct\n            llm_predictor.last_token_usage = net_tokens\n            net_embed_tokens = embed_model.total_tokens_used - start_embed_token_ct\n           ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/token_counter.py", "file_name": "token_counter.py"}, "index": 2, "child_indices": [], "ref_doc_id": "ee4b9ac3b34abb65842a80d4ae2390733c1542c1", "node_info": null}, "3": {"text": "- start_embed_token_ct\n            embed_model.last_token_usage = net_embed_tokens\n\n            # print outputs\n            logging.info(\n                f\"> [{method_name_str}] Total LLM token usage: {net_tokens} tokens\"\n            )\n            logging.info(\n                f\"> [{method_name_str}] Total embedding token usage: \"\n                f\"{net_embed_tokens} tokens\"\n            )\n\n            return f_return_val\n\n        return wrapped_llm_predict\n\n    return wrap\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/token_counter.py", "file_name": "token_counter.py"}, "index": 3, "child_indices": [], "ref_doc_id": "ee4b9ac3b34abb65842a80d4ae2390733c1542c1", "node_info": null}, "4": {"text": "This code file contains a function called llm_token_counter which is used as a decorator for methods in index/query classes that make calls to LLMs. It is used to count the total number of tokens used by the LLM and embedding models. It prints the output in the form of total LLM token usage and total embedding token usage. It also sets the last token usage attribute for the LLM predictor and embedding model.", "doc_id": null, "embedding": null, "extra_info": null, "index": 4, "child_indices": [0, 1, 2, 3], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"4": {"text": "This code file contains a function called llm_token_counter which is used as a decorator for methods in index/query classes that make calls to LLMs. It is used to count the total number of tokens used by the LLM and embedding models. It prints the output in the form of total LLM token usage and total embedding token usage. It also sets the last token usage attribute for the LLM predictor and embedding model.", "doc_id": null, "embedding": null, "extra_info": null, "index": 4, "child_indices": [0, 1, 2, 3], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}