{"index_struct": {"text": "\nThis code file tests the ability of the GPTListIndex class to accept arguments in a query. It mocks the LLMChain predict function with a generic response, and patches the OpenAI, LLMPredictor, and LLMChain classes. It then creates objects for a Document, an LLMPredictor, and a PromptHelper, and uses these objects to create a GPTListIndex object. Finally, it tests that the query returns the expected response, verifying that the GPTListIndex class is able to correctly pass arguments and return the expected response. The purpose of the code is to ensure that the query is functioning properly.", "doc_id": "463b68d7-e3d7-4e5a-8113-a993c489d097", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Test query runner.\"\"\"\n\nfrom typing import Any\nfrom unittest.mock import patch\n\nfrom gpt_index import PromptHelper\nfrom gpt_index.indices.list.base import GPTListIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import (\n    LLMChain,\n    LLMMetadata,\n    LLMPredictor,\n)\nfrom gpt_index.readers.schema.base import Document\n\n\ndef mock_llmchain_predict(**full_prompt_args: Any) -> str:\n    \"\"\"Mock LLMChain predict with a generic response.\"\"\"\n    return \"foo bar 2\"\n\n\n@patch.object(LLMChain, \"predict\", side_effect=mock_llmchain_predict)\n@patch(\"gpt_index.langchain_helpers.chain_wrapper.OpenAI\")\n@patch.object(LLMPredictor, \"get_llm_metadata\", return_value=LLMMetadata())\n@patch.object(LLMChain, \"__init__\", return_value=None)\ndef test_passing_args_to_query(\n    _mock_init: Any,\n    _mock_llm_metadata: Any,\n    _mock_openai: Any,\n    _mock_predict: Any,\n) -> None:\n    \"\"\"Test", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/query/test_query_runner.py", "file_name": "test_query_runner.py"}, "index": 0, "child_indices": [], "ref_doc_id": "d283765f8173ce6fa83ad61a8a33198283818051", "node_info": null}, "1": {"text": "_mock_predict: Any,\n) -> None:\n    \"\"\"Test passing args to query works.\n\n    Test that passing LLMPredictor from build index to query works.\n\n    \"\"\"\n    doc_text = \"Hello world.\"\n    doc = Document(doc_text)\n    llm_predictor = LLMPredictor()\n    prompt_helper = PromptHelper.from_llm_predictor(llm_predictor)\n    # index construction should not use llm_predictor at all\n    index = GPTListIndex(\n        [doc], llm_predictor=llm_predictor, prompt_helper=prompt_helper\n    )\n    # should use llm_predictor during query time\n    response = index.query(\"What is?\")\n    assert str(response) == \"foo bar 2\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/query/test_query_runner.py", "file_name": "test_query_runner.py"}, "index": 1, "child_indices": [], "ref_doc_id": "d283765f8173ce6fa83ad61a8a33198283818051", "node_info": null}, "2": {"text": "This code file tests the functionality of passing arguments to a query in the GPTListIndex class. It mocks the LLMChain predict function with a generic response, and patches the OpenAI, LLMPredictor, and LLMChain classes. It then creates a Document object, an LLMPredictor object, and a PromptHelper object. It then creates a GPTListIndex object with the Document, LLMPredictor, and PromptHelper objects as arguments. Finally, it tests that the query returns the expected response.", "doc_id": null, "embedding": null, "extra_info": null, "index": 2, "child_indices": [0, 1], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"2": {"text": "This code file tests the functionality of passing arguments to a query in the GPTListIndex class. It mocks the LLMChain predict function with a generic response, and patches the OpenAI, LLMPredictor, and LLMChain classes. It then creates a Document object, an LLMPredictor object, and a PromptHelper object. It then creates a GPTListIndex object with the Document, LLMPredictor, and PromptHelper objects as arguments. Finally, it tests that the query returns the expected response.", "doc_id": null, "embedding": null, "extra_info": null, "index": 2, "child_indices": [0, 1], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"d283765f8173ce6fa83ad61a8a33198283818051": {"text": "\"\"\"Test query runner.\"\"\"\n\nfrom typing import Any\nfrom unittest.mock import patch\n\nfrom gpt_index import PromptHelper\nfrom gpt_index.indices.list.base import GPTListIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import (\n    LLMChain,\n    LLMMetadata,\n    LLMPredictor,\n)\nfrom gpt_index.readers.schema.base import Document\n\n\ndef mock_llmchain_predict(**full_prompt_args: Any) -> str:\n    \"\"\"Mock LLMChain predict with a generic response.\"\"\"\n    return \"foo bar 2\"\n\n\n@patch.object(LLMChain, \"predict\", side_effect=mock_llmchain_predict)\n@patch(\"gpt_index.langchain_helpers.chain_wrapper.OpenAI\")\n@patch.object(LLMPredictor, \"get_llm_metadata\", return_value=LLMMetadata())\n@patch.object(LLMChain, \"__init__\", return_value=None)\ndef test_passing_args_to_query(\n    _mock_init: Any,\n    _mock_llm_metadata: Any,\n    _mock_openai: Any,\n    _mock_predict: Any,\n) -> None:\n    \"\"\"Test passing args to query works.\n\n    Test that passing LLMPredictor from build index to query works.\n\n    \"\"\"\n    doc_text = \"Hello world.\"\n    doc = Document(doc_text)\n    llm_predictor = LLMPredictor()\n    prompt_helper = PromptHelper.from_llm_predictor(llm_predictor)\n    # index construction should not use llm_predictor at all\n    index = GPTListIndex(\n        [doc], llm_predictor=llm_predictor, prompt_helper=prompt_helper\n    )\n    # should use llm_predictor during query time\n    response = index.query(\"What is?\")\n    assert str(response) == \"foo bar 2\"\n", "doc_id": "d283765f8173ce6fa83ad61a8a33198283818051", "embedding": null, "extra_info": {"file_path": "tests/indices/query/test_query_runner.py", "file_name": "test_query_runner.py"}, "__type__": "Document"}, "463b68d7-e3d7-4e5a-8113-a993c489d097": {"text": "\nThis code file tests the ability of the GPTListIndex class to accept arguments in a query. It mocks the LLMChain predict function with a generic response, and patches the OpenAI, LLMPredictor, and LLMChain classes. It then creates objects for a Document, an LLMPredictor, and a PromptHelper, and uses these objects to create a GPTListIndex object. Finally, it tests that the query returns the expected response, verifying that the GPTListIndex class is able to correctly pass arguments and return the expected response. The purpose of the code is to ensure that the query is functioning properly.", "doc_id": "463b68d7-e3d7-4e5a-8113-a993c489d097", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Test query runner.\"\"\"\n\nfrom typing import Any\nfrom unittest.mock import patch\n\nfrom gpt_index import PromptHelper\nfrom gpt_index.indices.list.base import GPTListIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import (\n    LLMChain,\n    LLMMetadata,\n    LLMPredictor,\n)\nfrom gpt_index.readers.schema.base import Document\n\n\ndef mock_llmchain_predict(**full_prompt_args: Any) -> str:\n    \"\"\"Mock LLMChain predict with a generic response.\"\"\"\n    return \"foo bar 2\"\n\n\n@patch.object(LLMChain, \"predict\", side_effect=mock_llmchain_predict)\n@patch(\"gpt_index.langchain_helpers.chain_wrapper.OpenAI\")\n@patch.object(LLMPredictor, \"get_llm_metadata\", return_value=LLMMetadata())\n@patch.object(LLMChain, \"__init__\", return_value=None)\ndef test_passing_args_to_query(\n    _mock_init: Any,\n    _mock_llm_metadata: Any,\n    _mock_openai: Any,\n    _mock_predict: Any,\n) -> None:\n    \"\"\"Test", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/query/test_query_runner.py", "file_name": "test_query_runner.py"}, "index": 0, "child_indices": [], "ref_doc_id": "d283765f8173ce6fa83ad61a8a33198283818051", "node_info": null}, "1": {"text": "_mock_predict: Any,\n) -> None:\n    \"\"\"Test passing args to query works.\n\n    Test that passing LLMPredictor from build index to query works.\n\n    \"\"\"\n    doc_text = \"Hello world.\"\n    doc = Document(doc_text)\n    llm_predictor = LLMPredictor()\n    prompt_helper = PromptHelper.from_llm_predictor(llm_predictor)\n    # index construction should not use llm_predictor at all\n    index = GPTListIndex(\n        [doc], llm_predictor=llm_predictor, prompt_helper=prompt_helper\n    )\n    # should use llm_predictor during query time\n    response = index.query(\"What is?\")\n    assert str(response) == \"foo bar 2\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/query/test_query_runner.py", "file_name": "test_query_runner.py"}, "index": 1, "child_indices": [], "ref_doc_id": "d283765f8173ce6fa83ad61a8a33198283818051", "node_info": null}, "2": {"text": "This code file tests the functionality of passing arguments to a query in the GPTListIndex class. It mocks the LLMChain predict function with a generic response, and patches the OpenAI, LLMPredictor, and LLMChain classes. It then creates a Document object, an LLMPredictor object, and a PromptHelper object. It then creates a GPTListIndex object with the Document, LLMPredictor, and PromptHelper objects as arguments. Finally, it tests that the query returns the expected response.", "doc_id": null, "embedding": null, "extra_info": null, "index": 2, "child_indices": [0, 1], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"2": {"text": "This code file tests the functionality of passing arguments to a query in the GPTListIndex class. It mocks the LLMChain predict function with a generic response, and patches the OpenAI, LLMPredictor, and LLMChain classes. It then creates a Document object, an LLMPredictor object, and a PromptHelper object. It then creates a GPTListIndex object with the Document, LLMPredictor, and PromptHelper objects as arguments. Finally, it tests that the query returns the expected response.", "doc_id": null, "embedding": null, "extra_info": null, "index": 2, "child_indices": [0, 1], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}