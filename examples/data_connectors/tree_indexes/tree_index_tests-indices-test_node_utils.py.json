{"index_struct": {"text": "\nThis code file tests a function from the node_utils module that splits a document into chunks of a specified size and creates nodes from the chunks. It uses a TokenTextSplitter to split the document into chunks and then creates nodes from the chunks. The purpose of the code is to ensure that the nodes created from the document have the desired chunk size and include the extra info if specified. The test checks that the nodes have the desired chunk size and that the extra info is included in the nodes if specified.", "doc_id": "aa90bab5-c74e-4e3e-836c-c353706e8f5e", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Test node utils.\"\"\"\n\nfrom typing import List\n\nimport pytest\n\nfrom gpt_index.indices.node_utils import get_nodes_from_document\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.readers.schema.base import Document\n\n\n@pytest.fixture\ndef text_splitter() -> TokenTextSplitter:\n    \"\"\"Get text splitter.\"\"\"\n    return TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n\n\n@pytest.fixture\ndef documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"\n    # NOTE: one document for now\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    return [\n        Document(doc_text, doc_id=\"test_doc_id\", extra_info={\"test_key\": \"test_val\"})\n    ]\n\n\ndef test_get_nodes_from_document(\n    documents: List[Document], text_splitter: TokenTextSplitter\n) -> None:\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_node_utils.py", "file_name": "test_node_utils.py"}, "index": 0, "child_indices": [], "ref_doc_id": "2c621db7da92be19edfbf5fb7f66fe0d8609091f", "node_info": null}, "1": {"text": "text_splitter: TokenTextSplitter\n) -> None:\n    \"\"\"Test get nodes from document have desired chunk size.\"\"\"\n    nodes = get_nodes_from_document(\n        documents[0],\n        text_splitter,\n        start_idx=0,\n        include_extra_info=False,\n    )\n    assert len(nodes) == 2\n    actual_chunk_sizes = [\n        len(text_splitter.tokenizer(node.get_text())) for node in nodes\n    ]\n    assert all(\n        chunk_size <= text_splitter._chunk_size for chunk_size in actual_chunk_sizes\n    )\n\n\ndef test_get_nodes_from_document_with_extra_info(\n    documents: List[Document], text_splitter: TokenTextSplitter\n) -> None:\n    \"\"\"Test get nodes from document with extra info have desired chunk size.\"\"\"\n    nodes = get_nodes_from_document(\n        documents[0],\n        text_splitter,\n        start_idx=0,\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_node_utils.py", "file_name": "test_node_utils.py"}, "index": 1, "child_indices": [], "ref_doc_id": "2c621db7da92be19edfbf5fb7f66fe0d8609091f", "node_info": null}, "2": {"text": "       start_idx=0,\n        include_extra_info=True,\n    )\n    assert len(nodes) == 3\n    actual_chunk_sizes = [\n        len(text_splitter.tokenizer(node.get_text())) for node in nodes\n    ]\n    assert all(\n        chunk_size <= text_splitter._chunk_size for chunk_size in actual_chunk_sizes\n    )\n    assert all([\"test_key: test_val\" in n.get_text() for n in nodes])\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_node_utils.py", "file_name": "test_node_utils.py"}, "index": 2, "child_indices": [], "ref_doc_id": "2c621db7da92be19edfbf5fb7f66fe0d8609091f", "node_info": null}, "3": {"text": "This code file tests the get_nodes_from_document function from the node_utils module. It uses a TokenTextSplitter to split a document into chunks of a specified size, and then creates nodes from the chunks. The test checks that the nodes have the desired chunk size and that the extra info is included in the nodes if specified.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 3, "child_indices": [0, 1, 2], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"3": {"text": "This code file tests the get_nodes_from_document function from the node_utils module. It uses a TokenTextSplitter to split a document into chunks of a specified size, and then creates nodes from the chunks. The test checks that the nodes have the desired chunk size and that the extra info is included in the nodes if specified.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 3, "child_indices": [0, 1, 2], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"2c621db7da92be19edfbf5fb7f66fe0d8609091f": {"text": "\"\"\"Test node utils.\"\"\"\n\nfrom typing import List\n\nimport pytest\n\nfrom gpt_index.indices.node_utils import get_nodes_from_document\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.readers.schema.base import Document\n\n\n@pytest.fixture\ndef text_splitter() -> TokenTextSplitter:\n    \"\"\"Get text splitter.\"\"\"\n    return TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n\n\n@pytest.fixture\ndef documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"\n    # NOTE: one document for now\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    return [\n        Document(doc_text, doc_id=\"test_doc_id\", extra_info={\"test_key\": \"test_val\"})\n    ]\n\n\ndef test_get_nodes_from_document(\n    documents: List[Document], text_splitter: TokenTextSplitter\n) -> None:\n    \"\"\"Test get nodes from document have desired chunk size.\"\"\"\n    nodes = get_nodes_from_document(\n        documents[0],\n        text_splitter,\n        start_idx=0,\n        include_extra_info=False,\n    )\n    assert len(nodes) == 2\n    actual_chunk_sizes = [\n        len(text_splitter.tokenizer(node.get_text())) for node in nodes\n    ]\n    assert all(\n        chunk_size <= text_splitter._chunk_size for chunk_size in actual_chunk_sizes\n    )\n\n\ndef test_get_nodes_from_document_with_extra_info(\n    documents: List[Document], text_splitter: TokenTextSplitter\n) -> None:\n    \"\"\"Test get nodes from document with extra info have desired chunk size.\"\"\"\n    nodes = get_nodes_from_document(\n        documents[0],\n        text_splitter,\n        start_idx=0,\n        include_extra_info=True,\n    )\n    assert len(nodes) == 3\n    actual_chunk_sizes = [\n        len(text_splitter.tokenizer(node.get_text())) for node in nodes\n    ]\n    assert all(\n        chunk_size <= text_splitter._chunk_size for chunk_size in actual_chunk_sizes\n    )\n    assert all([\"test_key: test_val\" in n.get_text() for n in nodes])\n", "doc_id": "2c621db7da92be19edfbf5fb7f66fe0d8609091f", "embedding": null, "extra_info": {"file_path": "tests/indices/test_node_utils.py", "file_name": "test_node_utils.py"}, "__type__": "Document"}, "aa90bab5-c74e-4e3e-836c-c353706e8f5e": {"text": "\nThis code file tests a function from the node_utils module that splits a document into chunks of a specified size and creates nodes from the chunks. It uses a TokenTextSplitter to split the document into chunks and then creates nodes from the chunks. The purpose of the code is to ensure that the nodes created from the document have the desired chunk size and include the extra info if specified. The test checks that the nodes have the desired chunk size and that the extra info is included in the nodes if specified.", "doc_id": "aa90bab5-c74e-4e3e-836c-c353706e8f5e", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Test node utils.\"\"\"\n\nfrom typing import List\n\nimport pytest\n\nfrom gpt_index.indices.node_utils import get_nodes_from_document\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.readers.schema.base import Document\n\n\n@pytest.fixture\ndef text_splitter() -> TokenTextSplitter:\n    \"\"\"Get text splitter.\"\"\"\n    return TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n\n\n@pytest.fixture\ndef documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"\n    # NOTE: one document for now\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    return [\n        Document(doc_text, doc_id=\"test_doc_id\", extra_info={\"test_key\": \"test_val\"})\n    ]\n\n\ndef test_get_nodes_from_document(\n    documents: List[Document], text_splitter: TokenTextSplitter\n) -> None:\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_node_utils.py", "file_name": "test_node_utils.py"}, "index": 0, "child_indices": [], "ref_doc_id": "2c621db7da92be19edfbf5fb7f66fe0d8609091f", "node_info": null}, "1": {"text": "text_splitter: TokenTextSplitter\n) -> None:\n    \"\"\"Test get nodes from document have desired chunk size.\"\"\"\n    nodes = get_nodes_from_document(\n        documents[0],\n        text_splitter,\n        start_idx=0,\n        include_extra_info=False,\n    )\n    assert len(nodes) == 2\n    actual_chunk_sizes = [\n        len(text_splitter.tokenizer(node.get_text())) for node in nodes\n    ]\n    assert all(\n        chunk_size <= text_splitter._chunk_size for chunk_size in actual_chunk_sizes\n    )\n\n\ndef test_get_nodes_from_document_with_extra_info(\n    documents: List[Document], text_splitter: TokenTextSplitter\n) -> None:\n    \"\"\"Test get nodes from document with extra info have desired chunk size.\"\"\"\n    nodes = get_nodes_from_document(\n        documents[0],\n        text_splitter,\n        start_idx=0,\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_node_utils.py", "file_name": "test_node_utils.py"}, "index": 1, "child_indices": [], "ref_doc_id": "2c621db7da92be19edfbf5fb7f66fe0d8609091f", "node_info": null}, "2": {"text": "       start_idx=0,\n        include_extra_info=True,\n    )\n    assert len(nodes) == 3\n    actual_chunk_sizes = [\n        len(text_splitter.tokenizer(node.get_text())) for node in nodes\n    ]\n    assert all(\n        chunk_size <= text_splitter._chunk_size for chunk_size in actual_chunk_sizes\n    )\n    assert all([\"test_key: test_val\" in n.get_text() for n in nodes])\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_node_utils.py", "file_name": "test_node_utils.py"}, "index": 2, "child_indices": [], "ref_doc_id": "2c621db7da92be19edfbf5fb7f66fe0d8609091f", "node_info": null}, "3": {"text": "This code file tests the get_nodes_from_document function from the node_utils module. It uses a TokenTextSplitter to split a document into chunks of a specified size, and then creates nodes from the chunks. The test checks that the nodes have the desired chunk size and that the extra info is included in the nodes if specified.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 3, "child_indices": [0, 1, 2], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"3": {"text": "This code file tests the get_nodes_from_document function from the node_utils module. It uses a TokenTextSplitter to split a document into chunks of a specified size, and then creates nodes from the chunks. The test checks that the nodes have the desired chunk size and that the extra info is included in the nodes if specified.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 3, "child_indices": [0, 1, 2], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}