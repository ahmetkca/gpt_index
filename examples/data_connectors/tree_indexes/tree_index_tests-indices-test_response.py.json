{"index_struct": {"text": "\nThis code file tests the response utils for the GPT-Index project. It uses the ResponseBuilder class to generate a response to a query string, which is determined by the PromptHelper class and predicted by the LLMPredictor class. The ResponseMode.COMPACT and ResponseMode.TREE_SUMMARIZE modes are also tested, which use the RefinePrompt and QuestionAnswerPrompt classes to generate the response. Mock functions are used to simulate the behavior of the classes. The purpose of the code is to ensure that the response is generated correctly. The code tests the response utils by using the ResponseBuilder class to generate a response to a query string, which is determined by the PromptHelper class and predicted by the LLMPredictor class. The ResponseMode.COMPACT and ResponseMode.TREE_SUMMARIZE modes are also tested, which use the RefinePrompt and QuestionAnswerPrompt classes to generate the response. Mock functions are used to simulate the behavior of the classes. The code is designed to test the accuracy of the response utils for the GPT-Index project.", "doc_id": "326487c4-b1db-48ed-aa47-970966eab9af", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Test response utils.\"\"\"\n\nfrom typing import Any, List\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom gpt_index.constants import MAX_CHUNK_OVERLAP, MAX_CHUNK_SIZE, NUM_OUTPUTS\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.response.builder import ResponseBuilder, ResponseMode, TextChunk\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\nfrom gpt_index.readers.schema.base import Document\nfrom tests.mock_utils.mock_decorator import patch_common\nfrom tests.mock_utils.mock_predict import mock_llmpredictor_predict\nfrom tests.mock_utils.mock_prompts import MOCK_REFINE_PROMPT, MOCK_TEXT_QA_PROMPT\n\n\n@pytest.fixture\ndef documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"\n    # NOTE: one document for now\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 0, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "1": {"text": "      \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    return [Document(doc_text)]\n\n\ndef mock_tokenizer(text: str) -> List[str]:\n    \"\"\"Mock tokenizer.\"\"\"\n    if text == \"\":\n        return []\n    tokens = text.split(\" \")\n    return tokens\n\n\n@patch_common\ndef test_give_response(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test give response.\"\"\"\n    prompt_helper = PromptHelper(MAX_CHUNK_SIZE, NUM_OUTPUTS, MAX_CHUNK_OVERLAP)\n    llm_predictor = LLMPredictor()\n    query_str = \"What is?\"\n\n    # test single line\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 1, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "2": {"text": "      llm_predictor,\n        MOCK_TEXT_QA_PROMPT,\n        MOCK_REFINE_PROMPT,\n        texts=[TextChunk(\"This is a single line.\")],\n    )\n    response = builder.get_response(query_str)\n    assert str(response) == \"What is?:This is a single line.\"\n\n    # test multiple lines\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n        MOCK_TEXT_QA_PROMPT,\n        MOCK_REFINE_PROMPT,\n        texts=[TextChunk(documents[0].get_text())],\n    )\n    response = builder.get_response(query_str)\n    assert str(response) == \"What is?:Hello world.\"\n\n\n@patch.object(LLMPredictor, \"total_tokens_used\", return_value=0)\n@patch.object(LLMPredictor, \"predict\", side_effect=mock_llmpredictor_predict)\n@patch.object(LLMPredictor, \"__init__\", return_value=None)\ndef test_compact_response(\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 2, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "3": {"text": "return_value=None)\ndef test_compact_response(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test give response.\"\"\"\n    # test response with ResponseMode.COMPACT\n    # NOTE: here we want to guarante that prompts have 0 extra tokens\n    mock_refine_prompt_tmpl = \"{query_str}{existing_answer}{context_msg}\"\n    mock_refine_prompt = RefinePrompt(mock_refine_prompt_tmpl)\n\n    mock_qa_prompt_tmpl = \"{context_str}{query_str}\"\n    mock_qa_prompt = QuestionAnswerPrompt(mock_qa_prompt_tmpl)\n\n    # max input size is 11, prompt is two tokens (the query) --> 9 tokens\n    # --> padding is 1 --> 8 tokens\n    prompt_helper = PromptHelper(\n        11, 0, 0, tokenizer=mock_tokenizer, separator=\"\\n\\n\", chunk_size_limit=4\n    )\n    cur_chunk_size =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 3, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "4": {"text": "   )\n    cur_chunk_size = prompt_helper.get_chunk_size_given_prompt(\"\", 1, padding=1)\n    # outside of compact, assert that chunk size is 4\n    assert cur_chunk_size == 4\n\n    # within compact, make sure that chunk size is 8\n    llm_predictor = LLMPredictor()\n    query_str = \"What is?\"\n    texts = [\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nbar\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\nanother\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nfoo\"),\n    ]\n\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n        mock_qa_prompt,\n        mock_refine_prompt,\n        texts=texts,\n    )\n    response = builder.get_response(query_str,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 4, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "5": {"text": "   )\n    response = builder.get_response(query_str, mode=ResponseMode.COMPACT)\n    assert str(response) == (\n        \"What is?:\" \"This\\n\\nis\\n\\na\\n\\nbar\\n\\n\" \"This\\n\\nis\\n\\na\\n\\ntest\"\n    )\n\n\n@patch.object(LLMPredictor, \"total_tokens_used\", return_value=0)\n@patch.object(LLMPredictor, \"predict\", side_effect=mock_llmpredictor_predict)\n@patch.object(LLMPredictor, \"__init__\", return_value=None)\ndef test_tree_summarize_response(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test give response.\"\"\"\n    # test response with ResponseMode.TREE_SUMMARIZE\n    # NOTE: here we want to guarante that prompts have 0 extra tokens\n    mock_refine_prompt_tmpl = \"{query_str}{existing_answer}{context_msg}\"\n    mock_refine_prompt =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 5, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "6": {"text": "   mock_refine_prompt = RefinePrompt(mock_refine_prompt_tmpl)\n\n    mock_qa_prompt_tmpl = \"{context_str}{query_str}\"\n    mock_qa_prompt = QuestionAnswerPrompt(mock_qa_prompt_tmpl)\n\n    # max input size is 12, prompt tokens is 2 (query_str)\n    # --> 10 tokens for 2 chunks -->\n    # 5 tokens per chunk, 1 is padding --> 4 tokens per chunk\n    prompt_helper = PromptHelper(12, 0, 0, tokenizer=mock_tokenizer, separator=\"\\n\\n\")\n\n    # within tree_summarize, make sure that chunk size is 8\n    llm_predictor = LLMPredictor()\n    query_str = \"What is?\"\n    texts = [\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nbar\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\nanother\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nfoo\"),\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 6, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "7": {"text": "   ]\n\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n        mock_qa_prompt,\n        mock_refine_prompt,\n        texts=texts,\n    )\n    response = builder.get_response(\n        query_str, mode=ResponseMode.TREE_SUMMARIZE, num_children=2\n    )\n    # TODO: fix this output, the \\n join appends unnecessary results at the end\n    assert str(response) == (\n        \"What is?:This\\n\\nis\\n\\na\\n\\nbar\\nThis\\n\" \"This\\n\\nis\\n\\nanother\\n\\ntest\\nThis\"\n    )\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 7, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "8": {"text": "This code file tests the response utils for the GPT-Index project. It tests the give_response and compact_response functions, which use the ResponseBuilder class to generate a response to a query string. The ResponseBuilder class uses the PromptHelper class to determine the maximum chunk size and number of outputs, as well as the LLMPredictor class to predict the response. The test_tree_summarize_response function also tests the ResponseMode.TREE_SUMMARIZE mode, which uses the RefinePrompt and QuestionAnswerPrompt classes to generate the response. The code also uses mock functions to simulate the behavior of the classes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 8, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"8": {"text": "This code file tests the response utils for the GPT-Index project. It tests the give_response and compact_response functions, which use the ResponseBuilder class to generate a response to a query string. The ResponseBuilder class uses the PromptHelper class to determine the maximum chunk size and number of outputs, as well as the LLMPredictor class to predict the response. The test_tree_summarize_response function also tests the ResponseMode.TREE_SUMMARIZE mode, which uses the RefinePrompt and QuestionAnswerPrompt classes to generate the response. The code also uses mock functions to simulate the behavior of the classes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 8, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"c2a0685b15adc5262f0ab5349e856d6210e6e7bb": {"text": "\"\"\"Test response utils.\"\"\"\n\nfrom typing import Any, List\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom gpt_index.constants import MAX_CHUNK_OVERLAP, MAX_CHUNK_SIZE, NUM_OUTPUTS\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.response.builder import ResponseBuilder, ResponseMode, TextChunk\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\nfrom gpt_index.readers.schema.base import Document\nfrom tests.mock_utils.mock_decorator import patch_common\nfrom tests.mock_utils.mock_predict import mock_llmpredictor_predict\nfrom tests.mock_utils.mock_prompts import MOCK_REFINE_PROMPT, MOCK_TEXT_QA_PROMPT\n\n\n@pytest.fixture\ndef documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"\n    # NOTE: one document for now\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    return [Document(doc_text)]\n\n\ndef mock_tokenizer(text: str) -> List[str]:\n    \"\"\"Mock tokenizer.\"\"\"\n    if text == \"\":\n        return []\n    tokens = text.split(\" \")\n    return tokens\n\n\n@patch_common\ndef test_give_response(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test give response.\"\"\"\n    prompt_helper = PromptHelper(MAX_CHUNK_SIZE, NUM_OUTPUTS, MAX_CHUNK_OVERLAP)\n    llm_predictor = LLMPredictor()\n    query_str = \"What is?\"\n\n    # test single line\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n        MOCK_TEXT_QA_PROMPT,\n        MOCK_REFINE_PROMPT,\n        texts=[TextChunk(\"This is a single line.\")],\n    )\n    response = builder.get_response(query_str)\n    assert str(response) == \"What is?:This is a single line.\"\n\n    # test multiple lines\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n        MOCK_TEXT_QA_PROMPT,\n        MOCK_REFINE_PROMPT,\n        texts=[TextChunk(documents[0].get_text())],\n    )\n    response = builder.get_response(query_str)\n    assert str(response) == \"What is?:Hello world.\"\n\n\n@patch.object(LLMPredictor, \"total_tokens_used\", return_value=0)\n@patch.object(LLMPredictor, \"predict\", side_effect=mock_llmpredictor_predict)\n@patch.object(LLMPredictor, \"__init__\", return_value=None)\ndef test_compact_response(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test give response.\"\"\"\n    # test response with ResponseMode.COMPACT\n    # NOTE: here we want to guarante that prompts have 0 extra tokens\n    mock_refine_prompt_tmpl = \"{query_str}{existing_answer}{context_msg}\"\n    mock_refine_prompt = RefinePrompt(mock_refine_prompt_tmpl)\n\n    mock_qa_prompt_tmpl = \"{context_str}{query_str}\"\n    mock_qa_prompt = QuestionAnswerPrompt(mock_qa_prompt_tmpl)\n\n    # max input size is 11, prompt is two tokens (the query) --> 9 tokens\n    # --> padding is 1 --> 8 tokens\n    prompt_helper = PromptHelper(\n        11, 0, 0, tokenizer=mock_tokenizer, separator=\"\\n\\n\", chunk_size_limit=4\n    )\n    cur_chunk_size = prompt_helper.get_chunk_size_given_prompt(\"\", 1, padding=1)\n    # outside of compact, assert that chunk size is 4\n    assert cur_chunk_size == 4\n\n    # within compact, make sure that chunk size is 8\n    llm_predictor = LLMPredictor()\n    query_str = \"What is?\"\n    texts = [\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nbar\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\nanother\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nfoo\"),\n    ]\n\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n        mock_qa_prompt,\n        mock_refine_prompt,\n        texts=texts,\n    )\n    response = builder.get_response(query_str, mode=ResponseMode.COMPACT)\n    assert str(response) == (\n        \"What is?:\" \"This\\n\\nis\\n\\na\\n\\nbar\\n\\n\" \"This\\n\\nis\\n\\na\\n\\ntest\"\n    )\n\n\n@patch.object(LLMPredictor, \"total_tokens_used\", return_value=0)\n@patch.object(LLMPredictor, \"predict\", side_effect=mock_llmpredictor_predict)\n@patch.object(LLMPredictor, \"__init__\", return_value=None)\ndef test_tree_summarize_response(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test give response.\"\"\"\n    # test response with ResponseMode.TREE_SUMMARIZE\n    # NOTE: here we want to guarante that prompts have 0 extra tokens\n    mock_refine_prompt_tmpl = \"{query_str}{existing_answer}{context_msg}\"\n    mock_refine_prompt = RefinePrompt(mock_refine_prompt_tmpl)\n\n    mock_qa_prompt_tmpl = \"{context_str}{query_str}\"\n    mock_qa_prompt = QuestionAnswerPrompt(mock_qa_prompt_tmpl)\n\n    # max input size is 12, prompt tokens is 2 (query_str)\n    # --> 10 tokens for 2 chunks -->\n    # 5 tokens per chunk, 1 is padding --> 4 tokens per chunk\n    prompt_helper = PromptHelper(12, 0, 0, tokenizer=mock_tokenizer, separator=\"\\n\\n\")\n\n    # within tree_summarize, make sure that chunk size is 8\n    llm_predictor = LLMPredictor()\n    query_str = \"What is?\"\n    texts = [\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nbar\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\nanother\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nfoo\"),\n    ]\n\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n        mock_qa_prompt,\n        mock_refine_prompt,\n        texts=texts,\n    )\n    response = builder.get_response(\n        query_str, mode=ResponseMode.TREE_SUMMARIZE, num_children=2\n    )\n    # TODO: fix this output, the \\n join appends unnecessary results at the end\n    assert str(response) == (\n        \"What is?:This\\n\\nis\\n\\na\\n\\nbar\\nThis\\n\" \"This\\n\\nis\\n\\nanother\\n\\ntest\\nThis\"\n    )\n", "doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "__type__": "Document"}, "326487c4-b1db-48ed-aa47-970966eab9af": {"text": "\nThis code file tests the response utils for the GPT-Index project. It uses the ResponseBuilder class to generate a response to a query string, which is determined by the PromptHelper class and predicted by the LLMPredictor class. The ResponseMode.COMPACT and ResponseMode.TREE_SUMMARIZE modes are also tested, which use the RefinePrompt and QuestionAnswerPrompt classes to generate the response. Mock functions are used to simulate the behavior of the classes. The purpose of the code is to ensure that the response is generated correctly. The code tests the response utils by using the ResponseBuilder class to generate a response to a query string, which is determined by the PromptHelper class and predicted by the LLMPredictor class. The ResponseMode.COMPACT and ResponseMode.TREE_SUMMARIZE modes are also tested, which use the RefinePrompt and QuestionAnswerPrompt classes to generate the response. Mock functions are used to simulate the behavior of the classes. The code is designed to test the accuracy of the response utils for the GPT-Index project.", "doc_id": "326487c4-b1db-48ed-aa47-970966eab9af", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Test response utils.\"\"\"\n\nfrom typing import Any, List\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom gpt_index.constants import MAX_CHUNK_OVERLAP, MAX_CHUNK_SIZE, NUM_OUTPUTS\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.response.builder import ResponseBuilder, ResponseMode, TextChunk\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\nfrom gpt_index.readers.schema.base import Document\nfrom tests.mock_utils.mock_decorator import patch_common\nfrom tests.mock_utils.mock_predict import mock_llmpredictor_predict\nfrom tests.mock_utils.mock_prompts import MOCK_REFINE_PROMPT, MOCK_TEXT_QA_PROMPT\n\n\n@pytest.fixture\ndef documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"\n    # NOTE: one document for now\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n    ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 0, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "1": {"text": "      \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    return [Document(doc_text)]\n\n\ndef mock_tokenizer(text: str) -> List[str]:\n    \"\"\"Mock tokenizer.\"\"\"\n    if text == \"\":\n        return []\n    tokens = text.split(\" \")\n    return tokens\n\n\n@patch_common\ndef test_give_response(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test give response.\"\"\"\n    prompt_helper = PromptHelper(MAX_CHUNK_SIZE, NUM_OUTPUTS, MAX_CHUNK_OVERLAP)\n    llm_predictor = LLMPredictor()\n    query_str = \"What is?\"\n\n    # test single line\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n      ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 1, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "2": {"text": "      llm_predictor,\n        MOCK_TEXT_QA_PROMPT,\n        MOCK_REFINE_PROMPT,\n        texts=[TextChunk(\"This is a single line.\")],\n    )\n    response = builder.get_response(query_str)\n    assert str(response) == \"What is?:This is a single line.\"\n\n    # test multiple lines\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n        MOCK_TEXT_QA_PROMPT,\n        MOCK_REFINE_PROMPT,\n        texts=[TextChunk(documents[0].get_text())],\n    )\n    response = builder.get_response(query_str)\n    assert str(response) == \"What is?:Hello world.\"\n\n\n@patch.object(LLMPredictor, \"total_tokens_used\", return_value=0)\n@patch.object(LLMPredictor, \"predict\", side_effect=mock_llmpredictor_predict)\n@patch.object(LLMPredictor, \"__init__\", return_value=None)\ndef test_compact_response(\n ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 2, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "3": {"text": "return_value=None)\ndef test_compact_response(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test give response.\"\"\"\n    # test response with ResponseMode.COMPACT\n    # NOTE: here we want to guarante that prompts have 0 extra tokens\n    mock_refine_prompt_tmpl = \"{query_str}{existing_answer}{context_msg}\"\n    mock_refine_prompt = RefinePrompt(mock_refine_prompt_tmpl)\n\n    mock_qa_prompt_tmpl = \"{context_str}{query_str}\"\n    mock_qa_prompt = QuestionAnswerPrompt(mock_qa_prompt_tmpl)\n\n    # max input size is 11, prompt is two tokens (the query) --> 9 tokens\n    # --> padding is 1 --> 8 tokens\n    prompt_helper = PromptHelper(\n        11, 0, 0, tokenizer=mock_tokenizer, separator=\"\\n\\n\", chunk_size_limit=4\n    )\n    cur_chunk_size =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 3, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "4": {"text": "   )\n    cur_chunk_size = prompt_helper.get_chunk_size_given_prompt(\"\", 1, padding=1)\n    # outside of compact, assert that chunk size is 4\n    assert cur_chunk_size == 4\n\n    # within compact, make sure that chunk size is 8\n    llm_predictor = LLMPredictor()\n    query_str = \"What is?\"\n    texts = [\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nbar\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\nanother\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nfoo\"),\n    ]\n\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n        mock_qa_prompt,\n        mock_refine_prompt,\n        texts=texts,\n    )\n    response = builder.get_response(query_str,", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 4, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "5": {"text": "   )\n    response = builder.get_response(query_str, mode=ResponseMode.COMPACT)\n    assert str(response) == (\n        \"What is?:\" \"This\\n\\nis\\n\\na\\n\\nbar\\n\\n\" \"This\\n\\nis\\n\\na\\n\\ntest\"\n    )\n\n\n@patch.object(LLMPredictor, \"total_tokens_used\", return_value=0)\n@patch.object(LLMPredictor, \"predict\", side_effect=mock_llmpredictor_predict)\n@patch.object(LLMPredictor, \"__init__\", return_value=None)\ndef test_tree_summarize_response(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test give response.\"\"\"\n    # test response with ResponseMode.TREE_SUMMARIZE\n    # NOTE: here we want to guarante that prompts have 0 extra tokens\n    mock_refine_prompt_tmpl = \"{query_str}{existing_answer}{context_msg}\"\n    mock_refine_prompt =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 5, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "6": {"text": "   mock_refine_prompt = RefinePrompt(mock_refine_prompt_tmpl)\n\n    mock_qa_prompt_tmpl = \"{context_str}{query_str}\"\n    mock_qa_prompt = QuestionAnswerPrompt(mock_qa_prompt_tmpl)\n\n    # max input size is 12, prompt tokens is 2 (query_str)\n    # --> 10 tokens for 2 chunks -->\n    # 5 tokens per chunk, 1 is padding --> 4 tokens per chunk\n    prompt_helper = PromptHelper(12, 0, 0, tokenizer=mock_tokenizer, separator=\"\\n\\n\")\n\n    # within tree_summarize, make sure that chunk size is 8\n    llm_predictor = LLMPredictor()\n    query_str = \"What is?\"\n    texts = [\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nbar\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\nanother\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nfoo\"),\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 6, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "7": {"text": "   ]\n\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n        mock_qa_prompt,\n        mock_refine_prompt,\n        texts=texts,\n    )\n    response = builder.get_response(\n        query_str, mode=ResponseMode.TREE_SUMMARIZE, num_children=2\n    )\n    # TODO: fix this output, the \\n join appends unnecessary results at the end\n    assert str(response) == (\n        \"What is?:This\\n\\nis\\n\\na\\n\\nbar\\nThis\\n\" \"This\\n\\nis\\n\\nanother\\n\\ntest\\nThis\"\n    )\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "index": 7, "child_indices": [], "ref_doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "node_info": null}, "8": {"text": "This code file tests the response utils for the GPT-Index project. It tests the give_response and compact_response functions, which use the ResponseBuilder class to generate a response to a query string. The ResponseBuilder class uses the PromptHelper class to determine the maximum chunk size and number of outputs, as well as the LLMPredictor class to predict the response. The test_tree_summarize_response function also tests the ResponseMode.TREE_SUMMARIZE mode, which uses the RefinePrompt and QuestionAnswerPrompt classes to generate the response. The code also uses mock functions to simulate the behavior of the classes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 8, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"8": {"text": "This code file tests the response utils for the GPT-Index project. It tests the give_response and compact_response functions, which use the ResponseBuilder class to generate a response to a query string. The ResponseBuilder class uses the PromptHelper class to determine the maximum chunk size and number of outputs, as well as the LLMPredictor class to predict the response. The test_tree_summarize_response function also tests the ResponseMode.TREE_SUMMARIZE mode, which uses the RefinePrompt and QuestionAnswerPrompt classes to generate the response. The code also uses mock functions to simulate the behavior of the classes.", "doc_id": null, "embedding": null, "extra_info": null, "index": 8, "child_indices": [0, 1, 2, 3, 4, 5, 6, 7], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}