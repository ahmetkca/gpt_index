{"index_struct": {"text": "\nThe code file tests the TokenTextSplitter class, which is used to split text into chunks of a specified size with a specified overlap. The split_text and truncate_text functions are tested to ensure that the TokenTextSplitter class is able to accurately and efficiently split text into the desired chunks. The code also tests the ability to split long tokens and take into account the chunk size used by an extra info string. The data structures used are strings and arrays, and the algorithms used are for splitting and truncating text. The purpose of the code is to ensure that the TokenTextSplitter class is able to accurately and efficiently split text into the desired chunks.", "doc_id": "630fa85c-340f-420f-ace5-09028e733477", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Test text splitter.\"\"\"\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\n\n\ndef test_split_token() -> None:\n    \"\"\"Test split normal token.\"\"\"\n    # tiktoken will say length is ~5k\n    token = \"foo bar\"\n    text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)\n    chunks = text_splitter.split_text(token)\n    assert chunks == [\"foo\", \"bar\"]\n\n    token = \"foo bar hello world\"\n    text_splitter = TokenTextSplitter(chunk_size=2, chunk_overlap=1)\n    chunks = text_splitter.split_text(token)\n    assert chunks == [\"foo bar\", \"bar hello\", \"hello world\"]\n\n\ndef test_truncate_token() -> None:\n    \"\"\"Test truncate normal token.\"\"\"\n    # tiktoken will say length is ~5k\n    token = \"foo bar\"\n    text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)\n    chunks = text_splitter.truncate_text(token)\n    assert chunks == \"foo\"\n\n\ndef test_split_long_token() ->", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/langchain_helpers/test_text_splitter.py", "file_name": "test_text_splitter.py"}, "index": 0, "child_indices": [], "ref_doc_id": "ddb2540512e86c257e1680bbd058b930257b3b9e", "node_info": null}, "1": {"text": " assert chunks == \"foo\"\n\n\ndef test_split_long_token() -> None:\n    \"\"\"Test split a really long token.\"\"\"\n    # tiktoken will say length is ~5k\n    token = \"a\" * 100\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    text_splitter.split_text(token)\n\n    token = (\"a\" * 49) + \"\\n\" + (\"a\" * 50)\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    chunks = text_splitter.split_text(token)\n    assert len(chunks[0]) == 49\n    assert len(chunks[1]) == 50\n\n\ndef test_split_with_extra_info_str() -> None:\n    \"\"\"Test split while taking into account chunk size used by extra info str.\"\"\"\n    text = \" \".join([\"foo\"] * 20)\n    extra_info_str = \"test_extra_info_str\"\n\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    chunks = text_splitter.split_text(text)\n    assert len(chunks) == 1\n\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/langchain_helpers/test_text_splitter.py", "file_name": "test_text_splitter.py"}, "index": 1, "child_indices": [], "ref_doc_id": "ddb2540512e86c257e1680bbd058b930257b3b9e", "node_info": null}, "2": {"text": "   assert len(chunks) == 1\n\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    chunks = text_splitter.split_text(text, extra_info_str=extra_info_str)\n    assert len(chunks) == 2\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/langchain_helpers/test_text_splitter.py", "file_name": "test_text_splitter.py"}, "index": 2, "child_indices": [], "ref_doc_id": "ddb2540512e86c257e1680bbd058b930257b3b9e", "node_info": null}, "3": {"text": "This code file tests the functionality of the TokenTextSplitter class, which is used to split text into chunks of a specified size with a specified overlap. The code tests the split_text and truncate_text functions, as well as the ability to split long tokens and take into account the chunk size used by an extra info string.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 3, "child_indices": [0, 1, 2], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"3": {"text": "This code file tests the functionality of the TokenTextSplitter class, which is used to split text into chunks of a specified size with a specified overlap. The code tests the split_text and truncate_text functions, as well as the ability to split long tokens and take into account the chunk size used by an extra info string.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 3, "child_indices": [0, 1, 2], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"ddb2540512e86c257e1680bbd058b930257b3b9e": {"text": "\"\"\"Test text splitter.\"\"\"\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\n\n\ndef test_split_token() -> None:\n    \"\"\"Test split normal token.\"\"\"\n    # tiktoken will say length is ~5k\n    token = \"foo bar\"\n    text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)\n    chunks = text_splitter.split_text(token)\n    assert chunks == [\"foo\", \"bar\"]\n\n    token = \"foo bar hello world\"\n    text_splitter = TokenTextSplitter(chunk_size=2, chunk_overlap=1)\n    chunks = text_splitter.split_text(token)\n    assert chunks == [\"foo bar\", \"bar hello\", \"hello world\"]\n\n\ndef test_truncate_token() -> None:\n    \"\"\"Test truncate normal token.\"\"\"\n    # tiktoken will say length is ~5k\n    token = \"foo bar\"\n    text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)\n    chunks = text_splitter.truncate_text(token)\n    assert chunks == \"foo\"\n\n\ndef test_split_long_token() -> None:\n    \"\"\"Test split a really long token.\"\"\"\n    # tiktoken will say length is ~5k\n    token = \"a\" * 100\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    text_splitter.split_text(token)\n\n    token = (\"a\" * 49) + \"\\n\" + (\"a\" * 50)\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    chunks = text_splitter.split_text(token)\n    assert len(chunks[0]) == 49\n    assert len(chunks[1]) == 50\n\n\ndef test_split_with_extra_info_str() -> None:\n    \"\"\"Test split while taking into account chunk size used by extra info str.\"\"\"\n    text = \" \".join([\"foo\"] * 20)\n    extra_info_str = \"test_extra_info_str\"\n\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    chunks = text_splitter.split_text(text)\n    assert len(chunks) == 1\n\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    chunks = text_splitter.split_text(text, extra_info_str=extra_info_str)\n    assert len(chunks) == 2\n", "doc_id": "ddb2540512e86c257e1680bbd058b930257b3b9e", "embedding": null, "extra_info": {"file_path": "tests/langchain_helpers/test_text_splitter.py", "file_name": "test_text_splitter.py"}, "__type__": "Document"}, "630fa85c-340f-420f-ace5-09028e733477": {"text": "\nThe code file tests the TokenTextSplitter class, which is used to split text into chunks of a specified size with a specified overlap. The split_text and truncate_text functions are tested to ensure that the TokenTextSplitter class is able to accurately and efficiently split text into the desired chunks. The code also tests the ability to split long tokens and take into account the chunk size used by an extra info string. The data structures used are strings and arrays, and the algorithms used are for splitting and truncating text. The purpose of the code is to ensure that the TokenTextSplitter class is able to accurately and efficiently split text into the desired chunks.", "doc_id": "630fa85c-340f-420f-ace5-09028e733477", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Test text splitter.\"\"\"\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\n\n\ndef test_split_token() -> None:\n    \"\"\"Test split normal token.\"\"\"\n    # tiktoken will say length is ~5k\n    token = \"foo bar\"\n    text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)\n    chunks = text_splitter.split_text(token)\n    assert chunks == [\"foo\", \"bar\"]\n\n    token = \"foo bar hello world\"\n    text_splitter = TokenTextSplitter(chunk_size=2, chunk_overlap=1)\n    chunks = text_splitter.split_text(token)\n    assert chunks == [\"foo bar\", \"bar hello\", \"hello world\"]\n\n\ndef test_truncate_token() -> None:\n    \"\"\"Test truncate normal token.\"\"\"\n    # tiktoken will say length is ~5k\n    token = \"foo bar\"\n    text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)\n    chunks = text_splitter.truncate_text(token)\n    assert chunks == \"foo\"\n\n\ndef test_split_long_token() ->", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/langchain_helpers/test_text_splitter.py", "file_name": "test_text_splitter.py"}, "index": 0, "child_indices": [], "ref_doc_id": "ddb2540512e86c257e1680bbd058b930257b3b9e", "node_info": null}, "1": {"text": " assert chunks == \"foo\"\n\n\ndef test_split_long_token() -> None:\n    \"\"\"Test split a really long token.\"\"\"\n    # tiktoken will say length is ~5k\n    token = \"a\" * 100\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    text_splitter.split_text(token)\n\n    token = (\"a\" * 49) + \"\\n\" + (\"a\" * 50)\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    chunks = text_splitter.split_text(token)\n    assert len(chunks[0]) == 49\n    assert len(chunks[1]) == 50\n\n\ndef test_split_with_extra_info_str() -> None:\n    \"\"\"Test split while taking into account chunk size used by extra info str.\"\"\"\n    text = \" \".join([\"foo\"] * 20)\n    extra_info_str = \"test_extra_info_str\"\n\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    chunks = text_splitter.split_text(text)\n    assert len(chunks) == 1\n\n   ", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/langchain_helpers/test_text_splitter.py", "file_name": "test_text_splitter.py"}, "index": 1, "child_indices": [], "ref_doc_id": "ddb2540512e86c257e1680bbd058b930257b3b9e", "node_info": null}, "2": {"text": "   assert len(chunks) == 1\n\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    chunks = text_splitter.split_text(text, extra_info_str=extra_info_str)\n    assert len(chunks) == 2\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/langchain_helpers/test_text_splitter.py", "file_name": "test_text_splitter.py"}, "index": 2, "child_indices": [], "ref_doc_id": "ddb2540512e86c257e1680bbd058b930257b3b9e", "node_info": null}, "3": {"text": "This code file tests the functionality of the TokenTextSplitter class, which is used to split text into chunks of a specified size with a specified overlap. The code tests the split_text and truncate_text functions, as well as the ability to split long tokens and take into account the chunk size used by an extra info string.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 3, "child_indices": [0, 1, 2], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"3": {"text": "This code file tests the functionality of the TokenTextSplitter class, which is used to split text into chunks of a specified size with a specified overlap. The code tests the split_text and truncate_text functions, as well as the ability to split long tokens and take into account the chunk size used by an extra info string.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 3, "child_indices": [0, 1, 2], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}