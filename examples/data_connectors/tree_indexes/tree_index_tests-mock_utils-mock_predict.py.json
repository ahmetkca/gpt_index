{"index_struct": {"text": "\nThis code file contains mock functions for simulating the behavior of the GPT-Index system. The functions cover a range of tasks, such as summarizing text, inserting nodes into a GPT tree, selecting nodes from a GPT tree, refining existing answers, extracting keywords from text, extracting keywords from questions, extracting schemas from text, and converting text to SQL queries. Each function takes a prompt argument and returns a response, and there is also a mock LLMChain predict function which returns a generic response. The code provides a testing environment for the GPT-Index system, allowing developers to simulate the system's behavior and test its functionality. The code utilizes various algorithms, data structures, and relationships between functions, classes, and variables to achieve its purpose.", "doc_id": "ea1cd8b6-297a-43c1-a4af-549c21d90158", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Mock predict.\"\"\"\n\nfrom typing import Any, Dict, Tuple\n\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.prompts.prompt_type import PromptType\nfrom gpt_index.token_counter.utils import mock_extract_keywords_response\n\n\ndef _mock_summary_predict(prompt_args: Dict) -> str:\n    \"\"\"Mock summary predict.\"\"\"\n    return prompt_args[\"context_str\"]\n\n\ndef _mock_insert_predict() -> str:\n    \"\"\"Mock insert predict.\n\n    Used in GPT tree index during insertion\n    to select the next node.\n\n    \"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_query_select() -> str:\n    \"\"\"Mock query predict.\n\n    Used in GPT tree index during query traversal\n    to select the next node.\n\n    \"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_answer(prompt_args: Dict) -> str:\n    \"\"\"Mock answer.\"\"\"\n    return prompt_args[\"query_str\"] + \":\" + prompt_args[\"context_str\"]\n\n\ndef _mock_refine(prompt_args: Dict) -> str:\n    \"\"\"Mock", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_predict.py", "file_name": "mock_predict.py"}, "index": 0, "child_indices": [], "ref_doc_id": "af0672c6bf667916d04434c0959d199c30af3bb8", "node_info": null}, "1": {"text": "Dict) -> str:\n    \"\"\"Mock refine.\"\"\"\n    return prompt_args[\"existing_answer\"]\n\n\ndef _mock_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"text\"])\n\n\ndef _mock_query_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock query keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"question\"])\n\n\ndef _mock_schema_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock schema extract.\"\"\"\n    return prompt_args[\"text\"]\n\n\ndef _mock_text_to_sql(prompt_args: Dict) -> str:\n    \"\"\"Mock text to sql.\"\"\"\n    # assume it's a select query\n    tokens = prompt_args[\"query_str\"].split(\":\")\n    table_name = tokens[0]\n    subtokens = tokens[1].split(\",\")\n    return \"SELECT \" + \", \".join(subtokens) + f\" FROM {table_name}\"\n\n\ndef", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_predict.py", "file_name": "mock_predict.py"}, "index": 1, "child_indices": [], "ref_doc_id": "af0672c6bf667916d04434c0959d199c30af3bb8", "node_info": null}, "2": {"text": "+ f\" FROM {table_name}\"\n\n\ndef mock_llmpredictor_predict(prompt: Prompt, **prompt_args: Any) -> Tuple[str, str]:\n    \"\"\"Mock predict method of LLMPredictor.\n\n    Depending on the prompt, return response.\n\n    \"\"\"\n    formatted_prompt = prompt.format(**prompt_args)\n    full_prompt_args = prompt.get_full_format_args(prompt_args)\n    if prompt.prompt_type == PromptType.SUMMARY:\n        response = _mock_summary_predict(full_prompt_args)\n    elif prompt.prompt_type == PromptType.TREE_INSERT:\n        response = _mock_insert_predict()\n    elif prompt.prompt_type == PromptType.TREE_SELECT:\n        response = _mock_query_select()\n    elif prompt.prompt_type == PromptType.REFINE:\n        response = _mock_refine(full_prompt_args)\n    elif prompt.prompt_type == PromptType.QUESTION_ANSWER:\n        response =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_predict.py", "file_name": "mock_predict.py"}, "index": 2, "child_indices": [], "ref_doc_id": "af0672c6bf667916d04434c0959d199c30af3bb8", "node_info": null}, "3": {"text": "PromptType.QUESTION_ANSWER:\n        response = _mock_answer(full_prompt_args)\n    elif prompt.prompt_type == PromptType.KEYWORD_EXTRACT:\n        response = _mock_keyword_extract(full_prompt_args)\n    elif prompt.prompt_type == PromptType.QUERY_KEYWORD_EXTRACT:\n        response = _mock_query_keyword_extract(full_prompt_args)\n    elif prompt.prompt_type == PromptType.SCHEMA_EXTRACT:\n        response = _mock_schema_extract(full_prompt_args)\n    elif prompt.prompt_type == PromptType.TEXT_TO_SQL:\n        response = _mock_text_to_sql(full_prompt_args)\n    else:\n        raise ValueError(\"Invalid prompt to use with mocks.\")\n\n    return response, formatted_prompt\n\n\ndef mock_llmchain_predict(**full_prompt_args: Any) -> str:\n    \"\"\"Mock LLMChain predict with a generic response.\"\"\"\n    return \"generic response from", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_predict.py", "file_name": "mock_predict.py"}, "index": 3, "child_indices": [], "ref_doc_id": "af0672c6bf667916d04434c0959d199c30af3bb8", "node_info": null}, "4": {"text": "LLMChain predict with a generic response.\"\"\"\n    return \"generic response from LLMChain.predict()\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_predict.py", "file_name": "mock_predict.py"}, "index": 4, "child_indices": [], "ref_doc_id": "af0672c6bf667916d04434c0959d199c30af3bb8", "node_info": null}, "5": {"text": "This code file contains mock functions for predicting responses to various prompts in the GPT-Index system. The functions simulate the behavior of the GPT-Index system, and are used for testing purposes. The functions cover a range of tasks, such as summarizing text, inserting nodes into a GPT tree, selecting nodes from a GPT tree, refining existing answers, extracting keywords from text, extracting keywords from questions, extracting schemas from text, and converting text to SQL queries. Each function takes a prompt argument and returns a response. Additionally, there is a mock LLMChain predict function which returns a generic response.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 5, "child_indices": [0, 1, 2, 3, 4], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"5": {"text": "This code file contains mock functions for predicting responses to various prompts in the GPT-Index system. The functions simulate the behavior of the GPT-Index system, and are used for testing purposes. The functions cover a range of tasks, such as summarizing text, inserting nodes into a GPT tree, selecting nodes from a GPT tree, refining existing answers, extracting keywords from text, extracting keywords from questions, extracting schemas from text, and converting text to SQL queries. Each function takes a prompt argument and returns a response. Additionally, there is a mock LLMChain predict function which returns a generic response.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 5, "child_indices": [0, 1, 2, 3, 4], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"af0672c6bf667916d04434c0959d199c30af3bb8": {"text": "\"\"\"Mock predict.\"\"\"\n\nfrom typing import Any, Dict, Tuple\n\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.prompts.prompt_type import PromptType\nfrom gpt_index.token_counter.utils import mock_extract_keywords_response\n\n\ndef _mock_summary_predict(prompt_args: Dict) -> str:\n    \"\"\"Mock summary predict.\"\"\"\n    return prompt_args[\"context_str\"]\n\n\ndef _mock_insert_predict() -> str:\n    \"\"\"Mock insert predict.\n\n    Used in GPT tree index during insertion\n    to select the next node.\n\n    \"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_query_select() -> str:\n    \"\"\"Mock query predict.\n\n    Used in GPT tree index during query traversal\n    to select the next node.\n\n    \"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_answer(prompt_args: Dict) -> str:\n    \"\"\"Mock answer.\"\"\"\n    return prompt_args[\"query_str\"] + \":\" + prompt_args[\"context_str\"]\n\n\ndef _mock_refine(prompt_args: Dict) -> str:\n    \"\"\"Mock refine.\"\"\"\n    return prompt_args[\"existing_answer\"]\n\n\ndef _mock_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"text\"])\n\n\ndef _mock_query_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock query keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"question\"])\n\n\ndef _mock_schema_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock schema extract.\"\"\"\n    return prompt_args[\"text\"]\n\n\ndef _mock_text_to_sql(prompt_args: Dict) -> str:\n    \"\"\"Mock text to sql.\"\"\"\n    # assume it's a select query\n    tokens = prompt_args[\"query_str\"].split(\":\")\n    table_name = tokens[0]\n    subtokens = tokens[1].split(\",\")\n    return \"SELECT \" + \", \".join(subtokens) + f\" FROM {table_name}\"\n\n\ndef mock_llmpredictor_predict(prompt: Prompt, **prompt_args: Any) -> Tuple[str, str]:\n    \"\"\"Mock predict method of LLMPredictor.\n\n    Depending on the prompt, return response.\n\n    \"\"\"\n    formatted_prompt = prompt.format(**prompt_args)\n    full_prompt_args = prompt.get_full_format_args(prompt_args)\n    if prompt.prompt_type == PromptType.SUMMARY:\n        response = _mock_summary_predict(full_prompt_args)\n    elif prompt.prompt_type == PromptType.TREE_INSERT:\n        response = _mock_insert_predict()\n    elif prompt.prompt_type == PromptType.TREE_SELECT:\n        response = _mock_query_select()\n    elif prompt.prompt_type == PromptType.REFINE:\n        response = _mock_refine(full_prompt_args)\n    elif prompt.prompt_type == PromptType.QUESTION_ANSWER:\n        response = _mock_answer(full_prompt_args)\n    elif prompt.prompt_type == PromptType.KEYWORD_EXTRACT:\n        response = _mock_keyword_extract(full_prompt_args)\n    elif prompt.prompt_type == PromptType.QUERY_KEYWORD_EXTRACT:\n        response = _mock_query_keyword_extract(full_prompt_args)\n    elif prompt.prompt_type == PromptType.SCHEMA_EXTRACT:\n        response = _mock_schema_extract(full_prompt_args)\n    elif prompt.prompt_type == PromptType.TEXT_TO_SQL:\n        response = _mock_text_to_sql(full_prompt_args)\n    else:\n        raise ValueError(\"Invalid prompt to use with mocks.\")\n\n    return response, formatted_prompt\n\n\ndef mock_llmchain_predict(**full_prompt_args: Any) -> str:\n    \"\"\"Mock LLMChain predict with a generic response.\"\"\"\n    return \"generic response from LLMChain.predict()\"\n", "doc_id": "af0672c6bf667916d04434c0959d199c30af3bb8", "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_predict.py", "file_name": "mock_predict.py"}, "__type__": "Document"}, "ea1cd8b6-297a-43c1-a4af-549c21d90158": {"text": "\nThis code file contains mock functions for simulating the behavior of the GPT-Index system. The functions cover a range of tasks, such as summarizing text, inserting nodes into a GPT tree, selecting nodes from a GPT tree, refining existing answers, extracting keywords from text, extracting keywords from questions, extracting schemas from text, and converting text to SQL queries. Each function takes a prompt argument and returns a response, and there is also a mock LLMChain predict function which returns a generic response. The code provides a testing environment for the GPT-Index system, allowing developers to simulate the system's behavior and test its functionality. The code utilizes various algorithms, data structures, and relationships between functions, classes, and variables to achieve its purpose.", "doc_id": "ea1cd8b6-297a-43c1-a4af-549c21d90158", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Mock predict.\"\"\"\n\nfrom typing import Any, Dict, Tuple\n\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.prompts.prompt_type import PromptType\nfrom gpt_index.token_counter.utils import mock_extract_keywords_response\n\n\ndef _mock_summary_predict(prompt_args: Dict) -> str:\n    \"\"\"Mock summary predict.\"\"\"\n    return prompt_args[\"context_str\"]\n\n\ndef _mock_insert_predict() -> str:\n    \"\"\"Mock insert predict.\n\n    Used in GPT tree index during insertion\n    to select the next node.\n\n    \"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_query_select() -> str:\n    \"\"\"Mock query predict.\n\n    Used in GPT tree index during query traversal\n    to select the next node.\n\n    \"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_answer(prompt_args: Dict) -> str:\n    \"\"\"Mock answer.\"\"\"\n    return prompt_args[\"query_str\"] + \":\" + prompt_args[\"context_str\"]\n\n\ndef _mock_refine(prompt_args: Dict) -> str:\n    \"\"\"Mock", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_predict.py", "file_name": "mock_predict.py"}, "index": 0, "child_indices": [], "ref_doc_id": "af0672c6bf667916d04434c0959d199c30af3bb8", "node_info": null}, "1": {"text": "Dict) -> str:\n    \"\"\"Mock refine.\"\"\"\n    return prompt_args[\"existing_answer\"]\n\n\ndef _mock_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"text\"])\n\n\ndef _mock_query_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock query keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"question\"])\n\n\ndef _mock_schema_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock schema extract.\"\"\"\n    return prompt_args[\"text\"]\n\n\ndef _mock_text_to_sql(prompt_args: Dict) -> str:\n    \"\"\"Mock text to sql.\"\"\"\n    # assume it's a select query\n    tokens = prompt_args[\"query_str\"].split(\":\")\n    table_name = tokens[0]\n    subtokens = tokens[1].split(\",\")\n    return \"SELECT \" + \", \".join(subtokens) + f\" FROM {table_name}\"\n\n\ndef", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_predict.py", "file_name": "mock_predict.py"}, "index": 1, "child_indices": [], "ref_doc_id": "af0672c6bf667916d04434c0959d199c30af3bb8", "node_info": null}, "2": {"text": "+ f\" FROM {table_name}\"\n\n\ndef mock_llmpredictor_predict(prompt: Prompt, **prompt_args: Any) -> Tuple[str, str]:\n    \"\"\"Mock predict method of LLMPredictor.\n\n    Depending on the prompt, return response.\n\n    \"\"\"\n    formatted_prompt = prompt.format(**prompt_args)\n    full_prompt_args = prompt.get_full_format_args(prompt_args)\n    if prompt.prompt_type == PromptType.SUMMARY:\n        response = _mock_summary_predict(full_prompt_args)\n    elif prompt.prompt_type == PromptType.TREE_INSERT:\n        response = _mock_insert_predict()\n    elif prompt.prompt_type == PromptType.TREE_SELECT:\n        response = _mock_query_select()\n    elif prompt.prompt_type == PromptType.REFINE:\n        response = _mock_refine(full_prompt_args)\n    elif prompt.prompt_type == PromptType.QUESTION_ANSWER:\n        response =", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_predict.py", "file_name": "mock_predict.py"}, "index": 2, "child_indices": [], "ref_doc_id": "af0672c6bf667916d04434c0959d199c30af3bb8", "node_info": null}, "3": {"text": "PromptType.QUESTION_ANSWER:\n        response = _mock_answer(full_prompt_args)\n    elif prompt.prompt_type == PromptType.KEYWORD_EXTRACT:\n        response = _mock_keyword_extract(full_prompt_args)\n    elif prompt.prompt_type == PromptType.QUERY_KEYWORD_EXTRACT:\n        response = _mock_query_keyword_extract(full_prompt_args)\n    elif prompt.prompt_type == PromptType.SCHEMA_EXTRACT:\n        response = _mock_schema_extract(full_prompt_args)\n    elif prompt.prompt_type == PromptType.TEXT_TO_SQL:\n        response = _mock_text_to_sql(full_prompt_args)\n    else:\n        raise ValueError(\"Invalid prompt to use with mocks.\")\n\n    return response, formatted_prompt\n\n\ndef mock_llmchain_predict(**full_prompt_args: Any) -> str:\n    \"\"\"Mock LLMChain predict with a generic response.\"\"\"\n    return \"generic response from", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_predict.py", "file_name": "mock_predict.py"}, "index": 3, "child_indices": [], "ref_doc_id": "af0672c6bf667916d04434c0959d199c30af3bb8", "node_info": null}, "4": {"text": "LLMChain predict with a generic response.\"\"\"\n    return \"generic response from LLMChain.predict()\"\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_predict.py", "file_name": "mock_predict.py"}, "index": 4, "child_indices": [], "ref_doc_id": "af0672c6bf667916d04434c0959d199c30af3bb8", "node_info": null}, "5": {"text": "This code file contains mock functions for predicting responses to various prompts in the GPT-Index system. The functions simulate the behavior of the GPT-Index system, and are used for testing purposes. The functions cover a range of tasks, such as summarizing text, inserting nodes into a GPT tree, selecting nodes from a GPT tree, refining existing answers, extracting keywords from text, extracting keywords from questions, extracting schemas from text, and converting text to SQL queries. Each function takes a prompt argument and returns a response. Additionally, there is a mock LLMChain predict function which returns a generic response.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 5, "child_indices": [0, 1, 2, 3, 4], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"5": {"text": "This code file contains mock functions for predicting responses to various prompts in the GPT-Index system. The functions simulate the behavior of the GPT-Index system, and are used for testing purposes. The functions cover a range of tasks, such as summarizing text, inserting nodes into a GPT tree, selecting nodes from a GPT tree, refining existing answers, extracting keywords from text, extracting keywords from questions, extracting schemas from text, and converting text to SQL queries. Each function takes a prompt argument and returns a response. Additionally, there is a mock LLMChain predict function which returns a generic response.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 5, "child_indices": [0, 1, 2, 3, 4], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}