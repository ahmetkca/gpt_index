{"index_struct": {"text": "\nThis code file contains two functions that are used to tokenize and extract keywords from a given text. The mock_tokenizer function splits the text into individual words and removes any empty strings. The mock_extract_keywords function is similar to the simple_extract_keywords function, but it does not filter out stopwords. Both functions use the List and Set data structures and the keyword_table.utils module. The purpose of the code is to provide a way to tokenize and extract keywords from a given text without filtering out stopwords. The mock_tokenizer function takes a text as an input and splits it into individual words, while the mock_extract_keywords function takes the tokenized words and extracts the keywords from them. The code utilizes the List and Set data structures to store the tokenized words and the extracted keywords, respectively. The keyword_table.utils module is used to help with the extraction of keywords. The code provides a way to tokenize and extract keywords from a given text without filtering out stopwords.", "doc_id": "388550e9-f19e-4a35-90de-16a537bc6cfd", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Mock utils.\"\"\"\n\nfrom typing import List, Optional, Set\n\nfrom gpt_index.indices.keyword_table.utils import simple_extract_keywords\n\n\ndef mock_tokenizer(text: str) -> List[str]:\n    \"\"\"Mock tokenizer.\"\"\"\n    tokens = text.split(\" \")\n    result = []\n    for token in tokens:\n        if token.strip() == \"\":\n            continue\n        result.append(token.strip())\n    return result\n\n\ndef mock_extract_keywords(\n    text_chunk: str, max_keywords: Optional[int] = None, filter_stopwords: bool = True\n) -> Set[str]:\n    \"\"\"Extract keywords (mock).\n\n    Same as simple_extract_keywords but without filtering stopwords.\n\n    \"\"\"\n    return simple_extract_keywords(\n        text_chunk, max_keywords=max_keywords, filter_stopwords=False\n    )\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_utils.py", "file_name": "mock_utils.py"}, "index": 0, "child_indices": [], "ref_doc_id": "894c95fe14837faf82dcd09828b0859c16101e81", "node_info": null}, "1": {"text": "This code file contains two functions, mock_tokenizer and mock_extract_keywords, which are used to tokenize and extract keywords from a given text. The mock_tokenizer function splits the text into individual words and removes any empty strings. The mock_extract_keywords function is similar to the simple_extract_keywords function, but it does not filter out stopwords. Both functions use the List and Set data structures and the keyword_table.utils module.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 1, "child_indices": [0], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"1": {"text": "This code file contains two functions, mock_tokenizer and mock_extract_keywords, which are used to tokenize and extract keywords from a given text. The mock_tokenizer function splits the text into individual words and removes any empty strings. The mock_extract_keywords function is similar to the simple_extract_keywords function, but it does not filter out stopwords. Both functions use the List and Set data structures and the keyword_table.utils module.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 1, "child_indices": [0], "ref_doc_id": null, "node_info": null}}}, "docstore": {"docs": {"894c95fe14837faf82dcd09828b0859c16101e81": {"text": "\"\"\"Mock utils.\"\"\"\n\nfrom typing import List, Optional, Set\n\nfrom gpt_index.indices.keyword_table.utils import simple_extract_keywords\n\n\ndef mock_tokenizer(text: str) -> List[str]:\n    \"\"\"Mock tokenizer.\"\"\"\n    tokens = text.split(\" \")\n    result = []\n    for token in tokens:\n        if token.strip() == \"\":\n            continue\n        result.append(token.strip())\n    return result\n\n\ndef mock_extract_keywords(\n    text_chunk: str, max_keywords: Optional[int] = None, filter_stopwords: bool = True\n) -> Set[str]:\n    \"\"\"Extract keywords (mock).\n\n    Same as simple_extract_keywords but without filtering stopwords.\n\n    \"\"\"\n    return simple_extract_keywords(\n        text_chunk, max_keywords=max_keywords, filter_stopwords=False\n    )\n", "doc_id": "894c95fe14837faf82dcd09828b0859c16101e81", "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_utils.py", "file_name": "mock_utils.py"}, "__type__": "Document"}, "388550e9-f19e-4a35-90de-16a537bc6cfd": {"text": "\nThis code file contains two functions that are used to tokenize and extract keywords from a given text. The mock_tokenizer function splits the text into individual words and removes any empty strings. The mock_extract_keywords function is similar to the simple_extract_keywords function, but it does not filter out stopwords. Both functions use the List and Set data structures and the keyword_table.utils module. The purpose of the code is to provide a way to tokenize and extract keywords from a given text without filtering out stopwords. The mock_tokenizer function takes a text as an input and splits it into individual words, while the mock_extract_keywords function takes the tokenized words and extracts the keywords from them. The code utilizes the List and Set data structures to store the tokenized words and the extracted keywords, respectively. The keyword_table.utils module is used to help with the extraction of keywords. The code provides a way to tokenize and extract keywords from a given text without filtering out stopwords.", "doc_id": "388550e9-f19e-4a35-90de-16a537bc6cfd", "embedding": null, "extra_info": null, "all_nodes": {"0": {"text": "\"\"\"Mock utils.\"\"\"\n\nfrom typing import List, Optional, Set\n\nfrom gpt_index.indices.keyword_table.utils import simple_extract_keywords\n\n\ndef mock_tokenizer(text: str) -> List[str]:\n    \"\"\"Mock tokenizer.\"\"\"\n    tokens = text.split(\" \")\n    result = []\n    for token in tokens:\n        if token.strip() == \"\":\n            continue\n        result.append(token.strip())\n    return result\n\n\ndef mock_extract_keywords(\n    text_chunk: str, max_keywords: Optional[int] = None, filter_stopwords: bool = True\n) -> Set[str]:\n    \"\"\"Extract keywords (mock).\n\n    Same as simple_extract_keywords but without filtering stopwords.\n\n    \"\"\"\n    return simple_extract_keywords(\n        text_chunk, max_keywords=max_keywords, filter_stopwords=False\n    )\n", "doc_id": null, "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_utils.py", "file_name": "mock_utils.py"}, "index": 0, "child_indices": [], "ref_doc_id": "894c95fe14837faf82dcd09828b0859c16101e81", "node_info": null}, "1": {"text": "This code file contains two functions, mock_tokenizer and mock_extract_keywords, which are used to tokenize and extract keywords from a given text. The mock_tokenizer function splits the text into individual words and removes any empty strings. The mock_extract_keywords function is similar to the simple_extract_keywords function, but it does not filter out stopwords. Both functions use the List and Set data structures and the keyword_table.utils module.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 1, "child_indices": [0], "ref_doc_id": null, "node_info": null}}, "root_nodes": {"1": {"text": "This code file contains two functions, mock_tokenizer and mock_extract_keywords, which are used to tokenize and extract keywords from a given text. The mock_tokenizer function splits the text into individual words and removes any empty strings. The mock_extract_keywords function is similar to the simple_extract_keywords function, but it does not filter out stopwords. Both functions use the List and Set data structures and the keyword_table.utils module.\n\"\"\"", "doc_id": null, "embedding": null, "extra_info": null, "index": 1, "child_indices": [0], "ref_doc_id": null, "node_info": null}}, "__type__": "tree"}}}}