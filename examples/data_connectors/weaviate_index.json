{"index_struct": {"text": null, "doc_id": "cce2181c-28ee-42a7-b5a2-b9bb94115ee5", "embedding": null, "extra_info": null, "class_prefix": "Gpt_Index_3460389699009219481"}, "docstore": {"docs": {"0fb583996cdc52a8429eff37f8fb6002a96f2e11": {"text": "[flake8]\nexclude =\n    .venv\n    __pycache__\n    notebooks\n    .ipynb_checkpoints\n# Recommend matching the black line length (default 88),\n# rather than using the flake8 default of 79:\nmax-line-length = 88\nextend-ignore =\n    # See https://github.com/PyCQA/pycodestyle/issues/373\n    E203,\n", "doc_id": "0fb583996cdc52a8429eff37f8fb6002a96f2e11", "embedding": null, "extra_info": {"file_path": ".flake8", "file_name": ".flake8"}, "__type__": "Document"}, "62c3c1b509dc1615c7ce16a8de81a41731e19b5b": {"text": "name: Build Package\n\n# Build package on its own without additional pip install\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n\njobs:\n  build:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      # You can use PyPy versions in python-version.\n      # For example, pypy-2.7 and pypy-3.8\n      matrix:\n        os: [ubuntu-latest, windows-latest]\n        python-version: [\"3.9\"]\n    steps:\n      - uses: actions/checkout@v3\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Install deps\n        run: |\n          python -m pip install --upgrade pip\n          python -m pip install wheel setuptools\n          python setup.py sdist bdist_wheel\n      - name: Use package\n        run: |\n          cd dist\n          python -m pip install --find-links=. gpt-index\n      - name: Run import\n        run: |\n          python -c \"import gpt_index\"\n        ", "doc_id": "62c3c1b509dc1615c7ce16a8de81a41731e19b5b", "embedding": null, "extra_info": {"file_path": ".github/workflows/build_package.yml", "file_name": "build_package.yml"}, "__type__": "Document"}, "2178f6b0129edb785dc4dc5b9571b07d020fdf77": {"text": "name: Linting\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    strategy:\n      # You can use PyPy versions in python-version.\n      # For example, pypy-2.7 and pypy-3.8\n      matrix:\n        python-version: [\"3.9\"]\n    steps:\n      - uses: actions/checkout@v3\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Install deps\n        run: |\n          python -m pip install --upgrade pip\n          pip install -r requirements.txt\n          pip install -r data_requirements.txt\n      - name: Run linter\n        run: make lint\n        ", "doc_id": "2178f6b0129edb785dc4dc5b9571b07d020fdf77", "embedding": null, "extra_info": {"file_path": ".github/workflows/lint.yml", "file_name": "lint.yml"}, "__type__": "Document"}, "4c86ee3bb4070091ed4568e8e31a032cb91bf183": {"text": "name: Unit Testing\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    strategy:\n      # You can use PyPy versions in python-version.\n      # For example, pypy-2.7 and pypy-3.8\n      matrix:\n        python-version: [\"3.9\", \"3.8\"]\n    steps:\n      - uses: actions/checkout@v3\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Install deps\n        run: |\n          python -m pip install --upgrade pip\n          pip install -r requirements.txt\n          pip install -r data_requirements.txt\n      - name: Run testing\n        run: pytest tests\n        ", "doc_id": "4c86ee3bb4070091ed4568e8e31a032cb91bf183", "embedding": null, "extra_info": {"file_path": ".github/workflows/unit_test.yml", "file_name": "unit_test.yml"}, "__type__": "Document"}, "0b89b543c23ca6b6ab74f3b6ea99653509ece673": {"text": ".DS_Store\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbin/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\netc/\ninclude/\nlib/\nlib64/\nparts/\nsdist/\nshare/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\nnotebooks/\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\npyvenv.cfg\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# Jetbrains\n.idea\n", "doc_id": "0b89b543c23ca6b6ab74f3b6ea99653509ece673", "embedding": null, "extra_info": {"file_path": ".gitignore", "file_name": ".gitignore"}, "__type__": "Document"}, "3170150b55fd587d7c76f0eb39382e751f0ccd48": {"text": "version: 2\nsphinx:\n  configuration: docs/conf.py\nbuild:\n  image: testing\nformats: all\npython:\n  version: 3.9\n  install:\n    - requirements: docs/requirements.txt\n    - method: pip\n      path: .", "doc_id": "3170150b55fd587d7c76f0eb39382e751f0ccd48", "embedding": null, "extra_info": {"file_path": ".readthedocs.yaml", "file_name": ".readthedocs.yaml"}, "__type__": "Document"}, "986d0c78b5b64bfac17a890c173753babbab9a64": {"text": "cff-version: 1.2.0\nmessage: \"If you use this software, please cite it as below.\"\nauthors:\n- family-names: \"Liu\"\n  given-names: \"Jerry\"\n  orcid: \"https://orcid.org/0000-0002-6694-3517\"\ntitle: \"GPT Index\"\ndoi: 10.5281/zenodo.1234\ndate-released: 2022-11-1\nurl: \"https://github.com/jerryjliu/gpt_index\"", "doc_id": "986d0c78b5b64bfac17a890c173753babbab9a64", "embedding": null, "extra_info": {"file_path": "CITATION.cff", "file_name": "CITATION.cff"}, "__type__": "Document"}, "024b24be1818f09f3b9cd631d9a2a851df2b99ae": {"text": "\n\n\ud83d\udca1 Contributing to GPT Index\n\nInterested in contributing to GPT Index? Here's how to get started! \n\n\n\n\n\nContributions that we're looking for:\n- Bug fixes\n- New features\n\nAll future tasks are tracked in Github Issues Page.\nPlease feel free to open an issue and/or assign an issue to yourself.\n\nAlso, join our Discord for discussions: https://discord.gg/dGcwcsnxhU.\n\n\n\n\n\nEnvironment Setup\n\nGPT Index is a Python package. We've tested primarily with Python versions >= 3.8. Here's a quick\nand dirty guide to getting your environment setup.\n\nFirst, create a fork of GPT Index, by clicking the \"Fork\" button on the GPT Index Github page.\nFollowing these steps for more details\non how to fork the repo and clone the forked repo.\n\nThen, create a new Python virtual environment. The command below creates an environment in `.venv`,\nand activates it:\n```bash\npython -m venv .venv\nsource .venv/bin/activate\n```\n\nInstall the required dependencies (this will also install gpt-index through `pip install -e .` \nso that you can start developing on it):\n\n```bash\npip install -r requirements.txt\n```\n\nNow you should be set! \n\n\n\n\n\n\nValidating your Change\n\nLet's make sure to `format/lint` our change. For bigger changes,\nlet's also make sure to `test` it and perhaps create an `example notebook`.\n\n\n\n\n\nFormatting/Linting\n\nYou can format and lint your changes with the following commands in the root directory:\n\n```bash\nmake format; make lint\n```\n\nWe run an assortment of linters: `black`, `isort`, `mypy`, `flake8`.\n\n\n\n\n\nTesting\n\nFor bigger changes, you'll want to create a unit test. Our tests are in the `tests` folder.\nWe use `pytest` for unit testing. To run all unit tests, run the following in the root dir:\n\n```bash\npytest tests\n```\n\n\n\n\n\nCreating an Example Notebook\n\nFor changes that involve entirely new features, it may be worth adding an example Jupyter notebook to showcase\nthis feature. \n\nExample notebooks can be found in this folder: https://github.com/jerryjliu/gpt_index/tree/main/examples.\n\n\n\n\n\n\nCreating a pull request\n\nSee these instructions\nto open a pull request against the main GPT Index repo.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "doc_id": "024b24be1818f09f3b9cd631d9a2a851df2b99ae", "embedding": null, "extra_info": {"file_path": "CONTRIBUTING.md", "file_name": "CONTRIBUTING.md"}, "__type__": "Document"}, "d1290334952fd430377e3f8cd28d1174593a507d": {"text": "The MIT License\n\nCopyright (c) Jerry Liu\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.", "doc_id": "d1290334952fd430377e3f8cd28d1174593a507d", "embedding": null, "extra_info": {"file_path": "LICENSE", "file_name": "LICENSE"}, "__type__": "Document"}, "567a63cc13b0be43db26fb4eaca6e6c9fbe90a24": {"text": "include gpt_index/py.typed\ninclude gpt_index/VERSION\ninclude LICENSE", "doc_id": "567a63cc13b0be43db26fb4eaca6e6c9fbe90a24", "embedding": null, "extra_info": {"file_path": "MANIFEST.in", "file_name": "MANIFEST.in"}, "__type__": "Document"}, "11f224f37191e11c667b547a80d4dafa15eae1a1": {"text": ".PHONY: format lint\n\nformat:\n\tblack .\n\tisort .\n\nlint:\n\tmypy .\n\tblack . --check\n\tisort . --check\n\tflake8 .\n\ntest:\n\tpytest tests", "doc_id": "11f224f37191e11c667b547a80d4dafa15eae1a1", "embedding": null, "extra_info": {"file_path": "Makefile", "file_name": "Makefile"}, "__type__": "Document"}, "19e81e1976e5791f2485776a341f44f792881557": {"text": "\n\n\ud83d\uddc2\ufe0f \ufe0fGPT Index\n\nGPT Index is a project consisting of a set of data structures designed to make it easier to \nuse large external knowledge bases with LLMs.\n\nPyPi: https://pypi.org/project/gpt-index/.\n\nDocumentation: https://gpt-index.readthedocs.io/en/latest/.\n\nTwitter: https://twitter.com/gpt_index.\n\nDiscord: https://discord.gg/dGcwcsnxhU.\n\n\n\n\n\n\ud83d\ude80 Overview\n\n**NOTE**: This README is not updated as frequently as the documentation. Please check out the documentation above for the latest updates!\n\n\n\n\n\nContext\n- LLMs are a phenomenonal piece of technology for knowledge generation and reasoning.\n- A big limitation of LLMs is context size (e.g. Davinci's limit is 4096 tokens. Large, but not infinite).\n- The ability to feed \"knowledge\" to LLMs is restricted to this limited prompt size and model weights.\n\n\n\n\n\nProposed Solution\n\nAt its core, GPT Index contains a toolkit of **index data structures** designed to easily connect LLM's with your external data.\nGPT Index helps to provide the following advantages:\n- Remove concerns over prompt size limitations.\n- Abstract common usage patterns to reduce boilerplate code in your LLM app.\n- Provide data connectors to your common data sources (Google Docs, Slack, etc.).\n- Provide cost transparency + tools that reduce cost while increasing performance.\n\n\nEach data structure offers distinct use cases and a variety of customizable parameters. These indices can then be \n*queried* in a general purpose manner, in order to achieve any task that you would typically achieve with an LLM:\n- Question-Answering\n- Summarization\n- Text Generation (Stories, TODO's, emails, etc.)\n- and more!\n\n\n\n\n\n\n\ud83d\udca1 Contributing\n\nInteresting in contributing? See our Contribution Guide for more details.\n\n\n\n\n\n\ud83d\udcc4 Documentation\n\nFull documentation can be found here: https://gpt-index.readthedocs.io/en/latest/. \n\nPlease check it out for the most up-to-date tutorials, how-to guides, references, and other resources! \n\n\n\n\n\n\n\ud83d\udcbb Example Usage\n\n```\npip install gpt-index\n```\n\nExamples are in the `examples` folder. Indices are in the `indices` folder (see list of indices below).\n\nTo build a simple vector store index:\n```python\nimport os\nos.environ[\"OPENAI_API_KEY\"] = 'YOUR_OPENAI_API_KEY'\n\nfrom gpt_index import GPTSimpleVectorIndex, SimpleDirectoryReader\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = GPTSimpleVectorIndex(documents)\n```\n\nTo save to and load from disk:\n```python\n\n\n\n\nsave to disk\nindex.save_to_disk('index.json')\n\n\n\n\nload from disk\nindex = GPTSimpleVectorIndex.load_from_disk('index.json')\n```\n\nTo query:\n```python\nindex.query(\"?\")\n```\n\n\n\n\n\n\ud83d\udd27 Dependencies\n\nThe main third-party package requirements are `tiktoken`, `openai`, and `langchain`.\n\nAll requirements should be contained within the `setup.py` file. To run the package locally without building the wheel, simply run `pip install -r requirements.txt`. \n\n\n\n\n\n\n\ud83d\udcd6 Citation\n\nReference to cite if you use GPT Index in a paper:\n\n```\n@software{Liu_GPT_Index_2022,\nauthor = {Liu, Jerry},\ndoi = {10.5281/zenodo.1234},\nmonth = {11},\ntitle = {{GPT Index}},\nurl = {https://github.com/jerryjliu/gpt_index},year = {2022}\n}\n```\n\n", "doc_id": "19e81e1976e5791f2485776a341f44f792881557", "embedding": null, "extra_info": {"file_path": "README.md", "file_name": "README.md"}, "__type__": "Document"}, "5b563dc31e23a89e7d920326e5a40f66ae33e727": {"text": "# requirements for external data\n\nwikipedia\npymongo\nslack_sdk\ndiscord.py\n\n# google\ngoogle-api-python-client\ngoogle-auth-httplib2\ngoogle-auth-oauthlib", "doc_id": "5b563dc31e23a89e7d920326e5a40f66ae33e727", "embedding": null, "extra_info": {"file_path": "data_requirements.txt", "file_name": "data_requirements.txt"}, "__type__": "Document"}, "5f666278b7d7a39df9abb1e5d2037e5aee954d4e": {"text": "\n\n\ud83e\uddea  Experimental\n\nThis section is for experiments, cool ideas, and more! \n\nCode here lives outside the base package. If a project is sufficiently interesting and validated, then we will move it into the core abstractions.\n", "doc_id": "5f666278b7d7a39df9abb1e5d2037e5aee954d4e", "embedding": null, "extra_info": {"file_path": "experimental/README.md", "file_name": "experimental/README.md"}, "__type__": "Document"}, "09e8676ed9c2cd06b8836e56e3473f5b54e05398": {"text": "{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 1,\n   \"id\": \"f445c1d1-acb9-431e-a7ff-50c41f064359\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stderr\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\\n\",\n      \"[nltk_data] Downloading package stopwords to\\n\",\n      \"[nltk_data]     /Users/jerryliu/nltk_data...\\n\",\n      \"[nltk_data]   Package stopwords is already up-to-date!\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"from utils import (\\n\",\n    \"    get_train_str,\\n\",\n    \"    get_train_and_eval_data,\\n\",\n    \"    get_eval_preds,\\n\",\n    \"    train_prompt\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"import warnings\\n\",\n    \"warnings.filterwarnings('ignore')\\n\",\n    \"warnings.simplefilter('ignore')\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 2,\n   \"id\": \"cf3cbd90-d5e1-4c30-a3bc-8b39fbd85d70\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# load up the titanic data\\n\",\n    \"train_df, train_labels, eval_df, eval_labels = get_train_and_eval_data('data/train.csv')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"fa2634f9-cb33-4f1e-81f9-3a3b285e2580\",\n   \"metadata\": {\n    \"tags\": []\n   },\n   \"source\": [\n    \"## Few-shot Prompting with GPT-3 for Titanic Dataset\\n\",\n    \"In this section, we can show how we can prompt GPT-3 on its own (without using GPT Index) to attain ~80% accuracy on Titanic! \\n\",\n    \"\\n\",\n    \"We can do this by simply providing a few example inputs. Or we can simply provide no example inputs at all (zero-shot). Both achieve the same results.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 3,\n   \"id\": \"d0698fd2-1361-49ae-8c17-8124e9b932a4\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"The following structured data is provided in \\\"Feature Name\\\":\\\"Feature Value\\\" format.\\n\",\n      \"Each datapoint describes a passenger on the Titanic.\\n\",\n      \"The task is to decide whether the passenger survived.\\n\",\n      \"Some example datapoints are given below: \\n\",\n      \"-------------------\\n\",\n      \"{train_str}\\n\",\n      \"-------------------\\n\",\n      \"Given this, predict whether the following passenger survived. Return answer as a number between 0 or 1. \\n\",\n      \"{eval_str}\\n\",\n      \"Survived: \\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# first demonstrate the prompt template\\n\",\n    \"print(train_prompt.template)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 4,\n   \"id\": \"4b39e2e7-be07-42f8-a27a-3419e84cfb2c\",\n   \"metadata\": {\n    \"scrolled\": true,\n    \"tags\": []\n   },\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"Example datapoints in `train_str`: \\n\",\n      \"This is the Data:\\n\",\n      \"Age:28.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:7.8958\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:male\\n\",\n      \"SibSp:0\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 0\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:17.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:7.925\\n\",\n      \"Parch:2\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:female\\n\",\n      \"SibSp:4\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 1\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:30.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:16.1\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:male\\n\",\n      \"SibSp:1\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 0\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:22.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:7.25\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:male\\n\",\n      \"SibSp:0\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 0\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:45.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:13.5\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:2\\n\",\n      \"Sex:female\\n\",\n      \"SibSp:0\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 1\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:25.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:0.0\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:male\\n\",\n      \"SibSp:0\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 1\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:18.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:20.2125\\n\",\n      \"Parch:1\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:male\\n\",\n      \"SibSp:1\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 0\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:33.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:9.5\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:male\\n\",\n      \"SibSp:0\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 0\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:24.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:65.0\\n\",\n      \"Parch:2\\n\",\n      \"Pclass:2\\n\",\n      \"Sex:female\\n\",\n      \"SibSp:1\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 1\\n\",\n      \"\\n\",\n      \"This is the Data:\\n\",\n      \"Age:26.0\\n\",\n      \"Embarked:S\\n\",\n      \"Fare:7.925\\n\",\n      \"Parch:0\\n\",\n      \"Pclass:3\\n\",\n      \"Sex:female\\n\",\n      \"SibSp:0\\n\",\n      \"This is the correct answer:\\n\",\n      \"Survived: 1\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# Get \\\"training\\\" prompt string \\n\",\n    \"train_n = 10\\n\",\n    \"eval_n = 40\\n\",\n    \"train_str = get_train_str(train_df, train_labels, train_n=train_n)\\n\",\n    \"print(f\\\"Example datapoints in `train_str`: \\\\n{train_str}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"819a06f7-3171-4edb-b90c-0a3eae308a04\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Do evaluation with the training prompt string\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"4a7f2202-518c-41a3-80ab-1e98bbcca903\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from sklearn.metrics import accuracy_score\\n\",\n    \"import numpy as np\\n\",\n    \"\\n\",\n    \"eval_preds = get_eval_preds(train_prompt, train_str, eval_df, n=eval_n)\\n\",\n    \"eval_label_chunk = eval_labels[:eval_n]\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 7,\n   \"id\": \"64323a4d-6eea-4e40-9eac-b2deed60192b\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"ACCURACY: 0.8\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"acc = accuracy_score(eval_label_chunk, np.array(eval_preds).round())\\n\",\n    \"print(f'ACCURACY: {acc}')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"11790d28-8f34-42dd-b11f-6aad21fd5f46\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Do evaluation with no training prompt string! \"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"aaf993e5-c363-4f18-a28f-09761e49cb6d\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from sklearn.metrics import accuracy_score\\n\",\n    \"import numpy as np\\n\",\n    \"\\n\",\n    \"eval_preds_null = get_eval_preds(train_prompt, \\\"\\\", eval_df, n=eval_n)\\n\",\n    \"eval_label_chunk = eval_labels[:eval_n]\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 11,\n   \"id\": \"c3b8bcd5-5972-4ce5-9aa1-57460cdde199\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"ACCURACY: 0.8\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"acc_null = accuracy_score(eval_label_chunk, np.array(eval_preds_null).round())\\n\",\n    \"print(f'ACCURACY: {acc_null}')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"8f0a5e4b-e627-4b47-a807-939813596594\",\n   \"metadata\": {\n    \"tags\": []\n   },\n   \"source\": [\n    \"## Extending with GPT List Index\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"42a1ca28-96e9-4cd2-bd48-0673917ad057\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Build Index\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 12,\n   \"id\": \"6c59b030-855d-4e27-89c3-74c972d1bf19\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from gpt_index import GPTListIndex\\n\",\n    \"from gpt_index.readers.schema.base import Document\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 13,\n   \"id\": \"8f9556de-e323-4318-bb71-cff75bf8c3c1\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"index = GPTListIndex([])\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"e27720fc-af36-40fd-8c55-41485248aa9f\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# insertion into index \\n\",\n    \"batch_size = 40\\n\",\n    \"num_train_chunks = 5\\n\",\n    \"\\n\",\n    \"for i in range(num_train_chunks):\\n\",\n    \"    print(f\\\"Inserting chunk: {i}/{num_train_chunks}\\\")\\n\",\n    \"    start_idx = i*batch_size\\n\",\n    \"    end_idx = (i+1)*batch_size\\n\",\n    \"    train_batch = train_df.iloc[start_idx:end_idx+batch_size]\\n\",\n    \"    labels_batch = train_labels.iloc[start_idx:end_idx+batch_size]\\n\",\n    \"    all_train_str = get_train_str(train_batch, labels_batch, train_n=batch_size)\\n\",\n    \"    index.insert(Document(all_train_str))\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"e78db088-6649-44db-b52a-766316713b96\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Query Index\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 15,\n   \"id\": \"9cb90564-1de2-412f-8318-d5280855004e\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from utils import query_str, qa_data_prompt, refine_prompt\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 16,\n   \"id\": \"77c1ae36-e0af-47bc-a656-4971af699755\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"'Which is the relationship between these features and predicting survival?'\"\n      ]\n     },\n     \"execution_count\": 16,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"query_str\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 17,\n   \"id\": \"c403710f-d4b3-4287-94f5-e275ea19b476\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"> Starting query: Which is the relationship between these features and predicting survival?\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"response = index.query(\\n\",\n    \"    query_str, \\n\",\n    \"    text_qa_template=qa_data_prompt, \\n\",\n    \"    refine_template=refine_prompt, \\n\",\n    \")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 19,\n   \"id\": \"d2545ab1-980a-4fbd-8add-7ef957801644\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"\\n\",\n      \"\\n\",\n      \"There is no definitive answer to this question, as the relationship between the features and predicting survival will vary depending on the data. However, some possible relationships include: age (younger passengers are more likely to survive), sex (females are more likely to survive), fare (passengers who paid more for their ticket are more likely to survive), and pclass (passengers in first or second class are more likely to survive).\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"print(response)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"d0d7d260-2283-49f6-ac40-35c7071cc54d\",\n   \"metadata\": {},\n   \"source\": [\n    \"#### Get Predictions and Evaluate\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 26,\n   \"id\": \"e7b98057-957c-48ef-be85-59ff9813d201\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"The following structured data is provided in \\\"Feature Name\\\":\\\"Feature Value\\\" format.\\n\",\n      \"Each datapoint describes a passenger on the Titanic.\\n\",\n      \"The task is to decide whether the passenger survived.\\n\",\n      \"We discovered the following relationship between features and survival:\\n\",\n      \"-------------------\\n\",\n      \"{train_str}\\n\",\n      \"-------------------\\n\",\n      \"Given this, predict whether the following passenger survived. \\n\",\n      \"Return answer as a number between 0 or 1. \\n\",\n      \"{eval_str}\\n\",\n      \"Survived: \\n\",\n      \"\\n\",\n      \"\\n\",\n      \"`train_str`: \\n\",\n      \"\\n\",\n      \"There is no definitive answer to this question, as the relationship between the features and predicting survival will vary depending on the data. However, some possible relationships include: age (younger passengers are more likely to survive), sex (females are more likely to survive), fare (passengers who paid more for their ticket are more likely to survive), and pclass (passengers in first or second class are more likely to survive).\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# get eval preds\\n\",\n    \"from utils import train_prompt_with_context\\n\",\n    \"\\n\",\n    \"train_str = response\\n\",\n    \"print(train_prompt_with_context.template)\\n\",\n    \"print(f'\\\\n\\\\n`train_str`: {train_str}')\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"659c6a3f-1c5d-4314-87dc-908e76d50e4a\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# do evaluation\\n\",\n    \"from sklearn.metrics import accuracy_score\\n\",\n    \"import numpy as np\\n\",\n    \"eval_n = 40\\n\",\n    \"eval_preds = get_eval_preds(train_prompt_with_context, train_str, eval_df, n=eval_n)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 28,\n   \"id\": \"7424e7d3-2576-42bc-b626-cf8088265004\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"ACCURACY: 0.85\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"eval_label_chunk = eval_labels[:eval_n]\\n\",\n    \"acc = accuracy_score(eval_label_chunk, np.array(eval_preds).round())\\n\",\n    \"print(f'ACCURACY: {acc}')\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"e010b497-eeed-4142-a8ac-f5545e85fcc2\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": []\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"gpt_retrieve_venv\",\n   \"language\": \"python\",\n   \"name\": \"gpt_retrieve_venv\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.4\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 5\n}\n", "doc_id": "09e8676ed9c2cd06b8836e56e3473f5b54e05398", "embedding": null, "extra_info": {"file_path": "experimental/classifier/TitanicModel.ipynb", "file_name": "TitanicModel.ipynb"}, "__type__": "Document"}, "ee8ec9935ad93ab3a4e6b42017174b65c2d21de5": {"text": "\"\"\"Helper functions for Titanic GPT-3 experiments.\"\"\"\n\n# form prompt, run GPT\nimport re\nfrom typing import List, Optional, Tuple\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfrom gpt_index.indices.utils import extract_numbers_given_response\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.base import Prompt\n\n\ndef get_train_and_eval_data(\n    csv_path: str,\n) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n    \"\"\"Get train and eval data.\"\"\"\n    df = pd.read_csv(csv_path)\n    label_col = \"Survived\"\n    cols_to_drop = [\"PassengerId\", \"Ticket\", \"Name\", \"Cabin\"]\n    df = df.drop(cols_to_drop, axis=1)\n    labels = df.pop(label_col)\n    train_df, eval_df, train_labels, eval_labels = train_test_split(\n        df, labels, test_size=0.25, random_state=0\n    )\n    return train_df, train_labels, eval_df, eval_labels\n\n\ndef get_sorted_dict_str(d: dict) -> str:\n    \"\"\"Get sorted dict string.\"\"\"\n    keys = sorted(list(d.keys()))\n    return \"\\n\".join([f\"{k}:{d[k]}\" for k in keys])\n\n\ndef get_label_str(labels: pd.Series, i: int) -> str:\n    \"\"\"Get label string.\"\"\"\n    return f\"{labels.name}: {labels.iloc[i]}\"\n\n\ndef get_train_str(\n    train_df: pd.DataFrame, train_labels: pd.Series, train_n: int = 10\n) -> str:\n    \"\"\"Get train str.\"\"\"\n    dict_list = train_df.to_dict(\"records\")[:train_n]\n    item_list = []\n    for i, d in enumerate(dict_list):\n        dict_str = get_sorted_dict_str(d)\n        label_str = get_label_str(train_labels, i)\n        item_str = (\n            f\"This is the Data:\\n{dict_str}\\nThis is the correct answer:\\n{label_str}\"\n        )\n        item_list.append(item_str)\n\n    return \"\\n\\n\".join(item_list)\n\n\ndef extract_float_given_response(response: str, n: int = 1) -> Optional[float]:\n    \"\"\"Extract number given the GPT-generated response.\n\n    Used by tree-structured indices.\n\n    \"\"\"\n    numbers = re.findall(r\"\\d+\\.\\d+\", response)\n    if len(numbers) == 0:\n        # if no floats, try extracting ints, and convert to float\n        new_numbers = extract_numbers_given_response(response, n=n)\n        if new_numbers is None:\n            return None\n        else:\n            return float(numbers[0])\n    else:\n        return float(numbers[0])\n\n\ndef get_eval_preds(\n    train_prompt: Prompt, train_str: str, eval_df: pd.DataFrame, n: int = 20\n) -> List:\n    \"\"\"Get eval preds.\"\"\"\n    llm_predictor = LLMPredictor()\n    eval_preds = []\n    for i in range(n):\n        eval_str = get_sorted_dict_str(eval_df.iloc[i].to_dict())\n        response, _ = llm_predictor.predict(\n            train_prompt, train_str=train_str, eval_str=eval_str\n        )\n        pred = extract_float_given_response(response)\n        print(f\"Getting preds: {i}/{n}: {pred}\")\n        if pred is None:\n            # something went wrong, impute a 0.5\n            eval_preds.append(0.5)\n        else:\n            eval_preds.append(pred)\n    return eval_preds\n\n\n# default train prompt\n\ntrain_prompt_str = (\n    \"The following structured data is provided in \"\n    '\"Feature Name\":\"Feature Value\" format.\\n'\n    \"Each datapoint describes a passenger on the Titanic.\\n\"\n    \"The task is to decide whether the passenger survived.\\n\"\n    \"Some example datapoints are given below: \\n\"\n    \"-------------------\\n\"\n    \"{train_str}\\n\"\n    \"-------------------\\n\"\n    \"Given this, predict whether the following passenger survived. \"\n    \"Return answer as a number between 0 or 1. \\n\"\n    \"{eval_str}\\n\"\n    \"Survived: \"\n)\n\ntrain_prompt = Prompt(\n    input_variables=[\"train_str\", \"eval_str\"], template=train_prompt_str\n)\n\n\n# prompt to summarize the data\nquery_str = \"Which is the relationship between these features and predicting survival?\"\nqa_data_str = (\n    \"The following structured data is provided in \"\n    '\"Feature Name\":\"Feature Value\" format.\\n'\n    \"Each datapoint describes a passenger on the Titanic.\\n\"\n    \"The task is to decide whether the passenger survived.\\n\"\n    \"Some example datapoints are given below: \\n\"\n    \"-------------------\\n\"\n    \"{context_str}\\n\"\n    \"-------------------\\n\"\n    \"Given this, answer the question: {query_str}\"\n)\n\nqa_data_prompt = Prompt(\n    input_variables=[\"context_str\", \"query_str\"], template=qa_data_str\n)\n\n# prompt to refine the answer\nrefine_str = (\n    \"The original question is as follows: {query_str}\\n\"\n    \"We have provided an existing answer: {existing_answer}\\n\"\n    \"The following structured data is provided in \"\n    '\"Feature Name\":\"Feature Value\" format.\\n'\n    \"Each datapoint describes a passenger on the Titanic.\\n\"\n    \"The task is to decide whether the passenger survived.\\n\"\n    \"We have the opportunity to refine the existing answer\"\n    \"(only if needed) with some more datapoints below.\\n\"\n    \"------------\\n\"\n    \"{context_msg}\\n\"\n    \"------------\\n\"\n    \"Given the new context, refine the original answer to better \"\n    \"answer the question. \"\n    \"If the context isn't useful, return the original answer.\"\n)\nrefine_prompt = Prompt(\n    input_variables=[\"query_str\", \"existing_answer\", \"context_msg\"],\n    template=refine_str,\n)\n\n\n# train prompt with refined context\n\ntrain_prompt_with_context_str = (\n    \"The following structured data is provided in \"\n    '\"Feature Name\":\"Feature Value\" format.\\n'\n    \"Each datapoint describes a passenger on the Titanic.\\n\"\n    \"The task is to decide whether the passenger survived.\\n\"\n    \"We discovered the following relationship between features and survival:\\n\"\n    \"-------------------\\n\"\n    \"{train_str}\\n\"\n    \"-------------------\\n\"\n    \"Given this, predict whether the following passenger survived. \\n\"\n    \"Return answer as a number between 0 or 1. \\n\"\n    \"{eval_str}\\n\"\n    \"Survived: \"\n)\n\ntrain_prompt_with_context = Prompt(\n    input_variables=[\"train_str\", \"eval_str\"], template=train_prompt_with_context_str\n)\n", "doc_id": "ee8ec9935ad93ab3a4e6b42017174b65c2d21de5", "embedding": null, "extra_info": {"file_path": "experimental/classifier/utils.py", "file_name": "utils.py"}, "__type__": "Document"}, "267577d47e497a0630bc454b3f74c4fd9a10ced4": {"text": "0.4.1\n", "doc_id": "267577d47e497a0630bc454b3f74c4fd9a10ced4", "embedding": null, "extra_info": {"file_path": "gpt_index/VERSION", "file_name": "VERSION"}, "__type__": "Document"}, "0054ff473fdab69e9283aafee2e4579f884d9e7b": {"text": "\"\"\"Init file of GPT Index.\"\"\"\nfrom pathlib import Path\n\nwith open(Path(__file__).absolute().parents[0] / \"VERSION\") as _f:\n    __version__ = _f.read().strip()\n\n\nfrom gpt_index.data_structs.struct_type import IndexStructType\n\n# embeddings\nfrom gpt_index.embeddings.langchain import LangchainEmbedding\nfrom gpt_index.embeddings.openai import OpenAIEmbedding\n\n# structured\nfrom gpt_index.indices.common.struct_store.base import SQLContextBuilder\n\n# indices\nfrom gpt_index.indices.keyword_table import (\n    GPTKeywordTableIndex,\n    GPTRAKEKeywordTableIndex,\n    GPTSimpleKeywordTableIndex,\n)\nfrom gpt_index.indices.list import GPTListIndex\n\n# prompt helper\nfrom gpt_index.indices.prompt_helper import PromptHelper\n\n# for composability\nfrom gpt_index.indices.query.schema import QueryConfig, QueryMode\nfrom gpt_index.indices.struct_store.sql import GPTSQLStructStoreIndex\nfrom gpt_index.indices.tree import GPTTreeIndex\nfrom gpt_index.indices.vector_store import (\n    GPTFaissIndex,\n    GPTPineconeIndex,\n    GPTQdrantIndex,\n    GPTSimpleVectorIndex,\n    GPTWeaviateIndex,\n)\n\n# langchain helper\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.memory_wrapper import GPTIndexMemory\nfrom gpt_index.langchain_helpers.sql_wrapper import SQLDatabase\n\n# prompts\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.prompts.prompts import (\n    KeywordExtractPrompt,\n    QueryKeywordExtractPrompt,\n    QuestionAnswerPrompt,\n    RefinePrompt,\n    SummaryPrompt,\n    TreeInsertPrompt,\n    TreeSelectMultiplePrompt,\n    TreeSelectPrompt,\n)\n\n# readers\nfrom gpt_index.readers import (\n    BeautifulSoupWebReader,\n    DiscordReader,\n    Document,\n    FaissReader,\n    GithubRepositoryReader,\n    GoogleDocsReader,\n    MboxReader,\n    NotionPageReader,\n    ObsidianReader,\n    PineconeReader,\n    QdrantReader,\n    RssReader,\n    SimpleDirectoryReader,\n    SimpleMongoReader,\n    SimpleWebPageReader,\n    SlackReader,\n    StringIterableReader,\n    TrafilaturaWebReader,\n    TwitterTweetReader,\n    WeaviateReader,\n    WikipediaReader,\n)\nfrom gpt_index.readers.download import download_loader\n\n# token predictor\nfrom gpt_index.token_counter.mock_chain_wrapper import MockLLMPredictor\nfrom gpt_index.token_counter.mock_embed_model import MockEmbedding\n\n__all__ = [\n    \"GPTKeywordTableIndex\",\n    \"GPTSimpleKeywordTableIndex\",\n    \"GPTRAKEKeywordTableIndex\",\n    \"GPTListIndex\",\n    \"GPTTreeIndex\",\n    \"GPTFaissIndex\",\n    \"GPTSimpleVectorIndex\",\n    \"GPTWeaviateIndex\",\n    \"GPTPineconeIndex\",\n    \"GPTQdrantIndex\",\n    \"GPTSQLStructStoreIndex\",\n    \"Prompt\",\n    \"LangchainEmbedding\",\n    \"OpenAIEmbedding\",\n    \"SummaryPrompt\",\n    \"TreeInsertPrompt\",\n    \"TreeSelectPrompt\",\n    \"TreeSelectMultiplePrompt\",\n    \"RefinePrompt\",\n    \"QuestionAnswerPrompt\",\n    \"KeywordExtractPrompt\",\n    \"QueryKeywordExtractPrompt\",\n    \"WikipediaReader\",\n    \"ObsidianReader\",\n    \"Document\",\n    \"SimpleDirectoryReader\",\n    \"SimpleMongoReader\",\n    \"NotionPageReader\",\n    \"GoogleDocsReader\",\n    \"MboxReader\",\n    \"SlackReader\",\n    \"StringIterableReader\",\n    \"WeaviateReader\",\n    \"FaissReader\",\n    \"PineconeReader\",\n    \"QdrantReader\",\n    \"DiscordReader\",\n    \"SimpleWebPageReader\",\n    \"RssReader\",\n    \"BeautifulSoupWebReader\",\n    \"TrafilaturaWebReader\",\n    \"LLMPredictor\",\n    \"MockLLMPredictor\",\n    \"MockEmbedding\",\n    \"SQLDatabase\",\n    \"GPTIndexMemory\",\n    \"SQLContextBuilder\",\n    \"PromptHelper\",\n    \"QueryConfig\",\n    \"QueryMode\",\n    \"IndexStructType\",\n    \"TwitterTweetReader\",\n    \"download_loader\",\n    \"GithubRepositoryReader\",\n]\n\nimport logging\nfrom logging import NullHandler\n\n# best practices for library logging:\n# https://docs.python.org/3/howto/logging.html#configuring-logging-for-a-library\nlogging.getLogger(__name__).addHandler(NullHandler())\n", "doc_id": "0054ff473fdab69e9283aafee2e4579f884d9e7b", "embedding": null, "extra_info": {"file_path": "gpt_index/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "7e8371c12d89bcde3b7b335df2c1772cd70229da": {"text": "\"\"\"This module contains all classes used for composing graphs over indices.\"\"\"\n\n\nfrom gpt_index.composability.graph import ComposableGraph\n\n__all__ = [\"ComposableGraph\"]\n", "doc_id": "7e8371c12d89bcde3b7b335df2c1772cd70229da", "embedding": null, "extra_info": {"file_path": "gpt_index/composability/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "90ca6826baf6988ea1d8f1c91b0cb85badcf00f4": {"text": "\"\"\"Composability graphs.\"\"\"\n\nimport json\nfrom typing import Any, Dict, List, Optional, Type, Union\n\nfrom gpt_index.data_structs.data_structs import IndexStruct\nfrom gpt_index.data_structs.struct_type import IndexStructType\nfrom gpt_index.docstore import DocumentStore\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.embeddings.openai import OpenAIEmbedding\nfrom gpt_index.indices.base import BaseGPTIndex\nfrom gpt_index.indices.keyword_table.base import GPTKeywordTableIndex\nfrom gpt_index.indices.list.base import GPTListIndex\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.query.query_runner import QueryRunner\nfrom gpt_index.indices.query.schema import QueryConfig\nfrom gpt_index.indices.registry import IndexRegistry\nfrom gpt_index.indices.struct_store.sql import GPTSQLStructStoreIndex\nfrom gpt_index.indices.tree.base import GPTTreeIndex\nfrom gpt_index.indices.vector_store.faiss import GPTFaissIndex\nfrom gpt_index.indices.vector_store.pinecone import GPTPineconeIndex\nfrom gpt_index.indices.vector_store.qdrant import GPTQdrantIndex\nfrom gpt_index.indices.vector_store.simple import GPTSimpleVectorIndex\nfrom gpt_index.indices.vector_store.weaviate import GPTWeaviateIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.response.schema import Response\n\n# TMP: refactor query config type\nQUERY_CONFIG_TYPE = Union[Dict, QueryConfig]\n\n\n# this is a map from type to outer index class\n# we extract the type_to_struct and type_to_query\n# fields from the index class\nDEFAULT_INDEX_REGISTRY_MAP: Dict[IndexStructType, Type[BaseGPTIndex]] = {\n    IndexStructType.TREE: GPTTreeIndex,\n    IndexStructType.LIST: GPTListIndex,\n    IndexStructType.KEYWORD_TABLE: GPTKeywordTableIndex,\n    IndexStructType.DICT: GPTFaissIndex,\n    IndexStructType.SIMPLE_DICT: GPTSimpleVectorIndex,\n    IndexStructType.WEAVIATE: GPTWeaviateIndex,\n    IndexStructType.PINECONE: GPTPineconeIndex,\n    IndexStructType.QDRANT: GPTQdrantIndex,\n    IndexStructType.SQL: GPTSQLStructStoreIndex,\n}\n\n\ndef _get_default_index_registry() -> IndexRegistry:\n    \"\"\"Get default index registry.\"\"\"\n    index_registry = IndexRegistry()\n    for index_type, index_class in DEFAULT_INDEX_REGISTRY_MAP.items():\n        index_registry.type_to_struct[index_type] = index_class.index_struct_cls\n        index_registry.type_to_query[index_type] = index_class.get_query_map()\n    return index_registry\n\n\ndef _safe_get_index_struct(\n    docstore: DocumentStore, index_struct_id: str\n) -> IndexStruct:\n    \"\"\"Try get index struct.\"\"\"\n    index_struct = docstore.get_document(index_struct_id)\n    if not isinstance(index_struct, IndexStruct):\n        raise ValueError(\"Invalid `index_struct_id` - must be an IndexStruct\")\n    return index_struct\n\n\nclass ComposableGraph:\n    \"\"\"Composable graph.\"\"\"\n\n    def __init__(\n        self,\n        docstore: DocumentStore,\n        index_registry: IndexRegistry,\n        index_struct: IndexStruct,\n        llm_predictor: Optional[LLMPredictor] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        chunk_size_limit: Optional[int] = None,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self._docstore = docstore\n        self._index_registry = index_registry\n        # this represents the \"root\" index struct\n        self._index_struct = index_struct\n\n        self._llm_predictor = llm_predictor or LLMPredictor()\n        self._prompt_helper = prompt_helper or PromptHelper.from_llm_predictor(\n            self._llm_predictor, chunk_size_limit=chunk_size_limit\n        )\n        self._embed_model = embed_model or OpenAIEmbedding()\n\n    @classmethod\n    def build_from_index(self, index: BaseGPTIndex) -> \"ComposableGraph\":\n        \"\"\"Build from index.\"\"\"\n        return ComposableGraph(\n            index.docstore,\n            index.index_registry,\n            # this represents the \"root\" index struct\n            index.index_struct,\n            llm_predictor=index.llm_predictor,\n            prompt_helper=index.prompt_helper,\n            embed_model=index.embed_model,\n        )\n\n    def query(\n        self,\n        query_str: str,\n        query_configs: Optional[List[QUERY_CONFIG_TYPE]] = None,\n    ) -> Response:\n        \"\"\"Query the index.\"\"\"\n        # go over all the indices and create a registry\n        query_runner = QueryRunner(\n            self._llm_predictor,\n            self._prompt_helper,\n            self._embed_model,\n            self._docstore,\n            self._index_registry,\n            query_configs=query_configs,\n            recursive=True,\n        )\n        return query_runner.query(query_str, self._index_struct)\n\n    def get_index(\n        self, index_struct_id: str, index_cls: Type[BaseGPTIndex], **kwargs: Any\n    ) -> BaseGPTIndex:\n        \"\"\"Get index.\"\"\"\n        index_struct = _safe_get_index_struct(self._docstore, index_struct_id)\n        return index_cls(\n            index_struct=index_struct,\n            docstore=self._docstore,\n            index_registry=self._index_registry,\n            **kwargs\n        )\n\n    @classmethod\n    def load_from_string(cls, index_string: str, **kwargs: Any) -> \"ComposableGraph\":\n        \"\"\"Load index from string (in JSON-format).\n\n        This method loads the index from a JSON string. The index data\n        structure itself is preserved completely. If the index is defined over\n        subindices, those subindices will also be preserved (and subindices of\n        those subindices, etc.).\n\n        Args:\n            save_path (str): The save_path of the file.\n\n        Returns:\n            BaseGPTIndex: The loaded index.\n\n        \"\"\"\n        result_dict = json.loads(index_string)\n        # TODO: this is hardcoded for now, allow it to be specified by the user\n        index_registry = _get_default_index_registry()\n        docstore = DocumentStore.load_from_dict(\n            result_dict[\"docstore\"], index_registry.type_to_struct\n        )\n        index_struct = _safe_get_index_struct(docstore, result_dict[\"index_struct_id\"])\n        return cls(docstore, index_registry, index_struct, **kwargs)\n\n    @classmethod\n    def load_from_disk(cls, save_path: str, **kwargs: Any) -> \"ComposableGraph\":\n        \"\"\"Load index from disk.\n\n        This method loads the index from a JSON file stored on disk. The index data\n        structure itself is preserved completely. If the index is defined over\n        subindices, those subindices will also be preserved (and subindices of\n        those subindices, etc.).\n\n        Args:\n            save_path (str): The save_path of the file.\n\n        Returns:\n            BaseGPTIndex: The loaded index.\n\n        \"\"\"\n        with open(save_path, \"r\") as f:\n            file_contents = f.read()\n            return cls.load_from_string(file_contents, **kwargs)\n\n    def save_to_string(self, **save_kwargs: Any) -> str:\n        \"\"\"Save to string.\n\n        This method stores the index into a JSON file stored on disk.\n\n        Args:\n            save_path (str): The save_path of the file.\n\n        \"\"\"\n        out_dict: Dict[str, Any] = {\n            \"index_struct_id\": self._index_struct.get_doc_id(),\n            \"docstore\": self._docstore.serialize_to_dict(),\n        }\n        return json.dumps(out_dict)\n\n    def save_to_disk(self, save_path: str, **save_kwargs: Any) -> None:\n        \"\"\"Save to file.\n\n        This method stores the index into a JSON file stored on disk.\n\n        Args:\n            save_path (str): The save_path of the file.\n\n        \"\"\"\n        index_string = self.save_to_string(**save_kwargs)\n        with open(save_path, \"w\") as f:\n            f.write(index_string)\n", "doc_id": "90ca6826baf6988ea1d8f1c91b0cb85badcf00f4", "embedding": null, "extra_info": {"file_path": "gpt_index/composability/graph.py", "file_name": "graph.py"}, "__type__": "Document"}, "d619a2462ae933d7bdde127f86bde58dac29b79b": {"text": "\"\"\"Set of constants.\"\"\"\n\nMAX_CHUNK_SIZE = 3900\nMAX_CHUNK_OVERLAP = 200\nNUM_OUTPUTS = 256\n", "doc_id": "d619a2462ae933d7bdde127f86bde58dac29b79b", "embedding": null, "extra_info": {"file_path": "gpt_index/constants.py", "file_name": "constants.py"}, "__type__": "Document"}, "73f0f8d7e49d0fc9866dbe8a9d799396b09b1a74": {"text": "\"\"\"Init file.\"\"\"\n\nfrom gpt_index.data_structs.data_structs import (\n    IndexDict,\n    IndexGraph,\n    IndexList,\n    KeywordTable,\n    Node,\n    QdrantIndexStruct,\n    SimpleIndexDict,\n    WeaviateIndexStruct,\n)\nfrom gpt_index.data_structs.table import StructDatapoint\n\n__all__ = [\n    \"Node\",\n    \"IndexGraph\",\n    \"KeywordTable\",\n    \"IndexList\",\n    \"IndexDict\",\n    \"SimpleIndexDict\",\n    \"WeaviateIndexStruct\",\n    \"QdrantIndexStruct\",\n    \"StructDatapoint\",\n]\n", "doc_id": "73f0f8d7e49d0fc9866dbe8a9d799396b09b1a74", "embedding": null, "extra_info": {"file_path": "gpt_index/data_structs/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "bb6752a483b33b4ef24085d9f0c7fc696de3a9fe": {"text": "\"\"\"File for core data structures.\"\"\"\n\nimport random\nimport sys\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional, Set\n\nfrom dataclasses_json import DataClassJsonMixin\n\nfrom gpt_index.schema import BaseDocument\nfrom gpt_index.utils import get_new_int_id\n\n\n@dataclass\nclass IndexStruct(BaseDocument, DataClassJsonMixin):\n    \"\"\"A base data struct for a GPT index.\"\"\"\n\n    # NOTE: the text field, inherited from BaseDocument,\n    # represents a summary of the content of the index struct.\n    # primarily used for composing indices with other indices\n\n    # NOTE: the doc_id field, inherited from BaseDocument,\n    # represents a unique identifier for the index struct\n    # that will be put in the docstore.\n    # Not all index_structs need to have a doc_id. Only index_structs that\n    # represent a complete data structure (e.g. IndexGraph, IndexList),\n    # and are used to compose a higher level index, will have a doc_id.\n\n\n@dataclass\nclass Node(IndexStruct):\n    \"\"\"A generic node of data.\n\n    Base struct used in most indices.\n\n    \"\"\"\n\n    def __post_init__(self) -> None:\n        \"\"\"Post init.\"\"\"\n        # NOTE: for Node objects, the text field is required\n        if self.text is None:\n            raise ValueError(\"text field not set.\")\n\n    # used for GPTTreeIndex\n    index: int = 0\n    child_indices: Set[int] = field(default_factory=set)\n\n    # embeddings\n    embedding: Optional[List[float]] = None\n\n    # reference document id\n    ref_doc_id: Optional[str] = None\n\n    # extra node info\n    node_info: Optional[Dict[str, Any]] = None\n\n    def get_text(self) -> str:\n        \"\"\"Get text.\"\"\"\n        text = super().get_text()\n        result_text = (\n            text if self.extra_info_str is None else f\"{self.extra_info_str}\\n\\n{text}\"\n        )\n        return result_text\n\n    @classmethod\n    def get_type(cls) -> str:\n        \"\"\"Get type.\"\"\"\n        # TODO: consolidate with IndexStructType\n        return \"node\"\n\n\n@dataclass\nclass IndexGraph(IndexStruct):\n    \"\"\"A graph representing the tree-structured index.\"\"\"\n\n    all_nodes: Dict[int, Node] = field(default_factory=dict)\n    root_nodes: Dict[int, Node] = field(default_factory=dict)\n\n    @property\n    def size(self) -> int:\n        \"\"\"Get the size of the graph.\"\"\"\n        return len(self.all_nodes)\n\n    def get_children(self, parent_node: Optional[Node]) -> Dict[int, Node]:\n        \"\"\"Get nodes given indices.\"\"\"\n        if parent_node is None:\n            return self.root_nodes\n        else:\n            return {i: self.all_nodes[i] for i in parent_node.child_indices}\n\n    def insert_under_parent(self, node: Node, parent_node: Optional[Node]) -> None:\n        \"\"\"Insert under parent node.\"\"\"\n        if node.index in self.all_nodes:\n            raise ValueError(\n                \"Cannot insert a new node with the same index as an existing node.\"\n            )\n        if parent_node is None:\n            self.root_nodes[node.index] = node\n        else:\n            parent_node.child_indices.add(node.index)\n\n        self.all_nodes[node.index] = node\n\n    @classmethod\n    def get_type(cls) -> str:\n        \"\"\"Get type.\"\"\"\n        return \"tree\"\n\n\n@dataclass\nclass KeywordTable(IndexStruct):\n    \"\"\"A table of keywords mapping keywords to text chunks.\"\"\"\n\n    table: Dict[str, Set[int]] = field(default_factory=dict)\n    text_chunks: Dict[int, Node] = field(default_factory=dict)\n\n    def _get_index(self) -> int:\n        \"\"\"Get the next index for the text chunk.\"\"\"\n        # randomly generate until we get a unique index\n        while True:\n            idx = random.randint(0, sys.maxsize)\n            if idx not in self.text_chunks:\n                break\n        return idx\n\n    def add_node(self, keywords: List[str], node: Node) -> int:\n        \"\"\"Add text to table.\"\"\"\n        cur_idx = self._get_index()\n        for keyword in keywords:\n            if keyword not in self.table:\n                self.table[keyword] = set()\n            self.table[keyword].add(cur_idx)\n        self.text_chunks[cur_idx] = node\n        return cur_idx\n\n    def get_texts(self, keyword: str) -> List[str]:\n        \"\"\"Get texts given keyword.\"\"\"\n        if keyword not in self.table:\n            raise ValueError(\"Keyword not found in table.\")\n        return [self.text_chunks[idx].get_text() for idx in self.table[keyword]]\n\n    @property\n    def keywords(self) -> Set[str]:\n        \"\"\"Get all keywords in the table.\"\"\"\n        return set(self.table.keys())\n\n    @property\n    def size(self) -> int:\n        \"\"\"Get the size of the table.\"\"\"\n        return len(self.table)\n\n    @classmethod\n    def get_type(cls) -> str:\n        \"\"\"Get type.\"\"\"\n        return \"keyword_table\"\n\n\n@dataclass\nclass IndexList(IndexStruct):\n    \"\"\"A list of documents.\"\"\"\n\n    nodes: List[Node] = field(default_factory=list)\n\n    def add_node(self, node: Node) -> None:\n        \"\"\"Add text to table, return current position in list.\"\"\"\n        # don't worry about child indices for now, nodes are all in order\n        self.nodes.append(node)\n\n    @classmethod\n    def get_type(cls) -> str:\n        \"\"\"Get type.\"\"\"\n        return \"list\"\n\n\n@dataclass\nclass BaseIndexDict(IndexStruct):\n    \"\"\"A simple dictionary of documents.\"\"\"\n\n    nodes_dict: Dict[int, Node] = field(default_factory=dict)\n    id_map: Dict[str, int] = field(default_factory=dict)\n\n    def add_node(\n        self,\n        node: Node,\n        text_id: Optional[str] = None,\n    ) -> str:\n        \"\"\"Add text to table, return current position in list.\"\"\"\n        int_id = get_new_int_id(set(self.nodes_dict.keys()))\n        if text_id in self.id_map:\n            raise ValueError(\"text_id cannot already exist in index.\")\n        elif text_id is not None and not isinstance(text_id, str):\n            raise ValueError(\"text_id must be a string.\")\n        elif text_id is None:\n            text_id = str(int_id)\n        self.id_map[text_id] = int_id\n\n        # don't worry about child indices for now, nodes are all in order\n        self.nodes_dict[int_id] = node\n        return text_id\n\n    def get_nodes(self, text_ids: List[str]) -> List[Node]:\n        \"\"\"Get nodes.\"\"\"\n        nodes = []\n        for text_id in text_ids:\n            if text_id not in self.id_map:\n                raise ValueError(\"text_id not found in id_map\")\n            elif not isinstance(text_id, str):\n                raise ValueError(\"text_id must be a string.\")\n            int_id = self.id_map[text_id]\n            if int_id not in self.nodes_dict:\n                raise ValueError(\"int_id not found in nodes_dict\")\n            nodes.append(self.nodes_dict[int_id])\n        return nodes\n\n    def get_node(self, text_id: str) -> Node:\n        \"\"\"Get node.\"\"\"\n        return self.get_nodes([text_id])[0]\n\n    @classmethod\n    def get_type(cls) -> str:\n        \"\"\"Get type.\"\"\"\n        return \"dict\"\n\n\n# TODO: this should be specific to FAISS\n@dataclass\nclass IndexDict(BaseIndexDict):\n    \"\"\"A dictionary of documents.\n\n    Note: this index structure is specifically used with the Faiss index.\n\n    \"\"\"\n\n    @classmethod\n    def get_type(cls) -> str:\n        \"\"\"Get type.\"\"\"\n        return \"dict\"\n\n\n@dataclass\nclass SimpleIndexDict(BaseIndexDict):\n    \"\"\"A simple dictionary of documents.\n\n    This index structure also contains an internal in-memory\n    embedding dict.\n\n    \"\"\"\n\n    embedding_dict: Dict[str, List[float]] = field(default_factory=dict)\n\n    def add_to_embedding_dict(self, text_id: str, embedding: List[float]) -> None:\n        \"\"\"Add embedding to dict.\"\"\"\n        if text_id not in self.id_map:\n            raise ValueError(\"text_id not found in id_map\")\n        elif not isinstance(text_id, str):\n            raise ValueError(\"text_id must be a string.\")\n        self.embedding_dict[text_id] = embedding\n\n    @classmethod\n    def get_type(cls) -> str:\n        \"\"\"Get type.\"\"\"\n        return \"simple_dict\"\n\n\n@dataclass\nclass WeaviateIndexStruct(IndexStruct):\n    \"\"\"A helper index struct for Weaviate.\n\n    In Weaviate, docs are stored in Weaviate directly.\n    This index struct helps to store the class prefix\n\n    \"\"\"\n\n    class_prefix: Optional[str] = None\n\n    def __post_init__(self) -> None:\n        \"\"\"Post init.\"\"\"\n        if self.class_prefix is None:\n            raise ValueError(\"class_prefix must be provided.\")\n\n    def get_class_prefix(self) -> str:\n        \"\"\"Get class prefix.\"\"\"\n        if self.class_prefix is None:\n            raise ValueError(\"class_prefix must be provided.\")\n        return self.class_prefix\n\n    @classmethod\n    def get_type(cls) -> str:\n        \"\"\"Get type.\"\"\"\n        return \"weaviate\"\n\n\n@dataclass\nclass PineconeIndexStruct(IndexStruct):\n    \"\"\"An index struct for Pinecone.\n\n    Docs are stored in Pinecone directly.\n\n    \"\"\"\n\n    @classmethod\n    def get_type(cls) -> str:\n        \"\"\"Get type.\"\"\"\n        return \"pinecone\"\n\n\n@dataclass\nclass QdrantIndexStruct(IndexStruct):\n    \"\"\"And index struct for Qdrant.\n\n    Docs are stored in Qdrant directly.\n    This index struct helps to store the collection name\n\n    \"\"\"\n\n    collection_name: Optional[str] = None\n\n    def __post_init__(self) -> None:\n        \"\"\"Post init.\"\"\"\n        if self.collection_name is None:\n            raise ValueError(\"collection_name must be provided.\")\n\n    def get_collection_name(self) -> str:\n        \"\"\"Get class prefix.\"\"\"\n        if self.collection_name is None:\n            raise ValueError(\"collection_name must be provided.\")\n        return self.collection_name\n\n    @classmethod\n    def get_type(cls) -> str:\n        \"\"\"Get type.\"\"\"\n        return \"qdrant\"\n", "doc_id": "bb6752a483b33b4ef24085d9f0c7fc696de3a9fe", "embedding": null, "extra_info": {"file_path": "gpt_index/data_structs/data_structs.py", "file_name": "data_structs.py"}, "__type__": "Document"}, "d75d8d1f3958884352e2c1148a6b9f1dd3e1f626": {"text": "\"\"\"IndexStructType class.\"\"\"\n\nfrom enum import Enum\n\n\nclass IndexStructType(str, Enum):\n    \"\"\"Index struct type. Identifier for a \"type\" of index.\n\n    Attributes:\n        TREE (\"tree\"): Tree index. See :ref:`Ref-Indices-Tree` for tree indices.\n        LIST (\"list\"): List index. See :ref:`Ref-Indices-List` for list indices.\n        KEYWORD_TABLE (\"keyword_table\"): Keyword table index. See\n            :ref:`Ref-Indices-Table`\n            for keyword table indices.\n        DICT (\"dict\"): Faiss Vector Store Index. See :ref:`Ref-Indices-VectorStore`\n            for more information on the Faiss vector store index.\n        SIMPLE_DICT (\"simple_dict\"): Simple Vector Store Index. See\n            :ref:`Ref-Indices-VectorStore`\n            for more information on the simple vector store index.\n        WEAVIATE (\"weaviate\"): Weaviate Vector Store Index.\n            See :ref:`Ref-Indices-VectorStore`\n            for more information on the Weaviate vector store index.\n        PINECONE (\"pinecone\"): Pinecone Vector Store Index.\n            See :ref:`Ref-Indices-VectorStore`\n            for more information on the Pinecone vector store index.\n        QDRANT (\"qdrant\"): Qdrant Vector Store Index.\n            See :ref:`Ref-Indices-VectorStore`\n            for more information on the Qdrant vector store index.\n\n        SQL (\"SQL\"): SQL Structured Store Index.\n            See :ref:`Ref-Indices-StructStore`\n            for more information on the SQL vector store index.\n\n    \"\"\"\n\n    # TODO: refactor so these are properties on the base class\n\n    NODE = \"node\"\n    TREE = \"tree\"\n    LIST = \"list\"\n    KEYWORD_TABLE = \"keyword_table\"\n    # for Faiss\n    # TODO: rename\n    DICT = \"dict\"\n    # for simple embedding index\n    SIMPLE_DICT = \"simple_dict\"\n    # for weaviate index\n    WEAVIATE = \"weaviate\"\n    # for pinecone index\n    PINECONE = \"pinecone\"\n    # for qdrant index\n    QDRANT = \"qdrant\"\n    # for SQL index\n    SQL = \"sql\"\n", "doc_id": "d75d8d1f3958884352e2c1148a6b9f1dd3e1f626", "embedding": null, "extra_info": {"file_path": "gpt_index/data_structs/struct_type.py", "file_name": "struct_type.py"}, "__type__": "Document"}, "a23a646d5f57acd8044f740f0b4a0b683aa20e7f": {"text": "\"\"\"Struct store schema.\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict\n\nfrom dataclasses_json import DataClassJsonMixin\n\nfrom gpt_index.data_structs.data_structs import IndexStruct\n\n\n@dataclass\nclass StructDatapoint(DataClassJsonMixin):\n    \"\"\"Struct outputs.\"\"\"\n\n    # map from field name to StructValue\n    fields: Dict[str, Any]\n\n\n@dataclass\nclass BaseStructTable(IndexStruct):\n    \"\"\"Struct outputs.\"\"\"\n\n\n@dataclass\nclass SQLStructTable(BaseStructTable):\n    \"\"\"SQL struct outputs.\"\"\"\n\n    context_dict: Dict[str, str] = field(default_factory=dict)\n\n    @classmethod\n    def get_type(cls) -> str:\n        \"\"\"Get type.\"\"\"\n        # TODO: consolidate with IndexStructType\n        return \"sql\"\n", "doc_id": "a23a646d5f57acd8044f740f0b4a0b683aa20e7f", "embedding": null, "extra_info": {"file_path": "gpt_index/data_structs/table.py", "file_name": "table.py"}, "__type__": "Document"}, "523dc2d189441cf16be18c8f3896fae904dc3473": {"text": "\"\"\"Document store.\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional, Type, Union\n\nfrom dataclasses_json import DataClassJsonMixin\n\nfrom gpt_index.data_structs.data_structs import IndexStruct\nfrom gpt_index.readers.schema.base import Document\nfrom gpt_index.utils import get_new_id\n\nDOC_TYPE = Union[IndexStruct, Document]\n\n# type key: used to store type of document\nTYPE_KEY = \"__type__\"\n\n\n@dataclass\nclass DocumentStore(DataClassJsonMixin):\n    \"\"\"Document store.\"\"\"\n\n    docs: Dict[str, DOC_TYPE] = field(default_factory=dict)\n\n    def serialize_to_dict(self) -> Dict[str, Any]:\n        \"\"\"Serialize to dict.\"\"\"\n        docs_dict = {}\n        for doc_id, doc in self.docs.items():\n            doc_dict = doc.to_dict()\n            doc_dict[TYPE_KEY] = doc.get_type()\n            docs_dict[doc_id] = doc_dict\n        return {\"docs\": docs_dict}\n\n    def contains_index_struct(self, exclude_ids: Optional[List[str]] = None) -> bool:\n        \"\"\"Check if contains index struct.\"\"\"\n        exclude_ids = exclude_ids or []\n        for doc in self.docs.values():\n            if isinstance(doc, IndexStruct) and doc.get_doc_id() not in exclude_ids:\n                return True\n        return False\n\n    @classmethod\n    def load_from_dict(\n        cls,\n        docs_dict: Dict[str, Any],\n        type_to_struct: Optional[Dict[str, Type[IndexStruct]]] = None,\n    ) -> \"DocumentStore\":\n        \"\"\"Load from dict.\"\"\"\n        docs_obj_dict = {}\n        for doc_id, doc_dict in docs_dict[\"docs\"].items():\n            doc_type = doc_dict.pop(TYPE_KEY, None)\n            if doc_type == \"Document\" or doc_type is None:\n                doc: DOC_TYPE = Document.from_dict(doc_dict)\n            else:\n                if type_to_struct is None:\n                    raise ValueError(\n                        \"type_to_struct must be provided if type is index struct.\"\n                    )\n                # try using IndexStructType to retrieve documents\n                if doc_type not in type_to_struct:\n                    raise ValueError(\n                        f\"doc_type {doc_type} not found in type_to_struct. \"\n                        \"Make sure that it was registered in the index registry.\"\n                    )\n                doc = type_to_struct[doc_type].from_dict(doc_dict)\n                # doc = index_struct_cls.from_dict(doc_dict)\n            docs_obj_dict[doc_id] = doc\n        return cls(docs=docs_obj_dict)\n\n    @classmethod\n    def from_documents(cls, docs: List[DOC_TYPE]) -> \"DocumentStore\":\n        \"\"\"Create from documents.\"\"\"\n        obj = cls()\n        obj.add_documents(docs)\n        return obj\n\n    def get_new_id(self) -> str:\n        \"\"\"Get a new ID.\"\"\"\n        return get_new_id(set(self.docs.keys()))\n\n    def update_docstore(self, other: \"DocumentStore\") -> None:\n        \"\"\"Update docstore.\"\"\"\n        self.docs.update(other.docs)\n\n    def add_documents(self, docs: List[DOC_TYPE], generate_id: bool = True) -> None:\n        \"\"\"Add a document to the store.\n\n        If generate_id = True, then generate id for doc if doc_id doesn't exist.\n\n        \"\"\"\n        for doc in docs:\n            if doc.is_doc_id_none:\n                if generate_id:\n                    doc.doc_id = self.get_new_id()\n                else:\n                    raise ValueError(\n                        \"doc_id not set (to generate id, please set generate_id=True).\"\n                    )\n\n            # NOTE: doc could already exist in the store, but we overwrite it\n            self.docs[doc.get_doc_id()] = doc\n\n    def get_document(self, doc_id: str, raise_error: bool = True) -> Optional[DOC_TYPE]:\n        \"\"\"Get a document from the store.\"\"\"\n        doc = self.docs.get(doc_id, None)\n        if doc is None and raise_error:\n            raise ValueError(f\"doc_id {doc_id} not found.\")\n        return doc\n\n    def document_exists(self, doc_id: str) -> bool:\n        \"\"\"Check if document exists.\"\"\"\n        return doc_id in self.docs\n\n    def delete_document(\n        self, doc_id: str, raise_error: bool = True\n    ) -> Optional[DOC_TYPE]:\n        \"\"\"Delete a document from the store.\"\"\"\n        doc = self.docs.pop(doc_id, None)\n        if doc is None and raise_error:\n            raise ValueError(f\"doc_id {doc_id} not found.\")\n        return doc\n\n    def __len__(self) -> int:\n        \"\"\"Get length.\"\"\"\n        return len(self.docs.keys())\n", "doc_id": "523dc2d189441cf16be18c8f3896fae904dc3473", "embedding": null, "extra_info": {"file_path": "gpt_index/docstore.py", "file_name": "docstore.py"}, "__type__": "Document"}, "1d4640565ae2765d9ca96a509dc9809217f62f2f": {"text": "\"\"\"Init file.\"\"\"\n", "doc_id": "1d4640565ae2765d9ca96a509dc9809217f62f2f", "embedding": null, "extra_info": {"file_path": "tests/token_predictor/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "6b44f11bc6046627318f35f08e5701d0cdce4208": {"text": "\"\"\"Base embeddings file.\"\"\"\n\nfrom abc import abstractmethod\nfrom enum import Enum\nfrom typing import Callable, List, Optional\n\nimport numpy as np\n\nfrom gpt_index.utils import globals_helper\n\n# TODO: change to numpy array\nEMB_TYPE = List\n\n\nclass SimilarityMode(str, Enum):\n    \"\"\"Modes for similarity/distance.\"\"\"\n\n    DEFAULT = \"cosine\"\n    DOT_PRODUCT = \"dot_product\"\n    EUCLIDEAN = \"euclidean\"\n\n\nclass BaseEmbedding:\n    \"\"\"Base class for embeddings.\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Init params.\"\"\"\n        self._total_tokens_used = 0\n        self._last_token_usage: Optional[int] = None\n        self._tokenizer: Callable = globals_helper.tokenizer\n\n    @abstractmethod\n    def _get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding.\"\"\"\n\n    def get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding.\"\"\"\n        query_embedding = self._get_query_embedding(query)\n        query_tokens_count = len(self._tokenizer(query))\n        self._total_tokens_used += query_tokens_count\n        return query_embedding\n\n    @abstractmethod\n    def _get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get text embedding.\"\"\"\n\n    def get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get text embedding.\"\"\"\n        text_embedding = self._get_text_embedding(text)\n        text_tokens_count = len(self._tokenizer(text))\n        self._total_tokens_used += text_tokens_count\n        return text_embedding\n\n    def similarity(\n        self,\n        embedding1: EMB_TYPE,\n        embedding2: EMB_TYPE,\n        mode: SimilarityMode = SimilarityMode.DEFAULT,\n    ) -> float:\n        \"\"\"Get embedding similarity.\"\"\"\n        if mode == SimilarityMode.EUCLIDEAN:\n            return float(np.linalg.norm(np.array(embedding1) - np.array(embedding2)))\n        elif mode == SimilarityMode.DOT_PRODUCT:\n            product = np.dot(embedding1, embedding2)\n            return product\n        else:\n            product = np.dot(embedding1, embedding2)\n            norm = np.linalg.norm(embedding1) * np.linalg.norm(embedding2)\n            return product / norm\n\n    @property\n    def total_tokens_used(self) -> int:\n        \"\"\"Get the total tokens used so far.\"\"\"\n        return self._total_tokens_used\n\n    @property\n    def last_token_usage(self) -> int:\n        \"\"\"Get the last token usage.\"\"\"\n        if self._last_token_usage is None:\n            return 0\n        return self._last_token_usage\n\n    @last_token_usage.setter\n    def last_token_usage(self, value: int) -> None:\n        \"\"\"Set the last token usage.\"\"\"\n        self._last_token_usage = value\n", "doc_id": "6b44f11bc6046627318f35f08e5701d0cdce4208", "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/base.py", "file_name": "base.py"}, "__type__": "Document"}, "8b01c1a38fd076f3f050d480e1a614de889880a0": {"text": "\"\"\"Langchain Embedding Wrapper Module.\"\"\"\n\n\nfrom typing import List\n\nfrom langchain.embeddings.base import Embeddings as LCEmbeddings\n\nfrom gpt_index.embeddings.base import BaseEmbedding\n\n\nclass LangchainEmbedding(BaseEmbedding):\n    \"\"\"External embeddings (taken from Langchain).\n\n    Args:\n        langchain_embedding (langchain.embeddings.Embeddings): Langchain\n            embeddings class.\n    \"\"\"\n\n    def __init__(self, langchain_embedding: LCEmbeddings) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__()\n        self._langchain_embedding = langchain_embedding\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding.\"\"\"\n        return self._langchain_embedding.embed_query(query)\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get text embedding.\"\"\"\n        return self._langchain_embedding.embed_documents([text])[0]\n", "doc_id": "8b01c1a38fd076f3f050d480e1a614de889880a0", "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/langchain.py", "file_name": "langchain.py"}, "__type__": "Document"}, "19f1d4072db017613451127614ed9fec18193f70": {"text": "\"\"\"OpenAI embeddings file.\"\"\"\n\nfrom enum import Enum\nfrom typing import List, Optional\n\nimport openai\nfrom tenacity import retry, stop_after_attempt, wait_random_exponential\n\nfrom gpt_index.embeddings.base import BaseEmbedding\n\n\nclass OpenAIEmbeddingMode(str, Enum):\n    \"\"\"OpenAI embedding mode.\"\"\"\n\n    SIMILARITY_MODE = \"similarity\"\n    TEXT_SEARCH_MODE = \"text_search\"\n\n\nclass OpenAIEmbeddingModelType(str, Enum):\n    \"\"\"OpenAI embedding model type.\"\"\"\n\n    DAVINCI = \"davinci\"\n    CURIE = \"curie\"\n    BABBAGE = \"babbage\"\n    ADA = \"ada\"\n    TEXT_EMBED_ADA_002 = \"text-embedding-ada-002\"\n\n\nclass OpenAIEmbeddingModeModel(str, Enum):\n    \"\"\"OpenAI embedding mode model.\"\"\"\n\n    # davinci\n    TEXT_SIMILARITY_DAVINCI = \"text-similarity-davinci-001\"\n    TEXT_SEARCH_DAVINCI_QUERY = \"text-search-davinci-query-001\"\n    TEXT_SEARCH_DAVINCI_DOC = \"text-search-davinci-doc-001\"\n\n    # curie\n    TEXT_SIMILARITY_CURIE = \"text-similarity-curie-001\"\n    TEXT_SEARCH_CURIE_QUERY = \"text-search-curie-query-001\"\n    TEXT_SEARCH_CURIE_DOC = \"text-search-curie-doc-001\"\n\n    # babbage\n    TEXT_SIMILARITY_BABBAGE = \"text-similarity-babbage-001\"\n    TEXT_SEARCH_BABBAGE_QUERY = \"text-search-babbage-query-001\"\n    TEXT_SEARCH_BABBAGE_DOC = \"text-search-babbage-doc-001\"\n\n    # ada\n    TEXT_SIMILARITY_ADA = \"text-similarity-ada-001\"\n    TEXT_SEARCH_ADA_QUERY = \"text-search-ada-query-001\"\n    TEXT_SEARCH_ADA_DOC = \"text-search-ada-doc-001\"\n\n    # text-embedding-ada-002\n    TEXT_EMBED_ADA_002 = \"text-embedding-ada-002\"\n\n\n# convenient shorthand\nOAEM = OpenAIEmbeddingMode\nOAEMT = OpenAIEmbeddingModelType\nOAEMM = OpenAIEmbeddingModeModel\n\nEMBED_MAX_TOKEN_LIMIT = 2048\n\n\n_QUERY_MODE_MODEL_DICT = {\n    (OAEM.SIMILARITY_MODE, \"davinci\"): OAEMM.TEXT_SIMILARITY_DAVINCI,\n    (OAEM.SIMILARITY_MODE, \"curie\"): OAEMM.TEXT_SIMILARITY_CURIE,\n    (OAEM.SIMILARITY_MODE, \"babbage\"): OAEMM.TEXT_SIMILARITY_BABBAGE,\n    (OAEM.SIMILARITY_MODE, \"ada\"): OAEMM.TEXT_SIMILARITY_ADA,\n    (OAEM.SIMILARITY_MODE, \"text-embedding-ada-002\"): OAEMM.TEXT_EMBED_ADA_002,\n    (OAEM.TEXT_SEARCH_MODE, \"davinci\"): OAEMM.TEXT_SEARCH_DAVINCI_QUERY,\n    (OAEM.TEXT_SEARCH_MODE, \"curie\"): OAEMM.TEXT_SEARCH_CURIE_QUERY,\n    (OAEM.TEXT_SEARCH_MODE, \"babbage\"): OAEMM.TEXT_SEARCH_BABBAGE_QUERY,\n    (OAEM.TEXT_SEARCH_MODE, \"ada\"): OAEMM.TEXT_SEARCH_ADA_QUERY,\n    (OAEM.TEXT_SEARCH_MODE, \"text-embedding-ada-002\"): OAEMM.TEXT_EMBED_ADA_002,\n}\n\n_TEXT_MODE_MODEL_DICT = {\n    (OAEM.SIMILARITY_MODE, \"davinci\"): OAEMM.TEXT_SIMILARITY_DAVINCI,\n    (OAEM.SIMILARITY_MODE, \"curie\"): OAEMM.TEXT_SIMILARITY_CURIE,\n    (OAEM.SIMILARITY_MODE, \"babbage\"): OAEMM.TEXT_SIMILARITY_BABBAGE,\n    (OAEM.SIMILARITY_MODE, \"ada\"): OAEMM.TEXT_SIMILARITY_ADA,\n    (OAEM.SIMILARITY_MODE, \"text-embedding-ada-002\"): OAEMM.TEXT_EMBED_ADA_002,\n    (OAEM.TEXT_SEARCH_MODE, \"davinci\"): OAEMM.TEXT_SEARCH_DAVINCI_DOC,\n    (OAEM.TEXT_SEARCH_MODE, \"curie\"): OAEMM.TEXT_SEARCH_CURIE_DOC,\n    (OAEM.TEXT_SEARCH_MODE, \"babbage\"): OAEMM.TEXT_SEARCH_BABBAGE_DOC,\n    (OAEM.TEXT_SEARCH_MODE, \"ada\"): OAEMM.TEXT_SEARCH_ADA_DOC,\n    (OAEM.TEXT_SEARCH_MODE, \"text-embedding-ada-002\"): OAEMM.TEXT_EMBED_ADA_002,\n}\n\n\n@retry(wait=wait_random_exponential(min=20, max=60), stop=stop_after_attempt(100))\ndef get_embedding(\n    text: str,\n    engine: Optional[str] = None,\n) -> List[float]:\n    \"\"\"Get embedding.\n\n    NOTE: Copied from OpenAI's embedding utils:\n    https://github.com/openai/openai-python/blob/main/openai/embeddings_utils.py\n\n    Copied here to avoid importing unnecessary dependencies\n    like matplotlib, plotly, scipy, sklearn.\n\n    \"\"\"\n    text = text.replace(\"\\n\", \" \")\n    return openai.Embedding.create(input=[text], engine=engine)[\"data\"][0][\"embedding\"]\n\n\nclass OpenAIEmbedding(BaseEmbedding):\n    \"\"\"OpenAI class for embeddings.\n\n    Args:\n        mode (str): Mode for embedding.\n            Defaults to OpenAIEmbeddingMode.TEXT_SEARCH_MODE.\n            Options are:\n\n            - OpenAIEmbeddingMode.SIMILARITY_MODE\n            - OpenAIEmbeddingMode.TEXT_SEARCH_MODE\n\n        model (str): Model for embedding.\n            Defaults to OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002.\n            Options are:\n\n            - OpenAIEmbeddingModelType.DAVINCI\n            - OpenAIEmbeddingModelType.CURIE\n            - OpenAIEmbeddingModelType.BABBAGE\n            - OpenAIEmbeddingModelType.ADA\n            - OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002\n\n        deployment_name (Optional[str]): Optional deployment of model. Defaults to None.\n            If this value is not None, mode and model will be ignored.\n            Only available for using AzureOpenAI.\n    \"\"\"\n\n    def __init__(\n        self,\n        mode: str = OpenAIEmbeddingMode.TEXT_SEARCH_MODE,\n        model: str = OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002,\n        deployment_name: Optional[str] = None,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__()\n        self.mode = OpenAIEmbeddingMode(mode)\n        self.model = OpenAIEmbeddingModelType(model)\n        self.deployment_name = deployment_name\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding.\"\"\"\n        if self.deployment_name is not None:\n            engine = self.deployment_name\n        else:\n            key = (self.mode, self.model)\n            if key not in _QUERY_MODE_MODEL_DICT:\n                raise ValueError(f\"Invalid mode, model combination: {key}\")\n            engine = _QUERY_MODE_MODEL_DICT[key]\n        return get_embedding(query, engine=engine)\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get text embedding.\"\"\"\n        if self.deployment_name is not None:\n            engine = self.deployment_name\n        else:\n            key = (self.mode, self.model)\n            if key not in _TEXT_MODE_MODEL_DICT:\n                raise ValueError(f\"Invalid mode, model combination: {key}\")\n            engine = _TEXT_MODE_MODEL_DICT[key]\n        return get_embedding(text, engine=engine)\n", "doc_id": "19f1d4072db017613451127614ed9fec18193f70", "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/openai.py", "file_name": "openai.py"}, "__type__": "Document"}, "b854a6fa5a966671ea193573c1ecc21b6ad63014": {"text": "\"\"\"Embedding utils for gpt index.\"\"\"\n\nfrom typing import List\n\n\ndef save_embedding(embedding: List[float], file_path: str) -> None:\n    \"\"\"Save embedding to file.\"\"\"\n    with open(file_path, \"w\") as f:\n        f.write(\",\".join([str(x) for x in embedding]))\n\n\ndef load_embedding(file_path: str) -> List[float]:\n    \"\"\"Load embedding from file. Will only return first embedding in file.\"\"\"\n    with open(file_path, \"r\") as f:\n        for line in f:\n            embedding = [float(x) for x in line.strip().split(\",\")]\n            break\n        return embedding\n", "doc_id": "b854a6fa5a966671ea193573c1ecc21b6ad63014", "embedding": null, "extra_info": {"file_path": "gpt_index/embeddings/utils.py", "file_name": "utils.py"}, "__type__": "Document"}, "e7bbbf6f654fcec74653151ee1feeea4b388c65b": {"text": "\"\"\"GPT Index data structures.\"\"\"\n\n# indices\nfrom gpt_index.indices.keyword_table.base import GPTKeywordTableIndex\nfrom gpt_index.indices.keyword_table.rake_base import GPTRAKEKeywordTableIndex\nfrom gpt_index.indices.keyword_table.simple_base import GPTSimpleKeywordTableIndex\nfrom gpt_index.indices.list.base import GPTListIndex\nfrom gpt_index.indices.tree.base import GPTTreeIndex\nfrom gpt_index.indices.vector_store.faiss import GPTFaissIndex\n\n__all__ = [\n    \"GPTKeywordTableIndex\",\n    \"GPTSimpleKeywordTableIndex\",\n    \"GPTRAKEKeywordTableIndex\",\n    \"GPTListIndex\",\n    \"GPTTreeIndex\",\n    \"GPTFaissIndex\",\n]\n", "doc_id": "e7bbbf6f654fcec74653151ee1feeea4b388c65b", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "2dec2853fc1bc65bc09aec5679d40f3cd702a74a": {"text": "\"\"\"Base index classes.\"\"\"\nimport json\nimport logging\nfrom abc import abstractmethod\nfrom typing import (\n    Any,\n    Dict,\n    Generic,\n    List,\n    Optional,\n    Sequence,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom gpt_index.data_structs.data_structs import IndexStruct, Node\nfrom gpt_index.docstore import DOC_TYPE, DocumentStore\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.embeddings.openai import OpenAIEmbedding\nfrom gpt_index.indices.node_utils import get_nodes_from_document\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.query_runner import QueryRunner\nfrom gpt_index.indices.query.schema import QueryConfig, QueryMode\nfrom gpt_index.indices.registry import IndexRegistry\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.readers.schema.base import Document\nfrom gpt_index.response.schema import Response\nfrom gpt_index.schema import BaseDocument\nfrom gpt_index.token_counter.token_counter import llm_token_counter\n\nIS = TypeVar(\"IS\", bound=IndexStruct)\n\n\nDOCUMENTS_INPUT = Union[BaseDocument, \"BaseGPTIndex\"]\n\n\nclass BaseGPTIndex(Generic[IS]):\n    \"\"\"Base GPT Index.\n\n    Args:\n        documents (Optional[Sequence[BaseDocument]]): List of documents to\n            build the index from.\n        llm_predictor (LLMPredictor): Optional LLMPredictor object. If not provided,\n            will use the default LLMPredictor (text-davinci-003)\n        prompt_helper (PromptHelper): Optional PromptHelper object. If not provided,\n            will use the default PromptHelper.\n        chunk_size_limit (Optional[int]): Optional chunk size limit. If not provided,\n            will use the default chunk size limit (4096 max input size).\n        include_extra_info (bool): Optional bool. If True, extra info (i.e. metadata)\n            of each Document will be prepended to its text to help with queries.\n            Default is True.\n\n    \"\"\"\n\n    index_struct_cls: Type[IS]\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[IS] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        docstore: Optional[DocumentStore] = None,\n        index_registry: Optional[IndexRegistry] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        chunk_size_limit: Optional[int] = None,\n        include_extra_info: bool = True,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        if index_struct is None and documents is None:\n            raise ValueError(\"One of documents or index_struct must be provided.\")\n        if index_struct is not None and documents is not None:\n            raise ValueError(\"Only one of documents or index_struct can be provided.\")\n\n        self._llm_predictor = llm_predictor or LLMPredictor()\n        # NOTE: the embed_model isn't used in all indices\n        self._embed_model = embed_model or OpenAIEmbedding()\n        self._include_extra_info = include_extra_info\n\n        # TODO: move out of base if we need custom params per index\n        self._prompt_helper = prompt_helper or PromptHelper.from_llm_predictor(\n            self._llm_predictor, chunk_size_limit=chunk_size_limit\n        )\n\n        # build index struct in the init function\n        self._docstore = docstore or DocumentStore()\n        self._index_registry = index_registry or IndexRegistry()\n\n        if index_struct is not None:\n            if not isinstance(index_struct, self.index_struct_cls):\n                raise ValueError(\n                    f\"index_struct must be of type {self.index_struct_cls}\"\n                )\n            self._index_struct = index_struct\n        else:\n            documents = cast(Sequence[DOCUMENTS_INPUT], documents)\n            documents = self._process_documents(\n                documents, self._docstore, self._index_registry\n            )\n            self._validate_documents(documents)\n            # TODO: introduce document store outside __init__ function\n            self._index_struct = self.build_index_from_documents(documents)\n        # update index registry and docstore with index_struct\n        self._update_index_registry_and_docstore()\n\n    @property\n    def prompt_helper(self) -> PromptHelper:\n        \"\"\"Get the prompt helper corresponding to the index.\"\"\"\n        return self._prompt_helper\n\n    @property\n    def docstore(self) -> DocumentStore:\n        \"\"\"Get the docstore corresponding to the index.\"\"\"\n        return self._docstore\n\n    @property\n    def index_registry(self) -> IndexRegistry:\n        \"\"\"Get the index registry corresponding to the index.\"\"\"\n        return self._index_registry\n\n    @property\n    def llm_predictor(self) -> LLMPredictor:\n        \"\"\"Get the llm predictor.\"\"\"\n        return self._llm_predictor\n\n    @property\n    def embed_model(self) -> BaseEmbedding:\n        \"\"\"Get the llm predictor.\"\"\"\n        return self._embed_model\n\n    def _update_index_registry_and_docstore(self) -> None:\n        \"\"\"Update index registry and docstore.\"\"\"\n        # update index registry with current struct\n        cur_type = self.index_struct_cls.get_type()\n        self._index_registry.type_to_struct[cur_type] = self.index_struct_cls\n        self._index_registry.type_to_query[cur_type] = self.get_query_map()\n\n        # update docstore with current struct\n        self._docstore.add_documents([self.index_struct])\n\n    def _process_documents(\n        self,\n        documents: Sequence[DOCUMENTS_INPUT],\n        docstore: DocumentStore,\n        index_registry: IndexRegistry,\n    ) -> List[BaseDocument]:\n        \"\"\"Process documents.\"\"\"\n        results: List[DOC_TYPE] = []\n        for doc in documents:\n            if isinstance(doc, BaseGPTIndex):\n                # if user passed in another index, we need to do the following:\n                # - update docstore with the docstore in the index\n                # - validate that the index is in the docstore\n                # - update the index registry\n\n                index_registry.update(doc.index_registry)\n                docstore.update_docstore(doc.docstore)\n                # assert that the doc exists within the docstore\n                sub_index_struct = doc.index_struct_with_text\n                if not docstore.document_exists(sub_index_struct.get_doc_id()):\n                    raise ValueError(\n                        \"The index struct of the sub-index must exist in the docstore. \"\n                        f\"Invalid doc ID: {sub_index_struct.get_doc_id()}\"\n                    )\n                results.append(sub_index_struct)\n            elif isinstance(doc, (Document, IndexStruct)):\n                results.append(doc)\n                # update docstore\n                docstore.add_documents([doc])\n            else:\n                raise ValueError(f\"Invalid document type: {type(doc)}.\")\n        return cast(List[BaseDocument], results)\n\n    def _validate_documents(self, documents: Sequence[BaseDocument]) -> None:\n        \"\"\"Validate documents.\"\"\"\n        for doc in documents:\n            if not isinstance(doc, BaseDocument):\n                raise ValueError(\"Documents must be of type BaseDocument.\")\n\n    @property\n    def index_struct(self) -> IS:\n        \"\"\"Get the index struct.\"\"\"\n        return self._index_struct\n\n    @property\n    def index_struct_with_text(self) -> IS:\n        \"\"\"Get the index struct with text.\n\n        If text not set, raise an error.\n        For use when composing indices with other indices.\n\n        \"\"\"\n        # make sure that we generate text for index struct\n        if self._index_struct.text is None:\n            # NOTE: set text to be empty string for now\n            raise ValueError(\n                \"Index must have text property set in order \"\n                \"to be composed with other indices. \"\n                \"In order to set text, please run `index.set_text()`.\"\n            )\n        return self._index_struct\n\n    def set_text(self, text: str) -> None:\n        \"\"\"Set summary text for index struct.\n\n        This allows index_struct_with_text to be used to compose indices\n        with other indices.\n\n        \"\"\"\n        self._index_struct.text = text\n\n    def set_extra_info(self, extra_info: Dict[str, Any]) -> None:\n        \"\"\"Set extra info (metadata) for index struct.\n\n        If this index is used as a subindex for a parent index, the metadata\n        will be propagated to all nodes derived from this subindex, in the\n        parent index.\n\n        \"\"\"\n        self._index_struct.extra_info = extra_info\n\n    def set_doc_id(self, doc_id: str) -> None:\n        \"\"\"Set doc_id for index struct.\n\n        This is used to uniquely identify the index struct in the docstore.\n        If you wish to delete the index struct, you can use this doc_id.\n\n        \"\"\"\n        old_doc_id = self._index_struct.get_doc_id()\n        self._index_struct.doc_id = doc_id\n        # Note: we also need to delete old doc_id, and update docstore\n        self._docstore.delete_document(old_doc_id)\n        self._docstore.add_documents([self._index_struct])\n\n    def get_doc_id(self) -> str:\n        \"\"\"Get doc_id for index struct.\n\n        If doc_id not set, raise an error.\n\n        \"\"\"\n        if self._index_struct.doc_id is None:\n            raise ValueError(\"Index must have doc_id property set.\")\n        return self._index_struct.doc_id\n\n    def _get_nodes_from_document(\n        self,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n        start_idx: int = 0,\n    ) -> List[Node]:\n        return get_nodes_from_document(\n            document=document,\n            text_splitter=text_splitter,\n            start_idx=start_idx,\n            include_extra_info=self._include_extra_info,\n        )\n\n    @abstractmethod\n    def _build_index_from_documents(self, documents: Sequence[BaseDocument]) -> IS:\n        \"\"\"Build the index from documents.\"\"\"\n\n    @llm_token_counter(\"build_index_from_documents\")\n    def build_index_from_documents(self, documents: Sequence[BaseDocument]) -> IS:\n        \"\"\"Build the index from documents.\"\"\"\n        return self._build_index_from_documents(documents)\n\n    @abstractmethod\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n\n    @llm_token_counter(\"insert\")\n    def insert(self, document: DOCUMENTS_INPUT, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\n\n        Args:\n            document (Union[BaseDocument, BaseGPTIndex]): document to insert\n\n        \"\"\"\n        processed_doc = self._process_documents(\n            [document], self._docstore, self._index_registry\n        )[0]\n        self._validate_documents([processed_doc])\n        self._insert(processed_doc, **insert_kwargs)\n\n    @abstractmethod\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n\n    def delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document from the index.\n\n        All nodes in the index related to the index will be deleted.\n\n        Args:\n            doc_id (str): document id\n            full_delete (bool): whether to delete the document from the docstore.\n                By default this is True.\n\n        \"\"\"\n        full_delete = delete_kwargs.pop(\"full_delete\", True)\n        logging.debug(f\"> Deleting document: {doc_id}\")\n        if full_delete:\n            self._docstore.delete_document(doc_id)\n        self._delete(doc_id, **delete_kwargs)\n\n    def update(self, document: DOCUMENTS_INPUT, **update_kwargs: Any) -> None:\n        \"\"\"Update a document.\n\n        This is equivalent to deleting the document and then inserting it again.\n\n        Args:\n            document (Union[BaseDocument, BaseGPTIndex]): document to update\n            insert_kwargs (Dict): kwargs to pass to insert\n            delete_kwargs (Dict): kwargs to pass to delete\n\n        \"\"\"\n        self.delete(document.get_doc_id(), **update_kwargs.pop(\"delete_kwargs\", {}))\n        self.insert(document, **update_kwargs.pop(\"insert_kwargs\", {}))\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Dict) -> None:\n        \"\"\"Preprocess query.\n\n        This allows subclasses to pass in additional query kwargs\n        to query, for instance arguments that are shared between the\n        index and the query class. By default, this does nothing.\n        This also allows subclasses to do validation.\n\n        \"\"\"\n        pass\n\n    def query(\n        self,\n        query_str: str,\n        mode: str = QueryMode.DEFAULT,\n        **query_kwargs: Any,\n    ) -> Response:\n        \"\"\"Answer a query.\n\n        When `query` is called, we query the index with the given `mode` and\n        `query_kwargs`. The `mode` determines the type of query to run, and\n        `query_kwargs` are parameters that are specific to the query type.\n\n        For a comprehensive documentation of available `mode` and `query_kwargs` to\n        query a given index, please visit :ref:`Ref-Query`.\n\n\n        \"\"\"\n        mode_enum = QueryMode(mode)\n        if mode_enum == QueryMode.RECURSIVE:\n            # TODO: deprecated, use ComposableGraph instead.\n            if \"query_configs\" not in query_kwargs:\n                raise ValueError(\"query_configs must be provided for recursive mode.\")\n            query_configs = query_kwargs[\"query_configs\"]\n            query_runner = QueryRunner(\n                self._llm_predictor,\n                self._prompt_helper,\n                self._embed_model,\n                self._docstore,\n                self._index_registry,\n                query_configs=query_configs,\n                recursive=True,\n            )\n            return query_runner.query(query_str, self._index_struct)\n        else:\n            self._preprocess_query(mode_enum, query_kwargs)\n            # TODO: pass in query config directly\n            query_config = QueryConfig(\n                index_struct_type=self._index_struct.get_type(),\n                query_mode=mode_enum,\n                query_kwargs=query_kwargs,\n            )\n            query_runner = QueryRunner(\n                self._llm_predictor,\n                self._prompt_helper,\n                self._embed_model,\n                self._docstore,\n                self._index_registry,\n                query_configs=[query_config],\n                recursive=False,\n            )\n            return query_runner.query(query_str, self._index_struct)\n\n    @classmethod\n    @abstractmethod\n    def get_query_map(cls) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n\n    @classmethod\n    def load_from_string(cls, index_string: str, **kwargs: Any) -> \"BaseGPTIndex\":\n        \"\"\"Load index from string (in JSON-format).\n\n        This method loads the index from a JSON string. The index data\n        structure itself is preserved completely. If the index is defined over\n        subindices, those subindices will also be preserved (and subindices of\n        those subindices, etc.).\n\n        NOTE: load_from_string should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_string` and `load_from_string` on that instead.\n\n        Args:\n            index_string (str): The index string (in JSON-format).\n\n        Returns:\n            BaseGPTIndex: The loaded index.\n\n        \"\"\"\n        result_dict = json.loads(index_string)\n        index_struct = cls.index_struct_cls.from_dict(result_dict[\"index_struct\"])\n        type_to_struct = {index_struct.get_type(): type(index_struct)}\n        docstore = DocumentStore.load_from_dict(\n            result_dict[\"docstore\"],\n            type_to_struct=type_to_struct,\n        )\n        return cls(index_struct=index_struct, docstore=docstore, **kwargs)\n\n    @classmethod\n    def load_from_disk(cls, save_path: str, **kwargs: Any) -> \"BaseGPTIndex\":\n        \"\"\"Load index from disk.\n\n        This method loads the index from a JSON file stored on disk. The index data\n        structure itself is preserved completely. If the index is defined over\n        subindices, those subindices will also be preserved (and subindices of\n        those subindices, etc.).\n\n        NOTE: load_from_disk should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_disk` and `load_from_disk` on that instead.\n\n        Args:\n            save_path (str): The save_path of the file.\n\n        Returns:\n            BaseGPTIndex: The loaded index.\n\n        \"\"\"\n        with open(save_path, \"r\") as f:\n            file_contents = f.read()\n            return cls.load_from_string(file_contents, **kwargs)\n\n    def save_to_string(self, **save_kwargs: Any) -> str:\n        \"\"\"Save to string.\n\n        This method stores the index into a JSON string.\n\n        NOTE: save_to_string should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_string` and `load_from_string` on that instead.\n\n        Returns:\n            str: The JSON string of the index.\n\n        \"\"\"\n        if self.docstore.contains_index_struct(\n            exclude_ids=[self.index_struct.get_doc_id()]\n        ):\n            raise ValueError(\n                \"Cannot call `save_to_string` on index if index is composed on top of \"\n                \"other indices. Please define a `ComposableGraph` and use \"\n                \"`save_to_string` and `load_from_string` on that instead.\"\n            )\n        out_dict: Dict[str, dict] = {\n            \"index_struct\": self.index_struct.to_dict(),\n            \"docstore\": self.docstore.serialize_to_dict(),\n        }\n        return json.dumps(out_dict, **save_kwargs)\n\n    def save_to_disk(self, save_path: str, **save_kwargs: Any) -> None:\n        \"\"\"Save to file.\n\n        This method stores the index into a JSON file stored on disk.\n\n        NOTE: save_to_disk should not be used for indices composed on top\n        of other indices. Please define a `ComposableGraph` and use\n        `save_to_disk` and `load_from_disk` on that instead.\n\n        Args:\n            save_path (str): The save_path of the file.\n\n        \"\"\"\n        index_string = self.save_to_string(**save_kwargs)\n        with open(save_path, \"w\") as f:\n            f.write(index_string)\n", "doc_id": "2dec2853fc1bc65bc09aec5679d40f3cd702a74a", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/base.py", "file_name": "base.py"}, "__type__": "Document"}, "c637335013c599b07de054fba07b47ecb86ad3e8": {"text": "\"\"\"Init params.\"\"\"\n", "doc_id": "c637335013c599b07de054fba07b47ecb86ad3e8", "embedding": null, "extra_info": {"file_path": "tests/langchain_helpers/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "c708cd82227bc6841cfe95d87f823b3237a44e67": {"text": "\"\"\"Common classes for structured operations.\"\"\"\n\nfrom typing import Dict, List, Optional, Sequence\n\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.response.builder import ResponseBuilder, TextChunk\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.sql_wrapper import SQLDatabase\nfrom gpt_index.prompts.default_prompts import (\n    DEFAULT_REFINE_TABLE_CONTEXT_PROMPT,\n    DEFAULT_TABLE_CONTEXT_PROMPT,\n    DEFAULT_TABLE_CONTEXT_QUERY,\n)\nfrom gpt_index.prompts.prompts import (\n    QuestionAnswerPrompt,\n    RefinePrompt,\n    RefineTableContextPrompt,\n    TableContextPrompt,\n)\nfrom gpt_index.schema import BaseDocument\n\n\nclass SQLContextBuilder:\n    \"\"\"Builder that builds context for a given set of SQL tables.\n\n    Args:\n        sql_database (Optional[SQLDatabase]): SQL database to use,\n        context_builder_prompt (Optional[TableContextPrompt]): A\n            Table Context Prompt (see :ref:`Prompt-Templates`).\n    \"\"\"\n\n    def __init__(\n        self,\n        sql_database: SQLDatabase,\n        llm_predictor: Optional[LLMPredictor] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        table_context_prompt: Optional[TableContextPrompt] = None,\n        refine_table_context_prompt: Optional[RefineTableContextPrompt] = None,\n        table_context_task: Optional[str] = None,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        # TODO: take in an entire index instead of forming a response builder\n        if sql_database is None:\n            raise ValueError(\"sql_database must be provided.\")\n        self._sql_database = sql_database\n        self._llm_predictor = llm_predictor or LLMPredictor()\n        self._prompt_helper = prompt_helper or PromptHelper.from_llm_predictor(\n            self._llm_predictor\n        )\n        self._table_context_prompt = (\n            table_context_prompt or DEFAULT_TABLE_CONTEXT_PROMPT\n        )\n        self._refine_table_context_prompt = (\n            refine_table_context_prompt or DEFAULT_REFINE_TABLE_CONTEXT_PROMPT\n        )\n        self._table_context_task = table_context_task or DEFAULT_TABLE_CONTEXT_QUERY\n\n    def build_all_context_from_documents(\n        self,\n        documents_dict: Dict[str, List[BaseDocument]],\n    ) -> Dict[str, str]:\n        \"\"\"Build context for all tables in the database.\"\"\"\n        context_dict = {}\n        for table_name in self._sql_database.get_table_names():\n            context_dict[table_name] = self.build_table_context_from_documents(\n                documents_dict[table_name], table_name\n            )\n        return context_dict\n\n    def build_table_context_from_documents(\n        self,\n        documents: Sequence[BaseDocument],\n        table_name: str,\n    ) -> str:\n        \"\"\"Build context from documents for a single table.\"\"\"\n        schema = self._sql_database.get_single_table_info(table_name)\n        prompt_with_schema = QuestionAnswerPrompt.from_prompt(\n            self._table_context_prompt.partial_format(schema=schema)\n        )\n        refine_prompt_with_schema = RefinePrompt.from_prompt(\n            self._refine_table_context_prompt.partial_format(schema=schema)\n        )\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            prompt_with_schema, 1\n        )\n        # we use the ResponseBuilder to iteratively go through all texts\n        response_builder = ResponseBuilder(\n            self._prompt_helper,\n            self._llm_predictor,\n            prompt_with_schema,\n            refine_prompt_with_schema,\n        )\n        for doc in documents:\n            text_chunks = text_splitter.split_text(doc.get_text())\n            for text_chunk in text_chunks:\n                response_builder.add_text_chunks([TextChunk(text_chunk)])\n\n        # feed in the \"query_str\" or the task\n        table_context = response_builder.get_response(self._table_context_task)\n        return table_context\n", "doc_id": "c708cd82227bc6841cfe95d87f823b3237a44e67", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/common/struct_store/base.py", "file_name": "base.py"}, "__type__": "Document"}, "cb36bdd5d0c7925d04e4e3fcb9363e168384f0ef": {"text": "\"\"\"Common classes/functions for tree index operations.\"\"\"\n\n\nimport logging\nfrom typing import Dict, Sequence\n\nfrom gpt_index.data_structs.data_structs import IndexGraph, Node\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.utils import get_sorted_node_list, truncate_text\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.prompts import SummaryPrompt\nfrom gpt_index.schema import BaseDocument\n\n\nclass GPTTreeIndexBuilder:\n    \"\"\"GPT tree index builder.\n\n    Helper class to build the tree-structured index,\n    or to synthesize an answer.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        num_children: int,\n        summary_prompt: SummaryPrompt,\n        llm_predictor: LLMPredictor,\n        prompt_helper: PromptHelper,\n    ) -> None:\n        \"\"\"Initialize with params.\"\"\"\n        if num_children < 2:\n            raise ValueError(\"Invalid number of children.\")\n        self.num_children = num_children\n        self.summary_prompt = summary_prompt\n        self._llm_predictor = llm_predictor\n        self._prompt_helper = prompt_helper\n\n    def _get_nodes_from_document(\n        self, start_idx: int, document: BaseDocument\n    ) -> Dict[int, Node]:\n        \"\"\"Add document to index.\"\"\"\n        # NOTE: summary prompt does not need to be partially formatted\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.summary_prompt, self.num_children\n        )\n        text_chunks = text_splitter.split_text(\n            document.get_text(), extra_info_str=document.extra_info_str\n        )\n        doc_nodes = {\n            (start_idx + i): Node(\n                text=t,\n                index=(start_idx + i),\n                ref_doc_id=document.get_doc_id(),\n                embedding=document.embedding,\n                extra_info=document.extra_info,\n            )\n            for i, t in enumerate(text_chunks)\n        }\n        return doc_nodes\n\n    def build_from_text(\n        self,\n        documents: Sequence[BaseDocument],\n        build_tree: bool = True,\n    ) -> IndexGraph:\n        \"\"\"Build from text.\n\n        Returns:\n            IndexGraph: graph object consisting of all_nodes, root_nodes\n\n        \"\"\"\n        all_nodes: Dict[int, Node] = {}\n        for d in documents:\n            all_nodes.update(self._get_nodes_from_document(len(all_nodes), d))\n\n        if build_tree:\n            # instantiate all_nodes from initial text chunks\n            root_nodes = self.build_index_from_nodes(all_nodes, all_nodes)\n        else:\n            # if build_tree is False, then don't surface any root nodes\n            root_nodes = {}\n        return IndexGraph(all_nodes=all_nodes, root_nodes=root_nodes)\n\n    def build_index_from_nodes(\n        self,\n        cur_nodes: Dict[int, Node],\n        all_nodes: Dict[int, Node],\n    ) -> Dict[int, Node]:\n        \"\"\"Consolidates chunks recursively, in a bottoms-up fashion.\"\"\"\n        cur_node_list = get_sorted_node_list(cur_nodes)\n        cur_index = len(all_nodes)\n        new_node_dict = {}\n        logging.info(\n            f\"> Building index from nodes: {len(cur_nodes) // self.num_children} chunks\"\n        )\n        for i in range(0, len(cur_node_list), self.num_children):\n            cur_nodes_chunk = cur_node_list[i : i + self.num_children]\n            text_chunk = self._prompt_helper.get_text_from_nodes(\n                cur_nodes_chunk, prompt=self.summary_prompt\n            )\n\n            new_summary, _ = self._llm_predictor.predict(\n                self.summary_prompt, context_str=text_chunk\n            )\n\n            logging.debug(\n                f\"> {i}/{len(cur_nodes)}, summary: {truncate_text(new_summary, 50)}\"\n            )\n            new_node = Node(\n                text=new_summary,\n                index=cur_index,\n                child_indices={n.index for n in cur_nodes_chunk},\n            )\n            new_node_dict[cur_index] = new_node\n            cur_index += 1\n\n        all_nodes.update(new_node_dict)\n\n        if len(new_node_dict) <= self.num_children:\n            return new_node_dict\n        else:\n            return self.build_index_from_nodes(new_node_dict, all_nodes)\n", "doc_id": "cb36bdd5d0c7925d04e4e3fcb9363e168384f0ef", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/common/tree/base.py", "file_name": "base.py"}, "__type__": "Document"}, "4c51f91a5611ecfd8531f01a74baf77f9f2484cf": {"text": "\n\n\ud83d\udd11 GPTKeywordIndex\n\nGPTKeywordIndex is a keyword-based table data structure (inspired by \"hash tables\").\n\n\n\n\n\nIndex Construction\n\nDuring index construction, GPTKeywordIndex first takes in a dataset of text documents as input, and chunks them up into smaller document chunks. For each text chunk, GPTKeywordIndex uses GPT to extract a set of relevant keywords with a **keyword extraction prompt**. (keywords can include short phrases, like \"new york city\"). These keywords are then stored in a table, referencing the same text chunk.\n\n\n\n\n\nQuery\n\nThere are three query modes: `default`, `simple`, and `rake`.\n\n**Default**\n\nDuring query-time, the GPTKeywordIndex extracts a set of relevant keywords from the query using a customized variant of the same **keyword extraction prompt**. These keywords are then used to fetch the set of candidate text chunk ID's. The text chunk ID's are ordered by number of matching keywords (from highest to lowest), and truncated after a cutoff $d$, which represents the maximum number of text chunks to consider.\n\nWe construct an answer using the _create and refine_ paradigm. An initial answer to the query is constructed using the first text chunk. The answer is then _refined_ through feeding in subsequent text chunks as context. Refinement could mean keeping the original answer, making small edits to the original answer, or rewriting the original answer completely.\n\n**Simple (Regex)**\nInstead of using GPT for keyword extraction, this mode uses a simple regex query to find words, filtering out stopwords.\n\n**RAKE**\nUse the popular RAKE keyword extractor.\n\n\n\n\n\nUsage\n\n```python\nfrom gpt_index import GPTKeywordTableIndex, SimpleDirectoryReader\n\n\n\n\n\nbuild index\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = GPTKeywordTableIndex(documents)\n\n\n\n\nsave index\nindex.save_to_disk('index_table.json')\n\n\n\n\nload index from disk\nindex = GPTKeywordTableIndex.load_from_disk('index_table.json')\n\n\n\n\nquery\nresponse = index.query(\"\", mode=\"default\")\n```\n\n\n\n\n\nFAQ/Additional\n\n**Runtime**\n\nWorst-case runtime to execute a query should be $O(k*c)$, where $k$ is the number of extracted keywords, and $c$ is the number of text chunks per query.\n\nHowever the number of queries to GPT is limited by $O(d)$, where $d$ is a\nuser-specified parameter indicating the maximum number of text chunks to query.\n\n**How much does this cost to run?**\n\nAssuming `num_chunks_per_query=10`, then this equates to \\$~0.40 per query.\n\n", "doc_id": "4c51f91a5611ecfd8531f01a74baf77f9f2484cf", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/README.md", "file_name": "gpt_index/indices/keyword_table/README.md"}, "__type__": "Document"}, "43a973b9b0920e2d6a499bee82a9aee62fe2321f": {"text": "\"\"\"Keyword Table Index Data Structures.\"\"\"\n\n# indices\nfrom gpt_index.indices.keyword_table.base import GPTKeywordTableIndex\nfrom gpt_index.indices.keyword_table.rake_base import GPTRAKEKeywordTableIndex\nfrom gpt_index.indices.keyword_table.simple_base import GPTSimpleKeywordTableIndex\n\n__all__ = [\n    \"GPTKeywordTableIndex\",\n    \"GPTSimpleKeywordTableIndex\",\n    \"GPTRAKEKeywordTableIndex\",\n]\n", "doc_id": "43a973b9b0920e2d6a499bee82a9aee62fe2321f", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "71bad2125cd5052fdc9fe562d68fc8ecf47ddb51": {"text": "\"\"\"Keyword-table based index.\n\nSimilar to a \"hash table\" in concept. GPT Index first tries\nto extract keywords from the source text, and stores the\nkeywords as keys per item. It similarly extracts keywords\nfrom the query text. Then, it tries to match those keywords to\nexisting keywords in the table.\n\n\"\"\"\n\nfrom abc import abstractmethod\nfrom typing import Any, Dict, Optional, Sequence, Set, Type\n\nfrom gpt_index.data_structs.data_structs import KeywordTable\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.keyword_table.utils import extract_keywords_given_response\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.keyword_table.query import (\n    GPTKeywordTableGPTQuery,\n    GPTKeywordTableRAKEQuery,\n    GPTKeywordTableSimpleQuery,\n)\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.default_prompts import (\n    DEFAULT_KEYWORD_EXTRACT_TEMPLATE,\n    DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE,\n)\nfrom gpt_index.prompts.prompts import KeywordExtractPrompt\nfrom gpt_index.schema import BaseDocument\n\nDQKET = DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE\n\n\nclass BaseGPTKeywordTableIndex(BaseGPTIndex[KeywordTable]):\n    \"\"\"GPT Keyword Table Index.\n\n    This index extracts keywords from the text, and maps each\n    keyword to the node(s) that it corresponds to. In this sense it mimicks a\n    \"hash table\". During index construction, the keyword table is constructed\n    by extracting keywords from each node and creating an internal mapping.\n\n    During query time, the keywords are extracted from the query text, and these\n    keywords are used to index into the keyword table. The retrieved nodes\n    are then used to answer the query.\n\n    Args:\n        keyword_extract_template (Optional[KeywordExtractPrompt]): A Keyword\n            Extraction Prompt\n            (see :ref:`Prompt-Templates`).\n\n    \"\"\"\n\n    index_struct_cls = KeywordTable\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[KeywordTable] = None,\n        keyword_extract_template: Optional[KeywordExtractPrompt] = None,\n        max_keywords_per_chunk: int = 10,\n        llm_predictor: Optional[LLMPredictor] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        # need to set parameters before building index in base class.\n        self.max_keywords_per_chunk = max_keywords_per_chunk\n        self.keyword_extract_template = (\n            keyword_extract_template or DEFAULT_KEYWORD_EXTRACT_TEMPLATE\n        )\n        # NOTE: Partially format keyword extract template here.\n        self.keyword_extract_template = self.keyword_extract_template.partial_format(\n            max_keywords=self.max_keywords_per_chunk\n        )\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            **kwargs,\n        )\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.keyword_extract_template, 1\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTKeywordTableGPTQuery,\n            QueryMode.SIMPLE: GPTKeywordTableSimpleQuery,\n            QueryMode.RAKE: GPTKeywordTableRAKEQuery,\n        }\n\n    @abstractmethod\n    def _extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n\n    def _build_index_from_documents(\n        self, documents: Sequence[BaseDocument]\n    ) -> KeywordTable:\n        \"\"\"Build the index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.keyword_extract_template, 1\n        )\n        # do simple concatenation\n        index_struct = KeywordTable(table={})\n        for d in documents:\n            nodes = self._get_nodes_from_document(d, text_splitter)\n            for n in nodes:\n                keywords = self._extract_keywords(n.get_text())\n                index_struct.add_node(list(keywords), n)\n\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        nodes = self._get_nodes_from_document(document, self._text_splitter)\n        for n in nodes:\n            keywords = self._extract_keywords(n.get_text())\n            self._index_struct.add_node(list(keywords), n)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        # get set of ids that correspond to node\n        node_idxs_to_delete = set()\n        for node_idx, node in self._index_struct.text_chunks.items():\n            if node.ref_doc_id != doc_id:\n                continue\n            node_idxs_to_delete.add(node_idx)\n        for node_idx in node_idxs_to_delete:\n            del self._index_struct.text_chunks[node_idx]\n\n        # delete node_idxs from keyword to node idxs mapping\n        keywords_to_delete = set()\n        for keyword, node_idxs in self._index_struct.table.items():\n            if node_idxs_to_delete.intersection(node_idxs):\n                self._index_struct.table[keyword] = node_idxs.difference(\n                    node_idxs_to_delete\n                )\n                if not self._index_struct.table[keyword]:\n                    keywords_to_delete.add(keyword)\n\n        for keyword in keywords_to_delete:\n            del self._index_struct.table[keyword]\n\n\nclass GPTKeywordTableIndex(BaseGPTKeywordTableIndex):\n    \"\"\"GPT Keyword Table Index.\n\n    This index uses a GPT model to extract keywords from the text.\n\n    \"\"\"\n\n    def _extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n        response, _ = self._llm_predictor.predict(\n            self.keyword_extract_template,\n            text=text,\n        )\n        keywords = extract_keywords_given_response(response, start_token=\"KEYWORDS:\")\n        return keywords\n", "doc_id": "71bad2125cd5052fdc9fe562d68fc8ecf47ddb51", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/base.py", "file_name": "base.py"}, "__type__": "Document"}, "61b26ce6dc58e56ef751b4db489efa42582a4dc6": {"text": "\"\"\"RAKE keyword-table based index.\n\nSimilar to GPTKeywordTableIndex, but uses RAKE instead of GPT.\n\n\"\"\"\n\nfrom typing import Set\n\nfrom gpt_index.indices.keyword_table.base import BaseGPTKeywordTableIndex\nfrom gpt_index.indices.keyword_table.utils import rake_extract_keywords\n\n\nclass GPTRAKEKeywordTableIndex(BaseGPTKeywordTableIndex):\n    \"\"\"GPT RAKE Keyword Table Index.\n\n    This index uses a RAKE keyword extractor to extract keywords from the text.\n\n    \"\"\"\n\n    def _extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n        return rake_extract_keywords(text, max_keywords=self.max_keywords_per_chunk)\n", "doc_id": "61b26ce6dc58e56ef751b4db489efa42582a4dc6", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/rake_base.py", "file_name": "rake_base.py"}, "__type__": "Document"}, "4d542c38da0d37d7504500a56ee9473f24f51eff": {"text": "\"\"\"Simple keyword-table based index.\n\nSimilar to GPTKeywordTableIndex, but uses a simpler keyword extraction\ntechnique that doesn't involve GPT - just uses regex.\n\n\"\"\"\n\nfrom typing import Set\n\nfrom gpt_index.indices.keyword_table.base import BaseGPTKeywordTableIndex\nfrom gpt_index.indices.keyword_table.utils import simple_extract_keywords\nfrom gpt_index.prompts.default_prompts import DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE\n\nDQKET = DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE\n\n\nclass GPTSimpleKeywordTableIndex(BaseGPTKeywordTableIndex):\n    \"\"\"GPT Simple Keyword Table Index.\n\n    This index uses a simple regex extractor to extract keywords from the text.\n\n    \"\"\"\n\n    def _extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n        return simple_extract_keywords(text, self.max_keywords_per_chunk)\n", "doc_id": "4d542c38da0d37d7504500a56ee9473f24f51eff", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/simple_base.py", "file_name": "simple_base.py"}, "__type__": "Document"}, "4afd893ac94d8ef62581ff3fdcf0ca7800e773ac": {"text": "\"\"\"Utils for keyword table.\"\"\"\n\nimport re\nfrom typing import Optional, Set\n\nimport nltk\nimport pandas as pd\n\nfrom gpt_index.indices.utils import expand_tokens_with_subtokens\nfrom gpt_index.utils import globals_helper\n\n\ndef simple_extract_keywords(\n    text_chunk: str, max_keywords: Optional[int] = None, filter_stopwords: bool = True\n) -> Set[str]:\n    \"\"\"Extract keywords with simple algorithm.\"\"\"\n    tokens = [t.strip().lower() for t in re.findall(r\"\\w+\", text_chunk)]\n    if filter_stopwords:\n        tokens = [t for t in tokens if t not in globals_helper.stopwords]\n    value_counts = pd.Series(tokens).value_counts()\n    keywords = value_counts.index.tolist()[:max_keywords]\n    return set(keywords)\n\n\ndef rake_extract_keywords(\n    text_chunk: str,\n    max_keywords: Optional[int] = None,\n    expand_with_subtokens: bool = True,\n) -> Set[str]:\n    \"\"\"Extract keywords with RAKE.\"\"\"\n    nltk.download(\"punkt\")\n    try:\n        from rake_nltk import Rake\n    except ImportError:\n        raise ImportError(\"Please install rake_nltk: `pip install rake_nltk`\")\n\n    r = Rake()\n    r.extract_keywords_from_text(text_chunk)\n    keywords = r.get_ranked_phrases()[:max_keywords]\n    if expand_with_subtokens:\n        return set(expand_tokens_with_subtokens(keywords))\n    else:\n        return set(keywords)\n\n\ndef extract_keywords_given_response(\n    response: str, lowercase: bool = True, start_token: str = \"\"\n) -> Set[str]:\n    \"\"\"Extract keywords given the GPT-generated response.\n\n    Used by keyword table indices.\n    Parses <start_token>: <word1>, <word2>, ... into [word1, word2, ...]\n    Raises exception if response doesn't start with <start_token>\n    \"\"\"\n    results = []\n    response = response.strip()  # Strip newlines from responses.\n\n    if response.startswith(start_token):\n        response = response[len(start_token) :]\n\n    keywords = response.split(\",\")\n    for k in keywords:\n        rk = k\n        if lowercase:\n            rk = rk.lower()\n        results.append(rk.strip())\n\n    # if keyword consists of multiple words, split into subwords\n    # (removing stopwords)\n    return expand_tokens_with_subtokens(set(results))\n", "doc_id": "4afd893ac94d8ef62581ff3fdcf0ca7800e773ac", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/keyword_table/utils.py", "file_name": "utils.py"}, "__type__": "Document"}, "fe7160c00aa003afe9d8f6f6d3352f11c279c10d": {"text": "\n\n\ud83d\udd17 GPTListIndex\n\n\n\n\n\nIndex Construction\n\nGPTListIndex is a simple list-based data structure. During index construction, GPTListIndex takes in a dataset of text documents as input, chunks them up into smaller document chunks, and concatenates them into a list. GPT is not called at all during index construction.\n\n\n\n\n\nQuery\n\nDuring query-time, GPT List Index constructs an answer using the _create and refine_ paradigm. An initial answer to the query is constructed using the first text chunk. The answer is then _refined_ through feeding in subsequent text chunks as context. Refinement could mean keeping the original answer, making small edits to the original answer, or rewriting the original answer completely.\n\n**Usage**\n\n```python\nfrom gpt_index import GPTListIndex, SimpleDirectoryReader\n\n\n\n\n\nbuild index\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = GPTListIndex(documents)\n\n\n\n\nsave index\nindex.save_to_disk('index_list.json')\n\n\n\n\nload index from disk\nindex = GPTListIndex.load_from_disk('index_list.json')\n\n\n\n\nquery\nresponse = index.query(\"\")\n\n```\n\n", "doc_id": "fe7160c00aa003afe9d8f6f6d3352f11c279c10d", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/list/README.md", "file_name": "gpt_index/indices/list/README.md"}, "__type__": "Document"}, "b24c607f33df4130279f81016b5fdc2a22d19fd0": {"text": "\"\"\"List-based data structures.\"\"\"\n\nfrom gpt_index.indices.list.base import GPTListIndex\n\n__all__ = [\n    \"GPTListIndex\",\n]\n", "doc_id": "b24c607f33df4130279f81016b5fdc2a22d19fd0", "embedding": null, "extra_info": {"file_path": "tests/indices/list/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "1a2d7ad3d7362260f2d3eae8d83d06e21bd1df97": {"text": "\"\"\"List index.\n\nA simple data structure where GPT Index iterates through document chunks\nin sequence in order to answer a given query.\n\n\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence, Type\n\nfrom gpt_index.data_structs.data_structs import IndexList\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.list.embedding_query import GPTListIndexEmbeddingQuery\nfrom gpt_index.indices.query.list.query import GPTListIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.schema import BaseDocument\n\n# This query is used to summarize the contents of the index.\nGENERATE_TEXT_QUERY = \"What is a concise summary of this document?\"\n\n\nclass GPTListIndex(BaseGPTIndex[IndexList]):\n    \"\"\"GPT List Index.\n\n    The list index is a simple data structure where nodes are stored in\n    a sequence. During index construction, the document texts are\n    chunked up, converted to nodes, and stored in a list.\n\n    During query time, the list index iterates through the nodes\n    with some optional filter parameters, and synthesizes an\n    answer from all the nodes.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n\n    \"\"\"\n\n    index_struct_cls = IndexList\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[IndexList] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            **kwargs,\n        )\n        # NOTE: when building the list index, text_qa_template is not partially\n        # formatted because we don't know the query ahead of time.\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTListIndexQuery,\n            QueryMode.EMBEDDING: GPTListIndexEmbeddingQuery,\n        }\n\n    def _build_index_from_documents(\n        self, documents: Sequence[BaseDocument]\n    ) -> IndexList:\n        \"\"\"Build the index from documents.\n\n        Args:\n            documents (List[BaseDocument]): A list of documents.\n\n        Returns:\n            IndexList: The created list index.\n        \"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n        index_struct = IndexList()\n        for d in documents:\n            nodes = self._get_nodes_from_document(d, text_splitter)\n            for n in nodes:\n                index_struct.add_node(n)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        nodes = self._get_nodes_from_document(document, self._text_splitter)\n        for n in nodes:\n            self._index_struct.add_node(n)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        cur_nodes = self._index_struct.nodes\n        nodes_to_keep = [n for n in cur_nodes if n.ref_doc_id != doc_id]\n        self._index_struct.nodes = nodes_to_keep\n", "doc_id": "1a2d7ad3d7362260f2d3eae8d83d06e21bd1df97", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/list/base.py", "file_name": "base.py"}, "__type__": "Document"}, "57e0df84f5302d7788c835809895fe5ae92d7f57": {"text": "\"\"\"General node utils.\"\"\"\n\n\nimport logging\nfrom typing import List\n\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.indices.utils import truncate_text\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.schema import BaseDocument\n\n\ndef get_nodes_from_document(\n    document: BaseDocument,\n    text_splitter: TokenTextSplitter,\n    start_idx: int = 0,\n    include_extra_info: bool = True,\n) -> List[Node]:\n    \"\"\"Add document to index.\"\"\"\n    text_chunks_with_overlap = text_splitter.split_text_with_overlaps(\n        document.get_text(),\n        extra_info_str=document.extra_info_str if include_extra_info else None,\n    )\n    nodes = []\n    index_counter = 0\n    for i, text_split in enumerate(text_chunks_with_overlap):\n        text_chunk = text_split.text_chunk\n        logging.debug(f\"> Adding chunk: {truncate_text(text_chunk, 50)}\")\n        index_pos_info = {\n            # NOTE: start is inclusive, end is exclusive\n            \"start\": index_counter - text_split.num_char_overlap,\n            \"end\": index_counter - text_split.num_char_overlap + len(text_chunk),\n        }\n        index_counter += len(text_chunk) + 1\n        # if embedding specified in document, pass it to the Node\n        node = Node(\n            text=text_chunk,\n            index=start_idx + i,\n            ref_doc_id=document.get_doc_id(),\n            embedding=document.embedding,\n            extra_info=document.extra_info if include_extra_info else None,\n            node_info=index_pos_info,\n        )\n        nodes.append(node)\n    return nodes\n", "doc_id": "57e0df84f5302d7788c835809895fe5ae92d7f57", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/node_utils.py", "file_name": "node_utils.py"}, "__type__": "Document"}, "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01": {"text": "\"\"\"General prompt helper that can help deal with token limitations.\n\nThe helper can split text. It can also concatenate text from Node\nstructs but keeping token limitations in mind.\n\n\"\"\"\n\nfrom typing import Callable, List, Optional\n\nfrom gpt_index.constants import MAX_CHUNK_OVERLAP\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.utils import globals_helper\n\n\nclass PromptHelper:\n    \"\"\"Prompt helper.\n\n    This utility helps us fill in the prompt, split the text,\n    and fill in context information according to necessary token limitations.\n\n    Args:\n        max_input_size (int): Maximum input size for the LLM.\n        num_output (int): Number of outputs for the LLM.\n        max_chunk_overlap (int): Maximum chunk overlap for the LLM.\n        embedding_limit (Optional[int]): Maximum number of embeddings to use.\n        chunk_size_limit (Optional[int]): Maximum chunk size to use.\n        tokenizer (Optional[Callable[[str], List]]): Tokenizer to use.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        max_input_size: int,\n        num_output: int,\n        max_chunk_overlap: int,\n        embedding_limit: Optional[int] = None,\n        chunk_size_limit: Optional[int] = None,\n        tokenizer: Optional[Callable[[str], List]] = None,\n        separator: str = \" \",\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self.max_input_size = max_input_size\n        self.num_output = num_output\n        self.max_chunk_overlap = max_chunk_overlap\n        self.embedding_limit = embedding_limit\n        self.chunk_size_limit = chunk_size_limit\n        # TODO: make configurable\n        self._tokenizer = tokenizer or globals_helper.tokenizer\n        self._separator = separator\n        self.use_chunk_size_limit = chunk_size_limit is not None\n\n    @classmethod\n    def from_llm_predictor(\n        self,\n        llm_predictor: LLMPredictor,\n        max_chunk_overlap: Optional[int] = None,\n        embedding_limit: Optional[int] = None,\n        chunk_size_limit: Optional[int] = None,\n        tokenizer: Optional[Callable[[str], List]] = None,\n    ) -> \"PromptHelper\":\n        \"\"\"Create from llm predictor.\n\n        This will autofill values like max_input_size and num_output.\n\n        \"\"\"\n        llm_metadata = llm_predictor.get_llm_metadata()\n        max_chunk_overlap = max_chunk_overlap or min(\n            MAX_CHUNK_OVERLAP, llm_metadata.max_input_size // 10\n        )\n        return self(\n            llm_metadata.max_input_size,\n            llm_metadata.num_output,\n            max_chunk_overlap,\n            embedding_limit=embedding_limit,\n            chunk_size_limit=chunk_size_limit,\n            tokenizer=tokenizer,\n        )\n\n    def get_chunk_size_given_prompt(\n        self, prompt_text: str, num_chunks: int, padding: Optional[int] = 1\n    ) -> int:\n        \"\"\"Get chunk size making sure we can also fit the prompt in.\n\n        Chunk size is computed based on a function of the total input size,\n        the prompt length, the number of outputs, and the number of chunks.\n\n        If padding is specified, then we subtract that from the chunk size.\n        By default we assume there is a padding of 1 (for the newline between chunks).\n\n        Limit by embedding_limit and chunk_size_limit if specified.\n\n        \"\"\"\n        prompt_tokens = self._tokenizer(prompt_text)\n        num_prompt_tokens = len(prompt_tokens)\n\n        # NOTE: if embedding limit is specified, then chunk_size must not be larger than\n        # embedding_limit\n        result = (\n            self.max_input_size - num_prompt_tokens - self.num_output\n        ) // num_chunks\n        if padding is not None:\n            result -= padding\n\n        if self.embedding_limit is not None:\n            result = min(result, self.embedding_limit)\n        if self.chunk_size_limit is not None and self.use_chunk_size_limit:\n            result = min(result, self.chunk_size_limit)\n\n        return result\n\n    def _get_empty_prompt_txt(self, prompt: Prompt) -> str:\n        \"\"\"Get empty prompt text.\n\n        Substitute empty strings in parts of the prompt that have\n        not yet been filled out. Skip variables that have already\n        been partially formatted. This is used to compute the initial tokens.\n\n        \"\"\"\n        fmt_dict = {\n            v: \"\" for v in prompt.input_variables if v not in prompt.partial_dict\n        }\n        empty_prompt_txt = prompt.format(**fmt_dict)\n        return empty_prompt_txt\n\n    def get_biggest_prompt(self, prompts: List[Prompt]) -> Prompt:\n        \"\"\"Get biggest prompt.\n\n        Oftentimes we need to fetch the biggest prompt, in order to\n        be the most conservative about chunking text. This\n        is a helper utility for that.\n\n        \"\"\"\n        empty_prompt_txts = [self._get_empty_prompt_txt(prompt) for prompt in prompts]\n        empty_prompt_txt_lens = [len(txt) for txt in empty_prompt_txts]\n        biggest_prompt = prompts[\n            empty_prompt_txt_lens.index(max(empty_prompt_txt_lens))\n        ]\n        return biggest_prompt\n\n    def get_text_splitter_given_prompt(\n        self, prompt: Prompt, num_chunks: int, padding: Optional[int] = 1\n    ) -> TokenTextSplitter:\n        \"\"\"Get text splitter given initial prompt.\n\n        Allows us to get the text splitter which will split up text according\n        to the desired chunk size.\n\n        \"\"\"\n        # generate empty_prompt_txt to compute initial tokens\n        empty_prompt_txt = self._get_empty_prompt_txt(prompt)\n        chunk_size = self.get_chunk_size_given_prompt(\n            empty_prompt_txt, num_chunks, padding=padding\n        )\n        text_splitter = TokenTextSplitter(\n            separator=self._separator,\n            chunk_size=chunk_size,\n            chunk_overlap=self.max_chunk_overlap // num_chunks,\n            tokenizer=self._tokenizer,\n        )\n        return text_splitter\n\n    def get_text_from_nodes(\n        self, node_list: List[Node], prompt: Optional[Prompt] = None\n    ) -> str:\n        \"\"\"Get text from nodes. Used by tree-structured indices.\"\"\"\n        num_nodes = len(node_list)\n        text_splitter = None\n        if prompt is not None:\n            # add padding given the newline character\n            text_splitter = self.get_text_splitter_given_prompt(\n                prompt,\n                num_nodes,\n                padding=1,\n            )\n        results = []\n        for node in node_list:\n            text = (\n                text_splitter.truncate_text(node.get_text())\n                if text_splitter is not None\n                else node.get_text()\n            )\n            results.append(text)\n\n        return \"\\n\".join(results)\n\n    def get_numbered_text_from_nodes(\n        self, node_list: List[Node], prompt: Optional[Prompt] = None\n    ) -> str:\n        \"\"\"Get text from nodes in the format of a numbered list.\n\n        Used by tree-structured indices.\n\n        \"\"\"\n        num_nodes = len(node_list)\n        text_splitter = None\n        if prompt is not None:\n            # add padding given the number, and the newlines\n            text_splitter = self.get_text_splitter_given_prompt(\n                prompt,\n                num_nodes,\n                padding=5,\n            )\n        results = []\n        number = 1\n        for node in node_list:\n            node_text = \" \".join(node.get_text().splitlines())\n            if text_splitter is not None:\n                node_text = text_splitter.truncate_text(node_text)\n            text = f\"({number}) {node_text}\"\n            results.append(text)\n            number += 1\n        return \"\\n\\n\".join(results)\n\n    def compact_text_chunks(self, prompt: Prompt, text_chunks: List[str]) -> List[str]:\n        \"\"\"Compact text chunks.\n\n        This will combine text chunks into consolidated chunks\n        that more fully \"pack\" the prompt template given the max_input_size.\n\n        \"\"\"\n        combined_str = \"\\n\\n\".join([c.strip() for c in text_chunks if c.strip()])\n        # resplit based on self.max_chunk_overlap\n        text_splitter = self.get_text_splitter_given_prompt(prompt, 1, padding=1)\n        return text_splitter.split_text(combined_str)\n", "doc_id": "3417a5c0eb0d7f9bea8bb6bc2828da5cc38fcd01", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/prompt_helper.py", "file_name": "prompt_helper.py"}, "__type__": "Document"}, "642b4f3bbe0ec820cf0734cec152c1766a7d0725": {"text": "\"\"\"Base query classes.\"\"\"\n\nimport logging\nimport re\nfrom abc import abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Dict, Generic, List, Optional, Tuple, TypeVar, cast\n\nfrom gpt_index.data_structs.data_structs import IndexStruct, Node\nfrom gpt_index.docstore import DocumentStore\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.embeddings.openai import OpenAIEmbedding\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.response.builder import ResponseBuilder, ResponseMode, TextChunk\nfrom gpt_index.indices.utils import truncate_text\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.default_prompts import (\n    DEFAULT_REFINE_PROMPT,\n    DEFAULT_TEXT_QA_PROMPT,\n)\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\nfrom gpt_index.response.schema import Response\nfrom gpt_index.token_counter.token_counter import llm_token_counter\n\nIS = TypeVar(\"IS\", bound=IndexStruct)\n\n\n@dataclass\nclass BaseQueryRunner:\n    \"\"\"Base query runner.\"\"\"\n\n    @abstractmethod\n    def query(self, query: str, index_struct: IndexStruct) -> Response:\n        \"\"\"Schedule a query.\"\"\"\n        raise NotImplementedError(\"Not implemented yet.\")\n\n\nclass BaseGPTIndexQuery(Generic[IS]):\n    \"\"\"Base GPT Index Query.\n\n    Helper class that is used to query an index. Can be called within `query`\n    method of a BaseGPTIndex object, or instantiated independently.\n\n    Args:\n        llm_predictor (LLMPredictor): Optional LLMPredictor object. If not provided,\n            will use the default LLMPredictor (text-davinci-003)\n        prompt_helper (PromptHelper): Optional PromptHelper object. If not provided,\n            will use the default PromptHelper.\n        required_keywords (List[str]): Optional list of keywords that must be present\n            in nodes. Can be used to query most indices (tree index is an exception).\n        exclude_keywords (List[str]): Optional list of keywords that must not be\n            present in nodes. Can be used to query most indices (tree index is an\n            exception).\n        response_mode (ResponseMode): Optional ResponseMode. If not provided, will\n            use the default ResponseMode.\n        text_qa_template (QuestionAnswerPrompt): Optional QuestionAnswerPrompt object.\n            If not provided, will use the default QuestionAnswerPrompt.\n        refine_template (RefinePrompt): Optional RefinePrompt object. If not provided,\n            will use the default RefinePrompt.\n        include_summary (bool): Optional bool. If True, will also use the summary\n            text of the index when generating a response (the summary text can be set\n            through `index.set_text(\"<text>\")`).\n        similarity_cutoff (float): Optional float. If set, will filter out nodes with\n            similarity below this cutoff threshold when computing the response\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IS,\n        # TODO: pass from superclass\n        llm_predictor: Optional[LLMPredictor] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        docstore: Optional[DocumentStore] = None,\n        query_runner: Optional[BaseQueryRunner] = None,\n        required_keywords: Optional[List[str]] = None,\n        exclude_keywords: Optional[List[str]] = None,\n        response_mode: ResponseMode = ResponseMode.DEFAULT,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        refine_template: Optional[RefinePrompt] = None,\n        include_summary: bool = False,\n        response_kwargs: Optional[Dict] = None,\n        similarity_cutoff: Optional[float] = None,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        if index_struct is None:\n            raise ValueError(\"index_struct must be provided.\")\n        self._validate_index_struct(index_struct)\n        self._index_struct = index_struct\n        self._llm_predictor = llm_predictor or LLMPredictor()\n        # NOTE: the embed_model isn't used in all indices\n        self._embed_model = embed_model or OpenAIEmbedding()\n        self._docstore = docstore\n        self._query_runner = query_runner\n        # TODO: make this a required param\n        if prompt_helper is None:\n            raise ValueError(\"prompt_helper must be provided.\")\n        self._prompt_helper = cast(PromptHelper, prompt_helper)\n\n        self._required_keywords = required_keywords\n        self._exclude_keywords = exclude_keywords\n        self._response_mode = ResponseMode(response_mode)\n\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        self.refine_template = refine_template or DEFAULT_REFINE_PROMPT\n        self._include_summary = include_summary\n\n        self._response_kwargs = response_kwargs or {}\n        self.response_builder = ResponseBuilder(\n            self._prompt_helper,\n            self._llm_predictor,\n            self.text_qa_template,\n            self.refine_template,\n        )\n\n        self.similarity_cutoff = similarity_cutoff\n\n    def _should_use_node(\n        self, node: Node, similarity_tracker: Optional[SimilarityTracker] = None\n    ) -> bool:\n        \"\"\"Run node through filters to determine if it should be used.\"\"\"\n        words = re.findall(r\"\\w+\", node.get_text())\n        if self._required_keywords is not None:\n            for w in self._required_keywords:\n                if w not in words:\n                    return False\n\n        if self._exclude_keywords is not None:\n            for w in self._exclude_keywords:\n                if w in words:\n                    return False\n\n        sim_cutoff_exists = (\n            similarity_tracker is not None and self.similarity_cutoff is not None\n        )\n\n        if sim_cutoff_exists:\n            similarity = cast(SimilarityTracker, similarity_tracker).find(node)\n            if similarity is None:\n                return False\n            if cast(float, similarity) < cast(float, self.similarity_cutoff):\n                return False\n\n        return True\n\n    def _get_text_from_node(\n        self,\n        query_str: str,\n        node: Node,\n        level: Optional[int] = None,\n    ) -> Tuple[TextChunk, Optional[Response]]:\n        \"\"\"Query a given node.\n\n        If node references a given document, then return the document.\n        If node references a given index, then query the index.\n\n        \"\"\"\n        level_str = \"\" if level is None else f\"[Level {level}]\"\n        fmt_text_chunk = truncate_text(node.get_text(), 50)\n        logging.debug(f\">{level_str} Searching in chunk: {fmt_text_chunk}\")\n\n        is_index_struct = False\n        # if self._query_runner is not None, assume we want to do a recursive\n        # query. In order to not perform a recursive query, make sure\n        # _query_runner is None.\n        if (\n            self._query_runner is not None\n            and node.ref_doc_id is not None\n            and self._docstore is not None\n        ):\n            doc = self._docstore.get_document(node.ref_doc_id, raise_error=True)\n            if isinstance(doc, IndexStruct):\n                is_index_struct = True\n\n        if is_index_struct:\n            query_runner = cast(BaseQueryRunner, self._query_runner)\n            response = query_runner.query(query_str, cast(IndexStruct, doc))\n            return TextChunk(str(response), is_answer=True), response\n        else:\n            text = node.get_text()\n            return TextChunk(text), None\n\n    @property\n    def index_struct(self) -> IS:\n        \"\"\"Get the index struct.\"\"\"\n        return self._index_struct\n\n    def _validate_index_struct(self, index_struct: IS) -> None:\n        \"\"\"Validate the index struct.\"\"\"\n        pass\n\n    def _give_response_for_nodes(\n        self, query_str: str, text_chunks: List[TextChunk]\n    ) -> str:\n        \"\"\"Give response for nodes.\"\"\"\n        self.response_builder.reset()\n        for text in text_chunks:\n            self.response_builder.add_text_chunks([text])\n        response = self.response_builder.get_response(\n            query_str,\n            mode=self._response_mode,\n            **self._response_kwargs,\n        )\n\n        return response or \"\"\n\n    def get_nodes_and_similarities_for_response(\n        self, query_str: str\n    ) -> List[Tuple[Node, Optional[float]]]:\n        \"\"\"Get list of tuples of node and similarity for response.\n\n        First part of the tuple is the node.\n        Second part of tuple is the distance from query to the node.\n        If not applicable, it's None.\n        \"\"\"\n        similarity_tracker = SimilarityTracker()\n        nodes = self._get_nodes_for_response(\n            query_str, similarity_tracker=similarity_tracker\n        )\n        nodes = [\n            node for node in nodes if self._should_use_node(node, similarity_tracker)\n        ]\n\n        # TODO: create a `display` method to allow subclasses to print the Node\n        return similarity_tracker.get_zipped_nodes(nodes)\n\n    @abstractmethod\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n\n    def _query(self, query_str: str) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        # TODO: remove _query and just use query\n        tuples = self.get_nodes_and_similarities_for_response(query_str)\n        node_texts = []\n        for node, similarity in tuples:\n            text, response = self._get_text_from_node(query_str, node)\n            self.response_builder.add_node(node, similarity=similarity)\n            if response is not None:\n                # these are source nodes from within this node (when it's an index)\n                for source_node in response.source_nodes:\n                    self.response_builder.add_source_node(source_node)\n            node_texts.append(text)\n\n        if self._response_mode != ResponseMode.NO_TEXT:\n            response_str = self._give_response_for_nodes(query_str, node_texts)\n        else:\n            response_str = None\n\n        return Response(response_str, source_nodes=self.response_builder.get_sources())\n\n    @llm_token_counter(\"query\")\n    def query(self, query_str: str) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        response = self._query(query_str)\n        # if include_summary is True, then include summary text in answer\n        # summary text is set through `set_text` on the underlying index.\n        # TODO: refactor response builder to be in the __init__\n        if self._response_mode != ResponseMode.NO_TEXT and self._include_summary:\n            response_builder = ResponseBuilder(\n                self._prompt_helper,\n                self._llm_predictor,\n                self.text_qa_template,\n                self.refine_template,\n                texts=[TextChunk(self._index_struct.get_text())],\n            )\n            # NOTE: use create and refine for now (default response mode)\n            response.response = response_builder.get_response(\n                query_str,\n                mode=self._response_mode,\n                prev_response=response.response,\n            )\n\n        return response\n", "doc_id": "642b4f3bbe0ec820cf0734cec152c1766a7d0725", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/base.py", "file_name": "base.py"}, "__type__": "Document"}, "f0bab417b4bf90ea4c52f882a52fbf041839974c": {"text": "\"\"\"Embedding utils for queries.\"\"\"\n\nfrom typing import Dict, List, Optional, Tuple\n\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.embeddings.openai import BaseEmbedding\n\n\ndef get_top_k_embeddings(\n    embed_model: BaseEmbedding,\n    query_embedding: List[float],\n    embeddings: List[List[float]],\n    similarity_top_k: Optional[int] = None,\n    embedding_ids: Optional[List] = None,\n) -> Tuple[List[float], List]:\n    \"\"\"Get top nodes by similarity to the query.\"\"\"\n    if embedding_ids is None:\n        embedding_ids = [i for i in range(len(embeddings))]\n\n    similarities = []\n    for emb in embeddings:\n        similarity = embed_model.similarity(query_embedding, emb)\n        similarities.append(similarity)\n\n    sorted_tups = sorted(\n        zip(similarities, embedding_ids), key=lambda x: x[0], reverse=True\n    )\n    similarity_top_k = similarity_top_k or len(sorted_tups)\n    result_tups = sorted_tups[:similarity_top_k]\n\n    result_similarities = [s for s, _ in result_tups]\n    result_ids = [n for _, n in result_tups]\n\n    return result_similarities, result_ids\n\n\nclass SimilarityTracker:\n    \"\"\"Helper class to manage node similarities during lifecycle of a single query.\"\"\"\n\n    # TODO: smarter way to store this information\n    lookup: Dict[str, float] = {}\n\n    def _hash(self, node: Node) -> str:\n        \"\"\"Generate a unique key for each node.\"\"\"\n        # TODO: Better way to get unique identifier of a node\n        return str(abs(hash(node.get_text())))\n\n    def add(self, node: Node, similarity: float) -> None:\n        \"\"\"Add a node and its similarity score.\"\"\"\n        node_hash = self._hash(node)\n        self.lookup[node_hash] = similarity\n\n    def find(self, node: Node) -> Optional[float]:\n        \"\"\"Find a node's similarity score.\"\"\"\n        node_hash = self._hash(node)\n        if node_hash not in self.lookup:\n            return None\n        return self.lookup[node_hash]\n\n    def get_zipped_nodes(self, nodes: List[Node]) -> List[Tuple[Node, Optional[float]]]:\n        \"\"\"Get a zipped list of nodes and their corresponding scores.\"\"\"\n        similarities = [self.find(node) for node in nodes]\n        return list(zip(nodes, similarities))\n", "doc_id": "f0bab417b4bf90ea4c52f882a52fbf041839974c", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/embedding_utils.py", "file_name": "embedding_utils.py"}, "__type__": "Document"}, "9b0b308e6e04d88056cbab99ebbdc33a0fac5a3d": {"text": "\"\"\"Query classes for keyword table indices.\"\"\"\n\nfrom gpt_index.indices.query.keyword_table.query import (\n    GPTKeywordTableGPTQuery,\n    GPTKeywordTableRAKEQuery,\n    GPTKeywordTableSimpleQuery,\n)\n\n__all__ = [\n    \"GPTKeywordTableGPTQuery\",\n    \"GPTKeywordTableRAKEQuery\",\n    \"GPTKeywordTableSimpleQuery\",\n]\n", "doc_id": "9b0b308e6e04d88056cbab99ebbdc33a0fac5a3d", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/keyword_table/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "731a15442e4eb480e1c5653f78dca14695ef8c95": {"text": "\"\"\"Query for GPTKeywordTableIndex.\"\"\"\nimport logging\nfrom abc import abstractmethod\nfrom collections import defaultdict\nfrom typing import Any, Dict, List, Optional\n\nfrom gpt_index.data_structs.data_structs import KeywordTable, Node\nfrom gpt_index.indices.keyword_table.utils import (\n    extract_keywords_given_response,\n    rake_extract_keywords,\n    simple_extract_keywords,\n)\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.utils import truncate_text\nfrom gpt_index.prompts.default_prompts import (\n    DEFAULT_KEYWORD_EXTRACT_TEMPLATE,\n    DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE,\n)\nfrom gpt_index.prompts.prompts import KeywordExtractPrompt, QueryKeywordExtractPrompt\n\nDQKET = DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE\n\n\nclass BaseGPTKeywordTableQuery(BaseGPTIndexQuery[KeywordTable]):\n    \"\"\"Base GPT Keyword Table Index Query.\n\n    Arguments are shared among subclasses.\n\n    Args:\n        keyword_extract_template (Optional[KeywordExtractPrompt]): A Keyword\n            Extraction Prompt\n            (see :ref:`Prompt-Templates`).\n        query_keyword_extract_template (Optional[QueryKeywordExtractPrompt]): A Query\n            Keyword Extraction\n            Prompt (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): A Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question Answering Prompt\n            (see :ref:`Prompt-Templates`).\n        max_keywords_per_query (int): Maximum number of keywords to extract from query.\n        num_chunks_per_query (int): Maximum number of text chunks to query.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: KeywordTable,\n        keyword_extract_template: Optional[KeywordExtractPrompt] = None,\n        query_keyword_extract_template: Optional[QueryKeywordExtractPrompt] = None,\n        max_keywords_per_query: int = 10,\n        num_chunks_per_query: int = 10,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(index_struct=index_struct, **kwargs)\n        self.max_keywords_per_query = max_keywords_per_query\n        self.num_chunks_per_query = num_chunks_per_query\n        self.keyword_extract_template = (\n            keyword_extract_template or DEFAULT_KEYWORD_EXTRACT_TEMPLATE\n        )\n        self.query_keyword_extract_template = query_keyword_extract_template or DQKET\n\n    @abstractmethod\n    def _get_keywords(self, query_str: str) -> List[str]:\n        \"\"\"Extract keywords.\"\"\"\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        logging.info(f\"> Starting query: {query_str}\")\n        keywords = self._get_keywords(query_str)\n        logging.info(f\"query keywords: {keywords}\")\n\n        # go through text chunks in order of most matching keywords\n        chunk_indices_count: Dict[int, int] = defaultdict(int)\n        keywords = [k for k in keywords if k in self.index_struct.keywords]\n        logging.info(f\"Extracted keywords: {keywords}\")\n        for k in keywords:\n            for text_chunk_idx in self.index_struct.table[k]:\n                chunk_indices_count[text_chunk_idx] += 1\n        sorted_chunk_indices = sorted(\n            list(chunk_indices_count.keys()),\n            key=lambda x: chunk_indices_count[x],\n            reverse=True,\n        )\n        sorted_chunk_indices = sorted_chunk_indices[: self.num_chunks_per_query]\n        sorted_nodes = [\n            self.index_struct.text_chunks[idx] for idx in sorted_chunk_indices\n        ]\n        # filter sorted nodes\n        sorted_nodes = [node for node in sorted_nodes if self._should_use_node(node)]\n\n        if logging.getLogger(__name__).getEffectiveLevel() == logging.DEBUG:\n            for chunk_idx, node in zip(sorted_chunk_indices, sorted_nodes):\n                logging.debug(\n                    f\"> Querying with idx: {chunk_idx}: \"\n                    f\"{truncate_text(node.get_text(), 50)}\"\n                )\n\n        return sorted_nodes\n\n\nclass GPTKeywordTableGPTQuery(BaseGPTKeywordTableQuery):\n    \"\"\"GPT Keyword Table Index Query.\n\n    Extracts keywords using GPT. Set when `mode=\"default\"` in `query` method of\n    `GPTKeywordTableIndex`.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    See BaseGPTKeywordTableQuery for arguments.\n\n    \"\"\"\n\n    def _get_keywords(self, query_str: str) -> List[str]:\n        \"\"\"Extract keywords.\"\"\"\n        response, _ = self._llm_predictor.predict(\n            self.query_keyword_extract_template,\n            max_keywords=self.max_keywords_per_query,\n            question=query_str,\n        )\n        keywords = extract_keywords_given_response(response, start_token=\"KEYWORDS:\")\n        return list(keywords)\n\n\nclass GPTKeywordTableSimpleQuery(BaseGPTKeywordTableQuery):\n    \"\"\"GPT Keyword Table Index Simple Query.\n\n    Extracts keywords using simple regex-based keyword extractor.\n    Set when `mode=\"simple\"` in `query` method of `GPTKeywordTableIndex`.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"simple\")\n\n    See BaseGPTKeywordTableQuery for arguments.\n\n    \"\"\"\n\n    def _get_keywords(self, query_str: str) -> List[str]:\n        \"\"\"Extract keywords.\"\"\"\n        return list(\n            simple_extract_keywords(query_str, max_keywords=self.max_keywords_per_query)\n        )\n\n\nclass GPTKeywordTableRAKEQuery(BaseGPTKeywordTableQuery):\n    \"\"\"GPT Keyword Table Index RAKE Query.\n\n    Extracts keywords using RAKE keyword extractor.\n    Set when `mode=\"rake\"` in `query` method of `GPTKeywordTableIndex`.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"rake\")\n\n    See BaseGPTKeywordTableQuery for arguments.\n\n    \"\"\"\n\n    def _get_keywords(self, query_str: str) -> List[str]:\n        \"\"\"Extract keywords.\"\"\"\n        return list(\n            rake_extract_keywords(query_str, max_keywords=self.max_keywords_per_query)\n        )\n", "doc_id": "731a15442e4eb480e1c5653f78dca14695ef8c95", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/keyword_table/query.py", "file_name": "query.py"}, "__type__": "Document"}, "795cc1c5df2839027cbf4cb474d8acb4cddbd939": {"text": "\"\"\"Query classes for list indices.\"\"\"\n\nfrom gpt_index.indices.query.list.embedding_query import GPTListIndexEmbeddingQuery\nfrom gpt_index.indices.query.list.query import GPTListIndexQuery\n\n__all__ = [\"GPTListIndexEmbeddingQuery\", \"GPTListIndexQuery\"]\n", "doc_id": "795cc1c5df2839027cbf4cb474d8acb4cddbd939", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/list/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "e71bcbd161ee6d598676e20c22af9f62e8b11045": {"text": "\"\"\"Embedding query for list index.\"\"\"\nimport logging\nfrom typing import Any, List, Optional, Tuple\n\nfrom gpt_index.data_structs.data_structs import IndexList, Node\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.embedding_utils import (\n    SimilarityTracker,\n    get_top_k_embeddings,\n)\nfrom gpt_index.indices.query.list.query import BaseGPTListIndexQuery\n\n\nclass GPTListIndexEmbeddingQuery(BaseGPTListIndexQuery):\n    \"\"\"GPTListIndex query.\n\n    An embedding-based query for GPTListIndex, which traverses\n    each node in sequence and retrieves top-k nodes by\n    embedding similarity to the query.\n    Set when `mode=\"embedding\"` in `query` method of `GPTListIndex`.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"embedding\")\n\n    See BaseGPTListIndexQuery for arguments.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexList,\n        similarity_top_k: Optional[int] = 1,\n        embed_model: Optional[BaseEmbedding] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            index_struct=index_struct,\n            embed_model=embed_model,\n            **kwargs,\n        )\n        self.similarity_top_k = similarity_top_k\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        nodes = self.index_struct.nodes\n        # top k nodes\n        query_embedding, node_embeddings = self._get_embeddings(query_str, nodes)\n\n        top_similarities, top_idxs = get_top_k_embeddings(\n            self._embed_model,\n            query_embedding,\n            node_embeddings,\n            similarity_top_k=self.similarity_top_k,\n            embedding_ids=list(range(len(nodes))),\n        )\n\n        top_k_nodes = [nodes[i] for i in top_idxs]\n\n        if similarity_tracker is not None:\n            for node, similarity in zip(top_k_nodes, top_similarities):\n                similarity_tracker.add(node, similarity)\n\n        logging.debug(f\"> Top {len(top_idxs)} nodes:\\n\")\n        nl = \"\\n\"\n        logging.debug(f\"{ nl.join([n.get_text() for n in top_k_nodes]) }\")\n        return top_k_nodes\n\n    def _get_embeddings(\n        self, query_str: str, nodes: List[Node]\n    ) -> Tuple[List[float], List[List[float]]]:\n        \"\"\"Get top nodes by similarity to the query.\"\"\"\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n        node_embeddings: List[List[float]] = []\n        for node in self.index_struct.nodes:\n            if node.embedding is not None:\n                text_embedding = node.embedding\n            else:\n                text_embedding = self._embed_model.get_text_embedding(node.get_text())\n                node.embedding = text_embedding\n\n            node_embeddings.append(text_embedding)\n        return query_embedding, node_embeddings\n", "doc_id": "e71bcbd161ee6d598676e20c22af9f62e8b11045", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/list/embedding_query.py", "file_name": "embedding_query.py"}, "__type__": "Document"}, "bd6aa1b66d7aceaee608581629f7c7cb293929b2": {"text": "\"\"\"Default query for GPTListIndex.\"\"\"\nfrom typing import List, Optional\n\nfrom gpt_index.data_structs.data_structs import IndexList, Node\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\n\n\nclass BaseGPTListIndexQuery(BaseGPTIndexQuery[IndexList]):\n    \"\"\"GPTListIndex query.\n\n    Arguments are shared among subclasses.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question Answering Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): A Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n\n    \"\"\"\n\n\nclass GPTListIndexQuery(BaseGPTListIndexQuery):\n    \"\"\"GPTListIndex query.\n\n    The default query mode for GPTListIndex, which traverses\n    each node in sequence and synthesizes a response across all nodes\n    (with an optional keyword filter).\n    Set when `mode=\"default\"` in `query` method of `GPTListIndex`.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    See BaseGPTListIndexQuery for arguments.\n\n    \"\"\"\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        return self.index_struct.nodes\n", "doc_id": "bd6aa1b66d7aceaee608581629f7c7cb293929b2", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/list/query.py", "file_name": "query.py"}, "__type__": "Document"}, "266fe6e986b73943246d77bc2f38623ad4ca18c7": {"text": "\"\"\"Query runner.\"\"\"\n\nfrom typing import Any, Dict, List, Optional, Union, cast\n\nfrom gpt_index.data_structs.data_structs import IndexStruct\nfrom gpt_index.docstore import DocumentStore\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.query.base import BaseQueryRunner\nfrom gpt_index.indices.query.schema import QueryConfig, QueryMode\nfrom gpt_index.indices.registry import IndexRegistry\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.response.schema import Response\n\n# TMP: refactor query config type\nQUERY_CONFIG_TYPE = Union[Dict, QueryConfig]\n\n\nclass QueryRunner(BaseQueryRunner):\n    \"\"\"Tool to take in a query request and perform a query with the right classes.\n\n    Higher-level wrapper over a given query.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        llm_predictor: LLMPredictor,\n        prompt_helper: PromptHelper,\n        embed_model: BaseEmbedding,\n        docstore: DocumentStore,\n        index_registry: IndexRegistry,\n        query_configs: Optional[List[QUERY_CONFIG_TYPE]] = None,\n        recursive: bool = False,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        config_dict: Dict[str, QueryConfig] = {}\n        if query_configs is None or len(query_configs) == 0:\n            query_config_objs: List[QueryConfig] = []\n        elif isinstance(query_configs[0], Dict):\n            query_config_objs = [\n                QueryConfig.from_dict(cast(Dict, qc)) for qc in query_configs\n            ]\n        else:\n            query_config_objs = [cast(QueryConfig, q) for q in query_configs]\n\n        for qc in query_config_objs:\n            config_dict[qc.index_struct_type] = qc\n\n        self._config_dict = config_dict\n        self._llm_predictor = llm_predictor\n        self._prompt_helper = prompt_helper\n        self._embed_model = embed_model\n        self._docstore = docstore\n        self._index_registry = index_registry\n        self._recursive = recursive\n\n    def _get_query_kwargs(self, config: QueryConfig) -> Dict[str, Any]:\n        \"\"\"Get query kwargs.\n\n        Also update with default arguments if not present.\n\n        \"\"\"\n        query_kwargs = {k: v for k, v in config.query_kwargs.items()}\n        if \"prompt_helper\" not in query_kwargs:\n            query_kwargs[\"prompt_helper\"] = self._prompt_helper\n        if \"llm_predictor\" not in query_kwargs:\n            query_kwargs[\"llm_predictor\"] = self._llm_predictor\n        if \"embed_model\" not in query_kwargs:\n            query_kwargs[\"embed_model\"] = self._embed_model\n        return query_kwargs\n\n    def query(self, query_str: str, index_struct: IndexStruct) -> Response:\n        \"\"\"Run query.\"\"\"\n        index_struct_type = index_struct.get_type()\n        if index_struct_type not in self._config_dict:\n            config = QueryConfig(\n                index_struct_type=index_struct_type, query_mode=QueryMode.DEFAULT\n            )\n        else:\n            config = self._config_dict[index_struct_type]\n        mode = config.query_mode\n\n        query_cls = self._index_registry.type_to_query[index_struct_type][mode]\n        # if recursive, pass self as query_runner to each individual query\n        query_runner = self if self._recursive else None\n        query_kwargs = self._get_query_kwargs(config)\n        query_obj = query_cls(\n            index_struct,\n            **query_kwargs,\n            query_runner=query_runner,\n            docstore=self._docstore,\n        )\n\n        return query_obj.query(query_str)\n", "doc_id": "266fe6e986b73943246d77bc2f38623ad4ca18c7", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/query_runner.py", "file_name": "query_runner.py"}, "__type__": "Document"}, "d7ed8486721f6f32f5c8ed69fbdce569a77b5ed0": {"text": "\"\"\"Query Configuration Schema.\n\nThis schema is used under the hood for all queries, but is primarily\nexposed for recursive queries over composable indices.\n\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Any, Dict\n\nfrom dataclasses_json import DataClassJsonMixin\n\n\nclass QueryMode(str, Enum):\n    \"\"\"Query mode enum.\n\n    Can be passed as the enum struct, or as the underlying string.\n\n    Attributes:\n        DEFAULT (\"default\"): Default query mode.\n        RETRIEVE (\"retrieve\"): Retrieve mode.\n        EMBEDDING (\"embedding\"): Embedding mode.\n        SUMMARIZE (\"summarize\"): Summarize mode. Used for hierarchical\n            summarization in the tree index.\n        SIMPLE (\"simple\"): Simple mode. Used for keyword extraction.\n        RAKE (\"rake\"): RAKE mode. Used for keyword extraction.\n        RECURSIVE (\"recursive\"): Recursive mode. Used to recursively query\n            over composed indices.\n\n    \"\"\"\n\n    DEFAULT = \"default\"\n    # a special \"retrieve\" query for tree index that retrieves that top nodes\n    RETRIEVE = \"retrieve\"\n    # embedding-based query\n    EMBEDDING = \"embedding\"\n\n    # to hierarchically summarize using tree\n    SUMMARIZE = \"summarize\"\n\n    # for keyword extractor\n    SIMPLE = \"simple\"\n    RAKE = \"rake\"\n\n    # recursive queries (composable queries)\n    # NOTE: deprecated\n    RECURSIVE = \"recursive\"\n\n    # for sql queries\n    SQL = \"sql\"\n\n\n@dataclass\nclass QueryConfig(DataClassJsonMixin):\n    \"\"\"Query config.\n\n    Used under the hood for all queries.\n    The user must explicitly specify a list of query config objects is passed during\n    a query call to define configurations for each individual subindex within an\n    overall composed index.\n\n    The user may choose to specify either the query config objects directly,\n    or as a list of JSON dictionaries. For instance, the following are equivalent:\n\n    .. code-block:: python\n\n        # using JSON dictionaries\n        query_configs = [\n            {\n                \"index_struct_type\": \"tree\",\n                \"query_mode\": \"default\",\n                \"query_kwargs\": {\n                    \"child_branch_factor\": 2\n                }\n            },\n            ...\n        ]\n        response = index.query(\n            \"<query_str>\", mode=\"recursive\", query_configs=query_configs\n        )\n\n    .. code-block:: python\n\n        query_configs = [\n            QueryConfig(\n                index_struct_type=IndexStructType.TREE,\n                query_mode=QueryMode.DEFAULT,\n                query_kwargs={\n                    \"child_branch_factor\": 2\n                }\n            )\n            ...\n        ]\n        response = index.query(\n            \"<query_str>\", mode=\"recursive\", query_configs=query_configs\n        )\n\n\n    Args:\n        index_struct_type (IndexStructType): The type of index struct.\n        query_mode (QueryMode): The query mode.\n        query_kwargs (Dict[str, Any], optional): The query kwargs. Defaults to {}.\n\n    \"\"\"\n\n    # index_struct_type: IndexStructType\n    index_struct_type: str\n    query_mode: QueryMode\n    query_kwargs: Dict[str, Any] = field(default_factory=dict)\n", "doc_id": "d7ed8486721f6f32f5c8ed69fbdce569a77b5ed0", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/schema.py", "file_name": "schema.py"}, "__type__": "Document"}, "b79e77bbf92c6bb7c326003d445709bae24fc168": {"text": "\"\"\"Default query for GPTFaissIndex.\"\"\"\nimport logging\nfrom typing import Any, Optional\n\nfrom gpt_index.data_structs.table import SQLStructTable\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.langchain_helpers.sql_wrapper import SQLDatabase\nfrom gpt_index.prompts.default_prompts import DEFAULT_TEXT_TO_SQL_PROMPT\nfrom gpt_index.prompts.prompts import TextToSQLPrompt\nfrom gpt_index.response.schema import Response\nfrom gpt_index.token_counter.token_counter import llm_token_counter\n\n\nclass GPTSQLStructStoreIndexQuery(BaseGPTIndexQuery[SQLStructTable]):\n    \"\"\"GPT SQL query over a structured database.\n\n    Runs raw SQL over a GPTSQLStructStoreIndex. No LLM calls are made here.\n    NOTE: this query cannot work with composed indices - if the index\n    contains subindices, those subindices will not be queried.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"sql\")\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: SQLStructTable,\n        sql_database: Optional[SQLDatabase] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(index_struct=index_struct, **kwargs)\n        if sql_database is None:\n            raise ValueError(\"sql_database must be provided.\")\n        self._sql_database = sql_database\n\n    @llm_token_counter(\"query\")\n    def query(self, query_str: str) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        # NOTE: override query method in order to fetch the right results.\n        # NOTE: since the query_str is a SQL query, it doesn't make sense\n        # to use ResponseBuilder anywhere.\n        response_str, extra_info = self._sql_database.run_sql(query_str)\n        response = Response(response=response_str, extra_info=extra_info)\n        return response\n\n\nclass GPTNLStructStoreIndexQuery(BaseGPTIndexQuery[SQLStructTable]):\n    \"\"\"GPT natural language query over a structured database.\n\n    Given a natural language query, we will extract the query to SQL.\n    Runs raw SQL over a GPTSQLStructStoreIndex. No LLM calls are made here.\n    NOTE: this query cannot work with composed indices - if the index\n    contains subindices, those subindices will not be queried.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"sql\")\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: SQLStructTable,\n        sql_database: Optional[SQLDatabase] = None,\n        ref_doc_id_column: Optional[str] = None,\n        text_to_sql_prompt: Optional[TextToSQLPrompt] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(index_struct=index_struct, **kwargs)\n        if sql_database is None:\n            raise ValueError(\"sql_database must be provided.\")\n        self._sql_database = sql_database\n        self._ref_doc_id_column = ref_doc_id_column\n        self._text_to_sql_prompt = text_to_sql_prompt or DEFAULT_TEXT_TO_SQL_PROMPT\n\n    def _parse_response_to_sql(self, response: str) -> str:\n        \"\"\"Parse response to SQL.\"\"\"\n        result_response = response.strip()\n        return result_response\n\n    def _get_all_tables_desc(self) -> str:\n        \"\"\"Get tables schema + optional context as a single string.\"\"\"\n        tables_desc = []\n        for table_name in self._sql_database.get_table_names():\n            table_desc = self._sql_database.get_single_table_info(table_name)\n            table_text = f\"Schema of table {table_name}:\\n\" f\"{table_desc}\\n\"\n            if table_name in self._index_struct.context_dict:\n                table_text += f\"Context of table {table_name}:\\n\"\n                table_text += self._index_struct.context_dict[table_name]\n            tables_desc.append(table_text)\n        return \"\\n\\n\".join(tables_desc)\n\n    def _query(self, query_str: str) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        table_desc_str = self._get_all_tables_desc()\n        logging.info(f\"table desc str: {table_desc_str}\")\n        response_str, _ = self._llm_predictor.predict(\n            self._text_to_sql_prompt, query_str=query_str, schema=table_desc_str\n        )\n\n        sql_query_str = self._parse_response_to_sql(response_str)\n        # assume that it's a valid SQL query\n        logging.debug(f\"> Predicted SQL query: {sql_query_str}\")\n\n        response_str, extra_info = self._sql_database.run_sql(sql_query_str)\n        response = Response(response=response_str, extra_info=extra_info)\n        return response\n", "doc_id": "b79e77bbf92c6bb7c326003d445709bae24fc168", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/struct_store/sql.py", "file_name": "sql.py"}, "__type__": "Document"}, "f269b72b009c4da94d70b83a9b6b9f03af0345da": {"text": "\"\"\"Query classes for tree indices.\"\"\"\n\nfrom gpt_index.indices.query.tree.embedding_query import GPTTreeIndexEmbeddingQuery\nfrom gpt_index.indices.query.tree.leaf_query import GPTTreeIndexLeafQuery\nfrom gpt_index.indices.query.tree.retrieve_query import GPTTreeIndexRetQuery\n\n__all__ = [\n    \"GPTTreeIndexLeafQuery\",\n    \"GPTTreeIndexRetQuery\",\n    \"GPTTreeIndexEmbeddingQuery\",\n]\n", "doc_id": "f269b72b009c4da94d70b83a9b6b9f03af0345da", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332": {"text": "\"\"\"Query Tree using embedding similarity between query and node text.\"\"\"\n\nimport logging\nfrom typing import Any, Dict, List, Optional, Tuple\n\nfrom gpt_index.data_structs.data_structs import IndexGraph, Node\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.tree.leaf_query import GPTTreeIndexLeafQuery\nfrom gpt_index.indices.utils import get_sorted_node_list\nfrom gpt_index.prompts.prompts import TreeSelectMultiplePrompt, TreeSelectPrompt\n\n\nclass GPTTreeIndexEmbeddingQuery(GPTTreeIndexLeafQuery):\n    \"\"\"\n    GPT Tree Index embedding query.\n\n    This class traverses the index graph using the embedding similarity between the\n    query and the node text.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"embedding\")\n\n    Args:\n        query_template (Optional[TreeSelectPrompt]): Tree Select Query Prompt\n            (see :ref:`Prompt-Templates`).\n        query_template_multiple (Optional[TreeSelectMultiplePrompt]): Tree Select\n            Query Prompt (Multiple)\n            (see :ref:`Prompt-Templates`).\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        child_branch_factor (int): Number of child nodes to consider at each level.\n            If child_branch_factor is 1, then the query will only choose one child node\n            to traverse for any given parent node.\n            If child_branch_factor is 2, then the query will choose two child nodes.\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexGraph,\n        query_template: Optional[TreeSelectPrompt] = None,\n        query_template_multiple: Optional[TreeSelectMultiplePrompt] = None,\n        child_branch_factor: int = 1,\n        embed_model: Optional[BaseEmbedding] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            index_struct,\n            query_template=query_template,\n            query_template_multiple=query_template_multiple,\n            child_branch_factor=child_branch_factor,\n            embed_model=embed_model,\n            **kwargs,\n        )\n        self.child_branch_factor = child_branch_factor\n\n    def _query_level(\n        self,\n        cur_nodes: Dict[int, Node],\n        query_str: str,\n        level: int = 0,\n    ) -> str:\n        cur_node_list = get_sorted_node_list(cur_nodes)\n\n        # Get the node with the highest similarity to the query\n        selected_node, selected_index = self._get_most_similar_node(\n            cur_node_list, query_str\n        )\n        logging.debug(\n            f\">[Level {level}] Node [{selected_index+1}] Summary text: \"\n            f\"{' '.join(selected_node.get_text().splitlines())}\"\n        )\n\n        # Get the response for the selected node\n        response = self._query_with_selected_node(selected_node, query_str, level=level)\n\n        return response\n\n    def _get_query_text_embedding_similarities(\n        self, query_str: str, nodes: List[Node]\n    ) -> List[float]:\n        \"\"\"\n        Get query text embedding similarity.\n\n        Cache the query embedding and the node text embedding.\n\n        \"\"\"\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n        similarities = []\n        for node in nodes:\n            if node.embedding is not None:\n                text_embedding = node.embedding\n            else:\n                text_embedding = self._embed_model.get_text_embedding(node.get_text())\n                node.embedding = text_embedding\n\n            similarity = self._embed_model.similarity(query_embedding, text_embedding)\n            similarities.append(similarity)\n        return similarities\n\n    def _get_most_similar_node(\n        self, nodes: List[Node], query_str: str\n    ) -> Tuple[Node, int]:\n        \"\"\"Get the node with the highest similarity to the query.\"\"\"\n        similarities = self._get_query_text_embedding_similarities(query_str, nodes)\n\n        selected_index = similarities.index(max(similarities))\n\n        selected_node = nodes[similarities.index(max(similarities))]\n        return selected_node, selected_index\n", "doc_id": "a5da8f1d8beb548e10422c7a5a2cf1c6328e9332", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/embedding_query.py", "file_name": "embedding_query.py"}, "__type__": "Document"}, "6317248da78605b9ee03504c5343c039423e3e4b": {"text": "\"\"\"Leaf query mechanism.\"\"\"\n\nimport logging\nfrom typing import Any, Dict, Optional, cast\n\nfrom gpt_index.data_structs.data_structs import IndexGraph, Node\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.response.builder import ResponseBuilder\nfrom gpt_index.indices.utils import (\n    extract_numbers_given_response,\n    get_sorted_node_list,\n    truncate_text,\n)\nfrom gpt_index.prompts.default_prompts import (\n    DEFAULT_QUERY_PROMPT,\n    DEFAULT_QUERY_PROMPT_MULTIPLE,\n)\nfrom gpt_index.prompts.prompts import TreeSelectMultiplePrompt, TreeSelectPrompt\nfrom gpt_index.response.schema import Response\n\n\nclass GPTTreeIndexLeafQuery(BaseGPTIndexQuery[IndexGraph]):\n    \"\"\"GPT Tree Index leaf query.\n\n    This class traverses the index graph and searches for a leaf node that can best\n    answer the query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    Args:\n        query_template (Optional[TreeSelectPrompt]): Tree Select Query Prompt\n            (see :ref:`Prompt-Templates`).\n        query_template_multiple (Optional[TreeSelectMultiplePrompt]): Tree Select\n            Query Prompt (Multiple)\n            (see :ref:`Prompt-Templates`).\n        child_branch_factor (int): Number of child nodes to consider at each level.\n            If child_branch_factor is 1, then the query will only choose one child node\n            to traverse for any given parent node.\n            If child_branch_factor is 2, then the query will choose two child nodes.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexGraph,\n        query_template: Optional[TreeSelectPrompt] = None,\n        query_template_multiple: Optional[TreeSelectMultiplePrompt] = None,\n        child_branch_factor: int = 1,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(index_struct, **kwargs)\n        self.query_template = query_template or DEFAULT_QUERY_PROMPT\n        self.query_template_multiple = (\n            query_template_multiple or DEFAULT_QUERY_PROMPT_MULTIPLE\n        )\n        self.child_branch_factor = child_branch_factor\n\n    def _query_with_selected_node(\n        self,\n        selected_node: Node,\n        query_str: str,\n        prev_response: Optional[str] = None,\n        level: int = 0,\n    ) -> str:\n        \"\"\"Get response for selected node.\n\n        If not leaf node, it will recursively call _query on the child nodes.\n        If prev_response is provided, we will update prev_response with the answer.\n\n        \"\"\"\n        if len(selected_node.child_indices) == 0:\n            response_builder = ResponseBuilder(\n                self._prompt_helper,\n                self._llm_predictor,\n                self.text_qa_template,\n                self.refine_template,\n            )\n            self.response_builder.add_node(selected_node)\n            # use response builder to get answer from node\n            node_text, _ = self._get_text_from_node(\n                query_str, selected_node, level=level\n            )\n            cur_response = response_builder.get_response_over_chunks(\n                query_str, [node_text], prev_response=prev_response\n            )\n            logging.debug(f\">[Level {level}] Current answer response: {cur_response} \")\n        else:\n            cur_response = self._query_level(\n                {\n                    i: self.index_struct.all_nodes[i]\n                    for i in selected_node.child_indices\n                },\n                query_str,\n                level=level + 1,\n            )\n\n        if prev_response is None:\n            return cur_response\n        else:\n            context_msg = \"\\n\".join([selected_node.get_text(), cur_response])\n            cur_response, formatted_refine_prompt = self._llm_predictor.predict(\n                self.refine_template,\n                query_str=query_str,\n                existing_answer=prev_response,\n                context_msg=context_msg,\n            )\n\n            logging.debug(f\">[Level {level}] Refine prompt: {formatted_refine_prompt}\")\n            logging.debug(f\">[Level {level}] Current refined response: {cur_response} \")\n            return cur_response\n\n    def _query_level(\n        self,\n        cur_nodes: Dict[int, Node],\n        query_str: str,\n        level: int = 0,\n    ) -> str:\n        \"\"\"Answer a query recursively.\"\"\"\n        cur_node_list = get_sorted_node_list(cur_nodes)\n\n        if self.child_branch_factor == 1:\n            query_template = self.query_template.partial_format(\n                num_chunks=len(cur_node_list), query_str=query_str\n            )\n            numbered_node_text = self._prompt_helper.get_numbered_text_from_nodes(\n                cur_node_list, prompt=query_template\n            )\n            response, formatted_query_prompt = self._llm_predictor.predict(\n                query_template,\n                context_list=numbered_node_text,\n            )\n        else:\n            query_template_multiple = self.query_template_multiple.partial_format(\n                num_chunks=len(cur_node_list),\n                query_str=query_str,\n                branching_factor=self.child_branch_factor,\n            )\n            numbered_node_text = self._prompt_helper.get_numbered_text_from_nodes(\n                cur_node_list, prompt=query_template_multiple\n            )\n            response, formatted_query_prompt = self._llm_predictor.predict(\n                query_template_multiple,\n                context_list=numbered_node_text,\n            )\n\n        logging.debug(\n            f\">[Level {level}] current prompt template: {formatted_query_prompt}\"\n        )\n\n        numbers = extract_numbers_given_response(response, n=self.child_branch_factor)\n        if numbers is None:\n            logging.debug(\n                f\">[Level {level}] Could not retrieve response - no numbers present\"\n            )\n            # just join text from current nodes as response\n            return response\n        result_response = None\n        for number_str in numbers:\n            number = int(number_str)\n            if number > len(cur_node_list):\n                logging.debug(\n                    f\">[Level {level}] Invalid response: {response} - \"\n                    f\"number {number} out of range\"\n                )\n                return response\n\n            # number is 1-indexed, so subtract 1\n            selected_node = cur_node_list[number - 1]\n\n            logging.info(\n                f\">[Level {level}] Selected node: \"\n                f\"[{number}]/[{','.join([str(int(n)) for n in numbers])}]\"\n            )\n            debug_str = \" \".join(selected_node.get_text().splitlines())\n            logging.debug(\n                f\">[Level {level}] Node \"\n                f\"[{number}] Summary text: \"\n                f\"{ truncate_text(debug_str, 100) }\"\n            )\n            result_response = self._query_with_selected_node(\n                selected_node,\n                query_str,\n                prev_response=result_response,\n                level=level,\n            )\n        # result_response should not be None\n        return cast(str, result_response)\n\n    def _query(self, query_str: str) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        # NOTE: this overrides the _query method in the base class\n        logging.info(f\"> Starting query: {query_str}\")\n        response_str = self._query_level(\n            self.index_struct.root_nodes,\n            query_str,\n            level=0,\n        ).strip()\n        return Response(response_str, source_nodes=self.response_builder.get_sources())\n", "doc_id": "6317248da78605b9ee03504c5343c039423e3e4b", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/leaf_query.py", "file_name": "leaf_query.py"}, "__type__": "Document"}, "71b67e07dcb1494c785428d1ecd9d9d6ca5f168a": {"text": "\"\"\"Retrieve query.\"\"\"\nimport logging\nfrom typing import List, Optional\n\nfrom gpt_index.data_structs.data_structs import IndexGraph, Node\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.utils import get_sorted_node_list\n\n\nclass GPTTreeIndexRetQuery(BaseGPTIndexQuery[IndexGraph]):\n    \"\"\"GPT Tree Index retrieve query.\n\n    This class directly retrieves the answer from the root nodes.\n\n    Unlike GPTTreeIndexLeafQuery, this class assumes the graph already stores\n    the answer (because it was constructed with a query_str), so it does not\n    attempt to parse information down the graph in order to synthesize an answer.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"retrieve\")\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n\n    \"\"\"\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        logging.info(f\"> Starting query: {query_str}\")\n        node_list = get_sorted_node_list(self.index_struct.root_nodes)\n        text_qa_template = self.text_qa_template.partial_format(query_str=query_str)\n        node_text = self._prompt_helper.get_text_from_nodes(\n            node_list, prompt=text_qa_template\n        )\n        return [Node(text=node_text)]\n", "doc_id": "71b67e07dcb1494c785428d1ecd9d9d6ca5f168a", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/retrieve_query.py", "file_name": "retrieve_query.py"}, "__type__": "Document"}, "46f8def5a3bd788e7448ee6c0ec10c1522f2a425": {"text": "\"\"\"Summarize query.\"\"\"\n\nimport logging\nfrom typing import Any, List, Optional, cast\n\nfrom gpt_index.data_structs.data_structs import IndexGraph, Node\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.response.builder import ResponseMode\nfrom gpt_index.indices.utils import get_sorted_node_list\n\n\nclass GPTTreeIndexSummarizeQuery(BaseGPTIndexQuery[IndexGraph]):\n    \"\"\"GPT Tree Index summarize query.\n\n    This class builds a query-specific tree from leaf nodes to return a response.\n    Using this query mode means that the tree index doesn't need to be built\n    when initialized, since we rebuild the tree for each query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"summarize\")\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexGraph,\n        num_children: int = 10,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        if \"response_mode\" in kwargs:\n            raise ValueError(\n                \"response_mode should not be specified for summarize query\"\n            )\n        response_kwargs = kwargs.pop(\"response_kwargs\", {})\n        response_kwargs.update(num_children=num_children)\n        super().__init__(\n            index_struct,\n            response_mode=ResponseMode.TREE_SUMMARIZE,\n            response_kwargs=response_kwargs,\n            **kwargs,\n        )\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        logging.info(f\"> Starting query: {query_str}\")\n        index_struct = cast(IndexGraph, self._index_struct)\n        sorted_node_list = get_sorted_node_list(index_struct.all_nodes)\n        return sorted_node_list\n", "doc_id": "46f8def5a3bd788e7448ee6c0ec10c1522f2a425", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/tree/summarize_query.py", "file_name": "summarize_query.py"}, "__type__": "Document"}, "1dd47d1956669dbe44b595ae46e20924311f7be6": {"text": "\"\"\"Query classes for vector store indices.\"\"\"\n\nfrom gpt_index.indices.query.vector_store.faiss import GPTFaissIndexQuery\nfrom gpt_index.indices.query.vector_store.simple import GPTSimpleVectorIndexQuery\nfrom gpt_index.indices.query.vector_store.weaviate import GPTWeaviateIndexQuery\n\n__all__ = [\"GPTFaissIndexQuery\", \"GPTSimpleVectorIndexQuery\", \"GPTWeaviateIndexQuery\"]\n", "doc_id": "1dd47d1956669dbe44b595ae46e20924311f7be6", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "c99a92ca905368d8cf79d8d42bfe270fa9e76733": {"text": "\"\"\"Base vector store index query.\"\"\"\n\n\nfrom typing import Any, Generic, Optional, TypeVar\n\nfrom gpt_index.data_structs.data_structs import IndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\n\nBID = TypeVar(\"BID\", bound=IndexStruct)\n\n\nclass BaseGPTVectorStoreIndexQuery(BaseGPTIndexQuery[BID], Generic[BID]):\n    \"\"\"Base vector store query.\"\"\"\n\n    def __init__(\n        self,\n        index_struct: BID,\n        embed_model: Optional[BaseEmbedding] = None,\n        similarity_top_k: Optional[int] = 1,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(index_struct=index_struct, embed_model=embed_model, **kwargs)\n        self.similarity_top_k = similarity_top_k\n", "doc_id": "c99a92ca905368d8cf79d8d42bfe270fa9e76733", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/base.py", "file_name": "base.py"}, "__type__": "Document"}, "1ea526708a54837eb2d2594ce803150877deeec3": {"text": "\"\"\"Default query for GPTFaissIndex.\"\"\"\n\nimport logging\nfrom typing import Any, List, Optional, cast\n\nimport numpy as np\n\nfrom gpt_index.data_structs.data_structs import IndexDict, Node\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.query.vector_store.base import BaseGPTVectorStoreIndexQuery\nfrom gpt_index.indices.utils import truncate_text\n\n\nclass GPTFaissIndexQuery(BaseGPTVectorStoreIndexQuery[IndexDict]):\n    \"\"\"GPTFaissIndex query.\n\n    An embedding-based query for GPTFaissIndex, which queries\n    an underlying Faiss index to retrieve top-k nodes by\n    embedding similarity to the query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        faiss_index (faiss.Index): A Faiss Index object (required)\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        similarity_top_k (int): Number of similar nodes to retrieve.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexDict,\n        faiss_index: Optional[Any] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        similarity_top_k: Optional[int] = 1,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            index_struct=index_struct,\n            embed_model=embed_model,\n            similarity_top_k=similarity_top_k,\n            **kwargs,\n        )\n        if faiss_index is None:\n            raise ValueError(\"faiss_index cannot be None.\")\n        # NOTE: cast to Any for now\n        self._faiss_index = cast(Any, faiss_index)\n        self._faiss_index = faiss_index\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n        query_embedding_np = np.array(query_embedding, dtype=\"float32\")[np.newaxis, :]\n        dists, indices = self._faiss_index.search(\n            query_embedding_np, self.similarity_top_k\n        )\n        dists = [d[0] for d in dists]\n        # if empty, then return an empty response\n        if len(indices) == 0:\n            return []\n\n        # returned dimension is 1 x k\n        node_idxs = list([str(i) for i in indices[0]])\n        top_k_nodes = self._index_struct.get_nodes(node_idxs)\n\n        if similarity_tracker is not None:\n            for node, similarity in zip(top_k_nodes, dists):\n                similarity_tracker.add(node, similarity)\n\n        if logging.getLogger(__name__).getEffectiveLevel() == logging.DEBUG:\n            fmt_txts = []\n            for node_idx, node_similarity, node in zip(node_idxs, dists, top_k_nodes):\n                fmt_txt = f\"> [Node {node_idx}] [Similarity score: \\\n                    {float(node_similarity):.6}] {truncate_text(node.get_text(), 100)}\"\n                fmt_txts.append(fmt_txt)\n            top_k_node_text = \"\\n\".join(fmt_txts)\n            logging.debug(f\"> Top {len(top_k_nodes)} nodes:\\n{top_k_node_text}\")\n\n        return top_k_nodes\n", "doc_id": "1ea526708a54837eb2d2594ce803150877deeec3", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/faiss.py", "file_name": "faiss.py"}, "__type__": "Document"}, "0260639a9de8f896a7e32e927fa8f257c442953b": {"text": "\"\"\"Pinecone vector store index query.\"\"\"\nimport logging\nfrom typing import Any, Dict, List, Optional, cast\n\nfrom gpt_index.data_structs.data_structs import IndexDict, Node\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.query.vector_store.base import BaseGPTVectorStoreIndexQuery\nfrom gpt_index.indices.utils import truncate_text\n\n\nclass GPTPineconeIndexQuery(BaseGPTVectorStoreIndexQuery[IndexDict]):\n    \"\"\"GPTPineconeIndex query.\n\n    An embedding-based query for GPTPineconeIndex, which queries\n    an undelrying Pinecone index to retrieve top-k nodes by\n    embedding similarity to the query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        pinecone_index (pinecone.Index): A Pinecone Index object (required)\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        similarity_top_k (int): Number of similar nodes to retrieve.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: IndexDict,\n        pinecone_index: Optional[Any] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        similarity_top_k: Optional[int] = 1,\n        pinecone_kwargs: Optional[Dict] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            index_struct=index_struct,\n            embed_model=embed_model,\n            similarity_top_k=similarity_top_k,\n            **kwargs,\n        )\n        if pinecone_index is None:\n            raise ValueError(\"pinecone_index cannot be None.\")\n        # NOTE: cast to Any for now\n        self._pinecone_index = cast(Any, pinecone_index)\n        self._pinecone_index = pinecone_index\n\n        self._pinecone_kwargs = pinecone_kwargs or {}\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n\n        response = self._pinecone_index.query(\n            query_embedding,\n            top_k=self.similarity_top_k,\n            include_values=True,\n            include_metadata=True,\n            **self._pinecone_kwargs,\n        )\n\n        top_k_nodes = []\n        top_k_ids = []\n        top_k_scores = []\n        for match in response.matches:\n            text = match.metadata[\"text\"]\n            node = Node(text=text, extra_info=match.metadata)\n            top_k_ids.append(match.id)\n            top_k_nodes.append(node)\n            top_k_scores.append(match.score)\n            if similarity_tracker is not None:\n                similarity_tracker.add(node, match.score)\n\n        if logging.getLogger(__name__).getEffectiveLevel() == logging.DEBUG:\n            fmt_txts = []\n            for node_idx, node_similarity, node in zip(\n                top_k_ids, top_k_scores, top_k_nodes\n            ):\n                fmt_txt = f\"> [Node {node_idx}] [Similarity score: \\\n                    {node_similarity:.6}] {truncate_text(node.get_text(), 100)}\"\n                fmt_txts.append(fmt_txt)\n            top_k_node_text = \"\\n\".join(fmt_txts)\n            logging.debug(f\"> Top {len(top_k_nodes)} nodes:\\n{top_k_node_text}\")\n\n        return top_k_nodes\n", "doc_id": "0260639a9de8f896a7e32e927fa8f257c442953b", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/pinecone.py", "file_name": "pinecone.py"}, "__type__": "Document"}, "666d66f5854bdfaa296829a9de353eddb6f46e69": {"text": "\"\"\"Qdrant vector store index query.\"\"\"\nimport logging\nfrom typing import Any, List, Optional, cast\n\nfrom gpt_index.data_structs import Node, QdrantIndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.query.vector_store.base import BaseGPTVectorStoreIndexQuery\nfrom gpt_index.indices.utils import truncate_text\n\n\nclass GPTQdrantIndexQuery(BaseGPTVectorStoreIndexQuery[QdrantIndexStruct]):\n    \"\"\"GPTQdrantIndex query.\n\n    An embedding-based query for GPTQdrantIndex, which queries\n    an undelrying Qdrant index to retrieve top-k nodes by\n    embedding similarity to the query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        similarity_top_k (int): Number of similar nodes to retrieve.\n        client (Optional[Any]): QdrantClient instance from `qdrant-client` package\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index_struct: QdrantIndexStruct,\n        embed_model: Optional[BaseEmbedding] = None,\n        similarity_top_k: int = 1,\n        client: Optional[Any] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            index_struct=index_struct,\n            embed_model=embed_model,\n            similarity_top_k=similarity_top_k,\n            **kwargs,\n        )\n\n        import_err_msg = (\n            \"`qdrant-client` package not found, please run `pip install qdrant-client`\"\n        )\n        try:\n            import qdrant_client  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n\n        if client is None:\n            raise ValueError(\"client cannot be None.\")\n\n        self._client = cast(qdrant_client.QdrantClient, client)\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        from qdrant_client.http.models.models import Payload\n\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n\n        response = self._client.search(\n            collection_name=self.index_struct.get_collection_name(),\n            query_vector=query_embedding,\n            limit=cast(int, self.similarity_top_k),\n        )\n\n        logging.debug(f\"> Top {len(response)} nodes:\")\n\n        nodes = []\n        for point in response:\n            payload = cast(Payload, point.payload)\n            node = Node(\n                doc_id=payload.get(\"doc_id\"),\n                text=payload.get(\"text\"),\n            )\n            nodes.append(node)\n\n            if similarity_tracker is not None:\n                similarity_tracker.add(node, point.score)\n\n            logging.debug(\n                f\"> [Node {point.id}] [Similarity score: {point.score:.6}] \"\n                f\"{truncate_text(str(payload.get('text')), 100)}\"\n            )\n\n        return nodes\n", "doc_id": "666d66f5854bdfaa296829a9de353eddb6f46e69", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/qdrant.py", "file_name": "qdrant.py"}, "__type__": "Document"}, "33f9551e7c7f50285582555c35fb754a11a4c61c": {"text": "\"\"\"Default query for GPTSimpleVectorIndex.\"\"\"\nimport logging\nfrom typing import List, Optional\n\nfrom gpt_index.data_structs.data_structs import Node, SimpleIndexDict\nfrom gpt_index.indices.query.embedding_utils import (\n    SimilarityTracker,\n    get_top_k_embeddings,\n)\nfrom gpt_index.indices.query.vector_store.base import BaseGPTVectorStoreIndexQuery\nfrom gpt_index.indices.utils import truncate_text\n\n\nclass GPTSimpleVectorIndexQuery(BaseGPTVectorStoreIndexQuery[SimpleIndexDict]):\n    \"\"\"GPTSimpleVectorIndex query.\n\n    An embedding-based query for GPTSimpleVectorIndex, which queries\n    an underlying dict-based embedding store to retrieve top-k nodes by\n    embedding similarity to the query.\n\n    .. code-block:: python\n\n        response = index.query(\"<query_str>\", mode=\"default\")\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[RefinePrompt]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        similarity_top_k (int): Number of similar nodes to retrieve.\n\n    \"\"\"\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        # TODO: consolidate with get_query_text_embedding_similarities\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n        items = self._index_struct.embedding_dict.items()\n        node_ids = [t[0] for t in items]\n        embeddings = [t[1] for t in items]\n\n        top_similarities, top_ids = get_top_k_embeddings(\n            self._embed_model,\n            query_embedding,\n            embeddings,\n            similarity_top_k=self.similarity_top_k,\n            embedding_ids=node_ids,\n        )\n        top_k_nodes = self._index_struct.get_nodes(top_ids)\n        if similarity_tracker is not None:\n            for node, similarity in zip(top_k_nodes, top_similarities):\n                similarity_tracker.add(node, similarity)\n\n        if logging.getLogger(__name__).getEffectiveLevel() == logging.DEBUG:\n            fmt_txts = []\n            for node_idx, node_similarity, node in zip(\n                top_ids, top_similarities, top_k_nodes\n            ):\n                fmt_txt = f\"> [Node {node_idx}] [Similarity score: \\\n                    {node_similarity:.6}] {truncate_text(node.get_text(), 100)}\"\n                fmt_txts.append(fmt_txt)\n            top_k_node_text = \"\\n\".join(fmt_txts)\n            logging.debug(f\"> Top {len(top_k_nodes)} nodes:\\n{top_k_node_text}\")\n\n        return top_k_nodes\n", "doc_id": "33f9551e7c7f50285582555c35fb754a11a4c61c", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/simple.py", "file_name": "simple.py"}, "__type__": "Document"}, "c74c84559cd6f32007980215425c5cdaeb0843b8": {"text": "\"\"\"Weaviate vector store index query.\"\"\"\n\n\nimport logging\nfrom typing import Any, List, Optional, cast\n\nfrom gpt_index.data_structs.data_structs import Node, WeaviateIndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.embedding_utils import SimilarityTracker\nfrom gpt_index.indices.utils import truncate_text\nfrom gpt_index.readers.weaviate.data_structs import WeaviateNode\n\n\nclass GPTWeaviateIndexQuery(BaseGPTIndexQuery[WeaviateIndexStruct]):\n    \"\"\"Base vector store query.\"\"\"\n\n    def __init__(\n        self,\n        index_struct: WeaviateIndexStruct,\n        embed_model: Optional[BaseEmbedding] = None,\n        similarity_top_k: Optional[int] = 1,\n        weaviate_client: Optional[Any] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(index_struct=index_struct, embed_model=embed_model, **kwargs)\n        self.similarity_top_k = similarity_top_k\n        import_err_msg = (\n            \"`weaviate` package not found, please run `pip install weaviate-client`\"\n        )\n        try:\n            import weaviate  # noqa: F401\n            from weaviate import Client  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n        self.client = cast(Client, weaviate_client)\n\n    def _get_nodes_for_response(\n        self,\n        query_str: str,\n        similarity_tracker: Optional[SimilarityTracker] = None,\n    ) -> List[Node]:\n        \"\"\"Get nodes for response.\"\"\"\n        query_embedding = self._embed_model.get_query_embedding(query_str)\n        nodes = WeaviateNode.to_gpt_index_list(\n            self.client,\n            self._index_struct.get_class_prefix(),\n            vector=query_embedding,\n            object_limit=self.similarity_top_k,\n        )\n        nodes = nodes[: self.similarity_top_k]\n        node_idxs = [str(i) for i in range(len(nodes))]\n\n        if logging.getLogger(__name__).getEffectiveLevel() == logging.DEBUG:\n            fmt_txts = []\n            for node_idx, node in zip(node_idxs, nodes):\n                fmt_txt = f\"> [Node {node_idx}] {truncate_text(node.get_text(), 100)}\"\n                fmt_txts.append(fmt_txt)\n            top_k_node_text = \"\\n\".join(fmt_txts)\n            logging.debug(f\"> Top {len(nodes)} nodes:\\n{top_k_node_text}\")\n        return nodes\n", "doc_id": "c74c84559cd6f32007980215425c5cdaeb0843b8", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/query/vector_store/weaviate.py", "file_name": "weaviate.py"}, "__type__": "Document"}, "4d36a4f799b01b46bb1527a5b95f7186cc3554b6": {"text": "\"\"\"Index registry.\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Type\n\nfrom gpt_index.data_structs.data_structs import IndexStruct\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\n\n# map from mode to query class\nQUERY_MAP_TYPE = Dict[str, Type[BaseGPTIndexQuery]]\n\n\n@dataclass\nclass IndexRegistry:\n    \"\"\"Index registry.\n\n    Stores mapping from index type to index_struct + queries.\n    NOTE: this cannot be easily serialized, so must be re-initialized\n    each time.\n    If the user defines custom IndexStruct or query classes,\n    they must be added to the registry manually.\n\n    \"\"\"\n\n    type_to_struct: Dict[str, Type[IndexStruct]] = field(default_factory=dict)\n    type_to_query: Dict[str, QUERY_MAP_TYPE] = field(default_factory=dict)\n\n    def update(self, other: \"IndexRegistry\") -> None:\n        \"\"\"Update the registry with another registry.\"\"\"\n        self.type_to_struct.update(other.type_to_struct)\n        self.type_to_query.update(other.type_to_query)\n", "doc_id": "4d36a4f799b01b46bb1527a5b95f7186cc3554b6", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/registry.py", "file_name": "registry.py"}, "__type__": "Document"}, "6b5adc8eec796f223f385f4911e32df83efc6785": {"text": "\"\"\"Response builder class.\n\nThis class provides general functions for taking in a set of text\nand generating a response.\n\nWill support different modes, from 1) stuffing chunks into prompt,\n2) create and refine separately over each chunk, 3) tree summarization.\n\n\"\"\"\nimport logging\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional\n\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.indices.common.tree.base import GPTTreeIndexBuilder\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.utils import get_sorted_node_list, truncate_text\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt, SummaryPrompt\nfrom gpt_index.response.schema import SourceNode\nfrom gpt_index.utils import temp_set_attrs\n\n\nclass ResponseMode(str, Enum):\n    \"\"\"Response modes.\"\"\"\n\n    DEFAULT = \"default\"\n    COMPACT = \"compact\"\n    TREE_SUMMARIZE = \"tree_summarize\"\n    NO_TEXT = \"no_text\"\n\n\n@dataclass\nclass TextChunk:\n    \"\"\"Response chunk.\"\"\"\n\n    text: str\n    # Whether this chunk is already a response\n    is_answer: bool = False\n\n\nclass ResponseBuilder:\n    \"\"\"Response builder class.\"\"\"\n\n    def __init__(\n        self,\n        prompt_helper: PromptHelper,\n        llm_predictor: LLMPredictor,\n        text_qa_template: QuestionAnswerPrompt,\n        refine_template: RefinePrompt,\n        texts: Optional[List[TextChunk]] = None,\n        nodes: Optional[List[Node]] = None,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self.prompt_helper = prompt_helper\n        self.llm_predictor = llm_predictor\n        self.text_qa_template = text_qa_template\n        self.refine_template = refine_template\n        self._texts = texts or []\n        nodes = nodes or []\n        self.source_nodes: List[SourceNode] = SourceNode.from_nodes(nodes)\n\n    def add_text_chunks(self, text_chunks: List[TextChunk]) -> None:\n        \"\"\"Add text chunk.\"\"\"\n        self._texts.extend(text_chunks)\n\n    def reset(self) -> None:\n        \"\"\"Clear text chunks.\"\"\"\n        self._texts = []\n\n    def add_node(self, node: Node, similarity: Optional[float] = None) -> None:\n        \"\"\"Add node.\"\"\"\n        self.source_nodes.append(SourceNode.from_node(node, similarity=similarity))\n\n    def add_source_node(self, source_node: SourceNode) -> None:\n        \"\"\"Add source node directly.\"\"\"\n        self.source_nodes.append(source_node)\n\n    def get_sources(self) -> List[SourceNode]:\n        \"\"\"Get sources.\"\"\"\n        return self.source_nodes\n\n    def refine_response_single(\n        self,\n        response: str,\n        query_str: str,\n        text_chunk: str,\n    ) -> str:\n        \"\"\"Refine response.\"\"\"\n        fmt_text_chunk = truncate_text(text_chunk, 50)\n        logging.debug(f\"> Refine context: {fmt_text_chunk}\")\n        # NOTE: partial format refine template with query_str and existing_answer here\n        refine_template = self.refine_template.partial_format(\n            query_str=query_str, existing_answer=response\n        )\n        refine_text_splitter = self.prompt_helper.get_text_splitter_given_prompt(\n            refine_template, 1\n        )\n        text_chunks = refine_text_splitter.split_text(text_chunk)\n        for cur_text_chunk in text_chunks:\n            response, _ = self.llm_predictor.predict(\n                refine_template,\n                context_msg=cur_text_chunk,\n            )\n            logging.debug(f\"> Refined response: {response}\")\n        return response\n\n    def give_response_single(\n        self,\n        query_str: str,\n        text_chunk: str,\n    ) -> str:\n        \"\"\"Give response given a query and a corresponding text chunk.\"\"\"\n        text_qa_template = self.text_qa_template.partial_format(query_str=query_str)\n        qa_text_splitter = self.prompt_helper.get_text_splitter_given_prompt(\n            text_qa_template, 1\n        )\n        text_chunks = qa_text_splitter.split_text(text_chunk)\n        response = None\n        # TODO: consolidate with loop in get_response_default\n        for cur_text_chunk in text_chunks:\n            if response is None:\n                response, _ = self.llm_predictor.predict(\n                    text_qa_template,\n                    context_str=cur_text_chunk,\n                )\n                logging.debug(f\"> Initial response: {response}\")\n            else:\n                response = self.refine_response_single(\n                    response,\n                    query_str,\n                    cur_text_chunk,\n                )\n        return response or \"\"\n\n    def get_response_over_chunks(\n        self,\n        query_str: str,\n        text_chunks: List[TextChunk],\n        prev_response: Optional[str] = None,\n    ) -> str:\n        \"\"\"Give response over chunks.\"\"\"\n        response = None\n        for text_chunk in text_chunks:\n            if prev_response is None:\n                # if this is the first chunk, and text chunk already\n                # is an answer, then return it\n                if text_chunk.is_answer:\n                    response = text_chunk.text\n                # otherwise give response\n                else:\n                    response = self.give_response_single(\n                        query_str,\n                        text_chunk.text,\n                    )\n            else:\n                response = self.refine_response_single(\n                    prev_response, query_str, text_chunk.text\n                )\n            prev_response = response\n        return response or \"Empty Response\"\n\n    def _get_response_default(\n        self, query_str: str, prev_response: Optional[str]\n    ) -> str:\n        return self.get_response_over_chunks(\n            query_str, self._texts, prev_response=prev_response\n        )\n\n    def _get_response_compact(\n        self, query_str: str, prev_response: Optional[str]\n    ) -> str:\n        \"\"\"Get compact response.\"\"\"\n        # use prompt helper to fix compact text_chunks under the prompt limitation\n        max_prompt = self.prompt_helper.get_biggest_prompt(\n            [self.text_qa_template, self.refine_template]\n        )\n        with temp_set_attrs(self.prompt_helper, use_chunk_size_limit=False):\n            new_texts = self.prompt_helper.compact_text_chunks(\n                max_prompt, [t.text for t in self._texts]\n            )\n            new_text_chunks = [TextChunk(text=t) for t in new_texts]\n            response = self.get_response_over_chunks(\n                query_str, new_text_chunks, prev_response=prev_response\n            )\n        return response\n\n    def _get_response_tree_summarize(\n        self,\n        query_str: str,\n        prev_response: Optional[str],\n        num_children: int = 10,\n    ) -> str:\n        \"\"\"Get tree summarize response.\"\"\"\n        text_qa_template = self.text_qa_template.partial_format(query_str=query_str)\n        summary_template = SummaryPrompt.from_prompt(text_qa_template)\n\n        # first join all the text chunks into a single text\n        all_text = \"\\n\\n\".join([t.text for t in self._texts])\n        # then get text splitter\n        text_splitter = self.prompt_helper.get_text_splitter_given_prompt(\n            summary_template, num_children\n        )\n        text_chunks = text_splitter.split_text(all_text)\n        all_nodes: Dict[int, Node] = {\n            i: Node(text=t) for i, t in enumerate(text_chunks)\n        }\n\n        index_builder = GPTTreeIndexBuilder(\n            num_children,\n            summary_template,\n            self.llm_predictor,\n            self.prompt_helper,\n        )\n        root_nodes = index_builder.build_index_from_nodes(all_nodes, all_nodes)\n        node_list = get_sorted_node_list(root_nodes)\n        node_text = self.prompt_helper.get_text_from_nodes(\n            node_list, prompt=text_qa_template\n        )\n        response = self.get_response_over_chunks(\n            query_str,\n            [TextChunk(node_text)],\n            prev_response=prev_response,\n        )\n        return response or \"Empty Response\"\n\n    def get_response(\n        self,\n        query_str: str,\n        prev_response: Optional[str] = None,\n        mode: ResponseMode = ResponseMode.DEFAULT,\n        **response_kwargs: Any,\n    ) -> str:\n        \"\"\"Get response.\"\"\"\n        if mode == ResponseMode.DEFAULT:\n            return self._get_response_default(query_str, prev_response)\n        elif mode == ResponseMode.COMPACT:\n            return self._get_response_compact(query_str, prev_response)\n        elif mode == ResponseMode.TREE_SUMMARIZE:\n            return self._get_response_tree_summarize(\n                query_str, prev_response, **response_kwargs\n            )\n        else:\n            raise ValueError(f\"Invalid mode: {mode}\")\n", "doc_id": "6b5adc8eec796f223f385f4911e32df83efc6785", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/response/builder.py", "file_name": "builder.py"}, "__type__": "Document"}, "3a3d24fbc1ebb0b17b6173f66959d30be00ce4b0": {"text": "\"\"\"Structured store indices.\"\"\"\n\nfrom gpt_index.indices.struct_store.sql import GPTSQLStructStoreIndex\n\n__all__ = [\"GPTSQLStructStoreIndex\"]\n", "doc_id": "3a3d24fbc1ebb0b17b6173f66959d30be00ce4b0", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "ed3819ef94005896d68caab2ee933ca64cb2f62c": {"text": "\"\"\"Struct store.\"\"\"\n\nimport logging\nimport re\nfrom abc import abstractmethod\nfrom typing import Any, Callable, Dict, Generic, Optional, Sequence, TypeVar\n\nfrom gpt_index.data_structs.table import BaseStructTable, StructDatapoint\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.utils import truncate_text\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_SCHEMA_EXTRACT_PROMPT\nfrom gpt_index.prompts.prompts import SchemaExtractPrompt\nfrom gpt_index.schema import BaseDocument\n\nBST = TypeVar(\"BST\", bound=BaseStructTable)\n\n\ndef default_output_parser(output: str) -> Optional[Dict[str, Any]]:\n    \"\"\"Parse output of schema extraction.\n\n    Attempt to parse the following format from the default prompt:\n    field1: <value>, field2: <value>, ...\n\n    \"\"\"\n    tups = output.split(\"\\n\")\n\n    fields = {}\n    for tup in tups:\n        if \":\" in tup:\n            tokens = tup.split(\":\")\n            field = re.sub(r\"\\W+\", \"\", tokens[0])\n            value = re.sub(r\"\\W+\", \"\", tokens[1])\n            fields[field] = value\n    return fields\n\n\nOUTPUT_PARSER_TYPE = Callable[[str], Optional[Dict[str, Any]]]\n\n\nclass BaseGPTStructStoreIndex(BaseGPTIndex[BST], Generic[BST]):\n    \"\"\"Base GPT Struct Store Index.\"\"\"\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[BST] = None,\n        schema_extract_prompt: Optional[SchemaExtractPrompt] = None,\n        output_parser: Optional[OUTPUT_PARSER_TYPE] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self.schema_extract_prompt = (\n            schema_extract_prompt or DEFAULT_SCHEMA_EXTRACT_PROMPT\n        )\n        self.output_parser = output_parser or default_output_parser\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            **kwargs,\n        )\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.schema_extract_prompt, 1\n        )\n\n    @abstractmethod\n    def _insert_datapoint(self, datapoint: StructDatapoint) -> None:\n        \"\"\"Insert datapoint into index.\"\"\"\n\n    @abstractmethod\n    def _get_col_types_map(self) -> Dict[str, type]:\n        \"\"\"Get col types map for schema.\"\"\"\n\n    def _clean_and_validate_fields(self, fields: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate fields with col_types_map.\"\"\"\n        new_fields = {}\n        col_types_map = self._get_col_types_map()\n        for field, value in fields.items():\n            clean_value = value\n            if field not in col_types_map:\n                continue\n            # if expected type is int or float, try to convert value to int or float\n            expected_type = col_types_map[field]\n            if expected_type == int:\n                try:\n                    clean_value = int(value)\n                except ValueError:\n                    continue\n            elif expected_type == float:\n                try:\n                    clean_value = float(value)\n                except ValueError:\n                    continue\n            else:\n                if len(value) == 0:\n                    continue\n                if not isinstance(value, col_types_map[field]):\n                    continue\n            new_fields[field] = clean_value\n        return new_fields\n\n    @abstractmethod\n    def _get_schema_text(self) -> str:\n        \"\"\"Get schema text for extracting relevant info from unstructured text.\"\"\"\n\n    def _add_document_to_index(\n        self,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        text_chunks = text_splitter.split_text(document.get_text())\n        fields = {}\n        for i, text_chunk in enumerate(text_chunks):\n            fmt_text_chunk = truncate_text(text_chunk, 50)\n            logging.info(f\"> Adding chunk {i}: {fmt_text_chunk}\")\n            # if embedding specified in document, pass it to the Node\n            schema_text = self._get_schema_text()\n            response_str, _ = self._llm_predictor.predict(\n                self.schema_extract_prompt,\n                text=text_chunk,\n                schema=schema_text,\n            )\n            cur_fields = self.output_parser(response_str)\n            if cur_fields is None:\n                continue\n            # validate fields with col_types_map\n            new_cur_fields = self._clean_and_validate_fields(cur_fields)\n            fields.update(new_cur_fields)\n\n        struct_datapoint = StructDatapoint(fields)\n        if struct_datapoint is not None:\n            self._insert_datapoint(struct_datapoint)\n            logging.debug(f\"> Added datapoint: {fields}\")\n\n    def _build_index_from_documents(self, documents: Sequence[BaseDocument]) -> BST:\n        \"\"\"Build index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.schema_extract_prompt, 1\n        )\n        index_struct = self.index_struct_cls()\n        for d in documents:\n            self._add_document_to_index(d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(document, self._text_splitter)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        raise NotImplementedError(\"Delete not implemented for Struct Store Index.\")\n", "doc_id": "ed3819ef94005896d68caab2ee933ca64cb2f62c", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/base.py", "file_name": "base.py"}, "__type__": "Document"}, "9b52c03318a639e578f5784857f4fa6403f1c7e2": {"text": "\"\"\"SQLite structured store.\"\"\"\n\nfrom typing import Any, Dict, List, Optional, Sequence, Type, cast\n\nfrom sqlalchemy import Table\n\nfrom gpt_index.data_structs.table import SQLStructTable, StructDatapoint\nfrom gpt_index.indices.base import DOCUMENTS_INPUT\nfrom gpt_index.indices.common.struct_store.base import SQLContextBuilder\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.struct_store.sql import (\n    GPTNLStructStoreIndexQuery,\n    GPTSQLStructStoreIndexQuery,\n)\nfrom gpt_index.indices.struct_store.base import BaseGPTStructStoreIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.sql_wrapper import SQLDatabase\nfrom gpt_index.schema import BaseDocument\n\n\nclass GPTSQLStructStoreIndex(BaseGPTStructStoreIndex[SQLStructTable]):\n    \"\"\"Base GPT SQL Struct Store Index.\n\n    The GPTSQLStructStoreIndex is an index that uses a SQL database\n    under the hood. During index construction, the data can be inferred\n    from unstructured documents given a schema extract prompt,\n    or it can be pre-loaded in the database.\n\n    During query time, the user can either specify a raw SQL query\n    or a natural language query to retrieve their data.\n\n    Args:\n        sql_database (Optional[SQLDatabase]): SQL database to use,\n            including table names to specify.\n            See :ref:`Ref-Struct-Store` for more details.\n        table_name (Optional[str]): Name of the table to use\n            for extracting data.\n            Either table_name or table must be specified.\n        table (Optional[Table]): SQLAlchemy Table object to use.\n            Specifying the Table object explicitly, instead of\n            the table name, allows you to pass in a view.\n            Either table_name or table must be specified.\n        table_context_dict (Optional[Dict[str, str]]): Optional table context to use.\n            If specified,\n            sql_context_builder and context_documents cannot be specified.\n        sql_context_builder (Optional[SQLContextBuilder]): SQL context builder.\n            If specified, the context builder will be used to build\n            context for the specified table, which will then be used during\n            query-time. Also if specified, context_documents must be specified,\n            and table_context cannot be specified.\n        context_documents_dict (Optional[Dict[str, List[BaseDocument]]]):\n            Optional context\n            documents to inform the sql_context_builder. Must be specified if\n            sql_context_builder is specified. Cannot be specified if table_context\n            is specified.\n\n    \"\"\"\n\n    index_struct_cls = SQLStructTable\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[SQLStructTable] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        sql_database: Optional[SQLDatabase] = None,\n        table_name: Optional[str] = None,\n        table: Optional[Table] = None,\n        ref_doc_id_column: Optional[str] = None,\n        table_context_dict: Optional[Dict[str, str]] = None,\n        sql_context_builder: Optional[SQLContextBuilder] = None,\n        context_documents_dict: Optional[Dict[str, List[BaseDocument]]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        # currently the user must specify a table info\n        if table_name is None and table is None:\n            raise ValueError(\"table_name must be specified\")\n        self.table_name = table_name or cast(Table, table).name\n        if sql_database is None:\n            raise ValueError(\"sql_database must be specified\")\n        self.sql_database = sql_database\n        if table is None:\n            table = self.sql_database.metadata_obj.tables[table_name]\n        # if ref_doc_id_column is specified, then we need to check that\n        # it is a valid column in the table\n        col_names = [c.name for c in table.c]\n        if ref_doc_id_column is not None and ref_doc_id_column not in col_names:\n            raise ValueError(\n                f\"ref_doc_id_column {ref_doc_id_column} not in table {table_name}\"\n            )\n        self.ref_doc_id_column = ref_doc_id_column\n        # then store python types of each column\n        self._col_types_map: Dict[str, type] = {\n            c.name: table.c[c.name].type.python_type for c in table.c\n        }\n\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            **kwargs,\n        )\n\n        # if context builder is specified, then add to context_dict\n        if table_context_dict is not None and (\n            sql_context_builder is not None or context_documents_dict is not None\n        ):\n            raise ValueError(\n                \"Cannot specify both table_context_dict and \"\n                \"sql_context_builder/context_documents_dict\"\n            )\n        if sql_context_builder is not None:\n            if context_documents_dict is None:\n                raise ValueError(\n                    \"context_documents_dict must be specified if \"\n                    \"sql_context_builder is specified\"\n                )\n            context_documents_dict = cast(\n                Dict[str, List[BaseDocument]], context_documents_dict\n            )\n            context_dict: Dict[\n                str, str\n            ] = sql_context_builder.build_all_context_from_documents(\n                context_documents_dict\n            )\n        elif table_context_dict is not None:\n            context_dict = table_context_dict\n        else:\n            context_dict = {}\n\n        # validate context_dict keys are valid table names\n        context_keys = set(context_dict.keys())\n        if not context_keys.issubset(set(self.sql_database.get_table_names())):\n            raise ValueError(\n                \"Invalid context table names: \"\n                f\"{context_keys - set(self.sql_database.get_table_names())}\"\n            )\n\n        self._index_struct.context_dict.update(context_dict)\n        self._sql_context_builder = sql_context_builder\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTNLStructStoreIndexQuery,\n            QueryMode.SQL: GPTSQLStructStoreIndexQuery,\n        }\n\n    def _get_col_types_map(self) -> Dict[str, type]:\n        \"\"\"Get col types map for schema.\"\"\"\n        return self._col_types_map\n\n    def _get_schema_text(self) -> str:\n        \"\"\"Insert datapoint into index.\"\"\"\n        return self.sql_database.get_single_table_info(self.table_name)\n\n    def _insert_datapoint(self, datapoint: StructDatapoint) -> None:\n        \"\"\"Insert datapoint into index.\"\"\"\n        datapoint_dict = datapoint.to_dict()[\"fields\"]\n        self.sql_database.insert_into_table(\n            self.table_name, cast(Dict[Any, Any], datapoint_dict)\n        )\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Any) -> None:\n        \"\"\"Preprocess query.\n\n        This allows subclasses to pass in additional query kwargs\n        to query, for instance arguments that are shared between the\n        index and the query class. By default, this does nothing.\n        This also allows subclasses to do validation.\n\n        \"\"\"\n        super()._preprocess_query(mode, query_kwargs)\n        # pass along sql_database, table_name\n        query_kwargs[\"sql_database\"] = self.sql_database\n        if mode == QueryMode.DEFAULT:\n            query_kwargs[\"ref_doc_id_column\"] = self.ref_doc_id_column\n", "doc_id": "9b52c03318a639e578f5784857f4fa6403f1c7e2", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/struct_store/sql.py", "file_name": "sql.py"}, "__type__": "Document"}, "aa53c642b9c9f3e8695dcbe11ea3a6a33628a1fd": {"text": "\n\n\ud83c\udf32 Tree Index\n\nCurrently the tree index refers to the `GPTTreeIndex` class. It organizes external data into a tree structure that can be queried.\n\n\n\n\n\nIndex Construction\n\nThe `GPTTreeIndex` first takes in a set of text documents as input. It then builds up a tree-index in a bottom-up fashion; each parent node is able to summarize the children nodes using a general **summarization prompt**; each intermediate node contains text summarizing the components below. Once the index is built, it can be saved to disk as a JSON and loaded for future use.\n\n\n\n\n\nQuery\n\nThere are two query modes: `default` and `retrieve`.\n\n**Default (GPTTreeIndexLeafQuery)**\n\nUsing a **query prompt template**, the GPTTreeIndex will be able to recursively perform tree traversal in a top-down fashion in order to answer a question. For example, in the very beginning GPT-3 is tasked with selecting between _n_ top-level nodes which best answers a provided query, by outputting a number as a multiple-choice problem. The GPTTreeIndex then uses the number to select the corresponding node, and the process repeats recursively among the children nodes until a leaf node is reached.\n\n**Retrieve (GPTTreeIndexRetQuery)**\n\nSimply use the root nodes as context to synthesize an answer to the query. This is especially effective if the tree is preseeded with a `query_str`.\n\n\n\n\n\nUsage\n\n```python\nfrom gpt_index import GPTTreeIndex, SimpleDirectoryReader\n\n\n\n\n\nbuild index\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = GPTTreeIndex(documents)\n\n\n\n\nsave index\nindex.save_to_disk('index_tree.json')\n\n\n\n\nload index from disk\nindex = GPTListIndex.load_from_disk('index_tree.json')\n\n\n\n\nquery\nresponse = index.query(\"\", mode=\"default\")\n```\n\n\n\n\n\nFAQ\n\n**Why build a tree? Why not just incrementally go through each chunk?**\n\nAlgorithmically speaking, $O(\\log N)$ is better than $O(N)$.\n\nMore broadly, building a tree helps us to test GPT's capabilities in modeling information in a hierarchy. It seems to me that our brains organize information in a similar way (citation needed). We can use this design to test how GPT can use its own hierarchy to answer questions.\n\nPractically speaking, it is much cheaper to do so and I want to limit my monthly spending (see below for costs).\n\n**How much does this cost to run?**\n\nWe currently use the Davinci model for good results. Unfortunately Davinci is quite expensive. The cost of building the tree is roughly\n$cN\\log(N)\\frac{p}{1000}$, where $p=4096$ is the prompt limit and $c$ is the cost per 1000 tokens ($0.02 as mentioned on the pricing page). The cost of querying the tree is roughly \n$c\\log(N)\\frac{p}{1000}$.\n\nFor the NYC example, this equates to \\$~0.40 per query.\n\n", "doc_id": "aa53c642b9c9f3e8695dcbe11ea3a6a33628a1fd", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/tree/README.md", "file_name": "gpt_index/indices/tree/README.md"}, "__type__": "Document"}, "c13b792b07486dc27964e134193fcbfbe2b877a5": {"text": "\"\"\"Tree-structured Index Data Structures.\"\"\"\n\n# indices\nfrom gpt_index.indices.tree.base import GPTTreeIndex\n\n__all__ = [\n    \"GPTTreeIndex\",\n]\n", "doc_id": "c13b792b07486dc27964e134193fcbfbe2b877a5", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/tree/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "e627d6f084fdbaefa7f0a5c7d67f5efba7ae8b39": {"text": "\"\"\"Tree-based index.\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence, Type\n\nfrom gpt_index.data_structs.data_structs import IndexGraph\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.common.tree.base import GPTTreeIndexBuilder\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.tree.embedding_query import GPTTreeIndexEmbeddingQuery\nfrom gpt_index.indices.query.tree.leaf_query import GPTTreeIndexLeafQuery\nfrom gpt_index.indices.query.tree.retrieve_query import GPTTreeIndexRetQuery\nfrom gpt_index.indices.query.tree.summarize_query import GPTTreeIndexSummarizeQuery\nfrom gpt_index.indices.tree.inserter import GPTIndexInserter\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.default_prompts import (\n    DEFAULT_INSERT_PROMPT,\n    DEFAULT_SUMMARY_PROMPT,\n)\nfrom gpt_index.prompts.prompts import SummaryPrompt, TreeInsertPrompt\nfrom gpt_index.schema import BaseDocument\n\nREQUIRE_TREE_MODES = {\n    QueryMode.DEFAULT,\n    QueryMode.EMBEDDING,\n    QueryMode.RETRIEVE,\n}\n\n\nclass GPTTreeIndex(BaseGPTIndex[IndexGraph]):\n    \"\"\"GPT Tree Index.\n\n    The tree index is a tree-structured index, where each node is a summary of\n    the children nodes. During index construction, the tree is constructed\n    in a bottoms-up fashion until we end up with a set of root_nodes.\n\n    There are a few different options during query time (see :ref:`Ref-Query`).\n    The main option is to traverse down the tree from the root nodes.\n    A secondary answer is to directly synthesize the answer from the root nodes.\n\n    Args:\n        summary_template (Optional[SummaryPrompt]): A Summarization Prompt\n            (see :ref:`Prompt-Templates`).\n        insert_prompt (Optional[TreeInsertPrompt]): An Tree Insertion Prompt\n            (see :ref:`Prompt-Templates`).\n        num_children (int): The number of children each node should have.\n        build_tree (bool): Whether to build the tree during index construction.\n\n    \"\"\"\n\n    index_struct_cls = IndexGraph\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[IndexGraph] = None,\n        summary_template: Optional[SummaryPrompt] = None,\n        insert_prompt: Optional[TreeInsertPrompt] = None,\n        num_children: int = 10,\n        llm_predictor: Optional[LLMPredictor] = None,\n        build_tree: bool = True,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        # need to set parameters before building index in base class.\n        self.num_children = num_children\n        self.summary_template = summary_template or DEFAULT_SUMMARY_PROMPT\n        self.insert_prompt: TreeInsertPrompt = insert_prompt or DEFAULT_INSERT_PROMPT\n        self.build_tree = build_tree\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            **kwargs,\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTTreeIndexLeafQuery,\n            QueryMode.EMBEDDING: GPTTreeIndexEmbeddingQuery,\n            QueryMode.RETRIEVE: GPTTreeIndexRetQuery,\n            QueryMode.SUMMARIZE: GPTTreeIndexSummarizeQuery,\n        }\n\n    def _validate_build_tree_required(self, mode: QueryMode) -> None:\n        \"\"\"Check if index supports modes that require trees.\"\"\"\n        if mode in REQUIRE_TREE_MODES and not self.build_tree:\n            raise ValueError(\n                \"Index was constructed without building trees, \"\n                f\"but mode {mode} requires trees.\"\n            )\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Any) -> None:\n        \"\"\"Query mode to class.\"\"\"\n        super()._preprocess_query(mode, query_kwargs)\n        self._validate_build_tree_required(mode)\n\n    def _build_index_from_documents(\n        self, documents: Sequence[BaseDocument]\n    ) -> IndexGraph:\n        \"\"\"Build the index from documents.\"\"\"\n        # do simple concatenation\n        index_builder = GPTTreeIndexBuilder(\n            self.num_children,\n            self.summary_template,\n            self._llm_predictor,\n            self._prompt_helper,\n        )\n        index_graph = index_builder.build_from_text(\n            documents, build_tree=self.build_tree\n        )\n        return index_graph\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        # TODO: allow to customize insert prompt\n        inserter = GPTIndexInserter(\n            self.index_struct,\n            num_children=self.num_children,\n            insert_prompt=self.insert_prompt,\n            summary_prompt=self.summary_template,\n            llm_predictor=self._llm_predictor,\n            prompt_helper=self._prompt_helper,\n        )\n        inserter.insert(document)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        raise NotImplementedError(\"Delete not implemented for tree index.\")\n", "doc_id": "e627d6f084fdbaefa7f0a5c7d67f5efba7ae8b39", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/tree/base.py", "file_name": "base.py"}, "__type__": "Document"}, "9fec09d19ce22bfae0b185a7f3c383cc178ef2e7": {"text": "\"\"\"GPT Tree Index inserter.\"\"\"\n\nfrom typing import Optional\n\nfrom gpt_index.data_structs.data_structs import IndexGraph, Node\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.utils import extract_numbers_given_response, get_sorted_node_list\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.prompts.default_prompts import (\n    DEFAULT_INSERT_PROMPT,\n    DEFAULT_SUMMARY_PROMPT,\n)\nfrom gpt_index.schema import BaseDocument\n\n\nclass GPTIndexInserter:\n    \"\"\"GPT Index inserter.\"\"\"\n\n    def __init__(\n        self,\n        index_graph: IndexGraph,\n        llm_predictor: LLMPredictor,\n        prompt_helper: PromptHelper,\n        num_children: int = 10,\n        insert_prompt: Prompt = DEFAULT_INSERT_PROMPT,\n        summary_prompt: Prompt = DEFAULT_SUMMARY_PROMPT,\n    ) -> None:\n        \"\"\"Initialize with params.\"\"\"\n        if num_children < 2:\n            raise ValueError(\"Invalid number of children.\")\n        self.num_children = num_children\n        self.summary_prompt = summary_prompt\n        self.insert_prompt = insert_prompt\n        self.index_graph = index_graph\n        self._llm_predictor = llm_predictor\n        self._prompt_helper = prompt_helper\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.summary_prompt, self.num_children\n        )\n\n    def _insert_under_parent_and_consolidate(\n        self, text_chunk: str, doc: BaseDocument, parent_node: Optional[Node]\n    ) -> None:\n        \"\"\"Insert node under parent and consolidate.\n\n        Consolidation will happen by dividing up child nodes, and creating a new\n        intermediate layer of nodes.\n\n        \"\"\"\n        # perform insertion\n        text_node = Node(\n            text=text_chunk,\n            index=self.index_graph.size,\n            ref_doc_id=doc.get_doc_id(),\n            embedding=doc.embedding,\n            extra_info=doc.extra_info,\n        )\n        self.index_graph.insert_under_parent(text_node, parent_node)\n\n        # if under num_children limit, then we're fine\n        if len(self.index_graph.get_children(parent_node)) <= self.num_children:\n            return\n        else:\n            # perform consolidation\n            cur_graph_nodes = self.index_graph.get_children(parent_node)\n            cur_graph_node_list = get_sorted_node_list(cur_graph_nodes)\n            # this layer is all leaf nodes, consolidate and split leaf nodes\n            cur_node_index = self.index_graph.size\n            # consolidate and split leaf nodes in half\n            # TODO: do better splitting (with a GPT prompt etc.)\n            half1 = cur_graph_node_list[: len(cur_graph_nodes) // 2]\n            half2 = cur_graph_node_list[len(cur_graph_nodes) // 2 :]\n\n            text_chunk1 = self._prompt_helper.get_text_from_nodes(\n                half1, prompt=self.summary_prompt\n            )\n            summary1, _ = self._llm_predictor.predict(\n                self.summary_prompt, context_str=text_chunk1\n            )\n            node1 = Node(\n                text=summary1,\n                index=cur_node_index,\n                child_indices={n.index for n in half1},\n            )\n\n            text_chunk2 = self._prompt_helper.get_text_from_nodes(\n                half2, prompt=self.summary_prompt\n            )\n            summary2, _ = self._llm_predictor.predict(\n                self.summary_prompt, context_str=text_chunk2\n            )\n            node2 = Node(\n                text=summary2,\n                index=cur_node_index + 1,\n                child_indices={n.index for n in half2},\n            )\n\n            # insert half1 and half2 as new children of parent_node\n            # first remove child indices from parent node\n            if parent_node is not None:\n                parent_node.child_indices = set()\n            else:\n                self.index_graph.root_nodes = {}\n            self.index_graph.insert_under_parent(node1, parent_node)\n            self.index_graph.insert_under_parent(node2, parent_node)\n\n    def _insert_node(\n        self, text_chunk: str, doc: BaseDocument, parent_node: Optional[Node]\n    ) -> None:\n        \"\"\"Insert node.\"\"\"\n        cur_graph_nodes = self.index_graph.get_children(parent_node)\n        cur_graph_node_list = get_sorted_node_list(cur_graph_nodes)\n        # if cur_graph_nodes is empty (start with empty graph), then insert under\n        # parent (insert new root node)\n        if len(cur_graph_nodes) == 0:\n            self._insert_under_parent_and_consolidate(text_chunk, doc, parent_node)\n        # check if leaf nodes, then just insert under parent\n        elif len(cur_graph_node_list[0].child_indices) == 0:\n            self._insert_under_parent_and_consolidate(text_chunk, doc, parent_node)\n        # else try to find the right summary node to insert under\n        else:\n            numbered_text = self._prompt_helper.get_numbered_text_from_nodes(\n                cur_graph_node_list, prompt=self.insert_prompt\n            )\n            response, _ = self._llm_predictor.predict(\n                self.insert_prompt,\n                new_chunk_text=text_chunk,\n                num_chunks=len(cur_graph_node_list),\n                context_list=numbered_text,\n            )\n            numbers = extract_numbers_given_response(response)\n            if numbers is None or len(numbers) == 0:\n                # NOTE: if we can't extract a number, then we just insert under parent\n                self._insert_under_parent_and_consolidate(text_chunk, doc, parent_node)\n            elif int(numbers[0]) > len(cur_graph_node_list):\n                # NOTE: if number is out of range, then we just insert under parent\n                self._insert_under_parent_and_consolidate(text_chunk, doc, parent_node)\n            else:\n                selected_node = cur_graph_node_list[int(numbers[0]) - 1]\n                self._insert_node(text_chunk, doc, selected_node)\n\n        # now we need to update summary for parent node, since we\n        # need to bubble updated summaries up the tree\n        if parent_node is not None:\n            # refetch children\n            cur_graph_nodes = self.index_graph.get_children(parent_node)\n            cur_graph_node_list = get_sorted_node_list(cur_graph_nodes)\n            text_chunk = self._prompt_helper.get_text_from_nodes(\n                cur_graph_node_list, prompt=self.summary_prompt\n            )\n            new_summary, _ = self._llm_predictor.predict(\n                self.summary_prompt, context_str=text_chunk\n            )\n\n            parent_node.text = new_summary\n\n    def insert(self, doc: BaseDocument) -> None:\n        \"\"\"Insert into index_graph.\"\"\"\n        text_chunks = self._text_splitter.split_text(doc.get_text())\n\n        for text_chunk in text_chunks:\n            self._insert_node(text_chunk, doc, None)\n", "doc_id": "9fec09d19ce22bfae0b185a7f3c383cc178ef2e7", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/tree/inserter.py", "file_name": "inserter.py"}, "__type__": "Document"}, "4622a34aedaeb346c6f275d45c96bb8ae57d202b": {"text": "\"\"\"Utilities for GPT indices.\"\"\"\nimport re\nfrom typing import Dict, List, Optional, Set\n\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.utils import globals_helper\n\n\ndef get_sorted_node_list(node_dict: Dict[int, Node]) -> List[Node]:\n    \"\"\"Get sorted node list. Used by tree-strutured indices.\"\"\"\n    sorted_indices = sorted(node_dict.keys())\n    return [node_dict[index] for index in sorted_indices]\n\n\ndef extract_numbers_given_response(response: str, n: int = 1) -> Optional[List[int]]:\n    \"\"\"Extract number given the GPT-generated response.\n\n    Used by tree-structured indices.\n\n    \"\"\"\n    numbers = re.findall(r\"\\d+\", response)\n    if len(numbers) == 0:\n        return None\n    else:\n        return numbers[:n]\n\n\ndef expand_tokens_with_subtokens(tokens: Set[str]) -> Set[str]:\n    \"\"\"Get subtokens from a list of tokens., filtering for stopwords.\"\"\"\n    results = set()\n    for token in tokens:\n        results.add(token)\n        sub_tokens = re.findall(r\"\\w+\", token)\n        if len(sub_tokens) > 1:\n            results.update({w for w in sub_tokens if w not in globals_helper.stopwords})\n\n    return results\n\n\ndef truncate_text(text: str, max_length: int) -> str:\n    \"\"\"Truncate text to a maximum length.\"\"\"\n    return text[: max_length - 3] + \"...\"\n", "doc_id": "4622a34aedaeb346c6f275d45c96bb8ae57d202b", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/utils.py", "file_name": "utils.py"}, "__type__": "Document"}, "095e81a317c46ee013bad276c00ee6523870019c": {"text": "\"\"\"Vector-store based data structures.\"\"\"\n\nfrom gpt_index.indices.vector_store.faiss import GPTFaissIndex\nfrom gpt_index.indices.vector_store.pinecone import GPTPineconeIndex\nfrom gpt_index.indices.vector_store.qdrant import GPTQdrantIndex\nfrom gpt_index.indices.vector_store.simple import GPTSimpleVectorIndex\nfrom gpt_index.indices.vector_store.weaviate import GPTWeaviateIndex\n\n__all__ = [\n    \"GPTFaissIndex\",\n    \"GPTSimpleVectorIndex\",\n    \"GPTWeaviateIndex\",\n    \"GPTPineconeIndex\",\n    \"GPTQdrantIndex\",\n]\n", "doc_id": "095e81a317c46ee013bad276c00ee6523870019c", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "e7576a555defa75d31ec9e716c927ee9b9e97d8f": {"text": "\"\"\"Base vector store index.\n\nAn index that that is built on top of an existing vector store.\n\n\"\"\"\n\nfrom abc import abstractmethod\nfrom typing import Any, Generic, Optional, Sequence, TypeVar\n\nfrom gpt_index.data_structs.data_structs import IndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.schema import BaseDocument\n\nBID = TypeVar(\"BID\", bound=IndexStruct)\n\n\nclass BaseGPTVectorStoreIndex(BaseGPTIndex[BID], Generic[BID]):\n    \"\"\"Base GPT Vector Store Index.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n    \"\"\"\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[BID] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            embed_model=embed_model,\n            **kwargs,\n        )\n        # NOTE: when building the vector store index, text_qa_template is not partially\n        # formatted because we don't know the query ahead of time.\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n\n    @abstractmethod\n    def _add_document_to_index(\n        self,\n        index_struct: BID,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n\n    def _build_index_from_documents(self, documents: Sequence[BaseDocument]) -> BID:\n        \"\"\"Build index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n        index_struct = self.index_struct_cls()\n        for d in documents:\n            self._add_document_to_index(index_struct, d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(self._index_struct, document, self._text_splitter)\n", "doc_id": "e7576a555defa75d31ec9e716c927ee9b9e97d8f", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/base.py", "file_name": "base.py"}, "__type__": "Document"}, "f211883caf76a36c2a3c024807976f40924947fc": {"text": "\"\"\"Faiss Vector store index.\n\nAn index that that is built on top of an existing vector store.\n\n\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence, Type, cast\n\nimport numpy as np\n\nfrom gpt_index.data_structs.data_structs import IndexDict\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.vector_store.faiss import GPTFaissIndexQuery\nfrom gpt_index.indices.vector_store.base import BaseGPTVectorStoreIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.schema import BaseDocument\n\n\nclass GPTFaissIndex(BaseGPTVectorStoreIndex[IndexDict]):\n    \"\"\"GPT Faiss Index.\n\n    The GPTFaissIndex is a data structure where nodes are keyed by\n    embeddings, and those embeddings are stored within a Faiss index.\n    During index construction, the document texts are chunked up,\n    converted to nodes with text; they are then encoded in\n    document embeddings stored within Faiss.\n\n    During query time, the index uses Faiss to query for the top\n    k most similar nodes, and synthesizes an answer from the\n    retrieved nodes.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        faiss_index (faiss.Index): A Faiss Index object (required). Note: the index\n            will be reset during index construction.\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n    \"\"\"\n\n    index_struct_cls = IndexDict\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[IndexDict] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        faiss_index: Optional[Any] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        import_err_msg = \"\"\"\n            `faiss` package not found. For instructions on\n            how to install `faiss` please visit\n            https://github.com/facebookresearch/faiss/wiki/Installing-Faiss\n        \"\"\"\n        try:\n            import faiss  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n\n        if faiss_index is None:\n            raise ValueError(\"faiss_index cannot be None.\")\n        if documents is not None and faiss_index.ntotal > 0:\n            raise ValueError(\n                \"If building a GPTFaissIndex from scratch, faiss_index must be empty.\"\n            )\n        self._faiss_index = cast(faiss.Index, faiss_index)\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            text_qa_template=text_qa_template,\n            llm_predictor=llm_predictor,\n            embed_model=embed_model,\n            **kwargs,\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTFaissIndexQuery,\n            QueryMode.EMBEDDING: GPTFaissIndexQuery,\n        }\n\n    def _add_document_to_index(\n        self,\n        index_struct: IndexDict,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        nodes = self._get_nodes_from_document(document, text_splitter)\n        for n in nodes:\n            # add to FAISS\n            # NOTE: embeddings won't be stored in Node but rather in underlying\n            # Faiss store\n            if n.embedding is None:\n                text_embedding = self._embed_model.get_text_embedding(n.get_text())\n            else:\n                text_embedding = n.embedding\n\n            text_embedding_np = np.array(text_embedding, dtype=\"float32\")[np.newaxis, :]\n            new_id = str(self._faiss_index.ntotal)\n            self._faiss_index.add(text_embedding_np)\n\n            # add to index\n            index_struct.add_node(n, text_id=new_id)\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Any) -> None:\n        \"\"\"Preprocess query.\n\n        This allows subclasses to pass in additional query kwargs\n        to query, for instance arguments that are shared between the\n        index and the query class. By default, this does nothing.\n        This also allows subclasses to do validation.\n\n        \"\"\"\n        super()._preprocess_query(mode, query_kwargs)\n        # pass along faiss_index\n        query_kwargs[\"faiss_index\"] = self._faiss_index\n\n    @classmethod\n    def load_from_disk(\n        cls, save_path: str, faiss_index_save_path: Optional[str] = None, **kwargs: Any\n    ) -> \"BaseGPTIndex\":\n        \"\"\"Load index from disk.\n\n        This method loads the index from a JSON file stored on disk. The index data\n        structure itself is preserved completely. If the index is defined over\n        subindices, those subindices will also be preserved (and subindices of\n        those subindices, etc.).\n        In GPTFaissIndex, we allow user to specify an additional\n        `faiss_index_save_path` to load faiss index from a file - that\n        way, the user does not have to recreate the faiss index outside\n        of this class.\n\n        Args:\n            save_path (str): The save_path of the file.\n            faiss_index_save_path (Optional[str]): The save_path of the\n                Faiss index file. If not specified, the Faiss index\n                will not be saved to disk.\n            **kwargs: Additional kwargs to pass to the index constructor.\n\n        Returns:\n            BaseGPTIndex: The loaded index.\n\n        \"\"\"\n        if faiss_index_save_path is not None:\n            import faiss\n\n            faiss_index = faiss.read_index(faiss_index_save_path)\n            return super().load_from_disk(save_path, faiss_index=faiss_index, **kwargs)\n        else:\n            return super().load_from_disk(save_path, **kwargs)\n\n    def save_to_disk(\n        self,\n        save_path: str,\n        faiss_index_save_path: Optional[str] = None,\n        **save_kwargs: Any,\n    ) -> None:\n        \"\"\"Save to file.\n\n        This method stores the index into a JSON file stored on disk.\n        In GPTFaissIndex, we allow user to specify an additional\n        `faiss_index_save_path` to save the faiss index to a file - that\n        way, the user can pass in the same argument in\n        `GPTFaissIndex.load_from_disk` without having to recreate\n        the Faiss index outside of this class.\n\n        Args:\n            save_path (str): The save_path of the file.\n            faiss_index_save_path (Optional[str]): The save_path of the\n                Faiss index file. If not specified, the Faiss index\n                will not be saved to disk.\n\n        \"\"\"\n        super().save_to_disk(save_path, **save_kwargs)\n\n        if faiss_index_save_path is not None:\n            import faiss\n\n            faiss.write_index(self._faiss_index, faiss_index_save_path)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        raise NotImplementedError(\"Delete not yet implemented for Faiss index.\")\n", "doc_id": "f211883caf76a36c2a3c024807976f40924947fc", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/faiss.py", "file_name": "faiss.py"}, "__type__": "Document"}, "b51addbca1a5a149e748be38f9cfe1d28e5577c3": {"text": "\"\"\"Pinecone Vector store index.\n\nAn index that that is built on top of an existing vector store.\n\n\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence, Type, cast\n\nfrom gpt_index.data_structs.data_structs import PineconeIndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.vector_store.pinecone import GPTPineconeIndexQuery\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.schema import BaseDocument\nfrom gpt_index.utils import get_new_id\n\n\nclass GPTPineconeIndex(BaseGPTIndex[PineconeIndexStruct]):\n    \"\"\"GPT Pinecone Index.\n\n    The GPTPineconeIndex is a data structure where nodes are keyed by\n    embeddings, and those embeddings are stored within a Pinecone index.\n    During index construction, the document texts are chunked up,\n    converted to nodes with text; they are then encoded in\n    document embeddings stored within Pinecone.\n\n    During query time, the index uses Pinecone to query for the top\n    k most similar nodes, and synthesizes an answer from the\n    retrieved nodes.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        chunk_size_limit (int): Maximum number of tokens per chunk. NOTE:\n            in Pinecone the default is 2048 due to metadata size restrictions.\n    \"\"\"\n\n    index_struct_cls = PineconeIndexStruct\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[PineconeIndexStruct] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        pinecone_index: Optional[Any] = None,\n        chunk_size_limit: int = 2048,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        import_err_msg = (\n            \"`pinecone` package not found, please run `pip install pinecone-client`\"\n        )\n        try:\n            import pinecone  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n        self._pinecone_index = cast(pinecone.Index, pinecone_index)\n\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            embed_model=embed_model,\n            chunk_size_limit=chunk_size_limit,\n            **kwargs,\n        )\n        # NOTE: when building the vector store index, text_qa_template is not partially\n        # formatted because we don't know the query ahead of time.\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTPineconeIndexQuery,\n            QueryMode.EMBEDDING: GPTPineconeIndexQuery,\n        }\n\n    def _add_document_to_index(\n        self,\n        index_struct: PineconeIndexStruct,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        nodes = self._get_nodes_from_document(document, text_splitter)\n        for n in nodes:\n            if n.embedding is None:\n                text_embedding = self._embed_model.get_text_embedding(n.get_text())\n            else:\n                text_embedding = n.embedding\n\n            while True:\n                new_id = get_new_id(set())\n                result = self._pinecone_index.fetch([new_id])\n                if len(result[\"vectors\"]) == 0:\n                    break\n\n            metadata = {\n                \"text\": n.get_text(),\n                \"doc_id\": document.get_doc_id(),\n            }\n\n            self._pinecone_index.upsert([(new_id, text_embedding, metadata)])\n\n    def _build_index_from_documents(\n        self, documents: Sequence[BaseDocument]\n    ) -> PineconeIndexStruct:\n        \"\"\"Build index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n        index_struct = self.index_struct_cls()\n        for d in documents:\n            self._add_document_to_index(index_struct, d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(self._index_struct, document, self._text_splitter)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        # delete by filtering on the doc_id metadata\n        self._pinecone_index.delete(filter={\"doc_id\": {\"$eq\": doc_id}})\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Any) -> None:\n        \"\"\"Query mode to class.\"\"\"\n        super()._preprocess_query(mode, query_kwargs)\n        # pass along pinecone client and info\n        query_kwargs[\"pinecone_index\"] = self._pinecone_index\n", "doc_id": "b51addbca1a5a149e748be38f9cfe1d28e5577c3", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/pinecone.py", "file_name": "pinecone.py"}, "__type__": "Document"}, "bf1118e9a20657b90f4e5cbb9ae924330db5f50f": {"text": "\"\"\"Qdrant vector store index.\n\nAn index that is built on top of an existing Qdrant collection.\n\n\"\"\"\nfrom typing import Any, Dict, Optional, Sequence, Type, cast\n\nfrom gpt_index.data_structs.data_structs import QdrantIndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.vector_store.qdrant import GPTQdrantIndexQuery\nfrom gpt_index.indices.vector_store.base import BaseGPTVectorStoreIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.schema import BaseDocument\nfrom gpt_index.utils import get_new_id\n\n\nclass GPTQdrantIndex(BaseGPTVectorStoreIndex[QdrantIndexStruct]):\n    \"\"\"GPT Qdrant Index.\n\n    The GPTQdrantIndex is a data structure where nodes are keyed by\n    embeddings, and those embeddings are stored within a Qdrant collection.\n    During index construction, the document texts are chunked up,\n    converted to nodes with text; they are then encoded in\n    document embeddings stored within Qdrant.\n\n    During query time, the index uses Qdrant to query for the top\n    k most similar nodes, and synthesizes an answer from the\n    retrieved nodes.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n        client (Optional[Any]): QdrantClient instance from `qdrant-client` package\n        collection_name: (Optional[str]): name of the Qdrant collection\n    \"\"\"\n\n    index_struct_cls = QdrantIndexStruct\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[QdrantIndexStruct] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        client: Optional[Any] = None,\n        collection_name: Optional[str] = None,\n        **kwargs: Any\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        import_err_msg = (\n            \"`qdrant-client` package not found, please run `pip install qdrant-client`\"\n        )\n        try:\n            import qdrant_client  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n\n        if client is None:\n            raise ValueError(\"client cannot be None.\")\n\n        if collection_name is None and index_struct is not None:\n            collection_name = index_struct.collection_name\n        if collection_name is None:\n            raise ValueError(\"collection_name cannot be None.\")\n\n        self._client = cast(qdrant_client.QdrantClient, client)\n        self._collection_name = collection_name\n        self._collection_initialized = self._collection_exists(collection_name)\n\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        super().__init__(\n            documents,\n            index_struct,\n            text_qa_template,\n            llm_predictor,\n            embed_model,\n            **kwargs\n        )\n        # NOTE: when building the vector store index, text_qa_template is not partially\n        # formatted because we don't know the query ahead of time.\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTQdrantIndexQuery,\n            QueryMode.EMBEDDING: GPTQdrantIndexQuery,\n        }\n\n    def _add_document_to_index(\n        self,\n        index_struct: QdrantIndexStruct,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        from qdrant_client.http import models as rest\n        from qdrant_client.http.exceptions import UnexpectedResponse\n\n        nodes = self._get_nodes_from_document(document, text_splitter)\n        for n in nodes:\n            if n.embedding is None:\n                text_embedding = self._embed_model.get_text_embedding(n.get_text())\n            else:\n                text_embedding = n.embedding\n\n            collection_name = index_struct.get_collection_name()\n\n            # Create the Qdrant collection, if it does not exist yet\n            if not self._collection_initialized:\n                self._create_collection(\n                    collection_name=collection_name,\n                    vector_size=len(text_embedding),\n                )\n                self._collection_initialized = True\n\n            while True:\n                new_id = get_new_id(set())\n                try:\n                    self._client.http.points_api.get_point(\n                        collection_name=collection_name, id=new_id\n                    )\n                except UnexpectedResponse:\n                    break\n\n            payload = {\n                \"doc_id\": document.get_doc_id(),\n                \"text\": n.get_text(),\n                \"index\": n.index,\n            }\n\n            self._client.upsert(\n                collection_name=collection_name,\n                points=[\n                    rest.PointStruct(\n                        id=new_id,\n                        vector=text_embedding,\n                        payload=payload,\n                    )\n                ],\n            )\n\n    def _build_index_from_documents(\n        self, documents: Sequence[BaseDocument]\n    ) -> QdrantIndexStruct:\n        \"\"\"Build index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n        index_struct = self.index_struct_cls(collection_name=self._collection_name)\n        for d in documents:\n            self._add_document_to_index(index_struct, d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(self.index_struct, document, self._text_splitter)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        from qdrant_client.http import models as rest\n\n        self._client.delete(\n            collection_name=self._collection_name,\n            points_selector=rest.Filter(\n                must=[\n                    rest.FieldCondition(\n                        key=\"doc_id\", match=rest.MatchValue(value=doc_id)\n                    )\n                ]\n            ),\n        )\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Any) -> None:\n        \"\"\"Query mode to class.\"\"\"\n        super()._preprocess_query(mode, query_kwargs)\n        # Pass along Qdrant client instance\n        query_kwargs[\"client\"] = self._client\n\n    def _create_collection(self, collection_name: str, vector_size: int) -> None:\n        \"\"\"Create a Qdrant collection.\"\"\"\n        from qdrant_client.http import models as rest\n\n        self._client.recreate_collection(\n            collection_name=collection_name,\n            vectors_config=rest.VectorParams(\n                size=vector_size,\n                distance=rest.Distance.COSINE,\n            ),\n        )\n\n    def _collection_exists(self, collection_name: str) -> bool:\n        from qdrant_client.http.exceptions import UnexpectedResponse\n\n        try:\n            response = self._client.http.collections_api.get_collection(collection_name)\n            return response.result is not None\n        except UnexpectedResponse:\n            return False\n", "doc_id": "bf1118e9a20657b90f4e5cbb9ae924330db5f50f", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/qdrant.py", "file_name": "qdrant.py"}, "__type__": "Document"}, "b901531911d54b57fd5436cb636d971e68fea306": {"text": "\"\"\"Simple vector store index.\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence, Type\n\nfrom gpt_index.data_structs.data_structs import SimpleIndexDict\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.vector_store.simple import GPTSimpleVectorIndexQuery\nfrom gpt_index.indices.vector_store.base import BaseGPTVectorStoreIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.schema import BaseDocument\nfrom gpt_index.utils import get_new_id\n\n\nclass GPTSimpleVectorIndex(BaseGPTVectorStoreIndex[SimpleIndexDict]):\n    \"\"\"GPT Simple Vector Index.\n\n    The GPTSimpleVectorIndex is a data structure where nodes are keyed by\n    embeddings, and those embeddings are stored within a simple dictionary.\n    During index construction, the document texts are chunked up,\n    converted to nodes with text; they are then encoded in\n    document embeddings stored within the dict.\n\n    During query time, the index uses the dict to query for the top\n    k most similar nodes, and synthesizes an answer from the\n    retrieved nodes.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n    \"\"\"\n\n    index_struct_cls = SimpleIndexDict\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[SimpleIndexDict] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            text_qa_template=text_qa_template,\n            llm_predictor=llm_predictor,\n            embed_model=embed_model,\n            **kwargs,\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTSimpleVectorIndexQuery,\n            QueryMode.EMBEDDING: GPTSimpleVectorIndexQuery,\n        }\n\n    def _add_document_to_index(\n        self,\n        index_struct: SimpleIndexDict,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        nodes = self._get_nodes_from_document(document, text_splitter)\n        for n in nodes:\n            # add to in-memory dict\n            # NOTE: embeddings won't be stored in Node but rather in underlying\n            # Faiss store\n            if n.embedding is None:\n                text_embedding = self._embed_model.get_text_embedding(n.get_text())\n            else:\n                text_embedding = n.embedding\n            new_id = get_new_id(set(index_struct.nodes_dict.keys()))\n\n            # add to index\n            index_struct.add_node(n, text_id=new_id)\n            # TODO: deprecate\n            index_struct.add_to_embedding_dict(new_id, text_embedding)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        text_ids_to_delete = set()\n        int_ids_to_delete = set()\n        for text_id, int_id in self.index_struct.id_map.items():\n            node = self.index_struct.nodes_dict[int_id]\n            if node.ref_doc_id != doc_id:\n                continue\n            text_ids_to_delete.add(text_id)\n            int_ids_to_delete.add(int_id)\n\n        for int_id, text_id in zip(int_ids_to_delete, text_ids_to_delete):\n            del self.index_struct.nodes_dict[int_id]\n            del self.index_struct.id_map[text_id]\n            del self.index_struct.embedding_dict[text_id]\n", "doc_id": "b901531911d54b57fd5436cb636d971e68fea306", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/simple.py", "file_name": "simple.py"}, "__type__": "Document"}, "9c650f2a0631e9cc1398df985038f660d8b10030": {"text": "\"\"\"Weaviate Vector store index.\n\nAn index that that is built on top of an existing vector store.\n\n\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence, Type, cast\n\nfrom gpt_index.data_structs.data_structs import WeaviateIndexStruct\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex\nfrom gpt_index.indices.query.base import BaseGPTIndexQuery\nfrom gpt_index.indices.query.schema import QueryMode\nfrom gpt_index.indices.query.vector_store.weaviate import GPTWeaviateIndexQuery\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt\nfrom gpt_index.readers.weaviate.data_structs import WeaviateNode\nfrom gpt_index.readers.weaviate.utils import get_default_class_prefix\nfrom gpt_index.schema import BaseDocument\n\n\nclass GPTWeaviateIndex(BaseGPTIndex[WeaviateIndexStruct]):\n    \"\"\"GPT Weaviate Index.\n\n    The GPTWeaviateIndex is a data structure where nodes are keyed by\n    embeddings, and those embeddings are stored within a Weaviate index.\n    During index construction, the document texts are chunked up,\n    converted to nodes with text; they are then encoded in\n    document embeddings stored within Weaviate.\n\n    During query time, the index uses Weaviate to query for the top\n    k most similar nodes, and synthesizes an answer from the\n    retrieved nodes.\n\n    Args:\n        text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n    \"\"\"\n\n    index_struct_cls = WeaviateIndexStruct\n\n    def __init__(\n        self,\n        documents: Optional[Sequence[DOCUMENTS_INPUT]] = None,\n        index_struct: Optional[WeaviateIndexStruct] = None,\n        text_qa_template: Optional[QuestionAnswerPrompt] = None,\n        llm_predictor: Optional[LLMPredictor] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        weaviate_client: Optional[Any] = None,\n        class_prefix: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        import_err_msg = (\n            \"`weaviate` package not found, please run `pip install weaviate-client`\"\n        )\n        try:\n            import weaviate  # noqa: F401\n            from weaviate import Client  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n\n        self.client = cast(Client, weaviate_client)\n        if index_struct is not None:\n            if class_prefix is not None:\n                raise ValueError(\n                    \"class_prefix must be None when index_struct is not None.\"\n                )\n            self.class_prefix = index_struct.get_class_prefix()\n        else:\n            self.class_prefix = class_prefix or get_default_class_prefix()\n        # try to create schema\n        WeaviateNode.create_schema(self.client, self.class_prefix)\n\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        super().__init__(\n            documents=documents,\n            index_struct=index_struct,\n            llm_predictor=llm_predictor,\n            embed_model=embed_model,\n            **kwargs,\n        )\n        # NOTE: when building the vector store index, text_qa_template is not partially\n        # formatted because we don't know the query ahead of time.\n        self._text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n\n    @classmethod\n    def get_query_map(self) -> Dict[str, Type[BaseGPTIndexQuery]]:\n        \"\"\"Get query map.\"\"\"\n        return {\n            QueryMode.DEFAULT: GPTWeaviateIndexQuery,\n            QueryMode.EMBEDDING: GPTWeaviateIndexQuery,\n        }\n\n    def _add_document_to_index(\n        self,\n        index_struct: WeaviateIndexStruct,\n        document: BaseDocument,\n        text_splitter: TokenTextSplitter,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        nodes = self._get_nodes_from_document(document, text_splitter)\n        for n in nodes:\n            if n.embedding is None:\n                n.embedding = self._embed_model.get_text_embedding(n.get_text())\n            WeaviateNode.from_gpt_index(self.client, n, index_struct.get_class_prefix())\n\n    def _build_index_from_documents(\n        self, documents: Sequence[BaseDocument]\n    ) -> WeaviateIndexStruct:\n        \"\"\"Build index from documents.\"\"\"\n        text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n            self.text_qa_template, 1\n        )\n        index_struct = self.index_struct_cls(class_prefix=self.class_prefix)\n        for d in documents:\n            self._add_document_to_index(index_struct, d, text_splitter)\n        return index_struct\n\n    def _insert(self, document: BaseDocument, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_document_to_index(self._index_struct, document, self._text_splitter)\n\n    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document.\"\"\"\n        WeaviateNode.delete_document(self.client, doc_id, self.class_prefix)\n\n    def _preprocess_query(self, mode: QueryMode, query_kwargs: Any) -> None:\n        \"\"\"Query mode to class.\"\"\"\n        super()._preprocess_query(mode, query_kwargs)\n        # pass along weaviate client and info\n        query_kwargs[\"weaviate_client\"] = self.client\n", "doc_id": "9c650f2a0631e9cc1398df985038f660d8b10030", "embedding": null, "extra_info": {"file_path": "gpt_index/indices/vector_store/weaviate.py", "file_name": "weaviate.py"}, "__type__": "Document"}, "ced105858f1e3558ef6ed5556d0e548659b84440": {"text": "\"\"\"Init file for langchain helpers.\"\"\"\n", "doc_id": "ced105858f1e3558ef6ed5556d0e548659b84440", "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "10540e3570e6348e14f5264d0904fd9dc4488e93": {"text": "\"\"\"Wrapper functions around an LLM chain.\"\"\"\n\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Any, Optional, Tuple\n\nimport openai\nfrom langchain import Cohere, LLMChain, OpenAI\nfrom langchain.llms import AI21\nfrom langchain.llms.base import BaseLLM\n\nfrom gpt_index.constants import MAX_CHUNK_SIZE, NUM_OUTPUTS\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.utils import (\n    ErrorToRetry,\n    globals_helper,\n    retry_on_exceptions_with_backoff,\n)\n\n\n@dataclass\nclass LLMMetadata:\n    \"\"\"LLM metadata.\n\n    We extract this metadata to help with our prompts.\n\n    \"\"\"\n\n    max_input_size: int = MAX_CHUNK_SIZE\n    num_output: int = NUM_OUTPUTS\n\n\ndef _get_llm_metadata(llm: BaseLLM) -> LLMMetadata:\n    \"\"\"Get LLM metadata from llm.\"\"\"\n    if not isinstance(llm, BaseLLM):\n        raise ValueError(\"llm must be an instance of langchain.llms.base.LLM\")\n    if isinstance(llm, OpenAI):\n        return LLMMetadata(\n            max_input_size=llm.modelname_to_contextsize(llm.model_name),\n            num_output=llm.max_tokens,\n        )\n    elif isinstance(llm, Cohere):\n        # TODO: figure out max input size for cohere\n        return LLMMetadata(num_output=llm.max_tokens)\n    elif isinstance(llm, AI21):\n        # TODO: figure out max input size for AI21\n        return LLMMetadata(num_output=llm.maxTokens)\n    else:\n        return LLMMetadata()\n\n\nclass LLMPredictor:\n    \"\"\"LLM predictor class.\n\n    Wrapper around an LLMChain from Langchain.\n\n    Args:\n        llm (Optional[langchain.llms.base.LLM]): LLM from Langchain to use\n            for predictions. Defaults to OpenAI's text-davinci-003 model.\n            Please see `Langchain's LLM Page\n            <https://langchain.readthedocs.io/en/latest/modules/llms.html>`_\n            for more details.\n\n        retry_on_throttling (bool): Whether to retry on rate limit errors.\n            Defaults to true.\n\n    \"\"\"\n\n    def __init__(\n        self, llm: Optional[BaseLLM] = None, retry_on_throttling: bool = True\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._llm = llm or OpenAI(temperature=0, model_name=\"text-davinci-003\")\n        self.retry_on_throttling = retry_on_throttling\n        self._total_tokens_used = 0\n        self.flag = True\n        self._last_token_usage: Optional[int] = None\n\n    def get_llm_metadata(self) -> LLMMetadata:\n        \"\"\"Get LLM metadata.\"\"\"\n        # TODO: refactor mocks in unit tests, this is a stopgap solution\n        if hasattr(self, \"_llm\"):\n            return _get_llm_metadata(self._llm)\n        else:\n            return LLMMetadata()\n\n    def _predict(self, prompt: Prompt, **prompt_args: Any) -> str:\n        \"\"\"Inner predict function.\n\n        If retry_on_throttling is true, we will retry on rate limit errors.\n\n        \"\"\"\n        llm_chain = LLMChain(prompt=prompt.get_langchain_prompt(), llm=self._llm)\n\n        # Note: we don't pass formatted_prompt to llm_chain.predict because\n        # langchain does the same formatting under the hood\n        full_prompt_args = prompt.get_full_format_args(prompt_args)\n        if self.retry_on_throttling:\n            llm_prediction = retry_on_exceptions_with_backoff(\n                lambda: llm_chain.predict(**full_prompt_args),\n                [\n                    ErrorToRetry(openai.error.RateLimitError),\n                    ErrorToRetry(openai.error.ServiceUnavailableError),\n                    ErrorToRetry(openai.error.TryAgain),\n                    ErrorToRetry(\n                        openai.error.APIConnectionError, lambda e: e.should_retry\n                    ),\n                ],\n            )\n        else:\n            llm_prediction = llm_chain.predict(**full_prompt_args)\n        return llm_prediction\n\n    def predict(self, prompt: Prompt, **prompt_args: Any) -> Tuple[str, str]:\n        \"\"\"Predict the answer to a query.\n\n        Args:\n            prompt (Prompt): Prompt to use for prediction.\n\n        Returns:\n            Tuple[str, str]: Tuple of the predicted answer and the formatted prompt.\n\n        \"\"\"\n        formatted_prompt = prompt.format(**prompt_args)\n        llm_prediction = self._predict(prompt, **prompt_args)\n        logging.debug(llm_prediction)\n\n        # We assume that the value of formatted_prompt is exactly the thing\n        # eventually sent to OpenAI, or whatever LLM downstream\n        prompt_tokens_count = self._count_tokens(formatted_prompt)\n        prediction_tokens_count = self._count_tokens(llm_prediction)\n        self._total_tokens_used += prompt_tokens_count + prediction_tokens_count\n        return llm_prediction, formatted_prompt\n\n    @property\n    def total_tokens_used(self) -> int:\n        \"\"\"Get the total tokens used so far.\"\"\"\n        return self._total_tokens_used\n\n    def _count_tokens(self, text: str) -> int:\n        tokens = globals_helper.tokenizer(text)\n        return len(tokens)\n\n    @property\n    def last_token_usage(self) -> int:\n        \"\"\"Get the last token usage.\"\"\"\n        if self._last_token_usage is None:\n            return 0\n        return self._last_token_usage\n\n    @last_token_usage.setter\n    def last_token_usage(self, value: int) -> None:\n        \"\"\"Set the last token usage.\"\"\"\n        self._last_token_usage = value\n", "doc_id": "10540e3570e6348e14f5264d0904fd9dc4488e93", "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/chain_wrapper.py", "file_name": "chain_wrapper.py"}, "__type__": "Document"}, "6af9d1a0133bbdf496857f225254cdf042edd0b3": {"text": "\"\"\"Langchain memory wrapper (for GPT Index).\"\"\"\n\nfrom typing import Any, Dict, List, Optional\n\nfrom langchain.chains.base import Memory\nfrom pydantic import Field\n\nfrom gpt_index.indices.base import BaseGPTIndex\nfrom gpt_index.readers.schema.base import Document\n\n\ndef get_prompt_input_key(inputs: Dict[str, Any], memory_variables: List[str]) -> str:\n    \"\"\"Get prompt input key.\n\n    Copied over from langchain.\n\n    \"\"\"\n    # \"stop\" is a special key that can be passed as input but is not used to\n    # format the prompt.\n    prompt_input_keys = list(set(inputs).difference(memory_variables + [\"stop\"]))\n    if len(prompt_input_keys) != 1:\n        raise ValueError(f\"One input key expected got {prompt_input_keys}\")\n    return prompt_input_keys[0]\n\n\nclass GPTIndexMemory(Memory):\n    \"\"\"Langchain memory wrapper (for GPT Index).\n\n    Args:\n        human_prefix (str): Prefix for human input. Defaults to \"Human\".\n        ai_prefix (str): Prefix for AI output. Defaults to \"AI\".\n        memory_key (str): Key for memory. Defaults to \"history\".\n        index (BaseGPTIndex): GPT Index instance.\n        query_kwargs (Dict[str, Any]): Keyword arguments for GPT Index query.\n        input_key (Optional[str]): Input key. Defaults to None.\n        output_key (Optional[str]): Output key. Defaults to None.\n\n    \"\"\"\n\n    human_prefix: str = \"Human\"\n    ai_prefix: str = \"AI\"\n    memory_key: str = \"history\"\n    index: BaseGPTIndex\n    query_kwargs: Dict = Field(default_factory=dict)\n    output_key: Optional[str] = None\n    input_key: Optional[str] = None\n\n    @property\n    def memory_variables(self) -> List[str]:\n        \"\"\"Return memory variables.\"\"\"\n        return [self.memory_key]\n\n    def _get_prompt_input_key(self, inputs: Dict[str, Any]) -> str:\n        if self.input_key is None:\n            prompt_input_key = get_prompt_input_key(inputs, self.memory_variables)\n        else:\n            prompt_input_key = self.input_key\n        return prompt_input_key\n\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n        \"\"\"Return key-value pairs given the text input to the chain.\"\"\"\n        prompt_input_key = self._get_prompt_input_key(inputs)\n        query_str = inputs[prompt_input_key]\n\n        # TODO: wrap in prompt\n        # TODO: add option to return the raw text\n        # NOTE: currently it's a hack\n        response = self.index.query(query_str, **self.query_kwargs)\n        return {self.memory_key: str(response)}\n\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\n        \"\"\"Save the context of this model run to memory.\"\"\"\n        prompt_input_key = self._get_prompt_input_key(inputs)\n        if self.output_key is None:\n            if len(outputs) != 1:\n                raise ValueError(f\"One output key expected, got {outputs.keys()}\")\n            output_key = list(outputs.keys())[0]\n        else:\n            output_key = self.output_key\n        human = f\"{self.human_prefix}: \" + inputs[prompt_input_key]\n        ai = f\"{self.ai_prefix}: \" + outputs[output_key]\n        doc_text = \"\\n\".join([human, ai])\n        doc = Document(text=doc_text)\n        self.index.insert(doc)\n\n    def clear(self) -> None:\n        \"\"\"Clear memory contents.\"\"\"\n        pass\n\n    def __repr__(self) -> str:\n        \"\"\"Return representation.\"\"\"\n        return \"GPTIndexMemory()\"\n", "doc_id": "6af9d1a0133bbdf496857f225254cdf042edd0b3", "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/memory_wrapper.py", "file_name": "memory_wrapper.py"}, "__type__": "Document"}, "e541c393c777483dca55d898b374d77f2dd7e023": {"text": "\"\"\"SQL wrapper around SQLDatabase in langchain.\"\"\"\nfrom typing import Any, Dict, List, Tuple\n\nfrom langchain.sql_database import SQLDatabase as LangchainSQLDatabase\nfrom sqlalchemy import MetaData, create_engine, insert\nfrom sqlalchemy.engine import Engine\n\n\nclass SQLDatabase(LangchainSQLDatabase):\n    \"\"\"SQL Database.\n\n    Wrapper around SQLDatabase object from langchain. Offers\n    some helper utilities for insertion and querying.\n    See `langchain documentation <https://tinyurl.com/4we5ku8j>`_ for more details:\n\n    Args:\n        *args: Arguments to pass to langchain SQLDatabase.\n        **kwargs: Keyword arguments to pass to langchain SQLDatabase.\n\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.metadata_obj = MetaData(bind=self._engine)\n        self.metadata_obj.reflect()\n\n    @property\n    def engine(self) -> Engine:\n        \"\"\"Return SQL Alchemy engine.\"\"\"\n        return self._engine\n\n    @classmethod\n    def from_uri(cls, database_uri: str, **kwargs: Any) -> \"SQLDatabase\":\n        \"\"\"Construct a SQLAlchemy engine from URI.\"\"\"\n        return cls(create_engine(database_uri), **kwargs)\n\n    def get_table_columns(self, table_name: str) -> List[dict]:\n        \"\"\"Get table columns.\"\"\"\n        return self._inspector.get_columns(table_name)\n\n    def get_single_table_info(self, table_name: str) -> str:\n        \"\"\"Get table info for a single table.\"\"\"\n        # same logic as table_info, but with specific table names\n        template = \"Table '{table_name}' has columns: {columns}.\"\n        columns = []\n        for column in self._inspector.get_columns(table_name):\n            columns.append(f\"{column['name']} ({str(column['type'])})\")\n        column_str = \", \".join(columns)\n        table_str = template.format(table_name=table_name, columns=column_str)\n        return table_str\n\n    def insert_into_table(self, table_name: str, data: dict) -> None:\n        \"\"\"Insert data into a table.\"\"\"\n        table = self.metadata_obj.tables[table_name]\n        stmt = insert(table).values(**data)\n        self._engine.execute(stmt)\n\n    def run_sql(self, command: str) -> Tuple[str, Dict]:\n        \"\"\"Execute a SQL statement and return a string representing the results.\n\n        If the statement returns rows, a string of the results is returned.\n        If the statement returns no rows, an empty string is returned.\n        \"\"\"\n        with self._engine.connect() as connection:\n            cursor = connection.exec_driver_sql(command)\n            if cursor.returns_rows:\n                result = cursor.fetchall()\n                return str(result), {\"result\": result}\n        return \"\", {}\n", "doc_id": "e541c393c777483dca55d898b374d77f2dd7e023", "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/sql_wrapper.py", "file_name": "sql_wrapper.py"}, "__type__": "Document"}, "b329d2f933f606b0fe34b6e40cf993b5ebe75d02": {"text": "\"\"\"Text splitter implementations.\"\"\"\nfrom dataclasses import dataclass\nfrom typing import Callable, List, Optional\n\nfrom langchain.text_splitter import TextSplitter\n\nfrom gpt_index.utils import globals_helper\n\n\n@dataclass\nclass TextSplit:\n    \"\"\"Text split with overlap.\n\n    Attributes:\n        text_chunk: The text string.\n        num_char_overlap: The number of overlapping characters with the previous chunk.\n    \"\"\"\n\n    text_chunk: str\n    num_char_overlap: int\n\n\nclass TokenTextSplitter(TextSplitter):\n    \"\"\"Implementation of splitting text that looks at word tokens.\"\"\"\n\n    def __init__(\n        self,\n        separator: str = \" \",\n        chunk_size: int = 4000,\n        chunk_overlap: int = 200,\n        tokenizer: Optional[Callable] = None,\n        backup_separators: Optional[List[str]] = [\"\\n\"],\n    ):\n        \"\"\"Initialize with parameters.\"\"\"\n        if chunk_overlap > chunk_size:\n            raise ValueError(\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\n                f\"({chunk_size}), should be smaller.\"\n            )\n        self._separator = separator\n        self._chunk_size = chunk_size\n        self._chunk_overlap = chunk_overlap\n        self.tokenizer = tokenizer or globals_helper.tokenizer\n        self._backup_separators = backup_separators\n\n    def _reduce_chunk_size(\n        self, start_idx: int, cur_idx: int, splits: List[str]\n    ) -> int:\n        \"\"\"Reduce the chunk size by reducing cur_idx.\n\n        Return the new cur_idx.\n\n        \"\"\"\n        current_doc_total = len(\n            self.tokenizer(self._separator.join(splits[start_idx:cur_idx]))\n        )\n        while current_doc_total > self._chunk_size:\n            percent_to_reduce = (\n                current_doc_total - self._chunk_size\n            ) / current_doc_total\n            num_to_reduce = int(percent_to_reduce * (cur_idx - start_idx)) + 1\n            cur_idx -= num_to_reduce\n            current_doc_total = len(\n                self.tokenizer(self._separator.join(splits[start_idx:cur_idx]))\n            )\n        return cur_idx\n\n    def _process_splits(self, splits: List[str], chunk_size: int) -> List[str]:\n        \"\"\"Process splits.\n\n        Specifically search for tokens that are too large for chunk size,\n        and see if we can separate those tokens more\n        (via backup separators if specified, or force chunking).\n\n        \"\"\"\n        new_splits = []\n        for split in splits:\n            num_cur_tokens = len(self.tokenizer(split))\n            if num_cur_tokens <= chunk_size:\n                new_splits.append(split)\n            else:\n                cur_splits = []\n                if self._backup_separators:\n                    for sep in self._backup_separators:\n                        if sep in split:\n                            cur_splits = split.split(sep)\n                            break\n                else:\n                    cur_splits = [split]\n\n                cur_splits2 = []\n                for cur_split in cur_splits:\n                    num_cur_tokens = len(self.tokenizer(cur_split))\n                    if num_cur_tokens <= chunk_size:\n                        cur_splits2.extend([cur_split])\n                    else:\n                        cur_split_chunks = [\n                            cur_split[i : i + chunk_size]\n                            for i in range(0, len(cur_split), chunk_size)\n                        ]\n                        cur_splits2.extend(cur_split_chunks)\n\n                new_splits.extend(cur_splits2)\n        return new_splits\n\n    def split_text(self, text: str, extra_info_str: Optional[str] = None) -> List[str]:\n        \"\"\"Split incoming text and return chunks.\"\"\"\n        text_slits = self.split_text_with_overlaps(text, extra_info_str=extra_info_str)\n        return [text_split.text_chunk for text_split in text_slits]\n\n    def split_text_with_overlaps(\n        self, text: str, extra_info_str: Optional[str] = None\n    ) -> List[TextSplit]:\n        \"\"\"Split incoming text and return chunks with overlap size.\"\"\"\n        if text == \"\":\n            return []\n\n        # NOTE: Consider extra info str that will be added to the chunk at query time\n        #       This reduces the effective chunk size that we can have\n        if extra_info_str is not None:\n            # NOTE: extra 2 newline chars for formatting when prepending in query\n            num_extra_tokens = len(self.tokenizer(f\"{extra_info_str}\\n\\n\")) + 1\n            effective_chunk_size = self._chunk_size - num_extra_tokens\n\n            if effective_chunk_size <= 0:\n                raise ValueError(\n                    \"Effective chunk size is non positive after considering extra_info\"\n                )\n        else:\n            effective_chunk_size = self._chunk_size\n\n        # First we naively split the large input into a bunch of smaller ones.\n        splits = text.split(self._separator)\n        splits = self._process_splits(splits, effective_chunk_size)\n        # We now want to combine these smaller pieces into medium size\n        # chunks to send to the LLM.\n        docs = []\n\n        start_idx = 0\n        cur_idx = 0\n        cur_total = 0\n        prev_idx = 0  # store the previous end index\n        while cur_idx < len(splits):\n            cur_token = splits[cur_idx]\n            num_cur_tokens = max(len(self.tokenizer(cur_token)), 1)\n            if num_cur_tokens > effective_chunk_size:\n                raise ValueError(\n                    \"A single term is larger than the allowed chunk size.\\n\"\n                    f\"Term size: {num_cur_tokens}\\n\"\n                    f\"Chunk size: {self._chunk_size}\"\n                    f\"Effective chunk size: {effective_chunk_size}\"\n                )\n            # If adding token to current_doc would exceed the chunk size:\n            # 1. First verify with tokenizer that current_doc\n            # 1. Update the docs list\n            if cur_total + num_cur_tokens > effective_chunk_size:\n                # NOTE: since we use a proxy for counting tokens, we want to\n                # run tokenizer across all of current_doc first. If\n                # the chunk is too big, then we will reduce text in pieces\n                cur_idx = self._reduce_chunk_size(start_idx, cur_idx, splits)\n                overlap = 0\n                # after first round, check if last chunk ended after this chunk begins\n                if prev_idx > 0 and prev_idx > start_idx:\n                    overlap = sum([len(splits[i]) for i in range(start_idx, prev_idx)])\n                docs.append(\n                    TextSplit(self._separator.join(splits[start_idx:cur_idx]), overlap)\n                )\n                prev_idx = cur_idx\n                # 2. Shrink the current_doc (from the front) until it is gets smaller\n                # than the overlap size\n                # NOTE: because counting tokens individually is an imperfect\n                # proxy (but much faster proxy) for the total number of tokens consumed,\n                # we need to enforce that start_idx <= cur_idx, otherwise\n                # start_idx has a chance of going out of bounds.\n                while cur_total > self._chunk_overlap and start_idx < cur_idx:\n                    cur_num_tokens = max(len(self.tokenizer(splits[start_idx])), 1)\n                    cur_total -= cur_num_tokens\n                    start_idx += 1\n            # Build up the current_doc with term d, and update the total counter with\n            # the number of the number of tokens in d, wrt self.tokenizer\n\n            # we reassign cur_token and num_cur_tokens, because cur_idx\n            # may have changed\n            cur_token = splits[cur_idx]\n            num_cur_tokens = max(len(self.tokenizer(cur_token)), 1)\n\n            cur_total += num_cur_tokens\n            cur_idx += 1\n        overlap = 0\n        # after first round, check if last chunk ended after this chunk begins\n        if prev_idx > start_idx:\n            overlap = sum([len(splits[i]) for i in range(start_idx, prev_idx)]) + len(\n                range(start_idx, prev_idx)\n            )\n        docs.append(TextSplit(self._separator.join(splits[start_idx:cur_idx]), overlap))\n        return docs\n\n    def truncate_text(self, text: str) -> str:\n        \"\"\"Truncate text in order to fit the underlying chunk size.\"\"\"\n        if text == \"\":\n            return \"\"\n        # First we naively split the large input into a bunch of smaller ones.\n        splits = text.split(self._separator)\n        splits = self._process_splits(splits, self._chunk_size)\n\n        start_idx = 0\n        cur_idx = 0\n        cur_total = 0\n        while cur_idx < len(splits):\n            cur_token = splits[cur_idx]\n            num_cur_tokens = max(len(self.tokenizer(cur_token)), 1)\n            if cur_total + num_cur_tokens > self._chunk_size:\n                cur_idx = self._reduce_chunk_size(start_idx, cur_idx, splits)\n                break\n            cur_total += num_cur_tokens\n            cur_idx += 1\n        return self._separator.join(splits[start_idx:cur_idx])\n", "doc_id": "b329d2f933f606b0fe34b6e40cf993b5ebe75d02", "embedding": null, "extra_info": {"file_path": "gpt_index/langchain_helpers/text_splitter.py", "file_name": "text_splitter.py"}, "__type__": "Document"}, "70a9a06c14c38bb373b8afc1d1e6b4b7febf726e": {"text": "\"\"\"Prompt class.\"\"\"\n\nfrom gpt_index.prompts.base import Prompt\n\n__all__ = [\"Prompt\"]\n", "doc_id": "70a9a06c14c38bb373b8afc1d1e6b4b7febf726e", "embedding": null, "extra_info": {"file_path": "gpt_index/prompts/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "7d31d14e71a85ec7b6245e0d2ffd714cce527961": {"text": "\"\"\"Base module for prompts.\"\"\"\nfrom copy import deepcopy\nfrom string import Formatter\nfrom typing import Any, Dict, List, Optional, Type, TypeVar\n\nfrom langchain import PromptTemplate as LangchainPrompt\n\nfrom gpt_index.prompts.prompt_type import PromptType\n\nPMT = TypeVar(\"PMT\", bound=\"Prompt\")\n\n\nclass Prompt:\n    \"\"\"Prompt class for GPT Index.\n\n    Wrapper around langchain's prompt class. Adds ability to:\n        - enforce certain prompt types\n        - partially fill values\n\n    \"\"\"\n\n    input_variables: List[str]\n    prompt_type: PromptType = PromptType.CUSTOM\n\n    def __init__(\n        self,\n        template: Optional[str] = None,\n        langchain_prompt: Optional[LangchainPrompt] = None,\n        **prompt_kwargs: Any,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        if langchain_prompt is None:\n            if template is None:\n                raise ValueError(\n                    \"`template` must be specified if `langchain_prompt` is None\"\n                )\n            # validate\n            tmpl_vars = {\n                v for _, v, _, _ in Formatter().parse(template) if v is not None\n            }\n            if tmpl_vars != set(self.input_variables):\n                raise ValueError(\n                    f\"Invalid template: {template}, variables do not match the \"\n                    f\"required input_variables: {self.input_variables}\"\n                )\n\n            self.prompt: LangchainPrompt = LangchainPrompt(\n                input_variables=self.input_variables, template=template, **prompt_kwargs\n            )\n        else:\n            if template:\n                raise ValueError(\n                    f\"Both template ({template}) and langchain_prompt \"\n                    f\"({langchain_prompt}) are provided, only one should be.\"\n                )\n            if set(langchain_prompt.input_variables) != set(self.input_variables):\n                raise ValueError(\n                    f\"Invalid prompt: {langchain_prompt}, variables do not match the \"\n                    f\"required input_variables: {self.input_variables}\"\n                )\n            self.prompt = langchain_prompt\n        self.partial_dict: Dict[str, Any] = {}\n        self.prompt_kwargs = prompt_kwargs\n\n    @classmethod\n    def from_langchain_prompt(\n        cls: Type[PMT], prompt: LangchainPrompt, **kwargs: Any\n    ) -> PMT:\n        \"\"\"Load prompt from LangChain prompt.\"\"\"\n        return cls(langchain_prompt=prompt, **kwargs)\n\n    def partial_format(self: PMT, **kwargs: Any) -> PMT:\n        \"\"\"Format the prompt partially.\n\n        Return an instance of itself.\n\n        \"\"\"\n        for k in kwargs.keys():\n            if k not in self.input_variables:\n                raise ValueError(\n                    f\"Invalid input variable: {k}, not found in input_variables\"\n                )\n\n        copy_obj = deepcopy(self)\n        copy_obj.partial_dict.update(kwargs)\n        return copy_obj\n\n    @classmethod\n    def from_prompt(cls: Type[PMT], prompt: \"Prompt\") -> PMT:\n        \"\"\"Create a prompt from an existing prompt.\n\n        Use case: If the existing prompt is already partially filled,\n        and the remaining fields satisfy the requirements of the\n        prompt class, then we can create a new prompt from the existing\n        partially filled prompt.\n\n        \"\"\"\n        template = prompt.prompt.template\n        tmpl_vars = {v for _, v, _, _ in Formatter().parse(template) if v is not None}\n        format_dict = {}\n        for var in tmpl_vars:\n            if var not in prompt.partial_dict:\n                format_dict[var] = f\"{{{var}}}\"\n\n        template_str = prompt.format(**format_dict)\n        cls_obj: PMT = cls(template_str, **prompt.prompt_kwargs)\n        return cls_obj\n\n    def get_langchain_prompt(self) -> LangchainPrompt:\n        \"\"\"Get langchain prompt.\"\"\"\n        return self.prompt\n\n    def format(self, **kwargs: Any) -> str:\n        \"\"\"Format the prompt.\"\"\"\n        kwargs.update(self.partial_dict)\n        return self.prompt.format(**kwargs)\n\n    def get_full_format_args(self, kwargs: Dict) -> Dict[str, Any]:\n        \"\"\"Get dict of all format args.\n\n        Hack to pass into Langchain to pass validation.\n\n        \"\"\"\n        kwargs.update(self.partial_dict)\n        return kwargs\n", "doc_id": "7d31d14e71a85ec7b6245e0d2ffd714cce527961", "embedding": null, "extra_info": {"file_path": "gpt_index/prompts/base.py", "file_name": "base.py"}, "__type__": "Document"}, "67d318229cee08ef396c5183c85a937be9a1a2db": {"text": "\"\"\"Set of default prompts.\"\"\"\n\nfrom gpt_index.prompts.prompts import (\n    KeywordExtractPrompt,\n    QueryKeywordExtractPrompt,\n    QuestionAnswerPrompt,\n    RefinePrompt,\n    RefineTableContextPrompt,\n    SchemaExtractPrompt,\n    SummaryPrompt,\n    TableContextPrompt,\n    TextToSQLPrompt,\n    TreeInsertPrompt,\n    TreeSelectMultiplePrompt,\n    TreeSelectPrompt,\n)\n\n############################################\n# Tree\n############################################\n\nDEFAULT_SUMMARY_PROMPT_TMPL = (\n    \"Write a summary of the following. Try to use only the \"\n    \"information provided. \"\n    \"Try to include as many key details as possible.\\n\"\n    \"\\n\"\n    \"\\n\"\n    \"{context_str}\\n\"\n    \"\\n\"\n    \"\\n\"\n    'SUMMARY:\"\"\"\\n'\n)\n\nDEFAULT_SUMMARY_PROMPT = SummaryPrompt(DEFAULT_SUMMARY_PROMPT_TMPL)\n\n# insert prompts\nDEFAULT_INSERT_PROMPT_TMPL = (\n    \"Context information is below. It is provided in a numbered list \"\n    \"(1 to {num_chunks}),\"\n    \"where each item in the list corresponds to a summary.\\n\"\n    \"---------------------\\n\"\n    \"{context_list}\"\n    \"---------------------\\n\"\n    \"Given the context information, here is a new piece of \"\n    \"information: {new_chunk_text}\\n\"\n    \"Answer with the number corresponding to the summary that should be updated. \"\n    \"The answer should be the number corresponding to the \"\n    \"summary that is most relevant to the question.\\n\"\n)\nDEFAULT_INSERT_PROMPT = TreeInsertPrompt(DEFAULT_INSERT_PROMPT_TMPL)\n\n\n# # single choice\nDEFAULT_QUERY_PROMPT_TMPL = (\n    \"Some choices are given below. It is provided in a numbered list \"\n    \"(1 to {num_chunks}),\"\n    \"where each item in the list corresponds to a summary.\\n\"\n    \"---------------------\\n\"\n    \"{context_list}\"\n    \"\\n---------------------\\n\"\n    \"Using only the choices above and not prior knowledge, return \"\n    \"the choice that is most relevant to the question: '{query_str}'\\n\"\n    \"Provide choice in the following format: 'ANSWER: <number>' and explain why \"\n    \"this summary was selected in relation to the question.\\n\"\n)\nDEFAULT_QUERY_PROMPT = TreeSelectPrompt(DEFAULT_QUERY_PROMPT_TMPL)\n\n# multiple choice\nDEFAULT_QUERY_PROMPT_MULTIPLE_TMPL = (\n    \"Some choices are given below. It is provided in a numbered \"\n    \"list (1 to {num_chunks}), \"\n    \"where each item in the list corresponds to a summary.\\n\"\n    \"---------------------\\n\"\n    \"{context_list}\"\n    \"\\n---------------------\\n\"\n    \"Using only the choices above and not prior knowledge, return the top choices \"\n    \"(no more than {branching_factor}, ranked by most relevant to least) that \"\n    \"are most relevant to the question: '{query_str}'\\n\"\n    \"Provide choices in the following format: 'ANSWER: <numbers>' and explain why \"\n    \"these summaries were selected in relation to the question.\\n\"\n)\nDEFAULT_QUERY_PROMPT_MULTIPLE = TreeSelectMultiplePrompt(\n    DEFAULT_QUERY_PROMPT_MULTIPLE_TMPL\n)\n\n\nDEFAULT_REFINE_PROMPT_TMPL = (\n    \"The original question is as follows: {query_str}\\n\"\n    \"We have provided an existing answer: {existing_answer}\\n\"\n    \"We have the opportunity to refine the existing answer\"\n    \"(only if needed) with some more context below.\\n\"\n    \"------------\\n\"\n    \"{context_msg}\\n\"\n    \"------------\\n\"\n    \"Given the new context, refine the original answer to better \"\n    \"answer the question. \"\n    \"If the context isn't useful, return the original answer.\"\n)\nDEFAULT_REFINE_PROMPT = RefinePrompt(DEFAULT_REFINE_PROMPT_TMPL)\n\n\nDEFAULT_TEXT_QA_PROMPT_TMPL = (\n    \"Context information is below. \\n\"\n    \"---------------------\\n\"\n    \"{context_str}\"\n    \"\\n---------------------\\n\"\n    \"Given the context information and not prior knowledge, \"\n    \"answer the question: {query_str}\\n\"\n)\nDEFAULT_TEXT_QA_PROMPT = QuestionAnswerPrompt(DEFAULT_TEXT_QA_PROMPT_TMPL)\n\n\n############################################\n# Keyword Table\n############################################\n\nDEFAULT_KEYWORD_EXTRACT_TEMPLATE_TMPL = (\n    \"Some text is provided below. Given the text, extract up to {max_keywords} \"\n    \"keywords from the text. Avoid stopwords.\"\n    \"---------------------\\n\"\n    \"{text}\\n\"\n    \"---------------------\\n\"\n    \"Provide keywords in the following comma-separated format: 'KEYWORDS: <keywords>'\\n\"\n)\nDEFAULT_KEYWORD_EXTRACT_TEMPLATE = KeywordExtractPrompt(\n    DEFAULT_KEYWORD_EXTRACT_TEMPLATE_TMPL\n)\n\n\n# NOTE: the keyword extraction for queries can be the same as\n# the one used to build the index, but here we tune it to see if performance is better.\nDEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE_TMPL = (\n    \"A question is provided below. Given the question, extract up to {max_keywords} \"\n    \"keywords from the text. Focus on extracting the keywords that we can use \"\n    \"to best lookup answers to the question. Avoid stopwords.\\n\"\n    \"---------------------\\n\"\n    \"{question}\\n\"\n    \"---------------------\\n\"\n    \"Provide keywords in the following comma-separated format: 'KEYWORDS: <keywords>'\\n\"\n)\nDEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE = QueryKeywordExtractPrompt(\n    DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE_TMPL\n)\n\n\n############################################\n# Structured Store\n############################################\n\nDEFAULT_SCHEMA_EXTRACT_TMPL = (\n    \"We wish to extract relevant fields from an unstructured text chunk into \"\n    \"a structured schema. We first provide the unstructured text, and then \"\n    \"we provide the schema that we wish to extract. \"\n    \"-----------text-----------\\n\"\n    \"{text}\\n\"\n    \"-----------schema-----------\\n\"\n    \"{schema}\\n\"\n    \"---------------------\\n\"\n    \"Given the text and schema, extract the relevant fields from the text in \"\n    \"the following format: \"\n    \"field1: <value>\\nfield2: <value>\\n...\\n\\n\"\n    \"If a field is not present in the text, don't include it in the output.\"\n    \"If no fields are present in the text, return a blank string.\\n\"\n    \"Fields: \"\n)\nDEFAULT_SCHEMA_EXTRACT_PROMPT = SchemaExtractPrompt(DEFAULT_SCHEMA_EXTRACT_TMPL)\n\n# NOTE: taken from langchain and adapted\n# https://tinyurl.com/b772sd77\nDEFAULT_TEXT_TO_SQL_TMPL = (\n    \"Given an input question, first create a syntactically correct SQL query \"\n    \"to run, then look at the results of the query and return the answer.\\n\"\n    \"Use the following format:\\n\"\n    'Question: \"Question here\"\\n'\n    'SQLQuery: \"SQL Query to run\"\\n'\n    \"The following is a schema of the table:\\n\"\n    \"---------------------\\n\"\n    \"{schema}\\n\"\n    \"---------------------\\n\"\n    \"Question: {query_str}\\n\"\n    \"SQLQuery: \"\n)\n\nDEFAULT_TEXT_TO_SQL_PROMPT = TextToSQLPrompt(DEFAULT_TEXT_TO_SQL_TMPL)\n\n\n# NOTE: by partially filling schema, we can reduce to a QuestionAnswer prompt\n# that we can feed to ur table\nDEFAULT_TABLE_CONTEXT_TMPL = (\n    \"We have provided a table schema below. \"\n    \"---------------------\\n\"\n    \"{schema}\\n\"\n    \"---------------------\\n\"\n    \"We have also provided context information below. \"\n    \"{context_str}\\n\"\n    \"---------------------\\n\"\n    \"Given the context information and the table schema, \"\n    \"give a response to the following task: {query_str}\"\n)\n\nDEFAULT_TABLE_CONTEXT_QUERY = (\n    \"Provide a high-level description of the table, \"\n    \"as well as a description of each column in the table. \"\n    \"Provide answers in the following format:\\n\"\n    \"TableDescription: <description>\\n\"\n    \"Column1Description: <description>\\n\"\n    \"Column2Description: <description>\\n\"\n    \"...\\n\\n\"\n)\n\nDEFAULT_TABLE_CONTEXT_PROMPT = TableContextPrompt(DEFAULT_TABLE_CONTEXT_TMPL)\n\n# NOTE: by partially filling schema, we can reduce to a RefinePrompt\n# that we can feed to ur table\nDEFAULT_REFINE_TABLE_CONTEXT_TMPL = (\n    \"We have provided a table schema below. \"\n    \"---------------------\\n\"\n    \"{schema}\\n\"\n    \"---------------------\\n\"\n    \"We have also provided some context information below. \"\n    \"{context_msg}\\n\"\n    \"---------------------\\n\"\n    \"Given the context information and the table schema, \"\n    \"give a response to the following task: {query_str}\\n\"\n    \"We have provided an existing answer: {existing_answer}\\n\"\n    \"Given the new context, refine the original answer to better \"\n    \"answer the question. \"\n    \"If the context isn't useful, return the original answer.\"\n)\nDEFAULT_REFINE_TABLE_CONTEXT_PROMPT = RefineTableContextPrompt(\n    DEFAULT_REFINE_TABLE_CONTEXT_TMPL\n)\n", "doc_id": "67d318229cee08ef396c5183c85a937be9a1a2db", "embedding": null, "extra_info": {"file_path": "gpt_index/prompts/default_prompts.py", "file_name": "default_prompts.py"}, "__type__": "Document"}, "6733ae7741e49b480a7fa692ec545ff3edec1a13": {"text": "\"\"\"Prompt types enum.\"\"\"\n\nfrom enum import Enum\n\n\nclass PromptType(str, Enum):\n    \"\"\"Prompt type.\"\"\"\n\n    # summarization\n    SUMMARY = \"summary\"\n    # tree insert node\n    TREE_INSERT = \"insert\"\n    # tree select query prompt\n    TREE_SELECT = \"tree_select\"\n    # tree select query prompt (multiple)\n    TREE_SELECT_MULTIPLE = \"tree_select_multiple\"\n    # question-answer\n    QUESTION_ANSWER = \"text_qa\"\n    # refine\n    REFINE = \"refine\"\n    # keyword extract\n    KEYWORD_EXTRACT = \"keyword_extract\"\n    # query keyword extract\n    QUERY_KEYWORD_EXTRACT = \"query_keyword_extract\"\n\n    # schema extract\n    SCHEMA_EXTRACT = \"schema_extract\"\n\n    # text to sql\n    TEXT_TO_SQL = \"text_to_sql\"\n\n    # table context\n    TABLE_CONTEXT = \"table_context\"\n\n    # custom (by default)\n    CUSTOM = \"custom\"\n", "doc_id": "6733ae7741e49b480a7fa692ec545ff3edec1a13", "embedding": null, "extra_info": {"file_path": "gpt_index/prompts/prompt_type.py", "file_name": "prompt_type.py"}, "__type__": "Document"}, "583437e9d61e15269b7b59a5121fa41dedd096c3": {"text": "\"\"\"Subclasses from base prompt.\"\"\"\nfrom typing import List\n\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.prompts.prompt_type import PromptType\n\n\nclass SummaryPrompt(Prompt):\n    \"\"\"Summary prompt.\n\n    Prompt to summarize the provided `context_str`.\n\n    Required template variables: `context_str`\n\n    Args:\n        template (str): Template for the prompt.\n        **prompt_kwargs: Keyword arguments for the prompt.\n\n    \"\"\"\n\n    prompt_type: PromptType = PromptType.SUMMARY\n    input_variables: List[str] = [\"context_str\"]\n\n\nclass TreeInsertPrompt(Prompt):\n    \"\"\"Tree Insert prompt.\n\n    Prompt to insert a new chunk of text `new_chunk_text` into the tree index.\n    More specifically, this prompt has the LLM select the relevant candidate\n    child node to continue tree traversal.\n\n    Required template variables: `num_chunks`, `context_list`, `new_chunk_text`\n\n    Args:\n        template (str): Template for the prompt.\n        **prompt_kwargs: Keyword arguments for the prompt.\n\n    \"\"\"\n\n    prompt_type: PromptType = PromptType.TREE_INSERT\n    input_variables: List[str] = [\"num_chunks\", \"context_list\", \"new_chunk_text\"]\n\n\nclass TreeSelectPrompt(Prompt):\n    \"\"\"Tree select prompt.\n\n    Prompt to select a candidate child node out of all child nodes\n    provided in `context_list`, given a query `query_str`. `num_chunks` is\n    the number of child nodes in `context_list`.\n\n    Required template variables: `num_chunks`, `context_list`, `query_str`\n\n    Args:\n        template (str): Template for the prompt.\n        **prompt_kwargs: Keyword arguments for the prompt.\n\n    \"\"\"\n\n    prompt_type: PromptType = PromptType.TREE_SELECT\n    input_variables: List[str] = [\"num_chunks\", \"context_list\", \"query_str\"]\n\n\nclass TreeSelectMultiplePrompt(Prompt):\n    \"\"\"Tree select multiple prompt.\n\n    Prompt to select multiple candidate child nodes out of all\n    child nodes provided in `context_list`, given a query `query_str`.\n    `branching_factor` refers to the number of child nodes to select, and\n    `num_chunks` is the number of child nodes in `context_list`.\n\n    Required template variables: `num_chunks`, `context_list`, `query_str`,\n        `branching_factor`\n\n    Args:\n        template (str): Template for the prompt.\n        **prompt_kwargs: Keyword arguments for the prompt.\n\n    \"\"\"\n\n    prompt_type = PromptType.TREE_SELECT_MULTIPLE\n    input_variables: List[str] = [\n        \"num_chunks\",\n        \"context_list\",\n        \"query_str\",\n        \"branching_factor\",\n    ]\n\n\nclass RefinePrompt(Prompt):\n    \"\"\"Refine prompt.\n\n    Prompt to refine an existing answer `existing_answer` given a context `context_msg`,\n    and a query `query_str`.\n\n    Required template variables: `query_str`, `existing_answer`, `context_msg`\n\n    Args:\n        template (str): Template for the prompt.\n        **prompt_kwargs: Keyword arguments for the prompt.\n\n    \"\"\"\n\n    # TODO: rename context_msg to context_str\n\n    prompt_type: PromptType = PromptType.REFINE\n    input_variables: List[str] = [\"query_str\", \"existing_answer\", \"context_msg\"]\n\n\nclass QuestionAnswerPrompt(Prompt):\n    \"\"\"Question Answer prompt.\n\n    Prompt to answer a question `query_str` given a context `context_str`.\n\n    Required template variables: `context_str`, `query_str`\n\n    Args:\n        template (str): Template for the prompt.\n        **prompt_kwargs: Keyword arguments for the prompt.\n\n    \"\"\"\n\n    prompt_type: PromptType = PromptType.QUESTION_ANSWER\n    input_variables: List[str] = [\"context_str\", \"query_str\"]\n\n\nclass KeywordExtractPrompt(Prompt):\n    \"\"\"Keyword extract prompt.\n\n    Prompt to extract keywords from a text `text` with a maximum of\n    `max_keywords` keywords.\n\n    Required template variables: `text`, `max_keywords`\n\n    Args:\n        template (str): Template for the prompt.\n        **prompt_kwargs: Keyword arguments for the prompt.\n\n    \"\"\"\n\n    prompt_type: PromptType = PromptType.KEYWORD_EXTRACT\n    input_variables: List[str] = [\"text\", \"max_keywords\"]\n\n\nclass QueryKeywordExtractPrompt(Prompt):\n    \"\"\"Query keyword extract prompt.\n\n    Prompt to extract keywords from a query `query_str` with a maximum\n    of `max_keywords` keywords.\n\n    Required template variables: `query_str`, `max_keywords`\n\n    Args:\n        template (str): Template for the prompt.\n        **prompt_kwargs: Keyword arguments for the prompt.\n\n    \"\"\"\n\n    prompt_type: PromptType = PromptType.QUERY_KEYWORD_EXTRACT\n    input_variables: List[str] = [\"question\", \"max_keywords\"]\n\n\nclass SchemaExtractPrompt(Prompt):\n    \"\"\"Schema extract prompt.\n\n    Prompt to extract schema from unstructured text `text`.\n\n    Required template variables: `text`, `schema`\n\n    Args:\n        template (str): Template for the prompt.\n        **prompt_kwargs: Keyword arguments for the prompt.\n\n    \"\"\"\n\n    prompt_type: PromptType = PromptType.SCHEMA_EXTRACT\n    input_variables: List[str] = [\"text\", \"schema\"]\n\n\nclass TextToSQLPrompt(Prompt):\n    \"\"\"Text to SQL prompt.\n\n    Prompt to translate a natural language query into SQL,\n    given a schema `schema`.\n\n    Required template variables: `query_str`, `schema`\n\n    Args:\n        template (str): Template for the prompt.\n        **prompt_kwargs: Keyword arguments for the prompt.\n\n    \"\"\"\n\n    prompt_type: PromptType = PromptType.TEXT_TO_SQL\n    input_variables: List[str] = [\"query_str\", \"schema\"]\n\n\nclass TableContextPrompt(Prompt):\n    \"\"\"Table context prompt.\n\n    Prompt to generate a table context given a table schema `schema`,\n    as well as unstructured text context `context_str`, and\n    a task `query_str`.\n    This includes both a high-level description of the table\n    as well as a description of each column in the table.\n\n    Args:\n        template (str): Template for the prompt.\n        **prompt_kwargs: Keyword arguments for the prompt.\n\n    \"\"\"\n\n    prompt_type: PromptType = PromptType.TABLE_CONTEXT\n    input_variables: List[str] = [\"schema\", \"context_str\", \"query_str\"]\n\n\nclass RefineTableContextPrompt(Prompt):\n    \"\"\"Refine Table context prompt.\n\n    Prompt to refine a table context given a table schema `schema`,\n    as well as unstructured text context `context_msg`, and\n    a task `query_str`.\n    This includes both a high-level description of the table\n    as well as a description of each column in the table.\n\n    Args:\n        template (str): Template for the prompt.\n        **prompt_kwargs: Keyword arguments for the prompt.\n\n    \"\"\"\n\n    # TODO: rename context_msg to context_str\n\n    prompt_type: PromptType = PromptType.TABLE_CONTEXT\n    input_variables: List[str] = [\n        \"schema\",\n        \"context_msg\",\n        \"query_str\",\n        \"existing_answer\",\n    ]\n", "doc_id": "583437e9d61e15269b7b59a5121fa41dedd096c3", "embedding": null, "extra_info": {"file_path": "gpt_index/prompts/prompts.py", "file_name": "prompts.py"}, "__type__": "Document"}, "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391": {"text": "", "doc_id": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391", "embedding": null, "extra_info": {"file_path": "gpt_index/py.typed", "file_name": "py.typed"}, "__type__": "Document"}, "846471c13398def5f566e12afc3117ddc30b9c88": {"text": "\"\"\"Data Connectors for GPT Index.\n\nThis module contains the data connectors for GPT Index. Each connector inherits\nfrom a `BaseReader` class, connects to a data source, and loads Document objects\nfrom that data source.\n\nYou may also choose to construct Document objects manually, for instance\nin our `Insert How-To Guide <../how_to/insert.html>`_. See below for the API\ndefinition of a Document - the bare minimum is a `text` property.\n\n\"\"\"\n\nfrom gpt_index.readers.discord_reader import DiscordReader\nfrom gpt_index.readers.faiss import FaissReader\n\n# readers\nfrom gpt_index.readers.file.base import SimpleDirectoryReader\nfrom gpt_index.readers.github_readers.github_repository_reader import (\n    GithubRepositoryReader,\n)\nfrom gpt_index.readers.google_readers.gdocs import GoogleDocsReader\nfrom gpt_index.readers.make_com.wrapper import MakeWrapper\nfrom gpt_index.readers.mbox import MboxReader\nfrom gpt_index.readers.mongo import SimpleMongoReader\nfrom gpt_index.readers.notion import NotionPageReader\nfrom gpt_index.readers.obsidian import ObsidianReader\nfrom gpt_index.readers.pinecone import PineconeReader\nfrom gpt_index.readers.qdrant import QdrantReader\nfrom gpt_index.readers.schema.base import Document\nfrom gpt_index.readers.slack import SlackReader\nfrom gpt_index.readers.string_iterable import StringIterableReader\nfrom gpt_index.readers.twitter import TwitterTweetReader\nfrom gpt_index.readers.weaviate.reader import WeaviateReader\nfrom gpt_index.readers.web import (\n    BeautifulSoupWebReader,\n    RssReader,\n    SimpleWebPageReader,\n    TrafilaturaWebReader,\n)\nfrom gpt_index.readers.wikipedia import WikipediaReader\nfrom gpt_index.readers.youtube_transcript import YoutubeTranscriptReader\n\n__all__ = [\n    \"WikipediaReader\",\n    \"YoutubeTranscriptReader\",\n    \"SimpleDirectoryReader\",\n    \"SimpleMongoReader\",\n    \"NotionPageReader\",\n    \"GoogleDocsReader\",\n    \"DiscordReader\",\n    \"SlackReader\",\n    \"WeaviateReader\",\n    \"PineconeReader\",\n    \"QdrantReader\",\n    \"FaissReader\",\n    \"Document\",\n    \"StringIterableReader\",\n    \"SimpleWebPageReader\",\n    \"BeautifulSoupWebReader\",\n    \"TrafilaturaWebReader\",\n    \"RssReader\",\n    \"MakeWrapper\",\n    \"TwitterTweetReader\",\n    \"ObsidianReader\",\n    \"GithubRepositoryReader\",\n    \"MboxReader\",\n]\n", "doc_id": "846471c13398def5f566e12afc3117ddc30b9c88", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/__init__.py", "file_name": "__init__.py"}, "__type__": "Document"}, "f5cc30de3365fe4a08a28f1e95a9bdd2b17e09a0": {"text": "\"\"\"Base reader class.\"\"\"\nfrom abc import abstractmethod\nfrom typing import Any, List\n\nfrom langchain.docstore.document import Document as LCDocument\n\nfrom gpt_index.readers.schema.base import Document\n\n\nclass BaseReader:\n    \"\"\"Utilities for loading data from a directory.\"\"\"\n\n    @abstractmethod\n    def load_data(self, *args: Any, **load_kwargs: Any) -> List[Document]:\n        \"\"\"Load data from the input directory.\"\"\"\n\n    def load_langchain_documents(self, **load_kwargs: Any) -> List[LCDocument]:\n        \"\"\"Load data in LangChain document format.\"\"\"\n        docs = self.load_data(**load_kwargs)\n        return [d.to_langchain_format() for d in docs]\n", "doc_id": "f5cc30de3365fe4a08a28f1e95a9bdd2b17e09a0", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/base.py", "file_name": "base.py"}, "__type__": "Document"}, "4098c3e4bb1452ddb95150e467c23f0d503b34e1": {"text": "\"\"\"Database Reader.\"\"\"\n\nfrom typing import Any, List, Optional\n\nfrom sqlalchemy import text\nfrom sqlalchemy.engine import Engine\n\nfrom gpt_index.langchain_helpers.sql_wrapper import SQLDatabase\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass DatabaseReader(BaseReader):\n    \"\"\"Simple Database reader.\n\n    Concatenates each row into Document used by GPT Index.\n\n    Args:\n        sql_database (Optional[SQLDatabase]): SQL database to use,\n            including table names to specify.\n            See :ref:`Ref-Struct-Store` for more details.\n\n        OR\n\n        engine (Optional[Engine]): SQLAlchemy Engine object of the database connection.\n\n        OR\n\n        uri (Optional[str]): uri of the database connection.\n\n        OR\n\n        scheme (Optional[str]): scheme of the database connection.\n        host (Optional[str]): host of the database connection.\n        port (Optional[int]): port of the database connection.\n        user (Optional[str]): user of the database connection.\n        password (Optional[str]): password of the database connection.\n        dbname (Optional[str]): dbname of the database connection.\n\n    Returns:\n        DatabaseReader: A DatabaseReader object.\n    \"\"\"\n\n    def __init__(\n        self,\n        sql_database: Optional[SQLDatabase] = None,\n        engine: Optional[Engine] = None,\n        uri: Optional[str] = None,\n        scheme: Optional[str] = None,\n        host: Optional[str] = None,\n        port: Optional[str] = None,\n        user: Optional[str] = None,\n        password: Optional[str] = None,\n        dbname: Optional[str] = None,\n        *args: Optional[Any],\n        **kwargs: Optional[Any],\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        if sql_database:\n            self.sql_database = sql_database\n        elif engine:\n            self.sql_database = SQLDatabase(engine, *args, **kwargs)\n        elif uri:\n            self.uri = uri\n            self.sql_database = SQLDatabase.from_uri(uri, *args, **kwargs)\n        elif scheme and host and port and user and password and dbname:\n            uri = f\"{scheme}://{user}:{password}@{host}:{port}/{dbname}\"\n            self.uri = uri\n            self.sql_database = SQLDatabase.from_uri(uri, *args, **kwargs)\n        else:\n            raise ValueError(\n                \"You must provide either a SQLDatabase, \"\n                \"a SQL Alchemy Engine, a valid connection URI, or a valid \"\n                \"set of credentials.\"\n            )\n\n    def load_data(self, query: str) -> List[Document]:\n        \"\"\"Query and load data from the Database, returning a list of Documents.\n\n        Args:\n            query (str): Query parameter to filter tables and rows.\n\n        Returns:\n            List[Document]: A list of Document objects.\n        \"\"\"\n        documents = []\n        with self.sql_database.engine.connect() as connection:\n            if query is None:\n                raise ValueError(\"A query parameter is necessary to filter the data\")\n            else:\n                result = connection.execute(text(query))\n\n            for item in result.fetchall():\n                documents.append(Document(item[0]))\n        return documents\n", "doc_id": "4098c3e4bb1452ddb95150e467c23f0d503b34e1", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/database.py", "file_name": "database.py"}, "__type__": "Document"}, "8d9a229a3248b1617b67774eac291a2594ec686e": {"text": "\"\"\"Discord reader.\n\nNote: this file is named discord_reader.py to avoid conflicts with the\ndiscord.py module.\n\n\"\"\"\n\nimport asyncio\nimport logging\nimport os\nfrom typing import List, Optional\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nasync def read_channel(\n    discord_token: str, channel_id: int, limit: Optional[int], oldest_first: bool\n) -> str:\n    \"\"\"Async read channel.\n\n    Note: This is our hack to create a synchronous interface to the\n    async discord.py API. We use the `asyncio` module to run\n    this function with `asyncio.get_event_loop().run_until_complete`.\n\n    \"\"\"\n    import discord  # noqa: F401\n\n    messages: List[discord.Message] = []\n\n    class CustomClient(discord.Client):\n        async def on_ready(self) -> None:\n            try:\n                logging.info(f\"{self.user} has connected to Discord!\")\n                channel = client.get_channel(channel_id)\n                # only work for text channels for now\n                if not isinstance(channel, discord.TextChannel):\n                    raise ValueError(\n                        f\"Channel {channel_id} is not a text channel. \"\n                        \"Only text channels are supported for now.\"\n                    )\n                # thread_dict maps thread_id to thread\n                thread_dict = {}\n                for thread in channel.threads:\n                    thread_dict[thread.id] = thread\n\n                async for msg in channel.history(\n                    limit=limit, oldest_first=oldest_first\n                ):\n                    messages.append(msg)\n                    if msg.id in thread_dict:\n                        thread = thread_dict[msg.id]\n                        async for thread_msg in thread.history(\n                            limit=limit, oldest_first=oldest_first\n                        ):\n                            messages.append(thread_msg)\n            except Exception as e:\n                logging.error(\"Encountered error: \" + str(e))\n            finally:\n                await self.close()\n\n    intents = discord.Intents.default()\n    intents.message_content = True\n    client = CustomClient(intents=intents)\n    await client.start(discord_token)\n\n    msg_txt_list = [m.content for m in messages]\n\n    return \"\\n\\n\".join(msg_txt_list)\n\n\nclass DiscordReader(BaseReader):\n    \"\"\"Discord reader.\n\n    Reads conversations from channels.\n\n    Args:\n        discord_token (Optional[str]): Discord token. If not provided, we\n            assume the environment variable `DISCORD_TOKEN` is set.\n\n    \"\"\"\n\n    def __init__(self, discord_token: Optional[str] = None) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            import discord  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`discord.py` package not found, please run `pip install discord.py`\"\n            )\n        if discord_token is None:\n            discord_token = os.environ[\"DISCORD_TOKEN\"]\n            if discord_token is None:\n                raise ValueError(\n                    \"Must specify `discord_token` or set environment \"\n                    \"variable `DISCORD_TOKEN`.\"\n                )\n\n        self.discord_token = discord_token\n\n    def _read_channel(\n        self, channel_id: int, limit: Optional[int] = None, oldest_first: bool = True\n    ) -> str:\n        \"\"\"Read channel.\"\"\"\n        result = asyncio.get_event_loop().run_until_complete(\n            read_channel(\n                self.discord_token, channel_id, limit=limit, oldest_first=oldest_first\n            )\n        )\n        return result\n\n    def load_data(\n        self,\n        channel_ids: List[int],\n        limit: Optional[int] = None,\n        oldest_first: bool = True,\n    ) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            channel_ids (List[int]): List of channel ids to read.\n            limit (Optional[int]): Maximum number of messages to read.\n            oldest_first (bool): Whether to read oldest messages first.\n                Defaults to `True`.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        results: List[Document] = []\n        for channel_id in channel_ids:\n            if not isinstance(channel_id, int):\n                raise ValueError(\n                    f\"Channel id {channel_id} must be an integer, \"\n                    f\"not {type(channel_id)}.\"\n                )\n            channel_content = self._read_channel(\n                channel_id, limit=limit, oldest_first=oldest_first\n            )\n            results.append(\n                Document(channel_content, extra_info={\"channel\": channel_id})\n            )\n        return results\n\n\nif __name__ == \"__main__\":\n    reader = DiscordReader()\n    logging.info(\"initialized reader\")\n    output = reader.load_data(channel_ids=[1057178784895348746], limit=10)\n    logging.info(output)\n", "doc_id": "8d9a229a3248b1617b67774eac291a2594ec686e", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/discord_reader.py", "file_name": "discord_reader.py"}, "__type__": "Document"}, "71585e29180e9ae11f0774b391bf9fd877e5ac54": {"text": "\"\"\"Download loader from the Loader Hub.\"\"\"\n\nimport json\nimport os\nimport subprocess\nimport sys\nfrom importlib import util\nfrom pathlib import Path\n\nimport pkg_resources\nimport requests\nfrom pkg_resources import DistributionNotFound\n\nfrom gpt_index.readers.base import BaseReader\n\nLOADER_HUB_URL = (\n    \"https://raw.githubusercontent.com/emptycrown/loader-hub/main/loader_hub\"\n)\n\n\ndef download_loader(loader_class: str) -> BaseReader:\n    \"\"\"Download a single loader from the Loader Hub.\n\n    Args:\n        loader_class: The name of the loader class you want to download,\n            such as `SimpleWebPageReader`.\n    Returns:\n        A Loader.\n    \"\"\"\n    response = requests.get(f\"{LOADER_HUB_URL}/library.json\")\n    library = json.loads(response.text)\n\n    # Look up the loader id (e.g. `web/simple_web`)\n    loader_id = library[loader_class][\"id\"]\n    dirpath = \".modules\"\n    loader_filename = loader_id.replace(\"/\", \"-\")\n    loader_path = f\"{dirpath}/{loader_filename}.py\"\n    requirements_path = f\"{dirpath}/{loader_filename}_requirements.txt\"\n\n    if not os.path.exists(dirpath):\n        # Create a new directory because it does not exist\n        os.makedirs(dirpath)\n\n    if not os.path.exists(loader_path):\n        response = requests.get(f\"{LOADER_HUB_URL}/{loader_id}/base.py\")\n        with open(loader_path, \"w\") as f:\n            f.write(response.text)\n\n    if not os.path.exists(requirements_path):\n        response = requests.get(f\"{LOADER_HUB_URL}/{loader_id}/requirements.txt\")\n        if response.status_code == 200:\n            with open(requirements_path, \"w\") as f:\n                f.write(response.text)\n\n    # Install dependencies if there are any and not already installed\n    if os.path.exists(requirements_path):\n        try:\n            requirements = pkg_resources.parse_requirements(\n                Path(requirements_path).open()\n            )\n            pkg_resources.require([str(r) for r in requirements])\n        except DistributionNotFound:\n            subprocess.check_call(\n                [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", requirements_path]\n            )\n\n    spec = util.spec_from_file_location(\"custom_loader\", location=loader_path)\n    if spec is None:\n        raise ValueError(f\"Could not find file: {loader_path}.\")\n    module = util.module_from_spec(spec)\n    spec.loader.exec_module(module)  # type: ignore\n\n    return getattr(module, loader_class)\n", "doc_id": "71585e29180e9ae11f0774b391bf9fd877e5ac54", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/download.py", "file_name": "download.py"}, "__type__": "Document"}, "82f076eb7558c8648d1b13d5ccec126b5cb6a5f3": {"text": "\"\"\"Faiss reader.\"\"\"\n\nfrom typing import Any, Dict, List\n\nimport numpy as np\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass FaissReader(BaseReader):\n    \"\"\"Faiss reader.\n\n    Retrieves documents through an existing in-memory Faiss index.\n    These documents can then be used in a downstream GPT Index data structure.\n    If you wish use Faiss itself as an index to to organize documents,\n    insert documents, and perform queries on them, please use GPTFaissIndex.\n\n    Args:\n        faiss_index (faiss.Index): A Faiss Index object (required)\n\n    \"\"\"\n\n    def __init__(self, index: Any):\n        \"\"\"Initialize with parameters.\"\"\"\n        import_err_msg = \"\"\"\n            `faiss` package not found. For instructions on\n            how to install `faiss` please visit\n            https://github.com/facebookresearch/faiss/wiki/Installing-Faiss\n        \"\"\"\n        try:\n            import faiss  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n\n        self._index = index\n\n    def load_data(\n        self,\n        query: np.ndarray,\n        id_to_text_map: Dict[str, str],\n        k: int = 4,\n        separate_documents: bool = True,\n    ) -> List[Document]:\n        \"\"\"Load data from Faiss.\n\n        Args:\n            query (np.ndarray): A 2D numpy array of query vectors.\n            id_to_text_map (Dict[str, str]): A map from ID's to text.\n            k (int): Number of nearest neighbors to retrieve. Defaults to 4.\n            separate_documents (Optional[bool]): Whether to return separate\n                documents. Defaults to True.\n        Returns:\n            List[Document]: A list of documents.\n\n        \"\"\"\n        dists, indices = self._index.search(query, k)\n        documents = []\n        for qidx in range(indices.shape[0]):\n            for didx in range(indices.shape[1]):\n                doc_id = indices[qidx, didx]\n                if doc_id not in id_to_text_map:\n                    raise ValueError(\n                        f\"Document ID {doc_id} not found in id_to_text_map.\"\n                    )\n                text = id_to_text_map[doc_id]\n                documents.append(Document(text=text))\n\n        if not separate_documents:\n            # join all documents into one\n            text_list = [doc.get_text() for doc in documents]\n            text = \"\\n\\n\".join(text_list)\n            documents = [Document(text=text)]\n\n        return documents\n", "doc_id": "82f076eb7558c8648d1b13d5ccec126b5cb6a5f3", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/faiss.py", "file_name": "faiss.py"}, "__type__": "Document"}, "d0027a089853c5ff8efcaa11483f67fc0e61b666": {"text": "\"\"\"Simple reader that reads files of different formats from a directory.\"\"\"\nimport logging\nfrom pathlib import Path\nfrom typing import Callable, Dict, List, Optional, Union\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.file.base_parser import BaseParser\nfrom gpt_index.readers.file.docs_parser import DocxParser, PDFParser\nfrom gpt_index.readers.file.epub_parser import EpubParser\nfrom gpt_index.readers.file.image_parser import ImageParser\nfrom gpt_index.readers.file.markdown_parser import MarkdownParser\nfrom gpt_index.readers.file.mbox_parser import MboxParser\nfrom gpt_index.readers.file.slides_parser import PptxParser\nfrom gpt_index.readers.file.tabular_parser import PandasCSVParser\nfrom gpt_index.readers.file.video_audio import VideoAudioParser\nfrom gpt_index.readers.schema.base import Document\n\nDEFAULT_FILE_EXTRACTOR: Dict[str, BaseParser] = {\n    \".pdf\": PDFParser(),\n    \".docx\": DocxParser(),\n    \".pptx\": PptxParser(),\n    \".jpg\": ImageParser(),\n    \".png\": ImageParser(),\n    \".jpeg\": ImageParser(),\n    \".mp3\": VideoAudioParser(),\n    \".mp4\": VideoAudioParser(),\n    \".csv\": PandasCSVParser(),\n    \".epub\": EpubParser(),\n    \".md\": MarkdownParser(),\n    \".mbox\": MboxParser(),\n}\n\n\nclass SimpleDirectoryReader(BaseReader):\n    \"\"\"Simple directory reader.\n\n    Can read files into separate documents, or concatenates\n    files into one document text.\n\n    Args:\n        input_dir (str): Path to the directory.\n        input_files (List): List of file paths to read (Optional; overrides input_dir)\n        exclude_hidden (bool): Whether to exclude hidden files (dotfiles).\n        errors (str): how encoding and decoding errors are to be handled,\n              see https://docs.python.org/3/library/functions.html#open\n        recursive (bool): Whether to recursively search in subdirectories.\n            False by default.\n        required_exts (Optional[List[str]]): List of required extensions.\n            Default is None.\n        file_extractor (Optional[Dict[str, BaseParser]]): A mapping of file\n            extension to a BaseParser class that specifies how to convert that file\n            to text. See DEFAULT_FILE_EXTRACTOR.\n        num_files_limit (Optional[int]): Maximum number of files to read.\n            Default is None.\n        file_metadata (Optional[Callable[str, Dict]]): A function that takes\n            in a filename and returns a Dict of metadata for the Document.\n            Default is None.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dir: Optional[str] = None,\n        input_files: Optional[List] = None,\n        exclude_hidden: bool = True,\n        errors: str = \"ignore\",\n        recursive: bool = False,\n        required_exts: Optional[List[str]] = None,\n        file_extractor: Optional[Dict[str, BaseParser]] = None,\n        num_files_limit: Optional[int] = None,\n        file_metadata: Optional[Callable[[str], Dict]] = None,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        super().__init__()\n\n        if not input_dir and not input_files:\n            raise ValueError(\"Must provide either `input_dir` or `input_files`.\")\n\n        self.errors = errors\n\n        self.recursive = recursive\n        self.exclude_hidden = exclude_hidden\n        self.required_exts = required_exts\n        self.num_files_limit = num_files_limit\n\n        if input_files:\n            self.input_files = []\n            for path in input_files:\n                input_file = Path(path)\n                self.input_files.append(input_file)\n        elif input_dir:\n            self.input_dir = Path(input_dir)\n            self.input_files = self._add_files(self.input_dir)\n\n        self.file_extractor = file_extractor or DEFAULT_FILE_EXTRACTOR\n        self.file_metadata = file_metadata\n\n    def _add_files(self, input_dir: Path) -> List[Path]:\n        \"\"\"Add files.\"\"\"\n        input_files = sorted(input_dir.iterdir())\n        new_input_files = []\n        dirs_to_explore = []\n        for input_file in input_files:\n            if input_file.is_dir():\n                if self.recursive:\n                    dirs_to_explore.append(input_file)\n            elif self.exclude_hidden and input_file.name.startswith(\".\"):\n                continue\n            elif (\n                self.required_exts is not None\n                and input_file.suffix not in self.required_exts\n            ):\n                continue\n            else:\n                new_input_files.append(input_file)\n\n        for dir_to_explore in dirs_to_explore:\n            sub_input_files = self._add_files(dir_to_explore)\n            new_input_files.extend(sub_input_files)\n\n        if self.num_files_limit is not None and self.num_files_limit > 0:\n            new_input_files = new_input_files[0 : self.num_files_limit]\n\n        # print total number of files added\n        logging.debug(\n            f\"> [SimpleDirectoryReader] Total files added: {len(new_input_files)}\"\n        )\n\n        return new_input_files\n\n    def load_data(self, concatenate: bool = False) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            concatenate (bool): whether to concatenate all files into one document.\n                If set to True, file metadata is ignored.\n                False by default.\n\n        Returns:\n            List[Document]: A list of documents.\n\n        \"\"\"\n        data: Union[str, List[str]] = \"\"\n        data_list: List[str] = []\n        metadata_list = []\n        for input_file in self.input_files:\n            if input_file.suffix in self.file_extractor:\n                parser = self.file_extractor[input_file.suffix]\n                if not parser.parser_config_set:\n                    parser.init_parser()\n                data = parser.parse_file(input_file, errors=self.errors)\n            else:\n                # do standard read\n                with open(input_file, \"r\", errors=self.errors) as f:\n                    data = f.read()\n            if isinstance(data, List):\n                data_list.extend(data)\n            else:\n                data_list.append(str(data))\n            if self.file_metadata is not None:\n                metadata_list.append(self.file_metadata(str(input_file)))\n\n        if concatenate:\n            return [Document(\"\\n\".join(data_list))]\n        elif self.file_metadata is not None:\n            return [Document(d, extra_info=m) for d, m in zip(data_list, metadata_list)]\n        else:\n            return [Document(d) for d in data_list]\n", "doc_id": "d0027a089853c5ff8efcaa11483f67fc0e61b666", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base.py", "file_name": "base.py"}, "__type__": "Document"}, "753a56f9797432f053fb96a72bdb782a2b20bd05": {"text": "\"\"\"Base parser and config class.\"\"\"\n\nfrom abc import abstractmethod\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Union\n\n\nclass BaseParser:\n    \"\"\"Base class for all parsers.\"\"\"\n\n    def __init__(self, parser_config: Optional[Dict] = None):\n        \"\"\"Init params.\"\"\"\n        self._parser_config = parser_config\n\n    def init_parser(self) -> None:\n        \"\"\"Init parser and store it.\"\"\"\n        parser_config = self._init_parser()\n        self._parser_config = parser_config\n\n    @property\n    def parser_config_set(self) -> bool:\n        \"\"\"Check if parser config is set.\"\"\"\n        return self._parser_config is not None\n\n    @property\n    def parser_config(self) -> Dict:\n        \"\"\"Check if parser config is set.\"\"\"\n        if self._parser_config is None:\n            raise ValueError(\"Parser config not set.\")\n        return self._parser_config\n\n    @abstractmethod\n    def _init_parser(self) -> Dict:\n        \"\"\"Initialize the parser with the config.\"\"\"\n\n    @abstractmethod\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> Union[str, List[str]]:\n        \"\"\"Parse file.\"\"\"\n", "doc_id": "753a56f9797432f053fb96a72bdb782a2b20bd05", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/base_parser.py", "file_name": "base_parser.py"}, "__type__": "Document"}, "ac3dce9de6352d92aa69fdf67294da228f5085c3": {"text": "\"\"\"Docs parser.\n\nContains parsers for docx, pdf files.\n\n\"\"\"\nfrom pathlib import Path\nfrom typing import Dict\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass PDFParser(BaseParser):\n    \"\"\"PDF parser.\"\"\"\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        return {}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> str:\n        \"\"\"Parse file.\"\"\"\n        try:\n            import PyPDF2\n        except ImportError:\n            raise ValueError(\"PyPDF2 is required to read PDF files.\")\n        text_list = []\n        with open(file, \"rb\") as fp:\n            # Create a PDF object\n            pdf = PyPDF2.PdfReader(fp)\n\n            # Get the number of pages in the PDF document\n            num_pages = len(pdf.pages)\n\n            # Iterate over every page\n            for page in range(num_pages):\n                # Extract the text from the page\n                page_text = pdf.pages[page].extract_text()\n                text_list.append(page_text)\n        text = \"\\n\".join(text_list)\n\n        return text\n\n\nclass DocxParser(BaseParser):\n    \"\"\"Docx parser.\"\"\"\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        return {}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> str:\n        \"\"\"Parse file.\"\"\"\n        try:\n            import docx2txt\n        except ImportError:\n            raise ValueError(\"docx2txt is required to read Microsoft Word files.\")\n\n        text = docx2txt.process(file)\n\n        return text\n", "doc_id": "ac3dce9de6352d92aa69fdf67294da228f5085c3", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/docs_parser.py", "file_name": "docs_parser.py"}, "__type__": "Document"}, "d04f28177f58cd45629b425083bd5affd6fba442": {"text": "\"\"\"Epub parser.\n\nContains parsers for epub files.\n\"\"\"\n\nfrom pathlib import Path\nfrom typing import Dict\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass EpubParser(BaseParser):\n    \"\"\"Epub Parser.\"\"\"\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        return {}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> str:\n        \"\"\"Parse file.\"\"\"\n        try:\n            import ebooklib\n            from ebooklib import epub\n        except ImportError:\n            raise ValueError(\"`EbookLib` is required to read Epub files.\")\n        try:\n            import html2text\n        except ImportError:\n            raise ValueError(\"`html2text` is required to parse Epub files.\")\n\n        text_list = []\n        book = epub.read_epub(file, options={\"ignore_ncx\": True})\n\n        # Iterate through all chapters.\n        for item in book.get_items():\n            # Chapters are typically located in epub documents items.\n            if item.get_type() == ebooklib.ITEM_DOCUMENT:\n                text_list.append(\n                    html2text.html2text(item.get_content().decode(\"utf-8\"))\n                )\n\n        text = \"\\n\".join(text_list)\n        return text\n", "doc_id": "d04f28177f58cd45629b425083bd5affd6fba442", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/epub_parser.py", "file_name": "epub_parser.py"}, "__type__": "Document"}, "618f6054d906179da2ba7ae65568378f60173c97": {"text": "\"\"\"Image parser.\n\nContains parsers for image files.\n\n\"\"\"\n\nimport re\nfrom pathlib import Path\nfrom typing import Dict\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass ImageParser(BaseParser):\n    \"\"\"Image parser.\n\n    Extract text from images using DONUT.\n\n    \"\"\"\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        try:\n            import torch  # noqa: F401\n        except ImportError:\n            raise ValueError(\"install pytorch to use the model\")\n        try:\n            from transformers import DonutProcessor, VisionEncoderDecoderModel\n        except ImportError:\n            raise ValueError(\"transformers is required for using DONUT model.\")\n        try:\n            import sentencepiece  # noqa: F401\n        except ImportError:\n            raise ValueError(\"sentencepiece is required for using DONUT model.\")\n        try:\n            from PIL import Image  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"PIL is required to read image files.\" \"Please run `pip install Pillow`\"\n            )\n\n        processor = DonutProcessor.from_pretrained(\n            \"naver-clova-ix/donut-base-finetuned-cord-v2\"\n        )\n        model = VisionEncoderDecoderModel.from_pretrained(\n            \"naver-clova-ix/donut-base-finetuned-cord-v2\"\n        )\n        return {\"processor\": processor, \"model\": model}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> str:\n        \"\"\"Parse file.\"\"\"\n        import torch\n        from PIL import Image\n\n        model = self.parser_config[\"model\"]\n        processor = self.parser_config[\"processor\"]\n\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        model.to(device)\n        # load document image\n        image = Image.open(file)\n        if image.mode != \"RGB\":\n            image = image.convert(\"RGB\")\n\n        # prepare decoder inputs\n        task_prompt = \"<s_cord-v2>\"\n        decoder_input_ids = processor.tokenizer(\n            task_prompt, add_special_tokens=False, return_tensors=\"pt\"\n        ).input_ids\n\n        pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n\n        outputs = model.generate(\n            pixel_values.to(device),\n            decoder_input_ids=decoder_input_ids.to(device),\n            max_length=model.decoder.config.max_position_embeddings,\n            early_stopping=True,\n            pad_token_id=processor.tokenizer.pad_token_id,\n            eos_token_id=processor.tokenizer.eos_token_id,\n            use_cache=True,\n            num_beams=1,\n            bad_words_ids=[[processor.tokenizer.unk_token_id]],\n            return_dict_in_generate=True,\n        )\n\n        sequence = processor.batch_decode(outputs.sequences)[0]\n        sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(\n            processor.tokenizer.pad_token, \"\"\n        )\n        # remove first task start token\n        sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()\n\n        return sequence\n", "doc_id": "618f6054d906179da2ba7ae65568378f60173c97", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/image_parser.py", "file_name": "image_parser.py"}, "__type__": "Document"}, "2785c7a340d0f872e5864e0c02e6ff6f150de538": {"text": "\"\"\"Markdown parser.\n\nContains parser for md files.\n\n\"\"\"\nimport re\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Tuple, Union, cast\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass MarkdownParser(BaseParser):\n    \"\"\"Markdown parser.\n\n    Extract text from markdown files.\n    Returns dictionary with keys as headers and values as the text between headers.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *args: Any,\n        remove_hyperlinks: bool = True,\n        remove_images: bool = True,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self._remove_hyperlinks = remove_hyperlinks\n        self._remove_images = remove_images\n\n    def markdown_to_tups(self, markdown_text: str) -> List[Tuple[Optional[str], str]]:\n        \"\"\"Convert a markdown file to a dictionary.\n\n        The keys are the headers and the values are the text under each header.\n\n        \"\"\"\n        markdown_tups: List[Tuple[Optional[str], str]] = []\n        lines = markdown_text.split(\"\\n\")\n\n        current_header = None\n        current_text = \"\"\n\n        for line in lines:\n            header_match = re.match(r\"^#+\\s\", line)\n            if header_match:\n                if current_header is not None:\n                    if current_text == \"\" or None:\n                        continue\n                    markdown_tups.append((current_header, current_text))\n\n                current_header = line\n                current_text = \"\"\n            else:\n                current_text += line + \"\\n\"\n        markdown_tups.append((current_header, current_text))\n\n        if current_header is not None:\n            # pass linting, assert keys are defined\n            markdown_tups = [\n                (re.sub(r\"#\", \"\", cast(str, key)).strip(), re.sub(r\"<.*?>\", \"\", value))\n                for key, value in markdown_tups\n            ]\n        else:\n            markdown_tups = [\n                (key, re.sub(\"\\n\", \"\", value)) for key, value in markdown_tups\n            ]\n\n        return markdown_tups\n\n    def remove_images(self, content: str) -> str:\n        \"\"\"Get a dictionary of a markdown file from its path.\"\"\"\n        pattern = r\"!{1}\\[\\[(.*)\\]\\]\"\n        content = re.sub(pattern, \"\", content)\n        return content\n\n    def remove_hyperlinks(self, content: str) -> str:\n        \"\"\"Get a dictionary of a markdown file from its path.\"\"\"\n        pattern = r\"\\[(.*?)\\]\\((.*?)\\)\"\n        content = re.sub(pattern, r\"\\1\", content)\n        return content\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Initialize the parser with the config.\"\"\"\n        return {}\n\n    def parse_tups(\n        self, filepath: Path, errors: str = \"ignore\"\n    ) -> List[Tuple[Optional[str], str]]:\n        \"\"\"Parse file into tuples.\"\"\"\n        with open(filepath, \"r\") as f:\n            content = f.read()\n        if self._remove_hyperlinks:\n            content = self.remove_hyperlinks(content)\n        if self._remove_images:\n            content = self.remove_images(content)\n        markdown_tups = self.markdown_to_tups(content)\n        return markdown_tups\n\n    def parse_file(\n        self, filepath: Path, errors: str = \"ignore\"\n    ) -> Union[str, List[str]]:\n        \"\"\"Parse file into string.\"\"\"\n        tups = self.parse_tups(filepath, errors=errors)\n        results = []\n        # TODO: don't include headers right now\n        for header, value in tups:\n            if header is None:\n                results.append(value)\n            else:\n                results.append(f\"\\n\\n{header}\\n{value}\")\n        return results\n", "doc_id": "2785c7a340d0f872e5864e0c02e6ff6f150de538", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/markdown_parser.py", "file_name": "markdown_parser.py"}, "__type__": "Document"}, "29f2fc552f2d6f0ec85e8fb06255667555ac9e94": {"text": "\"\"\"Mbox parser.\n\nContains simple parser for mbox files.\n\n\"\"\"\nfrom pathlib import Path\nfrom typing import Any, Dict, List\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass MboxParser(BaseParser):\n    \"\"\"Mbox parser.\n\n    Extract messages from mailbox files.\n    Returns string including date, subject, sender, receiver and\n    content for each message.\n\n    \"\"\"\n\n    DEFAULT_MESSAGE_FORMAT: str = (\n        \"Date: {_date}\\n\"\n        \"From: {_from}\\n\"\n        \"To: {_to}\\n\"\n        \"Subject: {_subject}\\n\"\n        \"Content: {_content}\"\n    )\n\n    def __init__(\n        self,\n        *args: Any,\n        max_count: int = 0,\n        message_format: str = DEFAULT_MESSAGE_FORMAT,\n        **kwargs: Any\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.max_count = max_count\n        self.message_format = message_format\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Initialize parser.\"\"\"\n        try:\n            from bs4 import BeautifulSoup  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`beautifulsoup4` package not found,\"\n                \"please run `pip install beautifulsoup4`\"\n            )\n        return {}\n\n    def parse_file(self, filepath: Path, errors: str = \"ignore\") -> List[str]:\n        \"\"\"Parse file into string.\"\"\"\n        # Import required libraries\n        import mailbox\n        from email.parser import BytesParser\n        from email.policy import default\n\n        from bs4 import BeautifulSoup\n\n        i = 0\n        results: List[str] = []\n        # Load file using mailbox\n        bytes_parser = BytesParser(policy=default).parse\n        mbox = mailbox.mbox(filepath, factory=bytes_parser)  # type: ignore\n\n        # Iterate through all messages\n        for _, _msg in enumerate(mbox):\n            msg: mailbox.mboxMessage = _msg\n            # Parse multipart messages\n            if msg.is_multipart():\n                for part in msg.walk():\n                    ctype = part.get_content_type()\n                    cdispo = str(part.get(\"Content-Disposition\"))\n                    if ctype == \"text/plain\" and \"attachment\" not in cdispo:\n                        content = part.get_payload(decode=True)  # decode\n                        break\n            # Get plain message payload for non-multipart messages\n            else:\n                content = msg.get_payload(decode=True)\n\n            # Parse message HTML content and remove unneeded whitespace\n            soup = BeautifulSoup(content)\n            stripped_content = \" \".join(soup.get_text().split())\n            # Format message to include date, sender, receiver and subject\n            msg_string = self.message_format.format(\n                _date=msg[\"date\"],\n                _from=msg[\"from\"],\n                _to=msg[\"to\"],\n                _subject=msg[\"subject\"],\n                _content=stripped_content,\n            )\n            # Add message string to results\n            results.append(msg_string)\n            # Increment counter and return if max count is met\n            i += 1\n            if self.max_count > 0 and i >= self.max_count:\n                break\n        return results\n", "doc_id": "29f2fc552f2d6f0ec85e8fb06255667555ac9e94", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/mbox_parser.py", "file_name": "mbox_parser.py"}, "__type__": "Document"}, "bf5faad5fc2e97987c1786f59411af5b69411313": {"text": "\"\"\"Slides parser.\n\nContains parsers for .pptx files.\n\n\"\"\"\n\nimport os\nfrom pathlib import Path\nfrom typing import Dict\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass PptxParser(BaseParser):\n    \"\"\"Powerpoint parser.\n\n    Extract text, caption images, and specify slides.\n\n    \"\"\"\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        try:\n            from pptx import Presentation  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"The package `python-pptx` is required to read Powerpoint files.\"\n            )\n        try:\n            import torch  # noqa: F401\n        except ImportError:\n            raise ValueError(\"The package `pytorch` is required to caption images.\")\n        try:\n            from transformers import (\n                AutoTokenizer,\n                VisionEncoderDecoderModel,\n                ViTFeatureExtractor,\n            )\n        except ImportError:\n            raise ValueError(\n                \"The package `transformers` is required to caption images.\"\n            )\n        try:\n            from PIL import Image  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"PIL is required to read image files.\" \"Please run `pip install Pillow`\"\n            )\n\n        model = VisionEncoderDecoderModel.from_pretrained(\n            \"nlpconnect/vit-gpt2-image-captioning\"\n        )\n        feature_extractor = ViTFeatureExtractor.from_pretrained(\n            \"nlpconnect/vit-gpt2-image-captioning\"\n        )\n        tokenizer = AutoTokenizer.from_pretrained(\n            \"nlpconnect/vit-gpt2-image-captioning\"\n        )\n\n        return {\n            \"feature_extractor\": feature_extractor,\n            \"model\": model,\n            \"tokenizer\": tokenizer,\n        }\n\n    def caption_image(self, tmp_image_file: str) -> str:\n        \"\"\"Generate text caption of image.\"\"\"\n        import torch\n        from PIL import Image\n\n        model = self.parser_config[\"model\"]\n        feature_extractor = self.parser_config[\"feature_extractor\"]\n        tokenizer = self.parser_config[\"tokenizer\"]\n\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        model.to(device)\n\n        max_length = 16\n        num_beams = 4\n        gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n\n        i_image = Image.open(tmp_image_file)\n        if i_image.mode != \"RGB\":\n            i_image = i_image.convert(mode=\"RGB\")\n\n        pixel_values = feature_extractor(\n            images=[i_image], return_tensors=\"pt\"\n        ).pixel_values\n        pixel_values = pixel_values.to(device)\n\n        output_ids = model.generate(pixel_values, **gen_kwargs)\n\n        preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n        return preds[0].strip()\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> str:\n        \"\"\"Parse file.\"\"\"\n        from pptx import Presentation\n\n        presentation = Presentation(file)\n        result = \"\"\n        for i, slide in enumerate(presentation.slides):\n            result += f\"\\n\\nSlide #{i}: \\n\"\n            for shape in slide.shapes:\n                if hasattr(shape, \"image\"):\n                    image = shape.image\n                    # get image \"file\" contents\n                    image_bytes = image.blob\n                    # temporarily save the image to feed into model\n                    image_filename = f\"tmp_image.{image.ext}\"\n                    with open(image_filename, \"wb\") as f:\n                        f.write(image_bytes)\n                    result += f\"\\n Image: {self.caption_image(image_filename)}\\n\\n\"\n\n                    os.remove(image_filename)\n                if hasattr(shape, \"text\"):\n                    result += f\"{shape.text}\\n\"\n\n        return result\n", "doc_id": "bf5faad5fc2e97987c1786f59411af5b69411313", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/slides_parser.py", "file_name": "slides_parser.py"}, "__type__": "Document"}, "d730452fdec35104b8b57cd3b6f80a982fb35957": {"text": "\"\"\"Tabular parser.\n\nContains parsers for tabular data files.\n\n\"\"\"\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Union\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass CSVParser(BaseParser):\n    \"\"\"CSV parser.\n\n    Args:\n        concat_rows (bool): whether to concatenate all rows into one document.\n            If set to False, a Document will be created for each row.\n            True by default.\n\n    \"\"\"\n\n    def __init__(self, *args: Any, concat_rows: bool = True, **kwargs: Any) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self._concat_rows = concat_rows\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        return {}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> Union[str, List[str]]:\n        \"\"\"Parse file.\n\n        Returns:\n            Union[str, List[str]]: a string or a List of strings.\n\n        \"\"\"\n        try:\n            import csv\n        except ImportError:\n            raise ValueError(\"csv module is required to read CSV files.\")\n        text_list = []\n        with open(file, \"r\") as fp:\n            csv_reader = csv.reader(fp)\n            for row in csv_reader:\n                text_list.append(\", \".join(row))\n        if self._concat_rows:\n            return \"\\n\".join(text_list)\n        else:\n            return text_list\n\n\nclass PandasCSVParser(BaseParser):\n    r\"\"\"Pandas-based CSV parser.\n\n    Parses CSVs using the separator detection from Pandas `read_csv`function.\n    If special parameters are required, use the `pandas_config` dict.\n\n    Args:\n        concat_rows (bool): whether to concatenate all rows into one document.\n            If set to False, a Document will be created for each row.\n            True by default.\n\n        col_joiner (str): Separator to use for joining cols per row.\n            Set to \", \" by default.\n\n        row_joiner (str): Separator to use for joining each row.\n            Only used when `concat_rows=True`.\n            Set to \"\\n\" by default.\n\n        pandas_config (dict): Options for the `pandas.read_csv` function call.\n            Refer to https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n            for more information.\n            Set to empty dict by default, this means pandas will try to figure\n            out the separators, table head, etc. on its own.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *args: Any,\n        concat_rows: bool = True,\n        col_joiner: str = \", \",\n        row_joiner: str = \"\\n\",\n        pandas_config: dict = {},\n        **kwargs: Any\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self._concat_rows = concat_rows\n        self._col_joiner = col_joiner\n        self._row_joiner = row_joiner\n        self._pandas_config = pandas_config\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        return {}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> Union[str, List[str]]:\n        \"\"\"Parse file.\"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ValueError(\"pandas module is required to read CSV files.\")\n\n        df = pd.read_csv(file, **self._pandas_config)\n\n        text_list = df.apply(\n            lambda row: (self._col_joiner).join(row.astype(str).tolist()), axis=1\n        ).tolist()\n\n        if self._concat_rows:\n            return (self._row_joiner).join(text_list)\n        else:\n            return text_list\n", "doc_id": "d730452fdec35104b8b57cd3b6f80a982fb35957", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/tabular_parser.py", "file_name": "tabular_parser.py"}, "__type__": "Document"}, "875d5164e96b8249b7c1672175e199266f13d110": {"text": "\"\"\"Video audio parser.\n\nContains parsers for mp3, mp4 files.\n\n\"\"\"\nfrom pathlib import Path\nfrom typing import Any, Dict, cast\n\nfrom gpt_index.readers.file.base_parser import BaseParser\n\n\nclass VideoAudioParser(BaseParser):\n    \"\"\"Video audio parser.\n\n    Extract text from transcript of video/audio files.\n\n    \"\"\"\n\n    def __init__(self, *args: Any, model_version: str = \"base\", **kwargs: Any) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self._model_version = model_version\n\n    def _init_parser(self) -> Dict:\n        \"\"\"Init parser.\"\"\"\n        try:\n            import whisper\n        except ImportError:\n            raise ValueError(\n                \"Please install OpenAI whisper model \"\n                \"'pip install git+https://github.com/openai/whisper.git' \"\n                \"to use the model\"\n            )\n\n        model = whisper.load_model(self._model_version)\n\n        return {\"model\": model}\n\n    def parse_file(self, file: Path, errors: str = \"ignore\") -> str:\n        \"\"\"Parse file.\"\"\"\n        import whisper\n\n        if file.name.endswith(\"mp4\"):\n            try:\n                from pydub import AudioSegment  # noqa: F401\n            except ImportError:\n                raise ValueError(\"Please install pydub 'pip install pydub' \")\n            # open file\n            video = AudioSegment.from_file(file, format=\"mp4\")\n\n            # Extract audio from video\n            audio = video.split_to_mono()[0]\n\n            file_str = str(file)[:-4] + \".mp3\"\n            # export file\n            audio.export(file_str, format=\"mp3\")\n\n        model = cast(whisper.Whisper, self.parser_config[\"model\"])\n        result = model.transcribe(str(file))\n\n        transcript = result[\"text\"]\n\n        return transcript\n", "doc_id": "875d5164e96b8249b7c1672175e199266f13d110", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/file/video_audio.py", "file_name": "video_audio.py"}, "__type__": "Document"}, "cde89efb045b4304cc4982ff8aa795ac8c522a23": {"text": "\"\"\"\nGithub API client for the GPT-Index library.\n\nThis module contains the Github API client for the GPT-Index library.\nIt is used by the Github readers to retrieve the data from Github.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional\n\nfrom dataclasses_json import DataClassJsonMixin\n\n\n@dataclass\nclass GitTreeResponseModel(DataClassJsonMixin):\n    \"\"\"\n    Dataclass for the response from the Github API's getTree endpoint.\n\n    Attributes:\n        - sha (str): SHA1 checksum ID of the tree.\n        - url (str): URL for the tree.\n        - tree (List[GitTreeObject]): List of objects in the tree.\n        - truncated (bool): Whether the tree is truncated.\n\n    Examples:\n        >>> tree = client.get_tree(\"owner\", \"repo\", \"branch\")\n        >>> tree.sha\n    \"\"\"\n\n    @dataclass\n    class GitTreeObject(DataClassJsonMixin):\n        \"\"\"\n        Dataclass for the objects in the tree.\n\n        Attributes:\n            - path (str): Path to the object.\n            - mode (str): Mode of the object.\n            - type (str): Type of the object.\n            - sha (str): SHA1 checksum ID of the object.\n            - url (str): URL for the object.\n            - size (Optional[int]): Size of the object (only for blobs).\n        \"\"\"\n\n        path: str\n        mode: str\n        type: str\n        sha: str\n        url: str\n        size: Optional[int] = None\n\n    sha: str\n    url: str\n    tree: List[GitTreeObject]\n    truncated: bool\n\n\n@dataclass\nclass GitBlobResponseModel(DataClassJsonMixin):\n    \"\"\"\n    Dataclass for the response from the Github API's getBlob endpoint.\n\n    Attributes:\n        - content (str): Content of the blob.\n        - encoding (str): Encoding of the blob.\n        - url (str): URL for the blob.\n        - sha (str): SHA1 checksum ID of the blob.\n        - size (int): Size of the blob.\n        - node_id (str): Node ID of the blob.\n    \"\"\"\n\n    content: str\n    encoding: str\n    url: str\n    sha: str\n    size: int\n    node_id: str\n\n\n@dataclass\nclass GitCommitResponseModel(DataClassJsonMixin):\n    \"\"\"\n    Dataclass for the response from the Github API's getCommit endpoint.\n\n    Attributes:\n        - tree (Tree): Tree object for the commit.\n    \"\"\"\n\n    @dataclass\n    class Commit(DataClassJsonMixin):\n        \"\"\"Dataclass for the commit object in the commit. (commit.commit).\"\"\"\n\n        @dataclass\n        class Tree(DataClassJsonMixin):\n            \"\"\"\n            Dataclass for the tree object in the commit.\n\n            Attributes:\n                - sha (str): SHA for the commit\n            \"\"\"\n\n            sha: str\n\n        tree: Tree\n\n    commit: Commit\n\n\n@dataclass\nclass GitBranchResponseModel(DataClassJsonMixin):\n    \"\"\"\n    Dataclass for the response from the Github API's getBranch endpoint.\n\n    Attributes:\n        - commit (Commit): Commit object for the branch.\n    \"\"\"\n\n    @dataclass\n    class Commit(DataClassJsonMixin):\n        \"\"\"Dataclass for the commit object in the branch. (commit.commit).\"\"\"\n\n        @dataclass\n        class Commit(DataClassJsonMixin):\n            \"\"\"Dataclass for the commit object in the commit. (commit.commit.tree).\"\"\"\n\n            @dataclass\n            class Tree(DataClassJsonMixin):\n                \"\"\"\n                Dataclass for the tree object in the commit.\n\n                Usage: commit.commit.tree.sha\n                \"\"\"\n\n                sha: str\n\n            tree: Tree\n\n        commit: Commit\n\n    commit: Commit\n\n\nclass GithubClient:\n    \"\"\"\n    An asynchronous client for interacting with the Github API.\n\n    This client is used for making API requests to Github.\n    It provides methods for accessing the Github API endpoints.\n    The client requires a Github token for authentication,\n    which can be passed as an argument or set as an environment variable.\n    If no Github token is provided, the client will raise a ValueError.\n\n    Examples:\n        >>> client = GithubClient(\"my_github_token\")\n        >>> branch_info = client.get_branch(\"owner\", \"repo\", \"branch\")\n    \"\"\"\n\n    DEFAULT_BASE_URL = \"https://api.github.com\"\n    DEFAULT_API_VERSION = \"2022-11-28\"\n\n    def __init__(\n        self,\n        github_token: Optional[str] = None,\n        base_url: str = DEFAULT_BASE_URL,\n        api_version: str = DEFAULT_API_VERSION,\n        verbose: bool = False,\n    ) -> None:\n        \"\"\"\n        Initialize the GithubClient.\n\n        Args:\n            - github_token (str): Github token for authentication.\n                If not provided, the client will try to get it from\n                the GITHUB_TOKEN environment variable.\n            - base_url (str): Base URL for the Github API\n                (defaults to \"https://api.github.com\").\n            - api_version (str): Github API version (defaults to \"2022-11-28\").\n\n        Raises:\n            ValueError: If no Github token is provided.\n        \"\"\"\n        if github_token is None:\n            github_token = os.getenv(\"GITHUB_TOKEN\")\n            if github_token is None:\n                raise ValueError(\n                    \"Please provide a Github token. \"\n                    + \"You can do so by passing it as an argument to the GithubReader,\"\n                    + \"or by setting the GITHUB_TOKEN environment variable.\"\n                )\n\n        self._base_url = base_url\n        self._api_version = api_version\n        self._verbose = verbose\n\n        self._endpoints = {\n            \"getTree\": \"/repos/{owner}/{repo}/git/trees/{tree_sha}\",\n            \"getBranch\": \"/repos/{owner}/{repo}/branches/{branch}\",\n            \"getBlob\": \"/repos/{owner}/{repo}/git/blobs/{file_sha}\",\n            \"getCommit\": \"/repos/{owner}/{repo}/commits/{commit_sha}\",\n        }\n\n        self._headers = {\n            \"Accept\": \"application/vnd.github+json\",\n            \"Authorization\": f\"Bearer {github_token}\",\n            \"X-GitHub-Api-Version\": f\"{self._api_version}\",\n        }\n\n    def get_all_endpoints(self) -> Dict[str, str]:\n        \"\"\"Get all available endpoints.\"\"\"\n        return {**self._endpoints}\n\n    async def request(\n        self,\n        endpoint: str,\n        method: str,\n        headers: Dict[str, Any] = {},\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"\n        Make an API request to the Github API.\n\n        This method is used for making API requests to the Github API.\n        It is used internally by the other methods in the client.\n\n        Args:\n            - `endpoint (str)`: Name of the endpoint to make the request to.\n            - `method (str)`: HTTP method to use for the request.\n            - `headers (dict)`: HTTP headers to include in the request.\n            - `**kwargs`: Keyword arguments to pass to the endpoint URL.\n\n        Returns:\n            - `response (httpx.Response)`: Response from the API request.\n\n        Raises:\n            - ImportError: If the `httpx` library is not installed.\n            - httpx.HTTPError: If the API request fails.\n\n        Examples:\n            >>> response = client.request(\"getTree\", \"GET\",\n                                owner=\"owner\", repo=\"repo\",\n                                tree_sha=\"tree_sha\")\n        \"\"\"\n        try:\n            import httpx\n        except ImportError:\n            raise ImportError(\n                \"Please install httpx to use the GithubRepositoryReader. \"\n                \"You can do so by running `pip install httpx`.\"\n            )\n\n        _headers = {**self._headers, **headers}\n\n        _client: httpx.AsyncClient\n        async with httpx.AsyncClient(\n            headers=_headers, base_url=self._base_url\n        ) as _client:\n            try:\n                response = await _client.request(\n                    method, url=self._endpoints[endpoint].format(**kwargs)\n                )\n            except httpx.HTTPError as excp:\n                print(f\"HTTP Exception for {excp.request.url} - {excp}\")\n                raise excp\n            return response\n\n    async def get_branch(\n        self, owner: str, repo: str, branch: str\n    ) -> GitBranchResponseModel:\n        \"\"\"\n        Get information about a branch. (Github API endpoint: getBranch).\n\n        Args:\n            - `owner (str)`: Owner of the repository.\n            - `repo (str)`: Name of the repository.\n            - `branch (str)`: Name of the branch.\n\n        Returns:\n            - `branch_info (GitBranchResponseModel)`: Information about the branch.\n\n        Examples:\n            >>> branch_info = client.get_branch(\"owner\", \"repo\", \"branch\")\n        \"\"\"\n        return GitBranchResponseModel.from_json(\n            (\n                await self.request(\n                    \"getBranch\", \"GET\", owner=owner, repo=repo, branch=branch\n                )\n            ).text\n        )\n\n    async def get_tree(\n        self, owner: str, repo: str, tree_sha: str\n    ) -> GitTreeResponseModel:\n        \"\"\"\n        Get information about a tree. (Github API endpoint: getTree).\n\n        Args:\n            - `owner (str)`: Owner of the repository.\n            - `repo (str)`: Name of the repository.\n            - `tree_sha (str)`: SHA of the tree.\n\n        Returns:\n            - `tree_info (GitTreeResponseModel)`: Information about the tree.\n\n        Examples:\n            >>> tree_info = client.get_tree(\"owner\", \"repo\", \"tree_sha\")\n        \"\"\"\n        return GitTreeResponseModel.from_json(\n            (\n                await self.request(\n                    \"getTree\", \"GET\", owner=owner, repo=repo, tree_sha=tree_sha\n                )\n            ).text\n        )\n\n    async def get_blob(\n        self, owner: str, repo: str, file_sha: str\n    ) -> GitBlobResponseModel:\n        \"\"\"\n        Get information about a blob. (Github API endpoint: getBlob).\n\n        Args:\n            - `owner (str)`: Owner of the repository.\n            - `repo (str)`: Name of the repository.\n            - `file_sha (str)`: SHA of the file.\n\n        Returns:\n            - `blob_info (GitBlobResponseModel)`: Information about the blob.\n\n        Examples:\n            >>> blob_info = client.get_blob(\"owner\", \"repo\", \"file_sha\")\n        \"\"\"\n        return GitBlobResponseModel.from_json(\n            (\n                await self.request(\n                    \"getBlob\", \"GET\", owner=owner, repo=repo, file_sha=file_sha\n                )\n            ).text\n        )\n\n    async def get_commit(\n        self, owner: str, repo: str, commit_sha: str\n    ) -> GitCommitResponseModel:\n        \"\"\"\n        Get information about a commit. (Github API endpoint: getCommit).\n\n        Args:\n            - `owner (str)`: Owner of the repository.\n            - `repo (str)`: Name of the repository.\n            - `commit_sha (str)`: SHA of the commit.\n\n        Returns:\n            - `commit_info (GitCommitResponseModel)`: Information about the commit.\n\n        Examples:\n            >>> commit_info = client.get_commit(\"owner\", \"repo\", \"commit_sha\")\n        \"\"\"\n        return GitCommitResponseModel.from_json(\n            (\n                await self.request(\n                    \"getCommit\", \"GET\", owner=owner, repo=repo, commit_sha=commit_sha\n                )\n            ).text\n        )\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    async def main() -> None:\n        \"\"\"Test the GithubClient.\"\"\"\n        client = GithubClient()\n        response = await client.get_tree(\n            owner=\"ahmetkca\", repo=\"CommitAI\", tree_sha=\"with-body\"\n        )\n\n        for obj in response.tree:\n            if obj.type == \"blob\":\n                print(obj.path)\n                print(obj.sha)\n                blob_response = await client.get_blob(\n                    owner=\"ahmetkca\", repo=\"CommitAI\", file_sha=obj.sha\n                )\n                print(blob_response.content)\n\n    asyncio.run(main())\n", "doc_id": "cde89efb045b4304cc4982ff8aa795ac8c522a23", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_api_client.py", "file_name": "github_api_client.py"}, "__type__": "Document"}, "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a": {"text": "\"\"\"\nGithub repository reader.\n\nRetrieves the contents of a Github repository and returns a list of documents.\nThe documents are either the contents of the files in the repository or\nthe text extracted from the files using the parser.\n\"\"\"\n\nimport asyncio\nimport base64\nimport binascii\nimport logging\nimport os\nimport pathlib\nimport tempfile\nfrom typing import Any, Callable, List, Optional, Tuple\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.file.base import DEFAULT_FILE_EXTRACTOR\nfrom gpt_index.readers.github_readers.github_api_client import (\n    GitBranchResponseModel,\n    GitCommitResponseModel,\n    GithubClient,\n    GitTreeResponseModel,\n)\nfrom gpt_index.readers.github_readers.utils import (\n    BufferedGitBlobDataIterator,\n    get_file_extension,\n    print_if_verbose,\n)\nfrom gpt_index.readers.schema.base import Document\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass GithubRepositoryReader(BaseReader):\n    \"\"\"\n    Github repository reader.\n\n    Retrieves the contents of a Github repository and returns a list of documents.\n    The documents are either the contents of the files in the repository or the text\n    extracted from the files using the parser.\n\n    Examples:\n        >>> reader = GithubRepositoryReader(\"owner\", \"repo\")\n        >>> branch_documents = reader.load_data(branch=\"branch\")\n        >>> commit_documents = reader.load_data(commit_sha=\"commit_sha\")\n\n    \"\"\"\n\n    def __init__(\n        self,\n        owner: str,\n        repo: str,\n        use_parser: bool = True,\n        verbose: bool = False,\n        github_token: Optional[str] = None,\n        concurrent_requests: int = 5,\n        ignore_file_extensions: Optional[List[str]] = None,\n        ignore_directories: Optional[List[str]] = None,\n    ):\n        \"\"\"\n        Initialize params.\n\n        Args:\n            - owner (str): Owner of the repository.\n            - repo (str): Name of the repository.\n            - use_parser (bool): Whether to use the parser to extract\n                the text from the files.\n            - verbose (bool): Whether to print verbose messages.\n            - github_token (str): Github token. If not provided,\n                it will be read from the GITHUB_TOKEN environment variable.\n            - concurrent_requests (int): Number of concurrent requests to\n                make to the Github API.\n            - ignore_file_extensions (List[str]): List of file extensions to ignore.\n                i.e. ['.png', '.jpg']\n            - ignore_directories (List[str]): List of directories to ignore.\n                i.e. ['node_modules', 'dist']\n\n        Raises:\n            - `ValueError`: If the github_token is not provided and\n                the GITHUB_TOKEN environment variable is not set.\n        \"\"\"\n        super().__init__()\n        if github_token is None:\n            github_token = os.getenv(\"GITHUB_TOKEN\")\n            if github_token is None:\n                raise ValueError(\n                    \"Please provide a Github token. \"\n                    \"You can do so by passing it as an argument or\"\n                    + \"by setting the GITHUB_TOKEN environment variable.\"\n                )\n\n        self._owner = owner\n        self._repo = repo\n        self._use_parser = use_parser\n        self._verbose = verbose\n        self._concurrent_requests = concurrent_requests\n        self._ignore_file_extensions = ignore_file_extensions\n        self._ignore_directories = ignore_directories\n\n        # Set up the event loop\n        try:\n            self._loop = asyncio.get_running_loop()\n        except RuntimeError:\n            # If there is no running loop, create a new one\n            self._loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(self._loop)\n\n        self._client = GithubClient(github_token)\n\n    def _load_data_from_commit(self, commit_sha: str) -> List[Document]:\n        \"\"\"\n        Load data from a commit.\n\n        Loads github repository data from a specific commit sha.\n\n        :param `commit`: commit sha\n\n        :return: list of documents\n        \"\"\"\n        commit_response: GitCommitResponseModel = self._loop.run_until_complete(\n            self._client.get_commit(self._owner, self._repo, commit_sha)\n        )\n\n        tree_sha = commit_response.commit.tree.sha\n        blobs_and_paths = self._loop.run_until_complete(self._recurse_tree(tree_sha))\n\n        print_if_verbose(self._verbose, f\"got {len(blobs_and_paths)} blobs\")\n\n        return self._loop.run_until_complete(\n            self._generate_documents(blobs_and_paths=blobs_and_paths)\n        )\n\n    def _load_data_from_branch(self, branch: str) -> List[Document]:\n        \"\"\"\n        Load data from a branch.\n\n        Loads github repository data from a specific branch.\n\n        :param `branch`: branch name\n\n        :return: list of documents\n        \"\"\"\n        branch_data: GitBranchResponseModel = self._loop.run_until_complete(\n            self._client.get_branch(self._owner, self._repo, branch)\n        )\n\n        tree_sha = branch_data.commit.commit.tree.sha\n        blobs_and_paths = self._loop.run_until_complete(self._recurse_tree(tree_sha))\n\n        print_if_verbose(self._verbose, f\"got {len(blobs_and_paths)} blobs\")\n\n        return self._loop.run_until_complete(\n            self._generate_documents(blobs_and_paths=blobs_and_paths)\n        )\n\n    def load_data(\n        self,\n        commit_sha: Optional[str] = None,\n        branch: Optional[str] = None,\n    ) -> List[Document]:\n        \"\"\"\n        Load data from a commit or a branch.\n\n        Loads github repository data from a specific commit sha or a branch.\n\n        :param `commit`: commit sha\n        :param `branch`: branch name\n\n        :return: list of documents\n        \"\"\"\n        if commit_sha is not None and branch is not None:\n            raise ValueError(\"You can only specify one of commit or branch.\")\n\n        if commit_sha is None and branch is None:\n            raise ValueError(\"You must specify one of commit or branch.\")\n\n        if commit_sha is not None:\n            return self._load_data_from_commit(commit_sha)\n\n        if branch is not None:\n            return self._load_data_from_branch(branch)\n\n        raise ValueError(\"You must specify one of commit or branch.\")\n\n    async def _recurse_tree(\n        self, tree_sha: str, current_path: str = \"\", current_depth: int = 0\n    ) -> Any:\n        \"\"\"\n        Recursively get all blob tree objects in a tree.\n\n        And construct their full path relative to the root of the repository.\n        (see GitTreeResponseModel.GitTreeObject in\n            github_api_client.py for more information)\n\n        :param `tree_sha`: sha of the tree to recurse\n        :param `current_path`: current path of the tree\n        :param `current_depth`: current depth of the tree\n        :return: list of tuples of\n            (tree object, file's full path realtive to the root of the repo)\n        \"\"\"\n        blobs_and_full_paths: List[Tuple[GitTreeResponseModel.GitTreeObject, str]] = []\n        print_if_verbose(\n            self._verbose, \"\\t\" * current_depth + f\"current path: {current_path}\"\n        )\n\n        tree_data: GitTreeResponseModel = await self._client.get_tree(\n            self._owner, self._repo, tree_sha\n        )\n        print_if_verbose(\n            self._verbose, \"\\t\" * current_depth + f\"processing tree {tree_sha}\"\n        )\n        for tree_obj in tree_data.tree:\n            file_path = os.path.join(current_path, tree_obj.path)\n            if tree_obj.type == \"tree\":\n                print_if_verbose(\n                    self._verbose,\n                    \"\\t\" * current_depth + f\"recursing into {tree_obj.path}\",\n                )\n                if self._ignore_directories is not None:\n                    if file_path in self._ignore_directories:\n                        print_if_verbose(\n                            self._verbose,\n                            \"\\t\" * current_depth\n                            + f\"ignoring tree {tree_obj.path} due to directory\",\n                        )\n                        continue\n\n                blobs_and_full_paths.extend(\n                    await self._recurse_tree(tree_obj.sha, file_path, current_depth + 1)\n                )\n            elif tree_obj.type == \"blob\":\n                print_if_verbose(\n                    self._verbose, \"\\t\" * current_depth + f\"found blob {tree_obj.path}\"\n                )\n                if self._ignore_file_extensions is not None:\n                    if get_file_extension(file_path) in self._ignore_file_extensions:\n                        print_if_verbose(\n                            self._verbose,\n                            \"\\t\" * current_depth\n                            + f\"ignoring blob {tree_obj.path} due to file extension\",\n                        )\n                        continue\n                blobs_and_full_paths.append((tree_obj, file_path))\n        return blobs_and_full_paths\n\n    async def _generate_documents(\n        self, blobs_and_paths: List[Tuple[GitTreeResponseModel.GitTreeObject, str]]\n    ) -> List[Document]:\n        \"\"\"\n        Generate documents from a list of blobs and their full paths.\n\n        :param `blobs_and_paths`: list of tuples of\n            (tree object, file's full path in the repo realtive to the root of the repo)\n        :return: list of documents\n        \"\"\"\n        buffered_iterator = BufferedGitBlobDataIterator(\n            blobs_and_paths=blobs_and_paths,\n            github_client=self._client,\n            owner=self._owner,\n            repo=self._repo,\n            loop=self._loop,\n            buffer_size=self._concurrent_requests,  # TODO: make this configurable\n            verbose=self._verbose,\n        )\n\n        documents = []\n        async for blob_data, full_path in buffered_iterator:\n            print_if_verbose(self._verbose, f\"generating document for {full_path}\")\n            assert (\n                blob_data.encoding == \"base64\"\n            ), f\"blob encoding {blob_data.encoding} not supported\"\n            decoded_bytes = None\n            try:\n                decoded_bytes = base64.b64decode(blob_data.content)\n                del blob_data.content\n            except binascii.Error:\n                print_if_verbose(\n                    self._verbose, f\"could not decode {full_path} as base64\"\n                )\n                continue\n\n            if self._use_parser:\n                document = self._parse_supported_file(\n                    file_path=full_path,\n                    file_content=decoded_bytes,\n                    tree_sha=blob_data.sha,\n                    tree_path=full_path,\n                )\n                if document is not None:\n                    documents.append(document)\n                else:\n                    continue\n\n            try:\n                if decoded_bytes is None:\n                    raise ValueError(\"decoded_bytes is None\")\n                decoded_text = decoded_bytes.decode(\"utf-8\")\n            except UnicodeDecodeError:\n                print_if_verbose(\n                    self._verbose, f\"could not decode {full_path} as utf-8\"\n                )\n                continue\n            print_if_verbose(\n                self._verbose,\n                f\"got {len(decoded_text)} characters\"\n                + f\"- adding to documents - {full_path}\",\n            )\n            document = Document(\n                text=decoded_text,\n                doc_id=blob_data.sha,\n                extra_info={\n                    \"file_path\": full_path,\n                    \"file_name\": full_path.split(\"/\")[-1],\n                },\n            )\n            documents.append(document)\n        return documents\n\n    def _parse_supported_file(\n        self, file_path: str, file_content: bytes, tree_sha: str, tree_path: str\n    ) -> Optional[Document]:\n        \"\"\"\n        Parse a file if it is supported by a parser.\n\n        :param `file_path`: path of the file in the repo\n        :param `file_content`: content of the file\n        :return: Document if the file is supported by a parser, None otherwise\n        \"\"\"\n        file_extension = get_file_extension(file_path)\n        if (parser := DEFAULT_FILE_EXTRACTOR.get(file_extension)) is not None:\n            parser.init_parser()\n            print_if_verbose(\n                self._verbose,\n                f\"parsing {file_path}\"\n                + f\"as {file_extension} with \"\n                + f\"{parser.__class__.__name__}\",\n            )\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                with tempfile.NamedTemporaryFile(\n                    dir=tmpdirname,\n                    suffix=f\".{file_extension}\",\n                    mode=\"w+b\",\n                    delete=False,\n                ) as tmpfile:\n                    print_if_verbose(\n                        self._verbose,\n                        \"created a temporary file\"\n                        + f\"{tmpfile.name} for parsing {file_path}\",\n                    )\n                    tmpfile.write(file_content)\n                    tmpfile.flush()\n                    tmpfile.close()\n                    try:\n                        parsed_file = parser.parse_file(pathlib.Path(tmpfile.name))\n                        parsed_file = \"\\n\\n\".join(parsed_file)\n                    except Exception as e:\n                        print_if_verbose(\n                            self._verbose, f\"error while parsing {file_path}\"\n                        )\n                        logger.error(\n                            \"Error while parsing \"\n                            + f\"{file_path} with \"\n                            + f\"{parser.__class__.__name__}:\\n{e}\"\n                        )\n                        parsed_file = None\n                    finally:\n                        os.remove(tmpfile.name)\n                    if parsed_file is None:\n                        return None\n                    return Document(\n                        text=parsed_file,\n                        doc_id=tree_sha,\n                        extra_info={\n                            \"file_path\": file_path,\n                            \"file_name\": tree_path,\n                        },\n                    )\n        return None\n\n\nif __name__ == \"__main__\":\n    import time\n\n    def timeit(func: Callable) -> Callable:\n        \"\"\"Time a function.\"\"\"\n\n        def wrapper(*args: Any, **kwargs: Any) -> None:\n            \"\"\"Callcuate time taken to run a function.\"\"\"\n            start = time.time()\n            func(*args, **kwargs)\n            end = time.time()\n            print(f\"Time taken: {end - start} seconds for {func.__name__}\")\n\n        return wrapper\n\n    reader1 = GithubRepositoryReader(\n        github_token=os.environ[\"GITHUB_TOKEN\"],\n        owner=\"jerryjliu\",\n        repo=\"gpt_index\",\n        use_parser=False,\n        verbose=True,\n        ignore_directories=[\"examples\"],\n    )\n\n    @timeit\n    def load_data_from_commit() -> None:\n        \"\"\"Load data from a commit.\"\"\"\n        documents = reader1.load_data(\n            commit_sha=\"22e198b3b166b5facd2843d6a62ac0db07894a13\"\n        )\n        for document in documents:\n            print(document.extra_info)\n\n    @timeit\n    def load_data_from_branch() -> None:\n        \"\"\"Load data from a branch.\"\"\"\n        documents = reader1.load_data(branch=\"main\")\n        for document in documents:\n            print(document.extra_info)\n\n    input(\"Press enter to load github repository from branch name...\")\n\n    load_data_from_branch()\n\n    input(\"Press enter to load github repository from commit sha...\")\n\n    load_data_from_commit()\n", "doc_id": "ec1c0caaaba1db2a4017ee3087cfa942ade43a9a", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/github_repository_reader.py", "file_name": "github_repository_reader.py"}, "__type__": "Document"}, "98449e52eda3d6ee33e51429362e457d29f7893e": {"text": "\"\"\"\nGithub readers utils.\n\nThis module contains utility functions for the Github readers.\n\"\"\"\nimport asyncio\nimport os\nimport time\nfrom abc import ABC, abstractmethod\nfrom typing import List, Tuple\n\nfrom gpt_index.readers.github_readers.github_api_client import (\n    GitBlobResponseModel,\n    GithubClient,\n    GitTreeResponseModel,\n)\n\n\ndef print_if_verbose(verbose: bool, message: str) -> None:\n    \"\"\"Log message if verbose is True.\"\"\"\n    if verbose:\n        print(message)\n\n\ndef get_file_extension(filename: str) -> str:\n    \"\"\"Get file extension.\"\"\"\n    return f\".{os.path.splitext(filename)[1][1:].lower()}\"\n\n\nclass BufferedAsyncIterator(ABC):\n    \"\"\"\n    Base class for buffered async iterators.\n\n    This class is to be used as a base class for async iterators\n    that need to buffer the results of an async operation.\n    The async operation is defined in the _fill_buffer method.\n    The _fill_buffer method is called when the buffer is empty.\n    \"\"\"\n\n    def __init__(self, buffer_size: int):\n        \"\"\"\n        Initialize params.\n\n        Args:\n            - `buffer_size (int)`: Size of the buffer.\n                It is also the number of items that will\n                be retrieved from the async operation at once.\n                see _fill_buffer. Defaults to 2. Setting it to 1\n                will result in the same behavior as a synchronous iterator.\n        \"\"\"\n        self._buffer_size = buffer_size\n        self._buffer: List[Tuple[GitBlobResponseModel, str]] = []\n        self._index = 0\n\n    @abstractmethod\n    async def _fill_buffer(self) -> None:\n        raise NotImplementedError\n\n    def __aiter__(self) -> \"BufferedAsyncIterator\":\n        \"\"\"Return the iterator object.\"\"\"\n        return self\n\n    async def __anext__(self) -> Tuple[GitBlobResponseModel, str]:\n        \"\"\"\n        Get next item.\n\n        Returns:\n            - `item (Tuple[GitBlobResponseModel, str])`: Next item.\n\n        Raises:\n            - `StopAsyncIteration`: If there are no more items.\n        \"\"\"\n        if not self._buffer:\n            await self._fill_buffer()\n\n        if not self._buffer:\n            raise StopAsyncIteration\n\n        item = self._buffer.pop(0)\n        self._index += 1\n        return item\n\n\nclass BufferedGitBlobDataIterator(BufferedAsyncIterator):\n    \"\"\"\n    Buffered async iterator for Git blobs.\n\n    This class is an async iterator that buffers the results of the get_blob operation.\n    It is used to retrieve the contents of the files in a Github repository.\n    getBlob endpoint supports up to 100 megabytes of content for blobs.\n    This concrete implementation of BufferedAsyncIterator allows you to lazily retrieve\n    the contents of the files in a Github repository.\n    Otherwise you would have to retrieve all the contents of\n    the files in the repository at once, which would\n    be problematic if the repository is large.\n    \"\"\"\n\n    def __init__(\n        self,\n        blobs_and_paths: List[Tuple[GitTreeResponseModel.GitTreeObject, str]],\n        github_client: GithubClient,\n        owner: str,\n        repo: str,\n        loop: asyncio.AbstractEventLoop,\n        buffer_size: int,\n        verbose: bool = False,\n    ):\n        \"\"\"\n        Initialize params.\n\n        Args:\n            - blobs_and_paths (List[Tuple[GitTreeResponseModel.GitTreeObject, str]]):\n                List of tuples containing the blob and the path of the file.\n            - github_client (GithubClient): Github client.\n            - owner (str): Owner of the repository.\n            - repo (str): Name of the repository.\n            - loop (asyncio.AbstractEventLoop): Event loop.\n            - buffer_size (int): Size of the buffer.\n        \"\"\"\n        super().__init__(buffer_size)\n        self._blobs_and_paths = blobs_and_paths\n        self._github_client = github_client\n        self._owner = owner\n        self._repo = repo\n        self._verbose = verbose\n        if loop is None:\n            loop = asyncio.get_event_loop()\n            if loop is None:\n                raise ValueError(\"No event loop found\")\n\n    async def _fill_buffer(self) -> None:\n        \"\"\"\n        Fill the buffer with the results of the get_blob operation.\n\n        The get_blob operation is called for each blob in the blobs_and_paths list.\n        The blobs are retrieved in batches of size buffer_size.\n        \"\"\"\n        del self._buffer[:]\n        self._buffer = []\n        start = self._index\n        end = min(start + self._buffer_size, len(self._blobs_and_paths))\n\n        if start >= end:\n            return\n\n        if self._verbose:\n            start_t = time.time()\n        results: List[GitBlobResponseModel] = await asyncio.gather(\n            *[\n                self._github_client.get_blob(self._owner, self._repo, blob.sha)\n                for blob, _ in self._blobs_and_paths[\n                    start:end\n                ]  # TODO: use batch_size instead of buffer_size for concurrent requests\n            ]\n        )\n        if self._verbose:\n            end_t = time.time()\n            blob_names_and_sizes = [\n                (blob.path, blob.size) for blob, _ in self._blobs_and_paths[start:end]\n            ]\n            print(\n                \"Time to get blobs (\"\n                + f\"{blob_names_and_sizes}\"\n                + f\"): {end_t - start_t:.2f} seconds\"\n            )\n\n        self._buffer = [\n            (result, path)\n            for result, (_, path) in zip(results, self._blobs_and_paths[start:end])\n        ]\n", "doc_id": "98449e52eda3d6ee33e51429362e457d29f7893e", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/github_readers/utils.py", "file_name": "utils.py"}, "__type__": "Document"}, "82e08c90b55f6a8bb3145d247bafd31416b8c71f": {"text": "\"\"\"Google docs reader.\"\"\"\n\nimport logging\nimport os\nfrom typing import Any, List\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\nSCOPES = [\"https://www.googleapis.com/auth/documents.readonly\"]\n\n\n# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nclass GoogleDocsReader(BaseReader):\n    \"\"\"Google Docs reader.\n\n    Reads a page from Google Docs\n\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            import google  # noqa: F401\n            import google_auth_oauthlib  # noqa: F401\n            import googleapiclient  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`google_auth_oauthlib`, `googleapiclient` and `google` \"\n                \"must be installed to use the GoogleDocsReader.\\n\"\n                \"Please run `pip install --upgrade google-api-python-client \"\n                \"google-auth-httplib2 google-auth-oauthlib`.\"\n            )\n\n    def load_data(self, document_ids: List[str]) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            document_ids (List[str]): a list of document ids.\n        \"\"\"\n        if document_ids is None:\n            raise ValueError('Must specify a \"document_ids\" in `load_kwargs`.')\n\n        results = []\n        for document_id in document_ids:\n            doc = self._load_doc(document_id)\n            results.append(Document(doc, extra_info={\"document_id\": document_id}))\n        return results\n\n    def _load_doc(self, document_id: str) -> str:\n        \"\"\"Load a document from Google Docs.\n\n        Args:\n            document_id: the document id.\n\n        Returns:\n            The document text.\n        \"\"\"\n        import googleapiclient.discovery as discovery\n\n        credentials = self._get_credentials()\n        docs_service = discovery.build(\"docs\", \"v1\", credentials=credentials)\n        doc = docs_service.documents().get(documentId=document_id).execute()\n        doc_content = doc.get(\"body\").get(\"content\")\n        return self._read_structural_elements(doc_content)\n\n    def _get_credentials(self) -> Any:\n        \"\"\"Get valid user credentials from storage.\n\n        The file token.json stores the user's access and refresh tokens, and is\n        created automatically when the authorization flow completes for the first\n        time.\n\n        Returns:\n            Credentials, the obtained credential.\n        \"\"\"\n        from google.auth.transport.requests import Request\n        from google.oauth2.credentials import Credentials\n        from google_auth_oauthlib.flow import InstalledAppFlow\n\n        creds = None\n        if os.path.exists(\"token.json\"):\n            creds = Credentials.from_authorized_user_file(\"token.json\", SCOPES)\n        # If there are no (valid) credentials available, let the user log in.\n        if not creds or not creds.valid:\n            if creds and creds.expired and creds.refresh_token:\n                creds.refresh(Request())\n            else:\n                flow = InstalledAppFlow.from_client_secrets_file(\n                    \"credentials.json\", SCOPES\n                )\n                creds = flow.run_local_server(port=0)\n            # Save the credentials for the next run\n            with open(\"token.json\", \"w\") as token:\n                token.write(creds.to_json())\n\n        return creds\n\n    def _read_paragraph_element(self, element: Any) -> Any:\n        \"\"\"Return the text in the given ParagraphElement.\n\n        Args:\n            element: a ParagraphElement from a Google Doc.\n        \"\"\"\n        text_run = element.get(\"textRun\")\n        if not text_run:\n            return \"\"\n        return text_run.get(\"content\")\n\n    def _read_structural_elements(self, elements: List[Any]) -> Any:\n        \"\"\"Recurse through a list of Structural Elements.\n\n        Read a document's text where text may be in nested elements.\n\n        Args:\n            elements: a list of Structural Elements.\n        \"\"\"\n        text = \"\"\n        for value in elements:\n            if \"paragraph\" in value:\n                elements = value.get(\"paragraph\").get(\"elements\")\n                for elem in elements:\n                    text += self._read_paragraph_element(elem)\n            elif \"table\" in value:\n                # The text in table cells are in nested Structural Elements\n                # and tables may be nested.\n                table = value.get(\"table\")\n                for row in table.get(\"tableRows\"):\n                    cells = row.get(\"tableCells\")\n                    for cell in cells:\n                        text += self._read_structural_elements(cell.get(\"content\"))\n            elif \"tableOfContents\" in value:\n                # The text in the TOC is also in a Structural Element.\n                toc = value.get(\"tableOfContents\")\n                text += self._read_structural_elements(toc.get(\"content\"))\n        return text\n\n\nif __name__ == \"__main__\":\n    reader = GoogleDocsReader()\n    logging.info(\n        reader.load_data(document_ids=[\"11ctUj_tEf5S8vs_dk8_BNi-Zk8wW5YFhXkKqtmU_4B8\"])\n    )\n", "doc_id": "82e08c90b55f6a8bb3145d247bafd31416b8c71f", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/google_readers/gdocs.py", "file_name": "gdocs.py"}, "__type__": "Document"}, "6c57299640176cee5463a5f48d05457667821f22": {"text": "\"\"\"Make.com API wrapper.\n\nCurrently cannot load documents.\n\n\"\"\"\n\nfrom typing import Any, List, Optional\n\nimport requests\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\nfrom gpt_index.response.schema import Response, SourceNode\n\n\nclass MakeWrapper(BaseReader):\n    \"\"\"Make reader.\"\"\"\n\n    def load_data(self, *args: Any, **load_kwargs: Any) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        NOTE: This is not implemented.\n\n        \"\"\"\n        raise NotImplementedError(\"Cannot load documents from Make.com API.\")\n\n    def pass_response_to_webhook(\n        self, webhook_url: str, response: Response, query: Optional[str] = None\n    ) -> None:\n        \"\"\"Pass response object to webhook.\n\n        Args:\n            webhook_url (str): Webhook URL.\n            response (Response): Response object.\n            query (Optional[str]): Query. Defaults to None.\n\n        \"\"\"\n        response_text = response.response\n        source_nodes = [n.to_dict() for n in response.source_nodes]\n        json_dict = {\n            \"response\": response_text,\n            \"source_nodes\": source_nodes,\n            \"query\": query,\n        }\n        r = requests.post(webhook_url, json=json_dict)\n        r.raise_for_status()\n\n\nif __name__ == \"__main__\":\n    wrapper = MakeWrapper()\n    test_response = Response(\n        response=\"test response\",\n        source_nodes=[SourceNode(source_text=\"test source\", doc_id=\"test id\")],\n    )\n    wrapper.pass_response_to_webhook(\n        \"https://hook.us1.make.com/asdfadsfasdfasdfd\",\n        test_response,\n        \"Test query\",\n    )\n", "doc_id": "6c57299640176cee5463a5f48d05457667821f22", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/make_com/wrapper.py", "file_name": "wrapper.py"}, "__type__": "Document"}, "b8ba0526790bd62d1a41d878474809af816f616e": {"text": "\"\"\"Simple reader for mbox (mailbox) files.\"\"\"\nimport os\nfrom pathlib import Path\nfrom typing import Any, List\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.file.mbox_parser import MboxParser\nfrom gpt_index.readers.schema.base import Document\n\n\nclass MboxReader(BaseReader):\n    \"\"\"Mbox e-mail reader.\n\n    Reads a set of e-mails saved in the mbox format.\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize.\"\"\"\n\n    def load_data(self, input_dir: str, **load_kwargs: Any) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        load_kwargs:\n            max_count (int): Maximum amount of messages to read.\n            message_format (str): Message format overriding default.\n        \"\"\"\n        docs: List[Document] = []\n        for (dirpath, dirnames, filenames) in os.walk(input_dir):\n            dirnames[:] = [d for d in dirnames if not d.startswith(\".\")]\n            for filename in filenames:\n                if filename.endswith(\".mbox\"):\n                    filepath = os.path.join(dirpath, filename)\n                    content = MboxParser(**load_kwargs).parse_file(Path(filepath))\n                    for msg in content:\n                        docs.append(Document(msg))\n        return docs\n", "doc_id": "b8ba0526790bd62d1a41d878474809af816f616e", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/mbox.py", "file_name": "mbox.py"}, "__type__": "Document"}, "2bc13de0428fd82ed9d665fc257405cf2de8929a": {"text": "\"\"\"Mongo client.\"\"\"\n\nfrom typing import Dict, List, Optional\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass SimpleMongoReader(BaseReader):\n    \"\"\"Simple mongo reader.\n\n    Concatenates each Mongo doc into Document used by GPT Index.\n\n    Args:\n        host (str): Mongo host.\n        port (int): Mongo port.\n        max_docs (int): Maximum number of documents to load.\n\n    \"\"\"\n\n    def __init__(self, host: str, port: int, max_docs: int = 1000) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            import pymongo  # noqa: F401\n            from pymongo import MongoClient  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`pymongo` package not found, please run `pip install pymongo`\"\n            )\n        self.client: MongoClient = MongoClient(host, port)\n        self.max_docs = max_docs\n\n    def load_data(\n        self, db_name: str, collection_name: str, query_dict: Optional[Dict] = None\n    ) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            db_name (str): name of the database.\n            collection_name (str): name of the collection.\n            query_dict (Optional[Dict]): query to filter documents.\n                Defaults to None\n\n        Returns:\n            List[Document]: A list of documents.\n\n        \"\"\"\n        documents = []\n        db = self.client[db_name]\n        if query_dict is None:\n            cursor = db[collection_name].find()\n        else:\n            cursor = db[collection_name].find(query_dict)\n\n        for item in cursor:\n            if \"text\" not in item:\n                raise ValueError(\"`text` field not found in Mongo document.\")\n            documents.append(Document(item[\"text\"]))\n        return documents\n", "doc_id": "2bc13de0428fd82ed9d665fc257405cf2de8929a", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/mongo.py", "file_name": "mongo.py"}, "__type__": "Document"}, "c7ee9e2bf5f208fdcdcdf35eaf126ee0abbefe83": {"text": "\"\"\"Notion reader.\"\"\"\nimport logging\nimport os\nfrom typing import Any, Dict, List, Optional\n\nimport requests  # type: ignore\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\nINTEGRATION_TOKEN_NAME = \"NOTION_INTEGRATION_TOKEN\"\nBLOCK_CHILD_URL_TMPL = \"https://api.notion.com/v1/blocks/{block_id}/children\"\nDATABASE_URL_TMPL = \"https://api.notion.com/v1/databases/{database_id}/query\"\nSEARCH_URL = \"https://api.notion.com/v1/search\"\n\n\n# TODO: Notion DB reader coming soon!\nclass NotionPageReader(BaseReader):\n    \"\"\"Notion Page reader.\n\n    Reads a set of Notion pages.\n\n    Args:\n        integration_token (str): Notion integration token.\n\n    \"\"\"\n\n    def __init__(self, integration_token: Optional[str] = None) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        if integration_token is None:\n            integration_token = os.getenv(INTEGRATION_TOKEN_NAME)\n            if integration_token is None:\n                raise ValueError(\n                    \"Must specify `integration_token` or set environment \"\n                    \"variable `NOTION_INTEGRATION_TOKEN`.\"\n                )\n        self.token = integration_token\n        self.headers = {\n            \"Authorization\": \"Bearer \" + self.token,\n            \"Content-Type\": \"application/json\",\n            \"Notion-Version\": \"2022-06-28\",\n        }\n\n    def _read_block(self, block_id: str, num_tabs: int = 0) -> str:\n        \"\"\"Read a block.\"\"\"\n        done = False\n        result_lines_arr = []\n        cur_block_id = block_id\n        while not done:\n            block_url = BLOCK_CHILD_URL_TMPL.format(block_id=cur_block_id)\n            query_dict: Dict[str, Any] = {}\n\n            res = requests.request(\n                \"GET\", block_url, headers=self.headers, json=query_dict\n            )\n            data = res.json()\n\n            for result in data[\"results\"]:\n                result_type = result[\"type\"]\n                result_obj = result[result_type]\n\n                cur_result_text_arr = []\n                if \"rich_text\" in result_obj:\n                    for rich_text in result_obj[\"rich_text\"]:\n                        # skip if doesn't have text object\n                        if \"text\" in rich_text:\n                            text = rich_text[\"text\"][\"content\"]\n                            prefix = \"\\t\" * num_tabs\n                            cur_result_text_arr.append(prefix + text)\n\n                result_block_id = result[\"id\"]\n                has_children = result[\"has_children\"]\n                if has_children:\n                    children_text = self._read_block(\n                        result_block_id, num_tabs=num_tabs + 1\n                    )\n                    cur_result_text_arr.append(children_text)\n\n                cur_result_text = \"\\n\".join(cur_result_text_arr)\n                result_lines_arr.append(cur_result_text)\n\n            if data[\"next_cursor\"] is None:\n                done = True\n                break\n            else:\n                cur_block_id = data[\"next_cursor\"]\n\n        result_lines = \"\\n\".join(result_lines_arr)\n        return result_lines\n\n    def read_page(self, page_id: str) -> str:\n        \"\"\"Read a page.\"\"\"\n        return self._read_block(page_id)\n\n    def query_database(\n        self, database_id: str, query_dict: Dict[str, Any] = {}\n    ) -> List[str]:\n        \"\"\"Get all the pages from a Notion database.\"\"\"\n        res = requests.post(\n            DATABASE_URL_TMPL.format(database_id=database_id),\n            headers=self.headers,\n            json=query_dict,\n        )\n        data = res.json()\n        page_ids = []\n        for result in data[\"results\"]:\n            page_id = result[\"id\"]\n            page_ids.append(page_id)\n\n        return page_ids\n\n    def search(self, query: str) -> List[str]:\n        \"\"\"Search Notion page given a text query.\"\"\"\n        done = False\n        next_cursor: Optional[str] = None\n        page_ids = []\n        while not done:\n            query_dict = {\n                \"query\": query,\n            }\n            if next_cursor is not None:\n                query_dict[\"start_cursor\"] = next_cursor\n            res = requests.post(SEARCH_URL, headers=self.headers, json=query_dict)\n            data = res.json()\n            for result in data[\"results\"]:\n                page_id = result[\"id\"]\n                page_ids.append(page_id)\n\n            if data[\"next_cursor\"] is None:\n                done = True\n                break\n            else:\n                next_cursor = data[\"next_cursor\"]\n        return page_ids\n\n    def load_data(\n        self, page_ids: List[str] = [], database_id: Optional[str] = None\n    ) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            page_ids (List[str]): List of page ids to load.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        if not page_ids and not database_id:\n            raise ValueError(\"Must specify either `page_ids` or `database_id`.\")\n        docs = []\n        if database_id is not None:\n            # get all the pages in the database\n            page_ids = self.query_database(database_id)\n            for page_id in page_ids:\n                page_text = self.read_page(page_id)\n                docs.append(Document(page_text, extra_info={\"page_id\": page_id}))\n        else:\n            for page_id in page_ids:\n                page_text = self.read_page(page_id)\n                docs.append(Document(page_text, extra_info={\"page_id\": page_id}))\n\n        return docs\n\n\nif __name__ == \"__main__\":\n    reader = NotionPageReader()\n    logging.info(reader.search(\"What I\"))\n", "doc_id": "c7ee9e2bf5f208fdcdcdf35eaf126ee0abbefe83", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/notion.py", "file_name": "notion.py"}, "__type__": "Document"}, "1404005433f2a8f5a4aaf2f0cb40dba48a0cd9e8": {"text": "\"\"\"Obsidian reader class.\n\nPass in the path to an Obsidian vault and it will parse all markdown\nfiles into a List of Documents,\nwith each Document containing text from under an Obsidian header.\n\n\"\"\"\nimport os\nfrom pathlib import Path\nfrom typing import Any, List\n\nfrom langchain.docstore.document import Document as LCDocument\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.file.markdown_parser import MarkdownParser\nfrom gpt_index.readers.schema.base import Document\n\n\nclass ObsidianReader(BaseReader):\n    \"\"\"Utilities for loading data from an Obsidian Vault.\n\n    Args:\n        input_dir (str): Path to the vault.\n\n    \"\"\"\n\n    def __init__(self, input_dir: str):\n        \"\"\"Init params.\"\"\"\n        self.input_dir = Path(input_dir)\n\n    def load_data(self, *args: Any, **load_kwargs: Any) -> List[Document]:\n        \"\"\"Load data from the input directory.\"\"\"\n        docs: List[str] = []\n        for dirpath, dirnames, filenames in os.walk(self.input_dir):\n            dirnames[:] = [d for d in dirnames if not d.startswith(\".\")]\n            for filename in filenames:\n                if filename.endswith(\".md\"):\n                    filepath = os.path.join(dirpath, filename)\n                    content = MarkdownParser().parse_file(Path(filepath))\n                    docs.extend(content)\n        return [Document(d) for d in docs]\n\n    def load_langchain_documents(self, **load_kwargs: Any) -> List[LCDocument]:\n        \"\"\"Load data in LangChain document format.\"\"\"\n        docs = self.load_data(**load_kwargs)\n        return [d.to_langchain_format() for d in docs]\n", "doc_id": "1404005433f2a8f5a4aaf2f0cb40dba48a0cd9e8", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/obsidian.py", "file_name": "obsidian.py"}, "__type__": "Document"}, "c121f325702354416b70e9e38021ce04ff265381": {"text": "\"\"\"Pinecone reader.\"\"\"\n\nfrom typing import Any, Dict, List, Optional\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass PineconeReader(BaseReader):\n    \"\"\"Pinecone reader.\n\n    Args:\n        api_key (str): Pinecone API key.\n        environment (str): Pinecone environment.\n    \"\"\"\n\n    def __init__(self, api_key: str, environment: str):\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            import pinecone  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`pinecone` package not found, please run `pip install pinecone-client`\"\n            )\n\n        self._api_key = api_key\n        self._environment = environment\n        pinecone.init(api_key=api_key, environment=environment)\n\n    def load_data(\n        self,\n        index_name: str,\n        id_to_text_map: Dict[str, str],\n        vector: Optional[List[float]],\n        top_k: int,\n        separate_documents: bool = True,\n        include_values: bool = True,\n        **query_kwargs: Any\n    ) -> List[Document]:\n        \"\"\"Load data from Pinecone.\n\n        Args:\n            index_name (str): Name of the index.\n            id_to_text_map (Dict[str, str]): A map from ID's to text.\n            separate_documents (Optional[bool]): Whether to return separate\n                documents per retrieved entry. Defaults to True.\n            vector (List[float]): Query vector.\n            top_k (int): Number of results to return.\n            include_values (bool): Whether to include the embedding in the response.\n                Defaults to True.\n            **query_kwargs: Keyword arguments to pass to the query.\n                Arguments are the exact same as those found in\n                Pinecone's reference documentation for the\n                query method.\n\n        Returns:\n            List[Document]: A list of documents.\n        \"\"\"\n        import pinecone\n\n        index = pinecone.Index(index_name)\n        if \"include_values\" not in query_kwargs:\n            query_kwargs[\"include_values\"] = True\n        response = index.query(top_k=top_k, vector=vector, **query_kwargs)\n\n        documents = []\n        for match in response.matches:\n            if match.id not in id_to_text_map:\n                raise ValueError(\"ID not found in id_to_text_map.\")\n            text = id_to_text_map[match.id]\n            embedding = match.values\n            if len(embedding) == 0:\n                embedding = None\n            documents.append(Document(text=text, embedding=embedding))\n\n        if not separate_documents:\n            text_list = [doc.get_text() for doc in documents]\n            text = \"\\n\\n\".join(text_list)\n            documents = [Document(text=text)]\n\n        return documents\n", "doc_id": "c121f325702354416b70e9e38021ce04ff265381", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/pinecone.py", "file_name": "pinecone.py"}, "__type__": "Document"}, "5088ae6098fcc16c6105a3cbf9b6b1c7dac5ec53": {"text": "\"\"\"Qdrant reader.\"\"\"\n\nfrom typing import List, Optional, cast\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass QdrantReader(BaseReader):\n    \"\"\"Qdrant reader.\n\n    Retrieve documents from existing Qdrant collections.\n\n    Args:\n        host: Host name of Qdrant service.\n        port: Port of the REST API interface. Default: 6333\n        grpc_port: Port of the gRPC interface. Default: 6334\n        prefer_grpc: If `true` - use gPRC interface whenever possible in custom methods.\n        https: If `true` - use HTTPS(SSL) protocol. Default: `false`\n        api_key: API key for authentication in Qdrant Cloud. Default: `None`\n        prefix:\n            If not `None` - add `prefix` to the REST URL path.\n            Example: `service/v1` will result in\n            `http://localhost:6333/service/v1/{qdrant-endpoint}` for REST API.\n            Default: `None`\n        timeout:\n            Timeout for REST and gRPC API requests.\n            Default: 5.0 seconds for REST and unlimited for gRPC\n    \"\"\"\n\n    def __init__(\n        self,\n        host: str,\n        port: int = 6333,\n        grpc_port: int = 6334,\n        prefer_grpc: bool = False,\n        https: Optional[bool] = None,\n        api_key: Optional[str] = None,\n        prefix: Optional[str] = None,\n        timeout: Optional[float] = None,\n    ):\n        \"\"\"Initialize with parameters.\"\"\"\n        import_err_msg = (\n            \"`qdrant-client` package not found, please run `pip install qdrant-client`\"\n        )\n        try:\n            import qdrant_client  # noqa: F401\n        except ImportError:\n            raise ValueError(import_err_msg)\n\n        self._client = qdrant_client.QdrantClient(\n            host=host,\n            port=port,\n            grpc_port=grpc_port,\n            prefer_grpc=prefer_grpc,\n            https=https,\n            api_key=api_key,\n            prefix=prefix,\n            timeout=timeout,\n        )\n\n    def load_data(\n        self,\n        collection_name: str,\n        query_vector: List[float],\n        limit: int = 10,\n    ) -> List[Document]:\n        \"\"\"Load data from Qdrant.\n\n        Args:\n            collection_name (str): Name of the Qdrant collection.\n            query_vector (List[float]): Query vector.\n            limit (int): Number of results to return.\n\n        Returns:\n            List[Document]: A list of documents.\n        \"\"\"\n        from qdrant_client.http.models.models import Payload\n\n        response = self._client.search(\n            collection_name=collection_name,\n            query_vector=query_vector,\n            with_vectors=True,\n            with_payload=True,\n            limit=limit,\n        )\n\n        documents = []\n        for point in response:\n            payload = cast(Payload, point)\n            try:\n                vector = cast(List[float], point.vector)\n            except ValueError as e:\n                raise ValueError(\"Could not cast vector to List[float].\") from e\n            document = Document(\n                doc_id=payload.get(\"doc_id\"),\n                text=payload.get(\"text\"),\n                embedding=vector,\n            )\n            documents.append(document)\n\n        return documents\n", "doc_id": "5088ae6098fcc16c6105a3cbf9b6b1c7dac5ec53", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/qdrant.py", "file_name": "qdrant.py"}, "__type__": "Document"}, "753bf93eae607ae72cccb26ebc7ec69ede6abb9e": {"text": "\"\"\"Base schema for readers.\"\"\"\nfrom dataclasses import dataclass\n\nfrom langchain.docstore.document import Document as LCDocument\n\nfrom gpt_index.schema import BaseDocument\n\n\n@dataclass\nclass Document(BaseDocument):\n    \"\"\"Generic interface for a data document.\n\n    This document connects to data sources.\n\n    \"\"\"\n\n    def __post_init__(self) -> None:\n        \"\"\"Post init.\"\"\"\n        if self.text is None:\n            raise ValueError(\"text field not set.\")\n\n    @classmethod\n    def get_type(cls) -> str:\n        \"\"\"Get Document type.\"\"\"\n        return \"Document\"\n\n    def to_langchain_format(self) -> LCDocument:\n        \"\"\"Convert struct to LangChain document format.\"\"\"\n        metadata = self.extra_info or {}\n        return LCDocument(page_content=self.text, metadata=metadata)\n\n    @classmethod\n    def from_langchain_format(cls, doc: LCDocument) -> \"Document\":\n        \"\"\"Convert struct from LangChain document format.\"\"\"\n        return cls(text=doc.page_content, extra_info=doc.metadata)\n", "doc_id": "753bf93eae607ae72cccb26ebc7ec69ede6abb9e", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/schema/base.py", "file_name": "base.py"}, "__type__": "Document"}, "a0db9d42d0b3dcf85ce915984e4159a8d57fd656": {"text": "\"\"\"Slack reader.\"\"\"\nimport logging\nimport os\nimport time\nfrom typing import List, Optional\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass SlackReader(BaseReader):\n    \"\"\"Slack reader.\n\n    Reads conversations from channels.\n\n    Args:\n        slack_token (Optional[str]): Slack token. If not provided, we\n            assume the environment variable `SLACK_BOT_TOKEN` is set.\n\n    \"\"\"\n\n    def __init__(self, slack_token: Optional[str] = None) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            from slack_sdk import WebClient\n        except ImportError:\n            raise ValueError(\n                \"`slack_sdk` package not found, please run `pip install slack_sdk`\"\n            )\n        if slack_token is None:\n            slack_token = os.environ[\"SLACK_BOT_TOKEN\"]\n            if slack_token is None:\n                raise ValueError(\n                    \"Must specify `slack_token` or set environment \"\n                    \"variable `SLACK_BOT_TOKEN`.\"\n                )\n        self.client = WebClient(token=slack_token)\n        res = self.client.api_test()\n        if not res[\"ok\"]:\n            raise ValueError(f\"Error initializing Slack API: {res['error']}\")\n\n    def _read_message(self, channel_id: str, message_ts: str) -> str:\n        from slack_sdk.errors import SlackApiError\n\n        \"\"\"Read a message.\"\"\"\n\n        messages_text = []\n        next_cursor = None\n        while True:\n            try:\n                # https://slack.com/api/conversations.replies\n                # List all replies to a message, including the message itself.\n                result = self.client.conversations_replies(\n                    channel=channel_id, ts=message_ts, cursor=next_cursor\n                )\n                messages = result[\"messages\"]\n                for message in messages:\n                    messages_text.append(message[\"text\"])\n\n                if not result[\"has_more\"]:\n                    break\n\n                next_cursor = result[\"response_metadata\"][\"next_cursor\"]\n            except SlackApiError as e:\n                if e.response[\"error\"] == \"ratelimited\":\n                    logging.error(\n                        \"Rate limit error reached, sleeping for: {} seconds\".format(\n                            e.response.headers[\"retry-after\"]\n                        )\n                    )\n                    time.sleep(int(e.response.headers[\"retry-after\"]))\n                else:\n                    logging.error(\"Error parsing conversation replies: {}\".format(e))\n\n        return \"\\n\\n\".join(messages_text)\n\n    def _read_channel(self, channel_id: str) -> str:\n        from slack_sdk.errors import SlackApiError\n\n        \"\"\"Read a channel.\"\"\"\n\n        result_messages = []\n        next_cursor = None\n        while True:\n            try:\n                # Call the conversations.history method using the WebClient\n                # conversations.history returns the first 100 messages by default\n                # These results are paginated,\n                # see: https://api.slack.com/methods/conversations.history$pagination\n                result = self.client.conversations_history(\n                    channel=channel_id, cursor=next_cursor\n                )\n                conversation_history = result[\"messages\"]\n                # Print results\n                logging.info(\n                    \"{} messages found in {}\".format(len(conversation_history), id)\n                )\n                for message in conversation_history:\n                    result_messages.append(\n                        self._read_message(channel_id, message[\"ts\"])\n                    )\n\n                if not result[\"has_more\"]:\n                    break\n                next_cursor = result[\"response_metadata\"][\"next_cursor\"]\n\n            except SlackApiError as e:\n                if e.response[\"error\"] == \"ratelimited\":\n                    logging.error(\n                        \"Rate limit error reached, sleeping for: {} seconds\".format(\n                            e.response.headers[\"retry-after\"]\n                        )\n                    )\n                    time.sleep(int(e.response.headers[\"retry-after\"]))\n                else:\n                    logging.error(\"Error parsing conversation replies: {}\".format(e))\n\n        return \"\\n\\n\".join(result_messages)\n\n    def load_data(self, channel_ids: List[str]) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            channel_ids (List[str]): List of channel ids to read.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        results = []\n        for channel_id in channel_ids:\n            channel_content = self._read_channel(channel_id)\n            results.append(\n                Document(channel_content, extra_info={\"channel\": channel_id})\n            )\n        return results\n\n\nif __name__ == \"__main__\":\n    reader = SlackReader()\n    logging.info(reader.load_data(channel_ids=[\"C04DC2VUY3F\"]))\n", "doc_id": "a0db9d42d0b3dcf85ce915984e4159a8d57fd656", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/slack.py", "file_name": "slack.py"}, "__type__": "Document"}, "62876b8e16ae2625adf3c433bdd1b3eaa70150c3": {"text": "\"\"\"Simple reader that turns an iterable of strings into a list of Documents.\"\"\"\nfrom typing import List\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass StringIterableReader(BaseReader):\n    \"\"\"String Iterable Reader.\n\n    Gets a list of documents, given an iterable (e.g. list) of strings.\n\n    Example:\n        .. code-block:: python\n\n            from gpt_index import StringIterableReader, GPTTreeIndex\n\n            documents = StringIterableReader().load_data(\n                texts=[\"I went to the store\", \"I bought an apple\"])\n            index = GPTTreeIndex(documents)\n            index.query(\"what did I buy?\")\n\n            # response should be something like \"You bought an apple.\"\n    \"\"\"\n\n    def load_data(self, texts: List[str]) -> List[Document]:\n        \"\"\"Load the data.\"\"\"\n        results = []\n        for text in texts:\n            results.append(Document(text))\n\n        return results\n", "doc_id": "62876b8e16ae2625adf3c433bdd1b3eaa70150c3", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/string_iterable.py", "file_name": "string_iterable.py"}, "__type__": "Document"}, "ee0e8f78b27c1a62e466e13d35e9bbeea3b425e9": {"text": "\"\"\"Simple reader that reads tweets of a twitter handle.\"\"\"\nfrom typing import Any, List, Optional\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass TwitterTweetReader(BaseReader):\n    \"\"\"Twitter tweets reader.\n\n    Read tweets of user twitter handle.\n\n    Check 'https://developer.twitter.com/en/docs/twitter-api/\\\n        getting-started/getting-access-to-the-twitter-api' \\\n        on how to get access to twitter API.\n\n    Args:\n        bearer_token (str): bearer_token that you get from twitter API.\n        num_tweets (Optional[int]): Number of tweets for each user twitter handle.\\\n            Default is 100 tweets.\n    \"\"\"\n\n    def __init__(\n        self,\n        bearer_token: str,\n        num_tweets: Optional[int] = 100,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        self.bearer_token = bearer_token\n        self.num_tweets = num_tweets\n\n    def load_data(\n        self, twitterhandles: List[str], **load_kwargs: Any\n    ) -> List[Document]:\n        \"\"\"Load tweets of twitter handles.\n\n        Args:\n            twitterhandles (List[str]): List of user twitter handles to read tweets.\n\n        \"\"\"\n        try:\n            import tweepy\n        except ImportError:\n            raise ValueError(\n                \"`tweepy` package not found, please run `pip install tweepy`\"\n            )\n\n        client = tweepy.Client(bearer_token=self.bearer_token)\n        results = []\n        for username in twitterhandles:\n            # tweets = api.user_timeline(screen_name=user, count=self.num_tweets)\n            user = client.get_user(username=username)\n            tweets = client.get_users_tweets(user.data.id, max_results=self.num_tweets)\n            response = \" \"\n            for tweet in tweets.data:\n                response = response + tweet.text + \"\\n\"\n            results.append(Document(response))\n        return results\n", "doc_id": "ee0e8f78b27c1a62e466e13d35e9bbeea3b425e9", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/twitter.py", "file_name": "twitter.py"}, "__type__": "Document"}, "460d2ae4fea64197bd58b237d3e9f7ac8ad5604e": {"text": "\"\"\"Weaviate-specific serializers for GPT Index data structures.\n\nContain conversion to and from dataclasses that GPT Index uses.\n\n\"\"\"\n\nimport json\nfrom abc import abstractmethod\nfrom typing import Any, Dict, Generic, List, Optional, TypeVar\n\nfrom gpt_index.data_structs.data_structs import IndexStruct, Node\nfrom gpt_index.readers.weaviate.utils import (\n    get_by_id,\n    parse_get_response,\n    validate_client,\n)\nfrom gpt_index.utils import get_new_id\n\nIS = TypeVar(\"IS\", bound=IndexStruct)\n\n\nclass BaseWeaviateIndexStruct(Generic[IS]):\n    \"\"\"Base Weaviate index struct.\"\"\"\n\n    @classmethod\n    @abstractmethod\n    def _class_name(cls, class_prefix: str) -> str:\n        \"\"\"Return class name.\"\"\"\n\n    @classmethod\n    def _get_common_properties(cls) -> List[Dict]:\n        \"\"\"Get common properties.\"\"\"\n        return [\n            {\n                \"dataType\": [\"string\"],\n                \"description\": \"Text property\",\n                \"name\": \"text\",\n            },\n            {\n                \"dataType\": [\"string\"],\n                \"description\": \"Document id\",\n                \"name\": \"doc_id\",\n            },\n            {\n                \"dataType\": [\"string\"],\n                \"description\": \"extra_info (in JSON)\",\n                \"name\": \"extra_info\",\n            },\n        ]\n\n    @classmethod\n    @abstractmethod\n    def _get_properties(cls) -> List[Dict]:\n        \"\"\"Get properties specific to each index struct.\n\n        Used in creating schema.\n\n        \"\"\"\n\n    @classmethod\n    def _get_by_id(cls, client: Any, object_id: str, class_prefix: str) -> Dict:\n        \"\"\"Get entry by id.\"\"\"\n        validate_client(client)\n        class_name = cls._class_name(class_prefix)\n        properties = cls._get_common_properties() + cls._get_properties()\n        prop_names = [p[\"name\"] for p in properties]\n        entry = get_by_id(client, object_id, class_name, prop_names)\n        return entry\n\n    @classmethod\n    def create_schema(cls, client: Any, class_prefix: str) -> None:\n        \"\"\"Create schema.\"\"\"\n        validate_client(client)\n        # first check if schema exists\n        schema = client.schema.get()\n        classes = schema[\"classes\"]\n        existing_class_names = {c[\"class\"] for c in classes}\n        # if schema already exists, don't create\n        class_name = cls._class_name(class_prefix)\n        if class_name in existing_class_names:\n            return\n\n        # get common properties\n        properties = cls._get_common_properties()\n        # get specific properties\n        properties.extend(cls._get_properties())\n        class_obj = {\n            \"class\": cls._class_name(class_prefix),  # <= note the capital \"A\".\n            \"description\": f\"Class for {class_name}\",\n            \"properties\": properties,\n        }\n        client.schema.create_class(class_obj)\n\n    @classmethod\n    @abstractmethod\n    def _entry_to_gpt_index(cls, entry: Dict) -> IS:\n        \"\"\"Convert to gpt index list.\"\"\"\n\n    @classmethod\n    def to_gpt_index_list(\n        cls,\n        client: Any,\n        class_prefix: str,\n        vector: Optional[List[float]] = None,\n        object_limit: Optional[int] = None,\n    ) -> List[IS]:\n        \"\"\"Convert to gpt index list.\"\"\"\n        validate_client(client)\n        class_name = cls._class_name(class_prefix)\n        properties = cls._get_common_properties() + cls._get_properties()\n        prop_names = [p[\"name\"] for p in properties]\n        query = client.query.get(class_name, prop_names).with_additional(\n            [\"id\", \"vector\"]\n        )\n        if vector is not None:\n            query = query.with_near_vector(\n                {\n                    \"vector\": vector,\n                }\n            )\n        if object_limit is not None:\n            query = query.with_limit(object_limit)\n        query_result = query.do()\n        parsed_result = parse_get_response(query_result)\n        entries = parsed_result[class_name]\n\n        results: List[IS] = []\n        for entry in entries:\n            results.append(cls._entry_to_gpt_index(entry))\n\n        return results\n\n    @classmethod\n    @abstractmethod\n    def _from_gpt_index(cls, client: Any, index: IS, class_prefix: str) -> str:\n        \"\"\"Convert from gpt index.\"\"\"\n\n    @classmethod\n    def from_gpt_index(cls, client: Any, index: IS, class_prefix: str) -> str:\n        \"\"\"Convert from gpt index.\"\"\"\n        validate_client(client)\n        index_id = cls._from_gpt_index(client, index, class_prefix)\n        client.batch.flush()\n        return index_id\n\n\nclass WeaviateNode(BaseWeaviateIndexStruct[Node]):\n    \"\"\"Weaviate node.\"\"\"\n\n    @classmethod\n    def _class_name(cls, class_prefix: str) -> str:\n        \"\"\"Return class name.\"\"\"\n        return f\"{class_prefix}_Node\"\n\n    @classmethod\n    def _get_properties(cls) -> List[Dict]:\n        \"\"\"Create schema.\"\"\"\n        return [\n            {\n                \"dataType\": [\"int\"],\n                \"description\": \"The index of the Node\",\n                \"name\": \"index\",\n            },\n            {\n                \"dataType\": [\"int[]\"],\n                \"description\": \"The child_indices of the Node\",\n                \"name\": \"child_indices\",\n            },\n            {\n                \"dataType\": [\"string\"],\n                \"description\": \"The ref_doc_id of the Node\",\n                \"name\": \"ref_doc_id\",\n            },\n            {\n                \"dataType\": [\"string\"],\n                \"description\": \"node_info (in JSON)\",\n                \"name\": \"node_info\",\n            },\n        ]\n\n    @classmethod\n    def _entry_to_gpt_index(cls, entry: Dict) -> Node:\n        \"\"\"Convert to gpt index list.\"\"\"\n        extra_info_str = entry[\"extra_info\"]\n        if extra_info_str == \"\":\n            extra_info = None\n        else:\n            extra_info = json.loads(extra_info_str)\n\n        node_info_str = entry[\"node_info\"]\n        if node_info_str == \"\":\n            node_info = None\n        else:\n            node_info = json.loads(node_info_str)\n        return Node(\n            text=entry[\"text\"],\n            doc_id=entry[\"doc_id\"],\n            index=int(entry[\"index\"]),\n            child_indices=entry[\"child_indices\"],\n            ref_doc_id=entry[\"ref_doc_id\"],\n            embedding=entry[\"_additional\"][\"vector\"],\n            extra_info=extra_info,\n            node_info=node_info,\n        )\n\n    @classmethod\n    def _from_gpt_index(cls, client: Any, node: Node, class_prefix: str) -> str:\n        \"\"\"Convert from gpt index.\"\"\"\n        node_dict = node.to_dict()\n        vector = node_dict.pop(\"embedding\")\n        extra_info = node_dict.pop(\"extra_info\")\n        # json-serialize the extra_info\n        extra_info_str = \"\"\n        if extra_info is not None:\n            extra_info_str = json.dumps(extra_info)\n        node_dict[\"extra_info\"] = extra_info_str\n        # json-serialize the node_info\n        node_info = node_dict.pop(\"node_info\")\n        node_info_str = \"\"\n        if node_info is not None:\n            node_info_str = json.dumps(node_info)\n        node_dict[\"node_info\"] = node_info_str\n\n        # TODO: account for existing nodes that are stored\n        node_id = get_new_id(set())\n        class_name = cls._class_name(class_prefix)\n        client.batch.add_data_object(node_dict, class_name, node_id, vector)\n\n        return node_id\n\n    @classmethod\n    def delete_document(cls, client: Any, ref_doc_id: str, class_prefix: str) -> None:\n        \"\"\"Delete entry.\"\"\"\n        validate_client(client)\n        # make sure that each entry\n        class_name = cls._class_name(class_prefix)\n        where_filter = {\n            \"path\": [\"ref_doc_id\"],\n            \"operator\": \"Equal\",\n            \"valueString\": ref_doc_id,\n        }\n        query = (\n            client.query.get(class_name)\n            .with_additional([\"id\"])\n            .with_where(where_filter)\n        )\n\n        query_result = query.do()\n        parsed_result = parse_get_response(query_result)\n        entries = parsed_result[class_name]\n        for entry in entries:\n            client.data_object.delete(entry[\"_additional\"][\"id\"], class_name)\n", "doc_id": "460d2ae4fea64197bd58b237d3e9f7ac8ad5604e", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/weaviate/data_structs.py", "file_name": "data_structs.py"}, "__type__": "Document"}, "9fb7548dd341b7bbe79a3a28e6bf3fd7ef78c44d": {"text": "\"\"\"Weaviate reader.\"\"\"\n\nfrom typing import Any, List, Optional\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass WeaviateReader(BaseReader):\n    \"\"\"Weaviate reader.\n\n    Retrieves documents from Weaviate through vector lookup. Allows option\n    to concatenate retrieved documents into one Document, or to return\n    separate Document objects per document.\n\n    Args:\n        host (str): host.\n        auth_client_secret (Optional[weaviate.auth.AuthCredentials]):\n            auth_client_secret.\n    \"\"\"\n\n    def __init__(\n        self,\n        host: str,\n        auth_client_secret: Optional[Any] = None,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            import weaviate  # noqa: F401\n            from weaviate import Client  # noqa: F401\n            from weaviate.auth import AuthCredentials  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`weaviate` package not found, please run `pip install weaviate-client`\"\n            )\n\n        self.client: Client = Client(host, auth_client_secret=auth_client_secret)\n\n    def load_data(\n        self,\n        class_name: Optional[str] = None,\n        properties: Optional[List[str]] = None,\n        graphql_query: Optional[str] = None,\n        separate_documents: Optional[bool] = True,\n    ) -> List[Document]:\n        \"\"\"Load data from Weaviate.\n\n        If `graphql_query` is not found in load_kwargs, we assume that\n        `class_name` and `properties` are provided.\n\n        Args:\n            class_name (Optional[str]): class_name to retrieve documents from.\n            properties (Optional[List[str]]): properties to retrieve from documents.\n            graphql_query (Optional[str]): Raw GraphQL Query.\n                We assume that the query is a Get query.\n            separate_documents (Optional[bool]): Whether to return separate\n                documents. Defaults to True.\n\n        Returns:\n            List[Document]: A list of documents.\n\n        \"\"\"\n        if class_name is not None and properties is not None:\n            props_txt = \"\\n\".join(properties)\n            graphql_query = f\"\"\"\n            {{\n                Get {{\n                    {class_name} {{\n                        {props_txt}\n                    }}\n                }}\n            }}\n            \"\"\"\n        elif graphql_query is not None:\n            pass\n        else:\n            raise ValueError(\n                \"Either `class_name` and `properties` must be specified, \"\n                \"or `graphql_query` must be specified.\"\n            )\n\n        response = self.client.query.raw(graphql_query)\n        if \"errors\" in response:\n            raise ValueError(\"Invalid query, got errors: {}\".format(response[\"errors\"]))\n\n        data_response = response[\"data\"]\n        if \"Get\" not in data_response:\n            raise ValueError(\"Invalid query response, must be a Get query.\")\n\n        if class_name is None:\n            # infer class_name if only graphql_query was provided\n            class_name = list(data_response[\"Get\"].keys())[0]\n        entries = data_response[\"Get\"][class_name]\n        documents = []\n        for entry in entries:\n            embedding = None\n            # for each entry, join properties into <property>:<value>\n            # separated by newlines\n            text_list = []\n            for k, v in entry.items():\n                if k == \"_additional\":\n                    if \"vector\" in v:\n                        embedding = v[\"vector\"]\n                    continue\n                text_list.append(f\"{k}: {v}\")\n\n            text = \"\\n\".join(text_list)\n            documents.append(Document(text=text, embedding=embedding))\n\n        if not separate_documents:\n            # join all documents into one\n            text_list = [doc.get_text() for doc in documents]\n            text = \"\\n\\n\".join(text_list)\n            documents = [Document(text=text)]\n\n        return documents\n", "doc_id": "9fb7548dd341b7bbe79a3a28e6bf3fd7ef78c44d", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/weaviate/reader.py", "file_name": "reader.py"}, "__type__": "Document"}, "82a86f061d187eb61fee194b2b8aa7c6d5b8930e": {"text": "\"\"\"Weaviate utils.\"\"\"\n\nfrom typing import Any, Dict, List, Set, cast\n\nfrom gpt_index.utils import get_new_int_id\n\nDEFAULT_CLASS_PREFIX_STUB = \"Gpt_Index\"\n\n\ndef get_default_class_prefix(current_id_set: Set = set()) -> str:\n    \"\"\"Get default class prefix.\"\"\"\n    return DEFAULT_CLASS_PREFIX_STUB + \"_\" + str(get_new_int_id(current_id_set))\n\n\ndef validate_client(client: Any) -> None:\n    \"\"\"Validate client and import weaviate library.\"\"\"\n    try:\n        import weaviate  # noqa: F401\n        from weaviate import Client\n\n        client = cast(Client, client)\n    except ImportError:\n        raise ValueError(\n            \"Weaviate is not installed. \"\n            \"Please install it with `pip install weaviate-client`.\"\n        )\n    cast(Client, client)\n\n\ndef parse_get_response(response: Dict) -> Dict:\n    \"\"\"Parse get response from Weaviate.\"\"\"\n    if \"errors\" in response:\n        raise ValueError(\"Invalid query, got errors: {}\".format(response[\"errors\"]))\n    data_response = response[\"data\"]\n    if \"Get\" not in data_response:\n        raise ValueError(\"Invalid query response, must be a Get query.\")\n\n    return data_response[\"Get\"]\n\n\ndef get_by_id(\n    client: Any, object_id: str, class_name: str, properties: List[str]\n) -> Dict:\n    \"\"\"Get response by id from Weaviate.\"\"\"\n    validate_client(client)\n\n    where_filter = {\"path\": [\"id\"], \"operator\": \"Equal\", \"valueString\": object_id}\n    query_result = (\n        client.query.get(class_name, properties)\n        .with_where(where_filter)\n        .with_additional([\"id\", \"vector\"])\n        .do()\n    )\n\n    parsed_result = parse_get_response(query_result)\n    entries = parsed_result[class_name]\n    if len(entries) == 0:\n        raise ValueError(\"No entry found for the given id\")\n    elif len(entries) > 1:\n        raise ValueError(\"More than one entry found for the given id\")\n    return entries[0]\n", "doc_id": "82a86f061d187eb61fee194b2b8aa7c6d5b8930e", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/weaviate/utils.py", "file_name": "utils.py"}, "__type__": "Document"}, "eac2fa9f945363412e1401c12d4b4c1fb14376b1": {"text": "\"\"\"Web scraper.\"\"\"\nimport logging\nfrom typing import Any, Callable, Dict, List, Optional, Tuple\n\nfrom langchain.utilities import RequestsWrapper\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass SimpleWebPageReader(BaseReader):\n    \"\"\"Simple web page reader.\n\n    Reads pages from the web.\n\n    Args:\n        html_to_text (bool): Whether to convert HTML to text.\n            Requires `html2text` package.\n\n    \"\"\"\n\n    def __init__(self, html_to_text: bool = False) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            import html2text  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`html2text` package not found, please run `pip install html2text`\"\n            )\n        self._html_to_text = html_to_text\n\n    def load_data(self, urls: List[str]) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            urls (List[str]): List of URLs to scrape.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        if not isinstance(urls, list):\n            raise ValueError(\"urls must be a list of strings.\")\n        requests = RequestsWrapper()\n        documents = []\n        for url in urls:\n            response = requests.run(url)\n            if self._html_to_text:\n                import html2text\n\n                response = html2text.html2text(response)\n\n            documents.append(Document(response))\n\n        return documents\n\n\nclass TrafilaturaWebReader(BaseReader):\n    \"\"\"Trafilatura web page reader.\n\n    Reads pages from the web.\n    Requires the `trafilatura` package.\n\n    \"\"\"\n\n    def __init__(self, error_on_missing: bool = False) -> None:\n        \"\"\"Initialize with parameters.\n\n        Args:\n            error_on_missing (bool): Throw an error when data cannot be parsed\n        \"\"\"\n        self.error_on_missing = error_on_missing\n        try:\n            import trafilatura  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`trafilatura` package not found, please run `pip install trafilatura`\"\n            )\n\n    def load_data(self, urls: List[str]) -> List[Document]:\n        \"\"\"Load data from the urls.\n\n        Args:\n            urls (List[str]): List of URLs to scrape.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        import trafilatura\n\n        if not isinstance(urls, list):\n            raise ValueError(\"urls must be a list of strings.\")\n        documents = []\n        for url in urls:\n            downloaded = trafilatura.fetch_url(url)\n            if not downloaded:\n                if self.error_on_missing:\n                    raise ValueError(f\"Trafilatura fails to get string from url: {url}\")\n                continue\n            response = trafilatura.extract(downloaded)\n            if not response:\n                if self.error_on_missing:\n                    raise ValueError(f\"Trafilatura fails to parse page: {url}\")\n                continue\n            documents.append(Document(response))\n\n        return documents\n\n\ndef _substack_reader(soup: Any) -> Tuple[str, Dict[str, Any]]:\n    \"\"\"Extract text from Substack blog post.\"\"\"\n    extra_info = {\n        \"Title of this Substack post\": soup.select_one(\"h1.post-title\").getText(),\n        \"Subtitle\": soup.select_one(\"h3.subtitle\").getText(),\n        \"Author\": soup.select_one(\"span.byline-names\").getText(),\n    }\n    text = soup.select_one(\"div.available-content\").getText()\n    return text, extra_info\n\n\nDEFAULT_WEBSITE_EXTRACTOR: Dict[str, Callable[[Any], Tuple[str, Dict[str, Any]]]] = {\n    \"substack.com\": _substack_reader,\n}\n\n\nclass BeautifulSoupWebReader(BaseReader):\n    \"\"\"BeautifulSoup web page reader.\n\n    Reads pages from the web.\n    Requires the `bs4` and `urllib` packages.\n\n    Args:\n        file_extractor (Optional[Dict[str, Callable]]): A mapping of website\n            hostname (e.g. google.com) to a function that specifies how to\n            extract text from the BeautifulSoup obj. See DEFAULT_WEBSITE_EXTRACTOR.\n    \"\"\"\n\n    def __init__(\n        self,\n        website_extractor: Optional[Dict[str, Callable]] = None,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            from urllib.parse import urlparse  # noqa: F401\n\n            import requests  # noqa: F401\n            from bs4 import BeautifulSoup  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`bs4`, `requests`, and `urllib` must be installed to scrape websites.\"\n                \"Please run `pip install bs4 requests urllib`.\"\n            )\n\n        self.website_extractor = website_extractor or DEFAULT_WEBSITE_EXTRACTOR\n\n    def load_data(\n        self, urls: List[str], custom_hostname: Optional[str] = None\n    ) -> List[Document]:\n        \"\"\"Load data from the urls.\n\n        Args:\n            urls (List[str]): List of URLs to scrape.\n            custom_hostname (Optional[str]): Force a certain hostname in the case\n                a website is displayed under custom URLs (e.g. Substack blogs)\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        from urllib.parse import urlparse\n\n        import requests\n        from bs4 import BeautifulSoup\n\n        documents = []\n        for url in urls:\n            try:\n                page = requests.get(url)\n            except Exception:\n                raise ValueError(f\"One of the inputs is not a valid url: {url}\")\n\n            hostname = custom_hostname or urlparse(url).hostname or \"\"\n\n            soup = BeautifulSoup(page.content, \"html.parser\")\n\n            data = \"\"\n            extra_info = {\"URL\": url}\n            if hostname in self.website_extractor:\n                data, metadata = self.website_extractor[hostname](soup)\n                extra_info.update(metadata)\n            else:\n                data = soup.getText()\n\n            documents.append(Document(data, extra_info=extra_info))\n\n        return documents\n\n\nclass RssReader(BaseReader):\n    \"\"\"RSS reader.\n\n    Reads content from an RSS feed.\n\n    \"\"\"\n\n    def __init__(self, html_to_text: bool = False) -> None:\n        \"\"\"Initialize with parameters.\n\n        Args:\n            html_to_text (bool): Whether to convert HTML to text.\n                Requires `html2text` package.\n\n        \"\"\"\n        try:\n            import feedparser  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`feedparser` package not found, please run `pip install feedparser`\"\n            )\n\n        if html_to_text:\n            try:\n                import html2text  # noqa: F401\n            except ImportError:\n                raise ValueError(\n                    \"`html2text` package not found, please run `pip install html2text`\"\n                )\n        self._html_to_text = html_to_text\n\n    def load_data(self, urls: List[str]) -> List[Document]:\n        \"\"\"Load data from RSS feeds.\n\n        Args:\n            urls (List[str]): List of RSS URLs to load.\n\n        Returns:\n            List[Document]: List of documents.\n\n        \"\"\"\n        import feedparser\n\n        if not isinstance(urls, list):\n            raise ValueError(\"urls must be a list of strings.\")\n\n        documents = []\n\n        for url in urls:\n            parsed = feedparser.parse(url)\n            for entry in parsed.entries:\n                if entry.content:\n                    data = entry.content[0].value\n                else:\n                    data = entry.description or entry.summary\n\n                if self._html_to_text:\n                    import html2text\n\n                    data = html2text.html2text(data)\n\n                extra_info = {\"title\": entry.title, \"link\": entry.link}\n                documents.append(Document(data, extra_info=extra_info))\n\n        return documents\n\n\nif __name__ == \"__main__\":\n    reader = SimpleWebPageReader()\n    logging.info(reader.load_data([\"http://www.google.com\"]))\n", "doc_id": "eac2fa9f945363412e1401c12d4b4c1fb14376b1", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/web.py", "file_name": "web.py"}, "__type__": "Document"}, "55400c6722fc856cbb287cec4987d87118429fb1": {"text": "\"\"\"Simple reader that reads wikipedia.\"\"\"\nfrom typing import Any, List\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass WikipediaReader(BaseReader):\n    \"\"\"Wikipedia reader.\n\n    Reads a page.\n\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            import wikipedia  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"`wikipedia` package not found, please run `pip install wikipedia`\"\n            )\n\n    def load_data(self, pages: List[str], **load_kwargs: Any) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            pages (List[str]): List of pages to read.\n\n        \"\"\"\n        import wikipedia\n\n        results = []\n        for page in pages:\n            page_content = wikipedia.page(page, **load_kwargs).content\n            results.append(Document(page_content))\n        return results\n", "doc_id": "55400c6722fc856cbb287cec4987d87118429fb1", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/wikipedia.py", "file_name": "wikipedia.py"}, "__type__": "Document"}, "969ca38b79231bd668d28e6cdc0cbc59a2f3745d": {"text": "\"\"\"Simple Reader that reads transcript of youtube video.\"\"\"\nfrom typing import Any, List\n\nfrom gpt_index.readers.base import BaseReader\nfrom gpt_index.readers.schema.base import Document\n\n\nclass YoutubeTranscriptReader(BaseReader):\n    \"\"\"Youtube Transcript reader.\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n\n    def load_data(self, ytlinks: List[str], **load_kwargs: Any) -> List[Document]:\n        \"\"\"Load data from the input directory.\n\n        Args:\n            pages (List[str]): List of youtube links \\\n                for which transcripts are to be read.\n\n        \"\"\"\n        try:\n            from youtube_transcript_api import YouTubeTranscriptApi\n        except ImportError:\n            raise ValueError(\n                \"`youtube_transcript_api` package not found, \\\n                    please run `pip install youtube-transcript-api`\"\n            )\n\n        results = []\n        for link in ytlinks:\n            video_id = link.split(\"?v=\")[-1]\n            srt = YouTubeTranscriptApi.get_transcript(video_id)\n            transcript = \"\"\n            for chunk in srt:\n                transcript = transcript + chunk[\"text\"] + \"\\n\"\n            results.append(Document(transcript))\n        return results\n", "doc_id": "969ca38b79231bd668d28e6cdc0cbc59a2f3745d", "embedding": null, "extra_info": {"file_path": "gpt_index/readers/youtube_transcript.py", "file_name": "youtube_transcript.py"}, "__type__": "Document"}, "fd2d3c7be5495fb5a7305634c05756b03290929d": {"text": "\"\"\"Response schema.\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional\n\nfrom dataclasses_json import DataClassJsonMixin\n\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.indices.utils import truncate_text\n\n\n@dataclass\nclass SourceNode(DataClassJsonMixin):\n    \"\"\"Source node.\n\n    User-facing class containing the source text and the corresponding document id.\n\n    \"\"\"\n\n    source_text: str\n    doc_id: Optional[str]\n    extra_info: Optional[Dict[str, Any]] = None\n    node_info: Optional[Dict[str, Any]] = None\n\n    # distance score between node and query, if applicable\n    similarity: Optional[float] = None\n\n    @classmethod\n    def from_node(cls, node: Node, similarity: Optional[float] = None) -> \"SourceNode\":\n        \"\"\"Create a SourceNode from a Node.\"\"\"\n        return cls(\n            source_text=node.get_text(),\n            doc_id=node.ref_doc_id,\n            extra_info=node.extra_info,\n            node_info=node.node_info,\n            similarity=similarity,\n        )\n\n    @classmethod\n    def from_nodes(cls, nodes: List[Node]) -> List[\"SourceNode\"]:\n        \"\"\"Create a list of SourceNodes from a list of Nodes.\"\"\"\n        return [cls.from_node(node) for node in nodes]\n\n\n@dataclass\nclass Response:\n    \"\"\"Response.\n\n    Attributes:\n        response: The response text.\n\n    \"\"\"\n\n    response: Optional[str]\n    source_nodes: List[SourceNode] = field(default_factory=list)\n    extra_info: Optional[Dict[str, Any]] = None\n\n    def __str__(self) -> str:\n        \"\"\"Convert to string representation.\"\"\"\n        return self.response or \"None\"\n\n    def get_formatted_sources(self, length: int = 100) -> str:\n        \"\"\"Get formatted sources text.\"\"\"\n        texts = []\n        for source_node in self.source_nodes:\n            fmt_text_chunk = truncate_text(source_node.source_text, length)\n            doc_id = source_node.doc_id or \"None\"\n            source_text = f\"> Source (Doc id: {doc_id}): {fmt_text_chunk}\"\n            texts.append(source_text)\n        return \"\\n\\n\".join(texts)\n", "doc_id": "fd2d3c7be5495fb5a7305634c05756b03290929d", "embedding": null, "extra_info": {"file_path": "gpt_index/response/schema.py", "file_name": "schema.py"}, "__type__": "Document"}, "ec467e5a74f81e11b21a7965c94c1986708445d6": {"text": "\"\"\"Base schema for data structures.\"\"\"\nfrom abc import abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional\n\nfrom dataclasses_json import DataClassJsonMixin\n\n\n@dataclass\nclass BaseDocument(DataClassJsonMixin):\n    \"\"\"Base document.\n\n    Generic abstract interfaces that captures both index structs\n    as well as documents.\n\n    \"\"\"\n\n    # TODO: consolidate fields from Document/IndexStruct into base class\n    text: Optional[str] = None\n    doc_id: Optional[str] = None\n    embedding: Optional[List[float]] = None\n\n    # extra fields\n    extra_info: Optional[Dict[str, Any]] = None\n\n    @classmethod\n    @abstractmethod\n    def get_type(cls) -> str:\n        \"\"\"Get Document type.\"\"\"\n\n    def get_text(self) -> str:\n        \"\"\"Get text.\"\"\"\n        if self.text is None:\n            raise ValueError(\"text field not set.\")\n        return self.text\n\n    def get_doc_id(self) -> str:\n        \"\"\"Get doc_id.\"\"\"\n        if self.doc_id is None:\n            raise ValueError(\"doc_id not set.\")\n        return self.doc_id\n\n    @property\n    def is_doc_id_none(self) -> bool:\n        \"\"\"Check if doc_id is None.\"\"\"\n        return self.doc_id is None\n\n    def get_embedding(self) -> List[float]:\n        \"\"\"Get embedding.\n\n        Errors if embedding is None.\n\n        \"\"\"\n        if self.embedding is None:\n            raise ValueError(\"embedding not set.\")\n        return self.embedding\n\n    @property\n    def extra_info_str(self) -> Optional[str]:\n        \"\"\"Extra info string.\"\"\"\n        if self.extra_info is None:\n            return None\n\n        return \"\\n\".join([f\"{k}: {str(v)}\" for k, v in self.extra_info.items()])\n", "doc_id": "ec467e5a74f81e11b21a7965c94c1986708445d6", "embedding": null, "extra_info": {"file_path": "gpt_index/schema.py", "file_name": "schema.py"}, "__type__": "Document"}, "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d": {"text": "\"\"\"Mock chain wrapper.\"\"\"\n\nfrom typing import Any, Dict\n\nfrom gpt_index.constants import NUM_OUTPUTS\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.prompts.prompt_type import PromptType\nfrom gpt_index.token_counter.utils import mock_extract_keywords_response\nfrom gpt_index.utils import globals_helper\n\n# TODO: consolidate with unit tests in tests/mock_utils/mock_predict.py\n\n\ndef _mock_summary_predict(max_tokens: int, prompt_args: Dict) -> str:\n    \"\"\"Mock summary predict.\"\"\"\n    # tokens in response shouldn't be larger than tokens in `context_str`\n    num_text_tokens = len(globals_helper.tokenizer(prompt_args[\"context_str\"]))\n    token_limit = min(num_text_tokens, max_tokens)\n    return \" \".join([\"summary\"] * token_limit)\n\n\ndef _mock_insert_predict() -> str:\n    \"\"\"Mock insert predict.\"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_query_select() -> str:\n    \"\"\"Mock query select.\"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_query_select_multiple(num_chunks: int) -> str:\n    \"\"\"Mock query select.\"\"\"\n    nums_str = \", \".join([str(i) for i in range(num_chunks)])\n    return f\"ANSWER: {nums_str}\"\n\n\ndef _mock_answer(max_tokens: int, prompt_args: Dict) -> str:\n    \"\"\"Mock answer.\"\"\"\n    # tokens in response shouldn't be larger than tokens in `text`\n    num_ctx_tokens = len(globals_helper.tokenizer(prompt_args[\"context_str\"]))\n    token_limit = min(num_ctx_tokens, max_tokens)\n    return \" \".join([\"answer\"] * token_limit)\n\n\ndef _mock_refine(max_tokens: int, prompt: Prompt, prompt_args: Dict) -> str:\n    \"\"\"Mock refine.\"\"\"\n    # tokens in response shouldn't be larger than tokens in\n    # `existing_answer` + `context_msg`\n    # NOTE: if existing_answer is not in prompt_args, we need to get it from the prompt\n    if \"existing_answer\" not in prompt_args:\n        existing_answer = prompt.partial_dict[\"existing_answer\"]\n    else:\n        existing_answer = prompt_args[\"existing_answer\"]\n    num_ctx_tokens = len(globals_helper.tokenizer(prompt_args[\"context_msg\"]))\n    num_exist_tokens = len(globals_helper.tokenizer(existing_answer))\n    token_limit = min(num_ctx_tokens + num_exist_tokens, max_tokens)\n    return \" \".join([\"answer\"] * token_limit)\n\n\ndef _mock_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"text\"])\n\n\ndef _mock_query_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock query keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"question\"])\n\n\nclass MockLLMPredictor(LLMPredictor):\n    \"\"\"Mock LLM Predictor.\"\"\"\n\n    def __init__(self, max_tokens: int = NUM_OUTPUTS) -> None:\n        \"\"\"Initialize params.\"\"\"\n        # NOTE: don't call super, we don't want to instantiate LLM\n        self.max_tokens = max_tokens\n        self._total_tokens_used = 0\n        self.flag = True\n        self._last_token_usage = None\n\n    def _predict(self, prompt: Prompt, **prompt_args: Any) -> str:\n        \"\"\"Mock predict.\"\"\"\n        prompt_str = prompt.prompt_type\n        if prompt_str == PromptType.SUMMARY:\n            return _mock_summary_predict(self.max_tokens, prompt_args)\n        elif prompt_str == PromptType.TREE_INSERT:\n            return _mock_insert_predict()\n        elif prompt_str == PromptType.TREE_SELECT:\n            return _mock_query_select()\n        elif prompt_str == PromptType.TREE_SELECT_MULTIPLE:\n            return _mock_query_select_multiple(prompt_args[\"num_chunks\"])\n        elif prompt_str == PromptType.REFINE:\n            return _mock_refine(self.max_tokens, prompt, prompt_args)\n        elif prompt_str == PromptType.QUESTION_ANSWER:\n            return _mock_answer(self.max_tokens, prompt_args)\n        elif prompt_str == PromptType.KEYWORD_EXTRACT:\n            return _mock_keyword_extract(prompt_args)\n        elif prompt_str == PromptType.QUERY_KEYWORD_EXTRACT:\n            return _mock_query_keyword_extract(prompt_args)\n        else:\n            raise ValueError(\"Invalid prompt type.\")\n", "doc_id": "7ef6e3539ca453e9aac62fb2b29a5fc8497fd37d", "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_chain_wrapper.py", "file_name": "mock_chain_wrapper.py"}, "__type__": "Document"}, "9ac4e9006efa30143675c5320e66fe9c1c7bf4a4": {"text": "\"\"\"Mock embedding model.\"\"\"\n\nfrom typing import Any, List\n\nfrom gpt_index.embeddings.base import BaseEmbedding\n\n\nclass MockEmbedding(BaseEmbedding):\n    \"\"\"Mock embedding.\n\n    Used for token prediction.\n\n    Args:\n        embed_dim (int): embedding dimension\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.embed_dim = embed_dim\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding.\"\"\"\n        return [0.5] * self.embed_dim\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get text embedding.\"\"\"\n        return [0.5] * self.embed_dim\n", "doc_id": "9ac4e9006efa30143675c5320e66fe9c1c7bf4a4", "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/mock_embed_model.py", "file_name": "mock_embed_model.py"}, "__type__": "Document"}, "ee4b9ac3b34abb65842a80d4ae2390733c1542c1": {"text": "\"\"\"Token counter function.\"\"\"\n\nimport logging\nfrom typing import Any, Callable, cast\n\nfrom gpt_index.embeddings.base import BaseEmbedding\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\n\n\ndef llm_token_counter(method_name_str: str) -> Callable:\n    \"\"\"\n    Use this as a decorator for methods in index/query classes that make calls to LLMs.\n\n    At the moment, this decorator can only be used on class instance methods with a\n    `_llm_predictor` attribute.\n\n    Do not use this on abstract methods.\n\n    For example, consider the class below:\n        .. code-block:: python\n            class GPTTreeIndexBuilder:\n            ...\n            @llm_token_counter(\"build_from_text\")\n            def build_from_text(self, documents: Sequence[BaseDocument]) -> IndexGraph:\n                ...\n\n    If you run `build_from_text()`, it will print the output in the form below:\n\n    ```\n    [build_from_text] Total token usage: <some-number> tokens\n    ```\n    \"\"\"\n\n    def wrap(f: Callable) -> Callable:\n        def wrapped_llm_predict(_self: Any, *args: Any, **kwargs: Any) -> Any:\n            llm_predictor = getattr(_self, \"_llm_predictor\", None)\n            if llm_predictor is None:\n                raise ValueError(\n                    \"Cannot use llm_token_counter on an instance \"\n                    \"without a _llm_predictor attribute.\"\n                )\n            llm_predictor = cast(LLMPredictor, llm_predictor)\n\n            embed_model = getattr(_self, \"_embed_model\", None)\n            if embed_model is None:\n                raise ValueError(\n                    \"Cannot use llm_token_counter on an instance \"\n                    \"without a _embed_model attribute.\"\n                )\n            embed_model = cast(BaseEmbedding, embed_model)\n\n            start_token_ct = llm_predictor.total_tokens_used\n            start_embed_token_ct = embed_model.total_tokens_used\n\n            f_return_val = f(_self, *args, **kwargs)\n\n            net_tokens = llm_predictor.total_tokens_used - start_token_ct\n            llm_predictor.last_token_usage = net_tokens\n            net_embed_tokens = embed_model.total_tokens_used - start_embed_token_ct\n            embed_model.last_token_usage = net_embed_tokens\n\n            # print outputs\n            logging.info(\n                f\"> [{method_name_str}] Total LLM token usage: {net_tokens} tokens\"\n            )\n            logging.info(\n                f\"> [{method_name_str}] Total embedding token usage: \"\n                f\"{net_embed_tokens} tokens\"\n            )\n\n            return f_return_val\n\n        return wrapped_llm_predict\n\n    return wrap\n", "doc_id": "ee4b9ac3b34abb65842a80d4ae2390733c1542c1", "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/token_counter.py", "file_name": "token_counter.py"}, "__type__": "Document"}, "fe6d373401194b3dab89e747b4126550d5204a27": {"text": "\"\"\"Token predictor utils.\"\"\"\nfrom typing import Optional\n\nfrom gpt_index.indices.keyword_table.utils import simple_extract_keywords\n\n\ndef mock_extract_keywords_response(\n    text_chunk: str, max_keywords: Optional[int] = None, filter_stopwords: bool = True\n) -> str:\n    \"\"\"Extract keywords mock response.\n\n    Same as simple_extract_keywords but without filtering stopwords.\n\n    \"\"\"\n    return \",\".join(\n        simple_extract_keywords(\n            text_chunk, max_keywords=max_keywords, filter_stopwords=False\n        )\n    )\n", "doc_id": "fe6d373401194b3dab89e747b4126550d5204a27", "embedding": null, "extra_info": {"file_path": "gpt_index/token_counter/utils.py", "file_name": "utils.py"}, "__type__": "Document"}, "c070558fc2f2babc0f4f950c573000ba295a80c7": {"text": "\"\"\"General utils functions.\"\"\"\n\nimport random\nimport sys\nimport time\nimport traceback\nimport uuid\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, Generator, List, Optional, Set, Type, cast\n\nimport nltk\n\n\nclass GlobalsHelper:\n    \"\"\"Helper to retrieve globals.\n\n    Helpful for global caching of certain variables that can be expensive to load.\n    (e.g. tokenization)\n\n    \"\"\"\n\n    _tokenizer: Optional[Callable[[str], List]] = None\n    _stopwords: Optional[List[str]] = None\n\n    @property\n    def tokenizer(self) -> Callable[[str], List]:\n        \"\"\"Get tokenizer.\"\"\"\n        if self._tokenizer is None:\n            # if python version >= 3.9, then use tiktoken\n            # else use GPT2TokenizerFast\n            if sys.version_info >= (3, 9):\n                tiktoken_import_err = (\n                    \"`tiktoken` package not found, please run `pip install tiktoken`\"\n                )\n                try:\n                    import tiktoken\n                except ImportError:\n                    raise ValueError(tiktoken_import_err)\n                enc = tiktoken.get_encoding(\"gpt2\")\n                self._tokenizer = cast(Callable[[str], List], enc.encode)\n            else:\n                import transformers\n\n                tokenizer = transformers.GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\n                def tokenizer_fn(text: str) -> List:\n                    return tokenizer(text)[\"input_ids\"]\n\n                self._tokenizer = tokenizer_fn\n        return self._tokenizer\n\n    @property\n    def stopwords(self) -> List[str]:\n        \"\"\"Get stopwords.\"\"\"\n        if self._stopwords is None:\n            try:\n                from nltk.corpus import stopwords\n            except ImportError:\n                raise ValueError(\n                    \"`nltk` package not found, please run `pip install nltk`\"\n                )\n            nltk.download(\"stopwords\")\n            self._stopwords = stopwords.words(\"english\")\n        return self._stopwords\n\n\nglobals_helper = GlobalsHelper()\n\n\ndef get_new_id(d: Set) -> str:\n    \"\"\"Get a new ID.\"\"\"\n    while True:\n        new_id = str(uuid.uuid4())\n        if new_id not in d:\n            break\n    return new_id\n\n\ndef get_new_int_id(d: Set) -> int:\n    \"\"\"Get a new integer ID.\"\"\"\n    while True:\n        new_id = random.randint(0, sys.maxsize)\n        if new_id not in d:\n            break\n    return new_id\n\n\n@contextmanager\ndef temp_set_attrs(obj: Any, **kwargs: Any) -> Generator:\n    \"\"\"Temporary setter.\n\n    Utility class for setting a temporary value for an attribute on a class.\n    Taken from: https://tinyurl.com/2p89xymh\n\n    \"\"\"\n    prev_values = {k: getattr(obj, k) for k in kwargs}\n    for k, v in kwargs.items():\n        setattr(obj, k, v)\n    try:\n        yield\n    finally:\n        for k, v in prev_values.items():\n            setattr(obj, k, v)\n\n\n@dataclass\nclass ErrorToRetry:\n    \"\"\"Exception types that should be retried.\n\n    Args:\n        exception_cls (Type[Exception]): Class of exception.\n        check_fn (Optional[Callable[[Any]], bool]]):\n            A function that takes an exception instance as input and returns\n            whether to retry.\n\n    \"\"\"\n\n    exception_cls: Type[Exception]\n    check_fn: Optional[Callable[[Any], bool]] = None\n\n\ndef retry_on_exceptions_with_backoff(\n    lambda_fn: Callable,\n    errors_to_retry: List[ErrorToRetry],\n    max_tries: int = 10,\n    min_backoff_secs: float = 0.5,\n    max_backoff_secs: float = 60.0,\n) -> Any:\n    \"\"\"Execute lambda function with retries and exponential backoff.\n\n    Args:\n        lambda_fn (Callable): Function to be called and output we want.\n        errors_to_retry (List[ErrorToRetry]): List of errors to retry.\n            At least one needs to be provided.\n        max_tries (int): Maximum number of tries, including the first. Defaults to 10.\n        min_backoff_secs (float): Minimum amount of backoff time between attempts.\n            Defaults to 0.5.\n        max_backoff_secs (float): Maximum amount of backoff time between attempts.\n            Defaults to 60.\n\n    \"\"\"\n    if not errors_to_retry:\n        raise ValueError(\"At least one error to retry needs to be provided\")\n\n    error_checks = {\n        error_to_retry.exception_cls: error_to_retry.check_fn\n        for error_to_retry in errors_to_retry\n    }\n    exception_class_tuples = tuple(error_checks.keys())\n\n    backoff_secs = min_backoff_secs\n    tries = 0\n\n    while True:\n        try:\n            return lambda_fn()\n        except exception_class_tuples as e:\n            traceback.print_exc()\n            tries += 1\n            if tries >= max_tries:\n                raise\n            check_fn = error_checks.get(e.__class__)\n            if check_fn and not check_fn(e):\n                raise\n            time.sleep(backoff_secs)\n            backoff_secs = min(backoff_secs * 2, max_backoff_secs)\n", "doc_id": "c070558fc2f2babc0f4f950c573000ba295a80c7", "embedding": null, "extra_info": {"file_path": "gpt_index/utils.py", "file_name": "utils.py"}, "__type__": "Document"}, "b545d6ada302f1b4f43a714dd4063b52e0cbe0a7": {"text": "[tool.isort]\nprofile = \"black\"\n\n[tool.mypy]\nignore_missing_imports = \"True\"\ndisallow_untyped_defs = \"True\"\nexclude = [\"notebooks\", \"build\"]", "doc_id": "b545d6ada302f1b4f43a714dd4063b52e0cbe0a7", "embedding": null, "extra_info": {"file_path": "pyproject.toml", "file_name": "pyproject.toml"}, "__type__": "Document"}, "11ee47eb5f86f10cdda33aef7dcba23112bf81f2": {"text": "-e .\n\n# For testing\npytest==7.2.1\npytest-dotenv==0.5.2\n\n# third-party (libraries)\nrake_nltk==1.0.6\nipython==8.9.0\n\n# linting stubs\ntypes-requests==2.28.11.8\ntypes-setuptools==67.1.0.0\n\n# linting\nblack==22.12.0\nisort==5.11.4\nmypy==0.991\nflake8==6.0.0\nflake8-docstrings==1.6.0\npylint==2.15.10\n", "doc_id": "11ee47eb5f86f10cdda33aef7dcba23112bf81f2", "embedding": null, "extra_info": {"file_path": "requirements.txt", "file_name": "requirements.txt"}, "__type__": "Document"}, "626dbe2923cb290cec18bc3e102f684c196abd4a": {"text": "\"\"\"Set up the package.\"\"\"\nimport sys\nfrom pathlib import Path\n\nfrom setuptools import find_packages, setup\n\nwith open(Path(__file__).absolute().parents[0] / \"gpt_index\" / \"VERSION\") as _f:\n    __version__ = _f.read().strip()\n\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as f:\n    long_description = f.read()\n\ninstall_requires = [\n    \"langchain\",\n    \"openai>=0.26.4\",\n    \"dataclasses_json\",\n    \"transformers\",\n    \"nltk\",\n    \"numpy\",\n    \"tenacity<8.2.0\",\n    \"pandas\",\n]\n\n# NOTE: if python version >= 3.9, install tiktoken\nif sys.version_info >= (3, 9):\n    install_requires.extend([\"tiktoken\"])\n\nsetup(\n    name=\"gpt_index\",\n    version=__version__,\n    packages=find_packages(),\n    description=\"Building an index of GPT summaries.\",\n    install_requires=install_requires,\n    long_description=long_description,\n    license=\"MIT\",\n    url=\"https://github.com/jerryjliu/gpt_index\",\n    include_package_data=True,\n    long_description_content_type=\"text/markdown\",\n)\n", "doc_id": "626dbe2923cb290cec18bc3e102f684c196abd4a", "embedding": null, "extra_info": {"file_path": "setup.py", "file_name": "setup.py"}, "__type__": "Document"}, "6aea22e046e0a392ed78df85b9cb73e44e66fbbb": {"text": "\"\"\"Test embedding functionalities.\"\"\"\n\nfrom collections import defaultdict\nfrom typing import Any, Dict, List, Tuple\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.embeddings.openai import OpenAIEmbedding\nfrom gpt_index.indices.query.tree.embedding_query import GPTTreeIndexEmbeddingQuery\nfrom gpt_index.indices.tree.base import GPTTreeIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import (\n    LLMChain,\n    LLMMetadata,\n    LLMPredictor,\n)\nfrom gpt_index.readers.schema.base import Document\nfrom tests.mock_utils.mock_decorator import patch_common\nfrom tests.mock_utils.mock_predict import mock_llmchain_predict\nfrom tests.mock_utils.mock_prompts import (\n    MOCK_INSERT_PROMPT,\n    MOCK_QUERY_PROMPT,\n    MOCK_REFINE_PROMPT,\n    MOCK_SUMMARY_PROMPT,\n    MOCK_TEXT_QA_PROMPT,\n)\n\n\ndef test_embedding_similarity() -> None:\n    \"\"\"Test embedding similarity.\"\"\"\n    embed_model = OpenAIEmbedding()\n    text_embedding = [3.0, 4.0, 0.0]\n    query_embedding = [0.0, 1.0, 0.0]\n    cosine = embed_model.similarity(query_embedding, text_embedding)\n    assert cosine == 0.8\n\n\n@pytest.fixture\ndef struct_kwargs() -> Tuple[Dict, Dict]:\n    \"\"\"Index kwargs.\"\"\"\n    index_kwargs = {\n        \"summary_template\": MOCK_SUMMARY_PROMPT,\n        \"insert_prompt\": MOCK_INSERT_PROMPT,\n        \"num_children\": 2,\n    }\n    query_kwargs = {\n        \"query_template\": MOCK_QUERY_PROMPT,\n        \"text_qa_template\": MOCK_TEXT_QA_PROMPT,\n        \"refine_template\": MOCK_REFINE_PROMPT,\n    }\n    return index_kwargs, query_kwargs\n\n\n@pytest.fixture\ndef documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"\n    # NOTE: one document for now\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    return [Document(doc_text)]\n\n\ndef _get_node_text_embedding_similarities(\n    query_embedding: List[float], nodes: List[Node]\n) -> List[float]:\n    \"\"\"Get node text embedding similarity.\"\"\"\n    text_similarity_map = defaultdict(lambda: 0.0)\n    text_similarity_map[\"Hello world.\"] = 0.9\n    text_similarity_map[\"This is a test.\"] = 0.8\n    text_similarity_map[\"This is another test.\"] = 0.7\n    text_similarity_map[\"This is a test v2.\"] = 0.6\n\n    similarities = []\n    for node in nodes:\n        similarities.append(text_similarity_map[node.get_text()])\n\n    return similarities\n\n\n@patch_common\n@patch.object(\n    GPTTreeIndexEmbeddingQuery,\n    \"_get_query_text_embedding_similarities\",\n    side_effect=_get_node_text_embedding_similarities,\n)\ndef test_embedding_query(\n    _mock_similarity: Any,\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    struct_kwargs: Dict,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test embedding query.\"\"\"\n    index_kwargs, query_kwargs = struct_kwargs\n    tree = GPTTreeIndex(documents, **index_kwargs)\n\n    # test embedding query\n    query_str = \"What is?\"\n    response = tree.query(query_str, mode=\"embedding\", **query_kwargs)\n    assert str(response) == (\"What is?:Hello world.\")\n\n\n@patch.object(LLMChain, \"predict\", side_effect=mock_llmchain_predict)\n@patch(\"gpt_index.langchain_helpers.chain_wrapper.OpenAI\")\n@patch.object(LLMPredictor, \"get_llm_metadata\", return_value=LLMMetadata())\n@patch.object(LLMChain, \"__init__\", return_value=None)\n@patch.object(\n    GPTTreeIndexEmbeddingQuery,\n    \"_get_query_text_embedding_similarities\",\n    side_effect=_get_node_text_embedding_similarities,\n)\ndef test_query_and_count_tokens(\n    _mock_similarity: Any,\n    _mock_llmchain: Any,\n    _mock_llm_metadata: Any,\n    _mock_init: Any,\n    _mock_predict: Any,\n    struct_kwargs: Dict,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test query and count tokens.\"\"\"\n    index_kwargs, query_kwargs = struct_kwargs\n    # mock_prompts.MOCK_SUMMARY_PROMPT_TMPL adds a \"\\n\" to the document text\n    document_token_count = 24\n    llmchain_mock_resp_token_count = 10\n    # build the tree\n    tree = GPTTreeIndex(documents, **index_kwargs)\n    assert (\n        tree._llm_predictor.total_tokens_used\n        == document_token_count + llmchain_mock_resp_token_count\n    )\n\n    # test embedding query\n    start_token_ct = tree._llm_predictor.total_tokens_used\n    query_str = \"What is?\"\n    # From MOCK_TEXT_QA_PROMPT, the prompt is 28 total\n    query_prompt_token_count = 28\n    tree.query(query_str, mode=\"embedding\", **query_kwargs)\n    assert (\n        tree._llm_predictor.total_tokens_used - start_token_ct\n        == query_prompt_token_count + llmchain_mock_resp_token_count\n    )\n", "doc_id": "6aea22e046e0a392ed78df85b9cb73e44e66fbbb", "embedding": null, "extra_info": {"file_path": "tests/indices/embedding/test_base.py", "file_name": "test_base.py"}, "__type__": "Document"}, "3766f7eb2961e7918fad3622ac9b52bc211a5c03": {"text": "\"\"\"Test keyword table index.\"\"\"\n\nfrom typing import Any, List\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom gpt_index.indices.keyword_table.simple_base import GPTSimpleKeywordTableIndex\nfrom gpt_index.readers.schema.base import Document\nfrom tests.mock_utils.mock_decorator import patch_common\nfrom tests.mock_utils.mock_utils import mock_extract_keywords\n\n\n@pytest.fixture\ndef documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"\n    # NOTE: one document for now\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    return [Document(doc_text)]\n\n\n@patch_common\n@patch(\n    \"gpt_index.indices.keyword_table.simple_base.simple_extract_keywords\",\n    mock_extract_keywords,\n)\ndef test_build_table(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test build table.\"\"\"\n    # test simple keyword table\n    # NOTE: here the keyword extraction isn't mocked because we're using\n    # the regex-based keyword extractor, not GPT\n    table = GPTSimpleKeywordTableIndex(documents)\n    table_chunks = {n.text for n in table.index_struct.text_chunks.values()}\n    assert len(table_chunks) == 4\n    assert \"Hello world.\" in table_chunks\n    assert \"This is a test.\" in table_chunks\n    assert \"This is another test.\" in table_chunks\n    assert \"This is a test v2.\" in table_chunks\n\n    # test that expected keys are present in table\n    # NOTE: in mock keyword extractor, stopwords are not filtered\n    assert table.index_struct.table.keys() == {\n        \"this\",\n        \"hello\",\n        \"world\",\n        \"test\",\n        \"another\",\n        \"v2\",\n        \"is\",\n        \"a\",\n        \"v2\",\n    }\n\n\n@patch_common\n@patch(\n    \"gpt_index.indices.keyword_table.simple_base.simple_extract_keywords\",\n    mock_extract_keywords,\n)\ndef test_insert(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test insert.\"\"\"\n    table = GPTSimpleKeywordTableIndex([])\n    assert len(table.index_struct.table.keys()) == 0\n    table.insert(documents[0])\n    table_chunks = {n.text for n in table.index_struct.text_chunks.values()}\n    assert \"Hello world.\" in table_chunks\n    assert \"This is a test.\" in table_chunks\n    assert \"This is another test.\" in table_chunks\n    assert \"This is a test v2.\" in table_chunks\n    # test that expected keys are present in table\n    # NOTE: in mock keyword extractor, stopwords are not filtered\n    assert table.index_struct.table.keys() == {\n        \"this\",\n        \"hello\",\n        \"world\",\n        \"test\",\n        \"another\",\n        \"v2\",\n        \"is\",\n        \"a\",\n        \"v2\",\n    }\n\n    # test insert with doc_id\n    document1 = Document(\"This is\", doc_id=\"test_id1\")\n    document2 = Document(\"test v3\", doc_id=\"test_id2\")\n    table = GPTSimpleKeywordTableIndex([])\n    table.insert(document1)\n    table.insert(document2)\n    chunk_index1_1 = list(table.index_struct.table[\"this\"])[0]\n    chunk_index1_2 = list(table.index_struct.table[\"is\"])[0]\n    chunk_index2_1 = list(table.index_struct.table[\"test\"])[0]\n    chunk_index2_2 = list(table.index_struct.table[\"v3\"])[0]\n    assert table.index_struct.text_chunks[chunk_index1_1].ref_doc_id == \"test_id1\"\n    assert table.index_struct.text_chunks[chunk_index1_2].ref_doc_id == \"test_id1\"\n    assert table.index_struct.text_chunks[chunk_index2_1].ref_doc_id == \"test_id2\"\n    assert table.index_struct.text_chunks[chunk_index2_2].ref_doc_id == \"test_id2\"\n\n\n@patch_common\n@patch(\n    \"gpt_index.indices.keyword_table.simple_base.simple_extract_keywords\",\n    mock_extract_keywords,\n)\ndef test_delete(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test insert.\"\"\"\n    new_documents = [\n        Document(\"Hello world.\\nThis is a test.\", doc_id=\"test_id_1\"),\n        Document(\"This is another test.\", doc_id=\"test_id_2\"),\n        Document(\"This is a test v2.\", doc_id=\"test_id_3\"),\n    ]\n\n    # test delete\n    table = GPTSimpleKeywordTableIndex(new_documents)\n    table.delete(\"test_id_1\")\n    assert len(table.index_struct.table.keys()) == 6\n    print(table.index_struct.table.keys())\n    assert len(table.index_struct.table[\"this\"]) == 2\n    node_texts = {n.text for n in table.index_struct.text_chunks.values()}\n    assert node_texts == {\"This is another test.\", \"This is a test v2.\"}\n\n    table = GPTSimpleKeywordTableIndex(new_documents)\n    table.delete(\"test_id_2\")\n    assert len(table.index_struct.table.keys()) == 7\n    assert len(table.index_struct.table[\"this\"]) == 2\n    node_texts = {n.text for n in table.index_struct.text_chunks.values()}\n    assert node_texts == {\"Hello world.\", \"This is a test.\", \"This is a test v2.\"}\n\n\n@patch_common\n@patch(\n    \"gpt_index.indices.keyword_table.simple_base.simple_extract_keywords\",\n    mock_extract_keywords,\n)\n@patch(\n    \"gpt_index.indices.query.keyword_table.query.simple_extract_keywords\",\n    mock_extract_keywords,\n)\ndef test_query(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test query.\"\"\"\n    # test simple keyword table\n    # NOTE: here the keyword extraction isn't mocked because we're using\n    # the regex-based keyword extractor, not GPT\n    table = GPTSimpleKeywordTableIndex(documents)\n\n    response = table.query(\"Hello\", mode=\"simple\")\n    assert str(response) == \"Hello:Hello world.\"\n\n    # try with filters\n    doc_text = (\n        \"Hello world\\n\" \"Hello foo\\n\" \"This is another test\\n\" \"This is a test v2\"\n    )\n    documents2 = [Document(doc_text)]\n    table2 = GPTSimpleKeywordTableIndex(documents2)\n    # NOTE: required keywords are somewhat redundant\n    response = table2.query(\"This\", mode=\"simple\", required_keywords=[\"v2\"])\n    assert str(response) == \"This:This is a test v2\"\n\n    # test exclude_keywords\n    response = table2.query(\"Hello\", mode=\"simple\", exclude_keywords=[\"world\"])\n    assert str(response) == \"Hello:Hello foo\"\n", "doc_id": "3766f7eb2961e7918fad3622ac9b52bc211a5c03", "embedding": null, "extra_info": {"file_path": "tests/indices/keyword_table/test_base.py", "file_name": "test_base.py"}, "__type__": "Document"}, "a565c5ede9769e63647e38e89a7ebe3be397c9c9": {"text": "\"\"\"Test utils.\"\"\"\n\nfrom gpt_index.indices.keyword_table.utils import extract_keywords_given_response\n\n\ndef test_expand_tokens_with_subtokens() -> None:\n    \"\"\"Test extract keywords given response.\"\"\"\n    response = \"foo bar, baz, Hello hello wOrld bye\"\n    keywords = extract_keywords_given_response(response)\n    assert keywords == {\n        \"foo bar\",\n        \"foo\",\n        \"bar\",\n        \"baz\",\n        \"hello hello world bye\",\n        \"hello\",\n        \"world\",\n        \"bye\",\n    }\n\n\ndef test_extract_keywords_with_start_delimiter() -> None:\n    \"\"\"Test extract keywords with start delimiter.\"\"\"\n    response = \"KEYWORDS: foo, bar, foobar\"\n    keywords = extract_keywords_given_response(response, start_token=\"KEYWORDS:\")\n    assert keywords == {\n        \"foo\",\n        \"bar\",\n        \"foobar\",\n    }\n\n    response = \"TOKENS: foo, bar, foobar\"\n    keywords = extract_keywords_given_response(response, start_token=\"TOKENS:\")\n    assert keywords == {\n        \"foo\",\n        \"bar\",\n        \"foobar\",\n    }\n", "doc_id": "a565c5ede9769e63647e38e89a7ebe3be397c9c9", "embedding": null, "extra_info": {"file_path": "tests/indices/keyword_table/test_utils.py", "file_name": "test_utils.py"}, "__type__": "Document"}, "e08da1f225af970a08546034c5064e568f7aa219": {"text": "\"\"\"Test list index.\"\"\"\n\nfrom pathlib import Path\nfrom tempfile import TemporaryDirectory\nfrom typing import Any, Dict, List, Optional, Tuple, cast\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.indices.list.base import GPTListIndex\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.query.list.embedding_query import GPTListIndexEmbeddingQuery\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.readers.schema.base import Document\nfrom gpt_index.utils import globals_helper\nfrom tests.mock_utils.mock_decorator import patch_common\nfrom tests.mock_utils.mock_predict import mock_llmpredictor_predict\nfrom tests.mock_utils.mock_prompts import MOCK_REFINE_PROMPT, MOCK_TEXT_QA_PROMPT\n\n\n@pytest.fixture\ndef struct_kwargs() -> Tuple[Dict, Dict]:\n    \"\"\"Index kwargs.\"\"\"\n    index_kwargs = {\n        \"text_qa_template\": MOCK_TEXT_QA_PROMPT,\n    }\n    query_kwargs = {\n        \"text_qa_template\": MOCK_TEXT_QA_PROMPT,\n        \"refine_template\": MOCK_REFINE_PROMPT,\n    }\n    return index_kwargs, query_kwargs\n\n\n@pytest.fixture\ndef documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"\n    # NOTE: one document for now\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    return [Document(doc_text)]\n\n\n@patch_common\ndef test_build_list(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test build list.\"\"\"\n    list_index = GPTListIndex(documents=documents)\n    assert len(list_index.index_struct.nodes) == 4\n    # check contents of nodes\n    assert list_index.index_struct.nodes[0].text == \"Hello world.\"\n    assert list_index.index_struct.nodes[1].text == \"This is a test.\"\n    assert list_index.index_struct.nodes[2].text == \"This is another test.\"\n    assert list_index.index_struct.nodes[3].text == \"This is a test v2.\"\n\n\n@patch_common\ndef test_build_list_multiple(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n) -> None:\n    \"\"\"Test build list multiple.\"\"\"\n    documents = [\n        Document(\"Hello world.\\nThis is a test.\"),\n        Document(\"This is another test.\\nThis is a test v2.\"),\n    ]\n    list_index = GPTListIndex(documents=documents)\n    assert len(list_index.index_struct.nodes) == 4\n    # check contents of nodes\n    assert list_index.index_struct.nodes[0].text == \"Hello world.\"\n    assert list_index.index_struct.nodes[1].text == \"This is a test.\"\n    assert list_index.index_struct.nodes[2].text == \"This is another test.\"\n    assert list_index.index_struct.nodes[3].text == \"This is a test v2.\"\n\n\n@patch_common\ndef test_list_insert(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test insert to list.\"\"\"\n    list_index = GPTListIndex([])\n    assert len(list_index.index_struct.nodes) == 0\n    list_index.insert(documents[0])\n    # check contents of nodes\n    assert list_index.index_struct.nodes[0].text == \"Hello world.\"\n    assert list_index.index_struct.nodes[1].text == \"This is a test.\"\n    assert list_index.index_struct.nodes[2].text == \"This is another test.\"\n    assert list_index.index_struct.nodes[3].text == \"This is a test v2.\"\n\n    # test insert with ID\n    document = documents[0]\n    document.doc_id = \"test_id\"\n    list_index = GPTListIndex([])\n    list_index.insert(document)\n    # check contents of nodes\n    for node in list_index.index_struct.nodes:\n        assert node.ref_doc_id == \"test_id\"\n\n\n@patch_common\ndef test_list_delete(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test insert to list and then delete.\"\"\"\n    new_documents = [\n        Document(\"Hello world.\\nThis is a test.\", doc_id=\"test_id_1\"),\n        Document(\"This is another test.\", doc_id=\"test_id_2\"),\n        Document(\"This is a test v2.\", doc_id=\"test_id_3\"),\n    ]\n\n    # delete from documents\n    list_index = GPTListIndex(new_documents)\n    # assert source doc is in docstore\n    source_doc = list_index.docstore.get_document(\"test_id_1\")\n    assert source_doc is not None\n    list_index.delete(\"test_id_1\")\n    assert len(list_index.index_struct.nodes) == 2\n    assert list_index.index_struct.nodes[0].ref_doc_id == \"test_id_2\"\n    assert list_index.index_struct.nodes[0].text == \"This is another test.\"\n    assert list_index.index_struct.nodes[1].ref_doc_id == \"test_id_3\"\n    assert list_index.index_struct.nodes[1].text == \"This is a test v2.\"\n    # check that not in docstore anymore\n    source_doc = list_index.docstore.get_document(\"test_id_1\", raise_error=False)\n    assert source_doc is None\n\n    list_index = GPTListIndex(new_documents)\n    list_index.delete(\"test_id_2\")\n    assert len(list_index.index_struct.nodes) == 3\n    assert list_index.index_struct.nodes[0].ref_doc_id == \"test_id_1\"\n    assert list_index.index_struct.nodes[0].text == \"Hello world.\"\n    assert list_index.index_struct.nodes[1].ref_doc_id == \"test_id_1\"\n    assert list_index.index_struct.nodes[1].text == \"This is a test.\"\n    assert list_index.index_struct.nodes[2].ref_doc_id == \"test_id_3\"\n    assert list_index.index_struct.nodes[2].text == \"This is a test v2.\"\n\n\ndef _get_embeddings(\n    query_str: str, nodes: List[Node]\n) -> Tuple[List[float], List[List[float]]]:\n    \"\"\"Get node text embedding similarity.\"\"\"\n    text_embed_map: Dict[str, List[float]] = {\n        \"Hello world.\": [1.0, 0.0, 0.0, 0.0, 0.0],\n        \"This is a test.\": [0.0, 1.0, 0.0, 0.0, 0.0],\n        \"This is another test.\": [0.0, 0.0, 1.0, 0.0, 0.0],\n        \"This is a test v2.\": [0.0, 0.0, 0.0, 1.0, 0.0],\n    }\n    node_embeddings = []\n    for node in nodes:\n        node_embeddings.append(text_embed_map[node.get_text()])\n\n    return [1.0, 0, 0, 0, 0], node_embeddings\n\n\n@patch_common\ndef test_query(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test list query.\"\"\"\n    index_kwargs, query_kwargs = struct_kwargs\n    index = GPTListIndex(documents, **index_kwargs)\n\n    query_str = \"What is?\"\n    response = index.query(query_str, mode=\"default\", **query_kwargs)\n    assert str(response) == (\"What is?:Hello world.\")\n    node_info = (\n        response.source_nodes[0].node_info if response.source_nodes[0].node_info else {}\n    )\n    assert node_info[\"start\"] == 0\n    assert node_info[\"end\"] == 12\n\n\n@patch.object(LLMPredictor, \"total_tokens_used\", return_value=0)\n@patch.object(LLMPredictor, \"predict\", side_effect=mock_llmpredictor_predict)\n@patch.object(LLMPredictor, \"__init__\", return_value=None)\ndef test_index_overlap(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test node info calculation with overlapping node text.\"\"\"\n    index_kwargs, query_kwargs = struct_kwargs\n\n    documents = [\n        Document(\n            \"Hello world. This is a test 1. This is a test 2.\"\n            + \" This is a test 3. This is a test 4. This is a test 5.\\n\"\n        )\n    ]\n\n    def _mock_text_splitter_with_space(\n        prompt: Prompt, num_chunks: int, padding: Optional[int] = 1\n    ) -> TokenTextSplitter:\n        \"\"\"Mock text splitter.\"\"\"\n        return TokenTextSplitter(\n            separator=\" \",\n            chunk_size=30,\n            chunk_overlap=10,\n            tokenizer=globals_helper.tokenizer,\n        )\n\n    with patch.object(\n        PromptHelper,\n        \"get_text_splitter_given_prompt\",\n        side_effect=_mock_text_splitter_with_space,\n    ):\n        index = GPTListIndex(documents, **index_kwargs)\n\n        query_str = \"What is?\"\n        response = index.query(query_str, mode=\"default\", **query_kwargs)\n        node_info_0 = (\n            response.source_nodes[0].node_info\n            if response.source_nodes[0].node_info\n            else {}\n        )\n        # First chunk: 'Hello world. This is a test 1. This is a test 2.\n        # This is a test 3. This is a test 4. This is a'\n        assert node_info_0[\"start\"] == 0  # start at the start\n        assert node_info_0[\"end\"] == 94  # Length of first chunk.\n\n        node_info_1 = (\n            response.source_nodes[1].node_info\n            if response.source_nodes[1].node_info\n            else {}\n        )\n        # Second chunk: 'This is a test 4. This is a test 5.\\n'\n        assert node_info_1[\"start\"] == 67  # Position of second chunk relative to start\n        assert node_info_1[\"end\"] == 103  # End index\n\n\n@patch_common\ndef test_query_with_keywords(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test list query with keywords.\"\"\"\n    index_kwargs, query_kwargs = struct_kwargs\n    index = GPTListIndex(documents, **index_kwargs)\n\n    # test query with keywords\n    query_str = \"What is?\"\n    query_kwargs.update({\"required_keywords\": [\"test\"]})\n    response = index.query(query_str, mode=\"default\", **query_kwargs)\n    assert str(response) == (\"What is?:This is a test.\")\n\n    query_kwargs.update({\"exclude_keywords\": [\"Hello\"]})\n    response = index.query(query_str, mode=\"default\", **query_kwargs)\n    assert str(response) == (\"What is?:This is a test.\")\n\n\n@patch_common\n@patch.object(\n    GPTListIndexEmbeddingQuery,\n    \"_get_embeddings\",\n    side_effect=_get_embeddings,\n)\ndef test_embedding_query(\n    _mock_similarity: Any,\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test embedding query.\"\"\"\n    index_kwargs, query_kwargs = struct_kwargs\n    index = GPTListIndex(documents, **index_kwargs)\n\n    # test embedding query\n    query_str = \"What is?\"\n    response = index.query(\n        query_str, mode=\"embedding\", similarity_top_k=1, **query_kwargs\n    )\n    assert str(response) == (\"What is?:Hello world.\")\n\n\n@patch_common\ndef test_extra_info(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test build/query with extra info.\"\"\"\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    extra_info = {\"extra_info\": \"extra_info\", \"foo\": \"bar\"}\n    new_document = Document(doc_text, extra_info=extra_info)\n    list_index = GPTListIndex(documents=[new_document])\n    assert list_index.index_struct.nodes[0].get_text() == (\n        \"extra_info: extra_info\\n\" \"foo: bar\\n\\n\" \"Hello world.\"\n    )\n    assert list_index.index_struct.nodes[3].get_text() == (\n        \"extra_info: extra_info\\n\" \"foo: bar\\n\\n\" \"This is a test v2.\"\n    )\n\n\n@patch_common\ndef test_to_from_disk(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test saving to disk and from disk.\"\"\"\n    list_index = GPTListIndex(documents=documents)\n    with TemporaryDirectory() as tmp_dir:\n        list_index.save_to_disk(str(Path(tmp_dir) / \"tmp.json\"))\n        new_list_index = cast(\n            GPTListIndex, GPTListIndex.load_from_disk(str(Path(tmp_dir) / \"tmp.json\"))\n        )\n        assert len(new_list_index.index_struct.nodes) == 4\n        # check contents of nodes\n        assert new_list_index.index_struct.nodes[0].text == \"Hello world.\"\n        assert new_list_index.index_struct.nodes[1].text == \"This is a test.\"\n        assert new_list_index.index_struct.nodes[2].text == \"This is another test.\"\n        assert new_list_index.index_struct.nodes[3].text == \"This is a test v2.\"\n\n\n@patch_common\ndef test_to_from_string(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test saving to disk and from disk.\"\"\"\n    list_index = GPTListIndex(documents=documents)\n    new_list_index = cast(\n        GPTListIndex, GPTListIndex.load_from_string(list_index.save_to_string())\n    )\n    assert len(new_list_index.index_struct.nodes) == 4\n    # check contents of nodes\n    assert new_list_index.index_struct.nodes[0].text == \"Hello world.\"\n    assert new_list_index.index_struct.nodes[1].text == \"This is a test.\"\n    assert new_list_index.index_struct.nodes[2].text == \"This is another test.\"\n    assert new_list_index.index_struct.nodes[3].text == \"This is a test v2.\"\n", "doc_id": "e08da1f225af970a08546034c5064e568f7aa219", "embedding": null, "extra_info": {"file_path": "tests/indices/list/test_base.py", "file_name": "test_base.py"}, "__type__": "Document"}, "d283765f8173ce6fa83ad61a8a33198283818051": {"text": "\"\"\"Test query runner.\"\"\"\n\nfrom typing import Any\nfrom unittest.mock import patch\n\nfrom gpt_index import PromptHelper\nfrom gpt_index.indices.list.base import GPTListIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import (\n    LLMChain,\n    LLMMetadata,\n    LLMPredictor,\n)\nfrom gpt_index.readers.schema.base import Document\n\n\ndef mock_llmchain_predict(**full_prompt_args: Any) -> str:\n    \"\"\"Mock LLMChain predict with a generic response.\"\"\"\n    return \"foo bar 2\"\n\n\n@patch.object(LLMChain, \"predict\", side_effect=mock_llmchain_predict)\n@patch(\"gpt_index.langchain_helpers.chain_wrapper.OpenAI\")\n@patch.object(LLMPredictor, \"get_llm_metadata\", return_value=LLMMetadata())\n@patch.object(LLMChain, \"__init__\", return_value=None)\ndef test_passing_args_to_query(\n    _mock_init: Any,\n    _mock_llm_metadata: Any,\n    _mock_openai: Any,\n    _mock_predict: Any,\n) -> None:\n    \"\"\"Test passing args to query works.\n\n    Test that passing LLMPredictor from build index to query works.\n\n    \"\"\"\n    doc_text = \"Hello world.\"\n    doc = Document(doc_text)\n    llm_predictor = LLMPredictor()\n    prompt_helper = PromptHelper.from_llm_predictor(llm_predictor)\n    # index construction should not use llm_predictor at all\n    index = GPTListIndex(\n        [doc], llm_predictor=llm_predictor, prompt_helper=prompt_helper\n    )\n    # should use llm_predictor during query time\n    response = index.query(\"What is?\")\n    assert str(response) == \"foo bar 2\"\n", "doc_id": "d283765f8173ce6fa83ad61a8a33198283818051", "embedding": null, "extra_info": {"file_path": "tests/indices/query/test_query_runner.py", "file_name": "test_query_runner.py"}, "__type__": "Document"}, "1ed25268212c3ce05ed55f5f93ff110b69836ea8": {"text": "\"\"\"Test recursive queries.\"\"\"\n\nfrom pathlib import Path\nfrom tempfile import TemporaryDirectory\nfrom typing import Any, Dict, List, Tuple\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom gpt_index.composability.graph import ComposableGraph\nfrom gpt_index.data_structs.struct_type import IndexStructType\nfrom gpt_index.indices.keyword_table.simple_base import GPTSimpleKeywordTableIndex\nfrom gpt_index.indices.list.base import GPTListIndex\nfrom gpt_index.indices.query.schema import QueryConfig, QueryMode\nfrom gpt_index.indices.tree.base import GPTTreeIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import (\n    LLMChain,\n    LLMMetadata,\n    LLMPredictor,\n)\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.readers.schema.base import Document\nfrom tests.mock_utils.mock_predict import (\n    mock_llmchain_predict,\n    mock_llmpredictor_predict,\n)\nfrom tests.mock_utils.mock_prompts import (\n    MOCK_INSERT_PROMPT,\n    MOCK_KEYWORD_EXTRACT_PROMPT,\n    MOCK_QUERY_KEYWORD_EXTRACT_PROMPT,\n    MOCK_QUERY_PROMPT,\n    MOCK_REFINE_PROMPT,\n    MOCK_SUMMARY_PROMPT,\n    MOCK_TEXT_QA_PROMPT,\n)\nfrom tests.mock_utils.mock_text_splitter import mock_token_splitter_newline\n\n\n@pytest.fixture\ndef struct_kwargs() -> Tuple[Dict, List]:\n    \"\"\"Index kwargs.\"\"\"\n    index_kwargs = {\n        \"tree\": {\n            \"summary_template\": MOCK_SUMMARY_PROMPT,\n            \"insert_prompt\": MOCK_INSERT_PROMPT,\n            \"num_children\": 2,\n        },\n        \"list\": {\n            \"text_qa_template\": MOCK_TEXT_QA_PROMPT,\n        },\n        \"table\": {\n            \"keyword_extract_template\": MOCK_KEYWORD_EXTRACT_PROMPT,\n        },\n    }\n    query_configs = [\n        QueryConfig(\n            index_struct_type=IndexStructType.TREE,\n            query_mode=QueryMode.DEFAULT,\n            query_kwargs={\n                \"query_template\": MOCK_QUERY_PROMPT,\n                \"text_qa_template\": MOCK_TEXT_QA_PROMPT,\n                \"refine_template\": MOCK_REFINE_PROMPT,\n            },\n        ),\n        QueryConfig(\n            index_struct_type=IndexStructType.LIST,\n            query_mode=QueryMode.DEFAULT,\n            query_kwargs={\n                \"text_qa_template\": MOCK_TEXT_QA_PROMPT,\n                \"refine_template\": MOCK_REFINE_PROMPT,\n            },\n        ),\n        QueryConfig(\n            index_struct_type=IndexStructType.KEYWORD_TABLE,\n            query_mode=QueryMode.DEFAULT,\n            query_kwargs={\n                \"query_keyword_extract_template\": MOCK_QUERY_KEYWORD_EXTRACT_PROMPT,\n                \"text_qa_template\": MOCK_TEXT_QA_PROMPT,\n                \"refine_template\": MOCK_REFINE_PROMPT,\n            },\n        ),\n    ]\n    return index_kwargs, query_configs\n\n\n@pytest.fixture\ndef documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"\n    docs = [\n        Document(\"This is a test v2.\"),\n        Document(\"This is another test.\"),\n        Document(\"This is a test.\"),\n        Document(\"Hello world.\"),\n        Document(\"Hello world.\"),\n        Document(\"This is a test.\"),\n        Document(\"This is another test.\"),\n        Document(\"This is a test v2.\"),\n    ]\n    return docs\n\n\n@patch.object(TokenTextSplitter, \"split_text\", side_effect=mock_token_splitter_newline)\n@patch.object(LLMPredictor, \"predict\", side_effect=mock_llmpredictor_predict)\n@patch.object(LLMPredictor, \"total_tokens_used\", return_value=0)\n@patch.object(LLMPredictor, \"__init__\", return_value=None)\ndef test_recursive_query_list_tree(\n    _mock_init: Any,\n    _mock_total_tokens_used: Any,\n    _mock_predict: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test query.\"\"\"\n    index_kwargs, query_configs = struct_kwargs\n    list_kwargs = index_kwargs[\"list\"]\n    tree_kwargs = index_kwargs[\"tree\"]\n    # try building a list for every two, then a tree\n    list1 = GPTListIndex(documents[0:2], **list_kwargs)\n    list1.set_text(\"summary1\")\n    list2 = GPTListIndex(documents[2:4], **list_kwargs)\n    list2.set_text(\"summary2\")\n    list3 = GPTListIndex(documents[4:6], **list_kwargs)\n    list3.set_text(\"summary3\")\n    list4 = GPTListIndex(documents[6:8], **list_kwargs)\n    list4.set_text(\"summary4\")\n\n    # there are two root nodes in this tree: one containing [list1, list2]\n    # and the other containing [list3, list4]\n    tree = GPTTreeIndex(\n        [\n            list1,\n            list2,\n            list3,\n            list4,\n        ],\n        **tree_kwargs\n    )\n    query_str = \"What is?\"\n    # query should first pick the left root node, then pick list1\n    # within list1, it should go through the first document and second document\n    response = tree.query(query_str, mode=\"recursive\", query_configs=query_configs)\n    assert str(response) == (\"What is?:This is a test v2.\")\n\n    # Also test a non-recursive query. This should not go down into the list\n    tree_query_kwargs = query_configs[0].query_kwargs\n    response = tree.query(query_str, mode=\"default\", **tree_query_kwargs)\n    assert str(response) == (\"What is?:summary1\")\n\n\n@patch.object(TokenTextSplitter, \"split_text\", side_effect=mock_token_splitter_newline)\n@patch.object(LLMPredictor, \"predict\", side_effect=mock_llmpredictor_predict)\n@patch.object(LLMPredictor, \"total_tokens_used\", return_value=0)\n@patch.object(LLMPredictor, \"__init__\", return_value=None)\ndef test_recursive_query_tree_list(\n    _mock_init: Any,\n    _mock_total_tokens_used: Any,\n    _mock_predict: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test query.\"\"\"\n    index_kwargs, query_configs = struct_kwargs\n    list_kwargs = index_kwargs[\"list\"]\n    tree_kwargs = index_kwargs[\"tree\"]\n    # try building a tree for a group of 4, then a list\n    # use a diff set of documents\n    tree1 = GPTTreeIndex(documents[2:6], **tree_kwargs)\n    tree2 = GPTTreeIndex(documents[:2] + documents[6:], **tree_kwargs)\n    tree1.set_text(\"tree_summary1\")\n    tree2.set_text(\"tree_summary2\")\n\n    # there are two root nodes in this tree: one containing [list1, list2]\n    # and the other containing [list3, list4]\n    list_index = GPTListIndex([tree1, tree2], **list_kwargs)\n    query_str = \"What is?\"\n    # query should first pick the left root node, then pick list1\n    # within list1, it should go through the first document and second document\n    response = list_index.query(\n        query_str, mode=\"recursive\", query_configs=query_configs\n    )\n    assert str(response) == (\"What is?:This is a test.\")\n\n    # Also test a non-recursive query. This should not go down into the list\n    list_query_kwargs = query_configs[1].query_kwargs\n    response = list_index.query(query_str, mode=\"default\", **list_query_kwargs)\n    assert str(response) == (\"What is?:tree_summary1\")\n\n\n@patch.object(TokenTextSplitter, \"split_text\", side_effect=mock_token_splitter_newline)\n@patch.object(LLMPredictor, \"predict\", side_effect=mock_llmpredictor_predict)\n@patch.object(LLMPredictor, \"total_tokens_used\", return_value=0)\n@patch.object(LLMPredictor, \"__init__\", return_value=None)\ndef test_recursive_query_table_list(\n    _mock_init: Any,\n    _mock_total_tokens_used: Any,\n    _mock_predict: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test query.\"\"\"\n    index_kwargs, query_configs = struct_kwargs\n    list_kwargs = index_kwargs[\"list\"]\n    table_kwargs = index_kwargs[\"table\"]\n    # try building a tree for a group of 4, then a list\n    # use a diff set of documents\n    table1 = GPTSimpleKeywordTableIndex(documents[4:6], **table_kwargs)\n    table2 = GPTSimpleKeywordTableIndex(documents[2:3], **table_kwargs)\n    table1.set_text(\"table_summary1\")\n    table2.set_text(\"table_summary2\")\n    table1.set_doc_id(\"table1\")\n    table2.set_doc_id(\"table2\")\n\n    list_index = GPTListIndex([table1, table2], **list_kwargs)\n    query_str = \"World?\"\n    response = list_index.query(\n        query_str, mode=\"recursive\", query_configs=query_configs\n    )\n    assert str(response) == (\"World?:Hello world.\")\n\n    query_str = \"Test?\"\n    response = list_index.query(\n        query_str, mode=\"recursive\", query_configs=query_configs\n    )\n    assert str(response) == (\"Test?:This is a test.\")\n\n    # test serialize and then back\n    with TemporaryDirectory() as tmpdir:\n        graph = ComposableGraph.build_from_index(list_index)\n        graph.save_to_disk(str(Path(tmpdir) / \"tmp.json\"))\n        graph = ComposableGraph.load_from_disk(str(Path(tmpdir) / \"tmp.json\"))\n        response = graph.query(query_str, query_configs=query_configs)\n        assert str(response) == (\"Test?:This is a test.\")\n\n        # test graph.get_index\n        test_table1 = graph.get_index(\"table1\", GPTSimpleKeywordTableIndex)\n        response = test_table1.query(\"Hello\")\n        assert str(response) == (\"Hello:Hello world.\")\n\n\n@patch.object(TokenTextSplitter, \"split_text\", side_effect=mock_token_splitter_newline)\n@patch.object(LLMPredictor, \"predict\", side_effect=mock_llmpredictor_predict)\n@patch.object(LLMPredictor, \"total_tokens_used\", return_value=0)\n@patch.object(LLMPredictor, \"__init__\", return_value=None)\ndef test_recursive_query_list_table(\n    _mock_init: Any,\n    _mock_total_tokens_used: Any,\n    _mock_predict: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test query.\"\"\"\n    index_kwargs, query_configs = struct_kwargs\n    list_kwargs = index_kwargs[\"list\"]\n    table_kwargs = index_kwargs[\"table\"]\n    # try building a tree for a group of 4, then a list\n    # use a diff set of documents\n    # try building a list for every two, then a tree\n    list1 = GPTListIndex(documents[0:2], **list_kwargs)\n    list1.set_text(\"foo bar\")\n    list2 = GPTListIndex(documents[2:4], **list_kwargs)\n    list2.set_text(\"apple orange\")\n    list3 = GPTListIndex(documents[4:6], **list_kwargs)\n    list3.set_text(\"toronto london\")\n    list4 = GPTListIndex(documents[6:8], **list_kwargs)\n    list4.set_text(\"cat dog\")\n\n    table = GPTSimpleKeywordTableIndex([list1, list2, list3, list4], **table_kwargs)\n    query_str = \"Foo?\"\n    response = table.query(query_str, mode=\"recursive\", query_configs=query_configs)\n    assert str(response) == (\"Foo?:This is a test v2.\")\n    query_str = \"Orange?\"\n    response = table.query(query_str, mode=\"recursive\", query_configs=query_configs)\n    assert str(response) == (\"Orange?:This is a test.\")\n    query_str = \"Cat?\"\n    response = table.query(query_str, mode=\"recursive\", query_configs=query_configs)\n    assert str(response) == (\"Cat?:This is another test.\")\n\n    # test serialize and then back\n    # use composable graph struct\n    with TemporaryDirectory() as tmpdir:\n        graph = ComposableGraph.build_from_index(table)\n        graph.save_to_disk(str(Path(tmpdir) / \"tmp.json\"))\n        graph = ComposableGraph.load_from_disk(str(Path(tmpdir) / \"tmp.json\"))\n        response = graph.query(query_str, query_configs=query_configs)\n        assert str(response) == (\"Cat?:This is another test.\")\n\n\n@patch.object(LLMChain, \"predict\", side_effect=mock_llmchain_predict)\n@patch(\"gpt_index.langchain_helpers.chain_wrapper.OpenAI\")\n@patch.object(LLMPredictor, \"get_llm_metadata\", return_value=LLMMetadata())\n@patch.object(LLMChain, \"__init__\", return_value=None)\ndef test_recursive_query_list_tree_token_count(\n    _mock_init: Any,\n    _mock_llm_metadata: Any,\n    _mock_llmchain: Any,\n    _mock_predict: Any,\n    documents: List[Document],\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test query.\"\"\"\n    index_kwargs, query_configs = struct_kwargs\n    list_kwargs = index_kwargs[\"list\"]\n    tree_kwargs = index_kwargs[\"tree\"]\n    # try building a list for every two, then a tree\n    list1 = GPTListIndex(documents[0:2], **list_kwargs)\n    list1.set_text(\"summary1\")\n    list2 = GPTListIndex(documents[2:4], **list_kwargs)\n    list2.set_text(\"summary2\")\n    list3 = GPTListIndex(documents[4:6], **list_kwargs)\n    list3.set_text(\"summary3\")\n    list4 = GPTListIndex(documents[6:8], **list_kwargs)\n    list4.set_text(\"summary4\")\n\n    # there are two root nodes in this tree: one containing [list1, list2]\n    # and the other containing [list3, list4]\n    # import pdb; pdb.set_trace()\n    tree = GPTTreeIndex(\n        [\n            list1,\n            list2,\n            list3,\n            list4,\n        ],\n        **tree_kwargs\n    )\n    # first pass prompt is \"summary1\\nsummary2\\n\" (6 tokens),\n    # response is the mock response (10 tokens)\n    # total is 16 tokens, multiply by 2 to get the total\n    assert tree._llm_predictor.total_tokens_used == 32\n\n    query_str = \"What is?\"\n    # query should first pick the left root node, then pick list1\n    # within list1, it should go through the first document and second document\n    start_token_ct = tree._llm_predictor.total_tokens_used\n    tree.query(query_str, mode=\"recursive\", query_configs=query_configs)\n    # prompt is which is 35 tokens, plus 10 for the mock response\n    assert tree._llm_predictor.total_tokens_used - start_token_ct == 45\n", "doc_id": "1ed25268212c3ce05ed55f5f93ff110b69836ea8", "embedding": null, "extra_info": {"file_path": "tests/indices/query/test_recursive.py", "file_name": "test_recursive.py"}, "__type__": "Document"}, "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94": {"text": "\"\"\"Test struct store indices.\"\"\"\n\nimport re\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport pytest\nfrom sqlalchemy import (\n    Column,\n    Integer,\n    MetaData,\n    String,\n    Table,\n    column,\n    create_engine,\n    delete,\n    select,\n)\n\nfrom gpt_index.indices.common.struct_store.base import SQLContextBuilder\nfrom gpt_index.indices.struct_store.base import default_output_parser\nfrom gpt_index.indices.struct_store.sql import GPTSQLStructStoreIndex\nfrom gpt_index.langchain_helpers.sql_wrapper import SQLDatabase\nfrom gpt_index.readers.schema.base import Document\nfrom gpt_index.schema import BaseDocument\nfrom tests.mock_utils.mock_decorator import patch_common\nfrom tests.mock_utils.mock_prompts import (\n    MOCK_REFINE_PROMPT,\n    MOCK_SCHEMA_EXTRACT_PROMPT,\n    MOCK_TABLE_CONTEXT_PROMPT,\n    MOCK_TEXT_QA_PROMPT,\n)\n\n\ndef _mock_output_parser(output: str) -> Optional[Dict[str, Any]]:\n    \"\"\"Mock output parser.\n\n    Split via commas instead of newlines, in order to fit\n    the format of the mock test document (newlines create\n    separate text chunks in the testing code).\n\n    \"\"\"\n    tups = output.split(\",\")\n\n    fields = {}\n    for tup in tups:\n        if \":\" in tup:\n            tokens = tup.split(\":\")\n            field = re.sub(r\"\\W+\", \"\", tokens[0])\n            value = re.sub(r\"\\W+\", \"\", tokens[1])\n            fields[field] = value\n    return fields\n\n\n@pytest.fixture\ndef struct_kwargs() -> Tuple[Dict, Dict]:\n    \"\"\"Index kwargs.\"\"\"\n    # NOTE: QuestionAnswer and Refine templates aren't technically used\n    index_kwargs = {\n        \"schema_extract_prompt\": MOCK_SCHEMA_EXTRACT_PROMPT,\n        \"output_parser\": _mock_output_parser,\n    }\n    query_kwargs = {\n        \"text_qa_template\": MOCK_TEXT_QA_PROMPT,\n        \"refine_template\": MOCK_REFINE_PROMPT,\n    }\n    return index_kwargs, query_kwargs\n\n\n@patch_common\ndef test_sql_index(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    struct_kwargs: Tuple[Dict, Dict],\n) -> None:\n    \"\"\"Test GPTSQLStructStoreIndex.\"\"\"\n    engine = create_engine(\"sqlite:///:memory:\")\n    metadata_obj = MetaData(bind=engine)\n    table_name = \"test_table\"\n    test_table = Table(\n        table_name,\n        metadata_obj,\n        Column(\"user_id\", Integer, primary_key=True),\n        Column(\"foo\", String(16), nullable=False),\n    )\n    metadata_obj.create_all()\n    # NOTE: we can use the default output parser for this\n    index_kwargs, _ = struct_kwargs\n    docs = [Document(text=\"user_id:2,foo:bar\"), Document(text=\"user_id:8,foo:hello\")]\n    sql_database = SQLDatabase(engine)\n    index = GPTSQLStructStoreIndex(\n        docs, sql_database=sql_database, table_name=table_name, **index_kwargs\n    )\n\n    # test that the document is inserted\n    stmt = select([column(\"user_id\"), column(\"foo\")]).select_from(test_table)\n    engine = index.sql_database.engine\n    with engine.connect() as connection:\n        results = connection.execute(stmt).fetchall()\n        assert results == [(2, \"bar\"), (8, \"hello\")]\n\n    # try with documents with more text chunks\n    delete_stmt = delete(test_table)\n    with engine.connect() as connection:\n        connection.execute(delete_stmt)\n    docs = [Document(text=\"user_id:2\\nfoo:bar\"), Document(text=\"user_id:8\\nfoo:hello\")]\n    index = GPTSQLStructStoreIndex(\n        docs, sql_database=sql_database, table_name=table_name, **index_kwargs\n    )\n    # test that the document is inserted\n    stmt = select([column(\"user_id\"), column(\"foo\")]).select_from(test_table)\n    engine = index.sql_database.engine\n    with engine.connect() as connection:\n        results = connection.execute(stmt).fetchall()\n        assert results == [(2, \"bar\"), (8, \"hello\")]\n\n\n@patch_common\ndef test_sql_index_with_context(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    struct_kwargs: Tuple[Dict, Dict],\n) -> None:\n    \"\"\"Test GPTSQLStructStoreIndex.\"\"\"\n    # test setting table_context_dict\n    engine = create_engine(\"sqlite:///:memory:\")\n    metadata_obj = MetaData(bind=engine)\n    table_name = \"test_table\"\n    test_table = Table(\n        table_name,\n        metadata_obj,\n        Column(\"user_id\", Integer, primary_key=True),\n        Column(\"foo\", String(16), nullable=False),\n    )\n    metadata_obj.create_all()\n    # NOTE: we can use the default output parser for this\n    index_kwargs, _ = struct_kwargs\n    docs = [Document(text=\"user_id:2,foo:bar\"), Document(text=\"user_id:8,foo:hello\")]\n    sql_database = SQLDatabase(engine)\n    table_context_dict = {\"test_table\": \"test_table_context\"}\n    index = GPTSQLStructStoreIndex(\n        docs,\n        sql_database=sql_database,\n        table_name=table_name,\n        table_context_dict=table_context_dict,\n        **index_kwargs\n    )\n    assert index._index_struct.context_dict == table_context_dict\n\n    # test setting sql_context_builder\n    delete_stmt = delete(test_table)\n    with engine.connect() as connection:\n        connection.execute(delete_stmt)\n    sql_database = SQLDatabase(engine)\n    # this should cause the mock QuestionAnswer prompt to run\n    sql_context_builder = SQLContextBuilder(\n        sql_database,\n        table_context_prompt=MOCK_TABLE_CONTEXT_PROMPT,\n        table_context_task=\"extract_test\",\n    )\n    context_documents_dict: Dict[str, List[BaseDocument]] = {\n        \"test_table\": [Document(\"test_table_context\")]\n    }\n    index = GPTSQLStructStoreIndex(\n        docs,\n        sql_database=sql_database,\n        table_name=table_name,\n        sql_context_builder=sql_context_builder,\n        context_documents_dict=context_documents_dict,\n        **index_kwargs\n    )\n    assert index._index_struct.context_dict == {\n        \"test_table\": \"extract_test:test_table_context\"\n    }\n\n    # test error if both are set\n    # TODO:\n\n\n@patch_common\ndef test_sql_index_query(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    struct_kwargs: Tuple[Dict, Dict],\n) -> None:\n    \"\"\"Test GPTSQLStructStoreIndex.\"\"\"\n    index_kwargs, query_kwargs = struct_kwargs\n    docs = [Document(text=\"user_id:2,foo:bar\"), Document(text=\"user_id:8,foo:hello\")]\n    engine = create_engine(\"sqlite:///:memory:\")\n    metadata_obj = MetaData(bind=engine)\n    table_name = \"test_table\"\n    # NOTE: table is created by tying to metadata_obj\n    Table(\n        table_name,\n        metadata_obj,\n        Column(\"user_id\", Integer, primary_key=True),\n        Column(\"foo\", String(16), nullable=False),\n    )\n    metadata_obj.create_all()\n    sql_database = SQLDatabase(engine)\n    # NOTE: we can use the default output parser for this\n    index = GPTSQLStructStoreIndex(\n        docs, sql_database=sql_database, table_name=table_name, **index_kwargs\n    )\n\n    # query the index with SQL\n    response = index.query(\n        \"SELECT user_id, foo FROM test_table\", mode=\"sql\", **query_kwargs\n    )\n    assert response.response == \"[(2, 'bar'), (8, 'hello')]\"\n\n    # query the index with natural language\n    response = index.query(\"test_table:user_id,foo\", mode=\"default\", **query_kwargs)\n    assert response.response == \"[(2, 'bar'), (8, 'hello')]\"\n\n\ndef test_default_output_parser() -> None:\n    \"\"\"Test default output parser.\"\"\"\n    test_str = \"user_id:2\\n\" \"foo:bar\\n\" \",,testing:testing2..\\n\" \"number:123,456,789\\n\"\n    fields = default_output_parser(test_str)\n    assert fields == {\n        \"user_id\": \"2\",\n        \"foo\": \"bar\",\n        \"testing\": \"testing2\",\n        \"number\": \"123456789\",\n    }\n", "doc_id": "49a136026ce2a0e2b09e2ab3da725e37dc5f5e94", "embedding": null, "extra_info": {"file_path": "tests/indices/struct_store/test_base.py", "file_name": "test_base.py"}, "__type__": "Document"}, "2c621db7da92be19edfbf5fb7f66fe0d8609091f": {"text": "\"\"\"Test node utils.\"\"\"\n\nfrom typing import List\n\nimport pytest\n\nfrom gpt_index.indices.node_utils import get_nodes_from_document\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.readers.schema.base import Document\n\n\n@pytest.fixture\ndef text_splitter() -> TokenTextSplitter:\n    \"\"\"Get text splitter.\"\"\"\n    return TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n\n\n@pytest.fixture\ndef documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"\n    # NOTE: one document for now\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    return [\n        Document(doc_text, doc_id=\"test_doc_id\", extra_info={\"test_key\": \"test_val\"})\n    ]\n\n\ndef test_get_nodes_from_document(\n    documents: List[Document], text_splitter: TokenTextSplitter\n) -> None:\n    \"\"\"Test get nodes from document have desired chunk size.\"\"\"\n    nodes = get_nodes_from_document(\n        documents[0],\n        text_splitter,\n        start_idx=0,\n        include_extra_info=False,\n    )\n    assert len(nodes) == 2\n    actual_chunk_sizes = [\n        len(text_splitter.tokenizer(node.get_text())) for node in nodes\n    ]\n    assert all(\n        chunk_size <= text_splitter._chunk_size for chunk_size in actual_chunk_sizes\n    )\n\n\ndef test_get_nodes_from_document_with_extra_info(\n    documents: List[Document], text_splitter: TokenTextSplitter\n) -> None:\n    \"\"\"Test get nodes from document with extra info have desired chunk size.\"\"\"\n    nodes = get_nodes_from_document(\n        documents[0],\n        text_splitter,\n        start_idx=0,\n        include_extra_info=True,\n    )\n    assert len(nodes) == 3\n    actual_chunk_sizes = [\n        len(text_splitter.tokenizer(node.get_text())) for node in nodes\n    ]\n    assert all(\n        chunk_size <= text_splitter._chunk_size for chunk_size in actual_chunk_sizes\n    )\n    assert all([\"test_key: test_val\" in n.get_text() for n in nodes])\n", "doc_id": "2c621db7da92be19edfbf5fb7f66fe0d8609091f", "embedding": null, "extra_info": {"file_path": "tests/indices/test_node_utils.py", "file_name": "test_node_utils.py"}, "__type__": "Document"}, "726d88a507fa99e69d3f782fb2697d21049f82e0": {"text": "\"\"\"Test PromptHelper.\"\"\"\nfrom typing import List\n\nfrom gpt_index.data_structs.data_structs import Node\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.prompts.base import Prompt\nfrom tests.mock_utils.mock_utils import mock_tokenizer\n\n\nclass TestPrompt(Prompt):\n    \"\"\"Test prompt class.\"\"\"\n\n    input_variables: List[str] = [\"text\"]\n\n\ndef test_get_chunk_size() -> None:\n    \"\"\"Test get chunk size given prompt.\"\"\"\n    # test with 1 chunk\n    empty_prompt_text = \"This is the prompt\"\n    prompt_helper = PromptHelper(\n        max_input_size=11, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    chunk_size = prompt_helper.get_chunk_size_given_prompt(\n        empty_prompt_text, 1, padding=0\n    )\n    assert chunk_size == 6\n\n    # test having 2 chunks\n    prompt_helper = PromptHelper(\n        max_input_size=11, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    chunk_size = prompt_helper.get_chunk_size_given_prompt(\n        empty_prompt_text, 2, padding=0\n    )\n    assert chunk_size == 3\n\n    # test with 2 chunks, and with chunk_size_limit\n    prompt_helper = PromptHelper(\n        max_input_size=11,\n        num_output=1,\n        max_chunk_overlap=0,\n        tokenizer=mock_tokenizer,\n        chunk_size_limit=2,\n    )\n    chunk_size = prompt_helper.get_chunk_size_given_prompt(\n        empty_prompt_text, 2, padding=0\n    )\n    assert chunk_size == 2\n\n    # test padding\n    prompt_helper = PromptHelper(\n        max_input_size=11, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    chunk_size = prompt_helper.get_chunk_size_given_prompt(\n        empty_prompt_text, 2, padding=1\n    )\n    assert chunk_size == 2\n\n\ndef test_get_text_splitter() -> None:\n    \"\"\"Test get text splitter.\"\"\"\n    test_prompt_text = \"This is the prompt{text}\"\n    test_prompt = TestPrompt(test_prompt_text)\n    prompt_helper = PromptHelper(\n        max_input_size=11, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    text_splitter = prompt_helper.get_text_splitter_given_prompt(\n        test_prompt, 2, padding=1\n    )\n    assert text_splitter._chunk_size == 2\n    test_text = \"Hello world foo Hello world bar\"\n    text_chunks = text_splitter.split_text(test_text)\n    assert text_chunks == [\"Hello world\", \"foo Hello\", \"world bar\"]\n    truncated_text = text_splitter.truncate_text(test_text)\n    assert truncated_text == \"Hello world\"\n\n    # test with chunk_size_limit\n    prompt_helper = PromptHelper(\n        max_input_size=11,\n        num_output=1,\n        max_chunk_overlap=0,\n        tokenizer=mock_tokenizer,\n        chunk_size_limit=1,\n    )\n    text_splitter = prompt_helper.get_text_splitter_given_prompt(\n        test_prompt, 2, padding=1\n    )\n    text_chunks = text_splitter.split_text(test_text)\n    assert text_chunks == [\"Hello\", \"world\", \"foo\", \"Hello\", \"world\", \"bar\"]\n\n\ndef test_get_text_splitter_partial() -> None:\n    \"\"\"Test get text splitter with a partially formatted prompt.\"\"\"\n\n    class TestPromptFoo(Prompt):\n        \"\"\"Test prompt class.\"\"\"\n\n        input_variables: List[str] = [\"foo\", \"text\"]\n\n    # test without partially formatting\n    test_prompt_text = \"This is the {foo} prompt{text}\"\n    test_prompt = TestPromptFoo(test_prompt_text)\n    prompt_helper = PromptHelper(\n        max_input_size=11, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    text_splitter = prompt_helper.get_text_splitter_given_prompt(\n        test_prompt, 2, padding=1\n    )\n    test_text = \"Hello world foo Hello world bar\"\n    text_chunks = text_splitter.split_text(test_text)\n    assert text_chunks == [\"Hello world\", \"foo Hello\", \"world bar\"]\n    truncated_text = text_splitter.truncate_text(test_text)\n    assert truncated_text == \"Hello world\"\n\n    # test with partially formatting\n    test_prompt = TestPromptFoo(test_prompt_text)\n    test_prompt = test_prompt.partial_format(foo=\"bar\")\n    prompt_helper = PromptHelper(\n        max_input_size=12, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    assert prompt_helper._get_empty_prompt_txt(test_prompt) == \"This is the bar prompt\"\n    text_splitter = prompt_helper.get_text_splitter_given_prompt(\n        test_prompt, 2, padding=1\n    )\n    test_text = \"Hello world foo Hello world bar\"\n    text_chunks = text_splitter.split_text(test_text)\n    assert text_chunks == [\"Hello world\", \"foo Hello\", \"world bar\"]\n    truncated_text = text_splitter.truncate_text(test_text)\n    assert truncated_text == \"Hello world\"\n\n\ndef test_get_text_from_nodes() -> None:\n    \"\"\"Test get_text_from_nodes.\"\"\"\n    # test prompt uses up one token\n    test_prompt_txt = \"test{text}\"\n    test_prompt = TestPrompt(test_prompt_txt)\n    # set max_input_size=11\n    # For each text chunk, there's 4 tokens for text + 1 for the padding\n    prompt_helper = PromptHelper(\n        max_input_size=11, num_output=0, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    node1 = Node(text=\"This is a test foo bar\")\n    node2 = Node(text=\"Hello world bar foo\")\n\n    response = prompt_helper.get_text_from_nodes([node1, node2], prompt=test_prompt)\n    assert str(response) == (\"This is a test\\n\" \"Hello world bar foo\")\n\n\ndef test_get_numbered_text_from_nodes() -> None:\n    \"\"\"Test get_text_from_nodes.\"\"\"\n    # test prompt uses up one token\n    test_prompt_txt = \"test{text}\"\n    test_prompt = TestPrompt(test_prompt_txt)\n    # set max_input_size=17\n    # For each text chunk, there's 3 for text, 5 for padding (including number)\n    prompt_helper = PromptHelper(\n        max_input_size=17, num_output=0, max_chunk_overlap=0, tokenizer=mock_tokenizer\n    )\n    node1 = Node(text=\"This is a test foo bar\")\n    node2 = Node(text=\"Hello world bar foo\")\n\n    response = prompt_helper.get_numbered_text_from_nodes(\n        [node1, node2], prompt=test_prompt\n    )\n    assert str(response) == (\"(1) This is a\\n\\n(2) Hello world bar\")\n\n\ndef test_compact_text() -> None:\n    \"\"\"Test compact text.\"\"\"\n    test_prompt_text = \"This is the prompt{text}\"\n    test_prompt = TestPrompt(test_prompt_text)\n    prompt_helper = PromptHelper(\n        max_input_size=9,\n        num_output=1,\n        max_chunk_overlap=0,\n        tokenizer=mock_tokenizer,\n        separator=\"\\n\\n\",\n    )\n    text_chunks = [\"Hello\", \"world\", \"foo\", \"Hello\", \"world\", \"bar\"]\n    compacted_chunks = prompt_helper.compact_text_chunks(test_prompt, text_chunks)\n    assert compacted_chunks == [\"Hello\\n\\nworld\\n\\nfoo\", \"Hello\\n\\nworld\\n\\nbar\"]\n\n\ndef test_get_biggest_prompt() -> None:\n    \"\"\"Test get_biggest_prompt from PromptHelper.\"\"\"\n    # NOTE: inputs don't matter\n    prompt_helper = PromptHelper(max_input_size=1, num_output=1, max_chunk_overlap=0)\n    prompt1 = TestPrompt(\"This is the prompt{text}\")\n    prompt2 = TestPrompt(\"This is the longer prompt{text}\")\n    prompt3 = TestPrompt(\"This is the {text}\")\n    biggest_prompt = prompt_helper.get_biggest_prompt([prompt1, prompt2, prompt3])\n    assert biggest_prompt.prompt.template == prompt2.prompt.template\n", "doc_id": "726d88a507fa99e69d3f782fb2697d21049f82e0", "embedding": null, "extra_info": {"file_path": "tests/indices/test_prompt_helper.py", "file_name": "test_prompt_helper.py"}, "__type__": "Document"}, "c2a0685b15adc5262f0ab5349e856d6210e6e7bb": {"text": "\"\"\"Test response utils.\"\"\"\n\nfrom typing import Any, List\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom gpt_index.constants import MAX_CHUNK_OVERLAP, MAX_CHUNK_SIZE, NUM_OUTPUTS\nfrom gpt_index.indices.prompt_helper import PromptHelper\nfrom gpt_index.indices.response.builder import ResponseBuilder, ResponseMode, TextChunk\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\nfrom gpt_index.readers.schema.base import Document\nfrom tests.mock_utils.mock_decorator import patch_common\nfrom tests.mock_utils.mock_predict import mock_llmpredictor_predict\nfrom tests.mock_utils.mock_prompts import MOCK_REFINE_PROMPT, MOCK_TEXT_QA_PROMPT\n\n\n@pytest.fixture\ndef documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"\n    # NOTE: one document for now\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    return [Document(doc_text)]\n\n\ndef mock_tokenizer(text: str) -> List[str]:\n    \"\"\"Mock tokenizer.\"\"\"\n    if text == \"\":\n        return []\n    tokens = text.split(\" \")\n    return tokens\n\n\n@patch_common\ndef test_give_response(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test give response.\"\"\"\n    prompt_helper = PromptHelper(MAX_CHUNK_SIZE, NUM_OUTPUTS, MAX_CHUNK_OVERLAP)\n    llm_predictor = LLMPredictor()\n    query_str = \"What is?\"\n\n    # test single line\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n        MOCK_TEXT_QA_PROMPT,\n        MOCK_REFINE_PROMPT,\n        texts=[TextChunk(\"This is a single line.\")],\n    )\n    response = builder.get_response(query_str)\n    assert str(response) == \"What is?:This is a single line.\"\n\n    # test multiple lines\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n        MOCK_TEXT_QA_PROMPT,\n        MOCK_REFINE_PROMPT,\n        texts=[TextChunk(documents[0].get_text())],\n    )\n    response = builder.get_response(query_str)\n    assert str(response) == \"What is?:Hello world.\"\n\n\n@patch.object(LLMPredictor, \"total_tokens_used\", return_value=0)\n@patch.object(LLMPredictor, \"predict\", side_effect=mock_llmpredictor_predict)\n@patch.object(LLMPredictor, \"__init__\", return_value=None)\ndef test_compact_response(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test give response.\"\"\"\n    # test response with ResponseMode.COMPACT\n    # NOTE: here we want to guarante that prompts have 0 extra tokens\n    mock_refine_prompt_tmpl = \"{query_str}{existing_answer}{context_msg}\"\n    mock_refine_prompt = RefinePrompt(mock_refine_prompt_tmpl)\n\n    mock_qa_prompt_tmpl = \"{context_str}{query_str}\"\n    mock_qa_prompt = QuestionAnswerPrompt(mock_qa_prompt_tmpl)\n\n    # max input size is 11, prompt is two tokens (the query) --> 9 tokens\n    # --> padding is 1 --> 8 tokens\n    prompt_helper = PromptHelper(\n        11, 0, 0, tokenizer=mock_tokenizer, separator=\"\\n\\n\", chunk_size_limit=4\n    )\n    cur_chunk_size = prompt_helper.get_chunk_size_given_prompt(\"\", 1, padding=1)\n    # outside of compact, assert that chunk size is 4\n    assert cur_chunk_size == 4\n\n    # within compact, make sure that chunk size is 8\n    llm_predictor = LLMPredictor()\n    query_str = \"What is?\"\n    texts = [\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nbar\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\nanother\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nfoo\"),\n    ]\n\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n        mock_qa_prompt,\n        mock_refine_prompt,\n        texts=texts,\n    )\n    response = builder.get_response(query_str, mode=ResponseMode.COMPACT)\n    assert str(response) == (\n        \"What is?:\" \"This\\n\\nis\\n\\na\\n\\nbar\\n\\n\" \"This\\n\\nis\\n\\na\\n\\ntest\"\n    )\n\n\n@patch.object(LLMPredictor, \"total_tokens_used\", return_value=0)\n@patch.object(LLMPredictor, \"predict\", side_effect=mock_llmpredictor_predict)\n@patch.object(LLMPredictor, \"__init__\", return_value=None)\ndef test_tree_summarize_response(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    documents: List[Document],\n) -> None:\n    \"\"\"Test give response.\"\"\"\n    # test response with ResponseMode.TREE_SUMMARIZE\n    # NOTE: here we want to guarante that prompts have 0 extra tokens\n    mock_refine_prompt_tmpl = \"{query_str}{existing_answer}{context_msg}\"\n    mock_refine_prompt = RefinePrompt(mock_refine_prompt_tmpl)\n\n    mock_qa_prompt_tmpl = \"{context_str}{query_str}\"\n    mock_qa_prompt = QuestionAnswerPrompt(mock_qa_prompt_tmpl)\n\n    # max input size is 12, prompt tokens is 2 (query_str)\n    # --> 10 tokens for 2 chunks -->\n    # 5 tokens per chunk, 1 is padding --> 4 tokens per chunk\n    prompt_helper = PromptHelper(12, 0, 0, tokenizer=mock_tokenizer, separator=\"\\n\\n\")\n\n    # within tree_summarize, make sure that chunk size is 8\n    llm_predictor = LLMPredictor()\n    query_str = \"What is?\"\n    texts = [\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nbar\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\nanother\\n\\ntest\"),\n        TextChunk(\"This\\n\\nis\\n\\na\\n\\nfoo\"),\n    ]\n\n    builder = ResponseBuilder(\n        prompt_helper,\n        llm_predictor,\n        mock_qa_prompt,\n        mock_refine_prompt,\n        texts=texts,\n    )\n    response = builder.get_response(\n        query_str, mode=ResponseMode.TREE_SUMMARIZE, num_children=2\n    )\n    # TODO: fix this output, the \\n join appends unnecessary results at the end\n    assert str(response) == (\n        \"What is?:This\\n\\nis\\n\\na\\n\\nbar\\nThis\\n\" \"This\\n\\nis\\n\\nanother\\n\\ntest\\nThis\"\n    )\n", "doc_id": "c2a0685b15adc5262f0ab5349e856d6210e6e7bb", "embedding": null, "extra_info": {"file_path": "tests/indices/test_response.py", "file_name": "test_response.py"}, "__type__": "Document"}, "8027ba56fe884bcf3ec48d3b08a69aac608a1252": {"text": "\"\"\"Test indices/utils.py.\"\"\"\nfrom gpt_index.indices.utils import expand_tokens_with_subtokens\n\n\ndef test_expand_tokens_with_subtokens() -> None:\n    \"\"\"Test expand tokens.\"\"\"\n    tokens = {\"foo bar\", \"baz\", \"hello hello world bye\"}\n    keywords = expand_tokens_with_subtokens(tokens)\n    assert keywords == {\n        \"foo bar\",\n        \"foo\",\n        \"bar\",\n        \"baz\",\n        \"hello hello world bye\",\n        \"hello\",\n        \"world\",\n        \"bye\",\n    }\n", "doc_id": "8027ba56fe884bcf3ec48d3b08a69aac608a1252", "embedding": null, "extra_info": {"file_path": "tests/indices/test_utils.py", "file_name": "test_utils.py"}, "__type__": "Document"}, "afb4aa398df65ab659e297655a54660dc23d269e": {"text": "\"\"\"Test tree index.\"\"\"\n\nfrom typing import Any, Dict, List, Optional, Tuple\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom gpt_index.data_structs.data_structs import IndexGraph, Node\nfrom gpt_index.indices.tree.base import GPTTreeIndex\nfrom gpt_index.langchain_helpers.chain_wrapper import (\n    LLMChain,\n    LLMMetadata,\n    LLMPredictor,\n)\nfrom gpt_index.readers.schema.base import Document\nfrom tests.mock_utils.mock_decorator import patch_common\nfrom tests.mock_utils.mock_predict import mock_llmchain_predict\nfrom tests.mock_utils.mock_prompts import (\n    MOCK_INSERT_PROMPT,\n    MOCK_QUERY_PROMPT,\n    MOCK_REFINE_PROMPT,\n    MOCK_SUMMARY_PROMPT,\n    MOCK_TEXT_QA_PROMPT,\n)\n\n\n@pytest.fixture\ndef struct_kwargs() -> Tuple[Dict, Dict]:\n    \"\"\"Index kwargs.\"\"\"\n    index_kwargs = {\n        \"summary_template\": MOCK_SUMMARY_PROMPT,\n        \"insert_prompt\": MOCK_INSERT_PROMPT,\n        \"num_children\": 2,\n    }\n    query_kwargs = {\n        \"query_template\": MOCK_QUERY_PROMPT,\n        \"text_qa_template\": MOCK_TEXT_QA_PROMPT,\n        \"refine_template\": MOCK_REFINE_PROMPT,\n    }\n    return index_kwargs, query_kwargs\n\n\n@pytest.fixture\ndef documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"\n    # NOTE: one document for now\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    return [Document(doc_text)]\n\n\ndef _get_left_or_right_node(\n    index_graph: IndexGraph, node: Optional[Node], left: bool = True\n) -> Node:\n    \"\"\"Get 'left' or 'right' node.\"\"\"\n    if node is None:\n        indices = set(index_graph.root_nodes.keys())\n    else:\n        indices = node.child_indices\n\n    index = min(indices) if left else max(indices)\n\n    if index not in index_graph.all_nodes:\n        raise ValueError(f\"Node {index} not in index_graph.all_nodes\")\n    return index_graph.all_nodes[index]\n\n\n@patch_common\ndef test_build_tree(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test build tree.\"\"\"\n    index_kwargs, _ = struct_kwargs\n    tree = GPTTreeIndex(documents, **index_kwargs)\n    assert len(tree.index_struct.all_nodes) == 6\n    # check contents of nodes\n    assert tree.index_struct.all_nodes[0].text == \"Hello world.\"\n    assert tree.index_struct.all_nodes[1].text == \"This is a test.\"\n    assert tree.index_struct.all_nodes[2].text == \"This is another test.\"\n    assert tree.index_struct.all_nodes[3].text == \"This is a test v2.\"\n    assert tree.index_struct.all_nodes[4].text == (\"Hello world.\\nThis is a test.\")\n    assert tree.index_struct.all_nodes[5].text == (\n        \"This is another test.\\nThis is a test v2.\"\n    )\n\n\n@patch_common\ndef test_build_tree_with_embed(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test build tree.\"\"\"\n    index_kwargs, _ = struct_kwargs\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    document = Document(doc_text, embedding=[0.1, 0.2, 0.3])\n    tree = GPTTreeIndex([document], **index_kwargs)\n    assert len(tree.index_struct.all_nodes) == 6\n    # check contents of nodes\n    assert tree.index_struct.all_nodes[0].text == \"Hello world.\"\n    assert tree.index_struct.all_nodes[1].text == \"This is a test.\"\n    assert tree.index_struct.all_nodes[2].text == \"This is another test.\"\n    assert tree.index_struct.all_nodes[3].text == \"This is a test v2.\"\n    # make sure all leaf nodes have embeddings\n    for i in range(4):\n        assert tree.index_struct.all_nodes[i].embedding == [0.1, 0.2, 0.3]\n    assert tree.index_struct.all_nodes[4].text == (\"Hello world.\\nThis is a test.\")\n    assert tree.index_struct.all_nodes[5].text == (\n        \"This is another test.\\nThis is a test v2.\"\n    )\n\n\n@patch_common\ndef test_build_tree_multiple(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test build tree.\"\"\"\n    new_docs = [\n        Document(\"Hello world.\\nThis is a test.\"),\n        Document(\"This is another test.\\nThis is a test v2.\"),\n    ]\n    index_kwargs, _ = struct_kwargs\n    tree = GPTTreeIndex(new_docs, **index_kwargs)\n    assert len(tree.index_struct.all_nodes) == 6\n    # check contents of nodes\n    assert tree.index_struct.all_nodes[0].text == \"Hello world.\"\n    assert tree.index_struct.all_nodes[1].text == \"This is a test.\"\n    assert tree.index_struct.all_nodes[2].text == \"This is another test.\"\n    assert tree.index_struct.all_nodes[3].text == \"This is a test v2.\"\n\n\n@patch_common\ndef test_query(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test query.\"\"\"\n    index_kwargs, query_kwargs = struct_kwargs\n    tree = GPTTreeIndex(documents, **index_kwargs)\n\n    # test default query\n    query_str = \"What is?\"\n    response = tree.query(query_str, mode=\"default\", **query_kwargs)\n    assert str(response) == (\"What is?:Hello world.\")\n\n\n@patch_common\ndef test_summarize_query(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test summarize query.\"\"\"\n    # create tree index without building tree\n    index_kwargs, orig_query_kwargs = struct_kwargs\n    index_kwargs = index_kwargs.copy()\n    index_kwargs.update({\"build_tree\": False})\n    tree = GPTTreeIndex(documents, **index_kwargs)\n\n    # test summarize query\n    query_str = \"What is?\"\n    query_kwargs: Dict[str, Any] = {\n        \"text_qa_template\": MOCK_TEXT_QA_PROMPT,\n        \"num_children\": 2,\n    }\n    # TODO: fix unit test later\n    response = tree.query(query_str, mode=\"summarize\", **query_kwargs)\n    assert str(response) == (\"What is?:Hello world.\")\n\n    # test that default query fails\n    with pytest.raises(ValueError):\n        tree.query(query_str, mode=\"default\", **orig_query_kwargs)\n\n\n@patch_common\ndef test_insert(\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test insert.\"\"\"\n    index_kwargs, _ = struct_kwargs\n    tree = GPTTreeIndex(documents, **index_kwargs)\n\n    # test insert\n    new_doc = Document(\"This is a new doc.\", doc_id=\"new_doc\")\n    tree.insert(new_doc)\n    # Before:\n    # Left root node: \"Hello world.\\nThis is a test.\"\n    # \"Hello world.\", \"This is a test\" are two children of the left root node\n    # After:\n    # \"Hello world.\\nThis is a test\\n.\\nThis is a new doc.\" is the left root node\n    # \"Hello world\", \"This is a test\\n.This is a new doc.\" are the children\n    # of the left root node.\n    # \"This is a test\", \"This is a new doc.\" are the children of\n    # \"This is a test\\n.This is a new doc.\"\n    left_root = _get_left_or_right_node(tree.index_struct, None)\n    assert left_root.text == \"Hello world.\\nThis is a test.\\nThis is a new doc.\"\n    left_root2 = _get_left_or_right_node(tree.index_struct, left_root)\n    right_root2 = _get_left_or_right_node(tree.index_struct, left_root, left=False)\n    assert left_root2.text == \"Hello world.\"\n    assert right_root2.text == \"This is a test.\\nThis is a new doc.\"\n    left_root3 = _get_left_or_right_node(tree.index_struct, right_root2)\n    right_root3 = _get_left_or_right_node(tree.index_struct, right_root2, left=False)\n    assert left_root3.text == \"This is a test.\"\n    assert right_root3.text == \"This is a new doc.\"\n    assert right_root3.ref_doc_id == \"new_doc\"\n\n    # test insert from empty (no_id)\n    tree = GPTTreeIndex([], **index_kwargs)\n    new_doc = Document(\"This is a new doc.\")\n    tree.insert(new_doc)\n    assert len(tree.index_struct.all_nodes) == 1\n    assert tree.index_struct.all_nodes[0].text == \"This is a new doc.\"\n\n    # test insert from empty (with_id)\n    tree = GPTTreeIndex([], **index_kwargs)\n    new_doc = Document(\"This is a new doc.\", doc_id=\"new_doc_test\")\n    tree.insert(new_doc)\n    assert len(tree.index_struct.all_nodes) == 1\n    assert tree.index_struct.all_nodes[0].text == \"This is a new doc.\"\n    assert tree.index_struct.all_nodes[0].ref_doc_id == \"new_doc_test\"\n\n\n@patch.object(LLMChain, \"predict\", side_effect=mock_llmchain_predict)\n@patch(\"gpt_index.langchain_helpers.chain_wrapper.OpenAI\")\n@patch.object(LLMPredictor, \"get_llm_metadata\", return_value=LLMMetadata())\n@patch.object(LLMChain, \"__init__\", return_value=None)\ndef test_build_and_count_tokens(\n    _mock_init: Any,\n    _mock_llm_metadata: Any,\n    _mock_llmchain: Any,\n    _mock_predict: Any,\n    documents: List[Document],\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test build and count tokens.\"\"\"\n    index_kwargs, _ = struct_kwargs\n    # mock_prompts.MOCK_SUMMARY_PROMPT_TMPL adds a \"\\n\" to the document text\n    # and the document is 23 tokens\n    document_token_count = 24\n    llmchain_mock_resp_token_count = 10\n    tree = GPTTreeIndex(documents, **index_kwargs)\n    assert (\n        tree._llm_predictor.total_tokens_used\n        == document_token_count + llmchain_mock_resp_token_count\n    )\n", "doc_id": "afb4aa398df65ab659e297655a54660dc23d269e", "embedding": null, "extra_info": {"file_path": "tests/indices/tree/test_base.py", "file_name": "test_base.py"}, "__type__": "Document"}, "a79e316284dd227757c065acf5e0401fcc19e75d": {"text": "\"\"\"Test Faiss index.\"\"\"\n\nimport sys\nfrom typing import Any, Dict, List, Tuple\nfrom unittest.mock import MagicMock, patch\n\nimport numpy as np\nimport pytest\n\nfrom gpt_index.embeddings.openai import OpenAIEmbedding\nfrom gpt_index.indices.vector_store.faiss import GPTFaissIndex\nfrom gpt_index.indices.vector_store.simple import GPTSimpleVectorIndex\nfrom gpt_index.readers.schema.base import Document\nfrom tests.mock_utils.mock_decorator import patch_common\nfrom tests.mock_utils.mock_prompts import MOCK_REFINE_PROMPT, MOCK_TEXT_QA_PROMPT\n\n\n@pytest.fixture\ndef struct_kwargs() -> Tuple[Dict, Dict]:\n    \"\"\"Index kwargs.\"\"\"\n    index_kwargs = {\n        \"text_qa_template\": MOCK_TEXT_QA_PROMPT,\n    }\n    query_kwargs = {\n        \"text_qa_template\": MOCK_TEXT_QA_PROMPT,\n        \"refine_template\": MOCK_REFINE_PROMPT,\n        \"similarity_top_k\": 1,\n    }\n    return index_kwargs, query_kwargs\n\n\n@pytest.fixture\ndef documents() -> List[Document]:\n    \"\"\"Get documents.\"\"\"\n    # NOTE: one document for now\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    return [Document(doc_text)]\n\n\nclass MockFaissIndex:\n    \"\"\"Mock Faiss index.\"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._index: Dict[int, np.ndarray] = {}\n\n    @property\n    def ntotal(self) -> int:\n        \"\"\"Get ntotal.\"\"\"\n        return len(self._index)\n\n    def add(self, vecs: np.ndarray) -> None:\n        \"\"\"Add vectors to index.\"\"\"\n        for vec in vecs:\n            new_id = len(self._index)\n            self._index[new_id] = vec\n\n    def reset(self) -> None:\n        \"\"\"Reset index.\"\"\"\n        self._index = {}\n\n    def search(self, vec: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Search index.\"\"\"\n        # assume query vec is of the form 1 x k\n        # index_mat is n x k\n        index_mat = np.array(list(self._index.values()))\n        # compute distances\n        distances = np.linalg.norm(index_mat - vec, axis=1)\n\n        indices = np.argsort(distances)[:k]\n        sorted_distances = distances[indices][:k]\n\n        # return distances and indices\n        return sorted_distances[np.newaxis, :], indices[np.newaxis, :]\n\n\ndef mock_get_text_embedding(text: str) -> List[float]:\n    \"\"\"Mock get text embedding.\"\"\"\n    # assume dimensions are 5\n    if text == \"Hello world.\":\n        return [1, 0, 0, 0, 0]\n    elif text == \"This is a test.\":\n        return [0, 1, 0, 0, 0]\n    elif text == \"This is another test.\":\n        return [0, 0, 1, 0, 0]\n    elif text == \"This is a test v2.\":\n        return [0, 0, 0, 1, 0]\n    elif text == \"This is a test v3.\":\n        return [0, 0, 0, 0, 1]\n    elif text == \"This is bar test.\":\n        return [0, 0, 1, 0, 0]\n    elif text == \"Hello world backup.\":\n        # this is used when \"Hello world.\" is deleted.\n        return [1, 0, 0, 0, 0]\n    else:\n        raise ValueError(\"Invalid text for `mock_get_text_embedding`.\")\n\n\ndef mock_get_query_embedding(query: str) -> List[float]:\n    \"\"\"Mock get query embedding.\"\"\"\n    return [0, 0, 1, 0, 0]\n\n\n@patch_common\n@patch.object(\n    OpenAIEmbedding, \"get_text_embedding\", side_effect=mock_get_text_embedding\n)\ndef test_build_faiss(\n    _mock_embed: Any,\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test build GPTFaissIndex.\"\"\"\n    # NOTE: mock faiss import\n    sys.modules[\"faiss\"] = MagicMock()\n    # NOTE: mock faiss index\n    faiss_index = MockFaissIndex()\n\n    index_kwargs, query_kwargs = struct_kwargs\n\n    index = GPTFaissIndex(documents=documents, faiss_index=faiss_index, **index_kwargs)\n    assert len(index.index_struct.nodes_dict) == 4\n    # check contents of nodes\n    assert index.index_struct.get_node(\"0\").text == \"Hello world.\"\n    assert index.index_struct.get_node(\"1\").text == \"This is a test.\"\n    assert index.index_struct.get_node(\"2\").text == \"This is another test.\"\n    assert index.index_struct.get_node(\"3\").text == \"This is a test v2.\"\n\n\n@patch_common\n@patch.object(\n    OpenAIEmbedding, \"get_text_embedding\", side_effect=mock_get_text_embedding\n)\ndef test_faiss_insert(\n    _mock_embed: Any,\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test build GPTFaissIndex.\"\"\"\n    # NOTE: mock faiss import\n    sys.modules[\"faiss\"] = MagicMock()\n    # NOTE: mock faiss index\n    faiss_index = MockFaissIndex()\n\n    index_kwargs, query_kwargs = struct_kwargs\n\n    index = GPTFaissIndex(documents=documents, faiss_index=faiss_index, **index_kwargs)\n    # insert into index\n    index.insert(Document(text=\"This is a test v3.\"))\n\n    # check contenst of nodes\n    assert index.index_struct.get_node(\"3\").text == \"This is a test v2.\"\n    assert index.index_struct.get_node(\"4\").text == \"This is a test v3.\"\n\n\n@patch_common\n@patch.object(\n    OpenAIEmbedding, \"get_text_embedding\", side_effect=mock_get_text_embedding\n)\n@patch.object(\n    OpenAIEmbedding, \"get_query_embedding\", side_effect=mock_get_query_embedding\n)\ndef test_faiss_query(\n    _mock_query_embed: Any,\n    _mock_text_embed: Any,\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test embedding query.\"\"\"\n    # NOTE: mock faiss import\n    sys.modules[\"faiss\"] = MagicMock()\n    # NOTE: mock faiss index\n    faiss_index = MockFaissIndex()\n\n    index_kwargs, query_kwargs = struct_kwargs\n    index = GPTFaissIndex(documents, faiss_index=faiss_index, **index_kwargs)\n\n    # test embedding query\n    query_str = \"What is?\"\n    response = index.query(query_str, **query_kwargs)\n    assert str(response) == (\"What is?:This is another test.\")\n\n\n@patch_common\n@patch.object(\n    OpenAIEmbedding, \"get_text_embedding\", side_effect=mock_get_text_embedding\n)\ndef test_build_simple(\n    _mock_embed: Any,\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test build GPTFaissIndex.\"\"\"\n    index_kwargs, query_kwargs = struct_kwargs\n\n    index = GPTSimpleVectorIndex(documents=documents, **index_kwargs)\n    assert len(index.index_struct.nodes_dict) == 4\n    # check contents of nodes\n    actual_node_tups = [\n        (\"Hello world.\", [1, 0, 0, 0, 0]),\n        (\"This is a test.\", [0, 1, 0, 0, 0]),\n        (\"This is another test.\", [0, 0, 1, 0, 0]),\n        (\"This is a test v2.\", [0, 0, 0, 1, 0]),\n    ]\n    for text_id in index.index_struct.id_map.keys():\n        node = index.index_struct.get_node(text_id)\n        embedding = index.index_struct.embedding_dict[text_id]\n        assert (node.text, embedding) in actual_node_tups\n\n\n@patch_common\n@patch.object(\n    OpenAIEmbedding, \"get_text_embedding\", side_effect=mock_get_text_embedding\n)\ndef test_simple_insert(\n    _mock_embed: Any,\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test build GPTFaissIndex.\"\"\"\n    index_kwargs, query_kwargs = struct_kwargs\n\n    index = GPTSimpleVectorIndex(documents=documents, **index_kwargs)\n    # insert into index\n    index.insert(Document(text=\"This is a test v3.\"))\n\n    # check contenst of nodes\n    actual_node_tups = [\n        (\"Hello world.\", [1, 0, 0, 0, 0]),\n        (\"This is a test.\", [0, 1, 0, 0, 0]),\n        (\"This is another test.\", [0, 0, 1, 0, 0]),\n        (\"This is a test v2.\", [0, 0, 0, 1, 0]),\n        (\"This is a test v3.\", [0, 0, 0, 0, 1]),\n    ]\n    for text_id in index.index_struct.id_map.keys():\n        node = index.index_struct.get_node(text_id)\n        embedding = index.index_struct.embedding_dict[text_id]\n        assert (node.text, embedding) in actual_node_tups\n\n\n@patch_common\n@patch.object(\n    OpenAIEmbedding, \"get_text_embedding\", side_effect=mock_get_text_embedding\n)\ndef test_simple_delete(\n    _mock_embed: Any,\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_splitter_overlap: Any,\n    _mock_splitter: Any,\n    documents: List[Document],\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test build GPTFaissIndex.\"\"\"\n    index_kwargs, query_kwargs = struct_kwargs\n\n    new_documents = [\n        Document(\"Hello world.\", doc_id=\"test_id_0\"),\n        Document(\"This is a test.\", doc_id=\"test_id_1\"),\n        Document(\"This is another test.\", doc_id=\"test_id_2\"),\n        Document(\"This is a test v2.\", doc_id=\"test_id_3\"),\n    ]\n    index = GPTSimpleVectorIndex(documents=new_documents, **index_kwargs)\n\n    # test delete\n    index.delete(\"test_id_0\")\n    assert len(index.index_struct.nodes_dict) == 3\n    assert len(index.index_struct.id_map) == 3\n    actual_node_tups = [\n        (\"This is a test.\", [0, 1, 0, 0, 0], \"test_id_1\"),\n        (\"This is another test.\", [0, 0, 1, 0, 0], \"test_id_2\"),\n        (\"This is a test v2.\", [0, 0, 0, 1, 0], \"test_id_3\"),\n    ]\n    for text_id in index.index_struct.id_map.keys():\n        node = index.index_struct.get_node(text_id)\n        embedding = index.index_struct.embedding_dict[text_id]\n        assert (node.text, embedding, node.ref_doc_id) in actual_node_tups\n\n    # test insert\n    index.insert(Document(\"Hello world backup.\", doc_id=\"test_id_0\"))\n    assert len(index.index_struct.nodes_dict) == 4\n    assert len(index.index_struct.id_map) == 4\n    actual_node_tups = [\n        (\"Hello world backup.\", [1, 0, 0, 0, 0], \"test_id_0\"),\n        (\"This is a test.\", [0, 1, 0, 0, 0], \"test_id_1\"),\n        (\"This is another test.\", [0, 0, 1, 0, 0], \"test_id_2\"),\n        (\"This is a test v2.\", [0, 0, 0, 1, 0], \"test_id_3\"),\n    ]\n    for text_id in index.index_struct.id_map.keys():\n        node = index.index_struct.get_node(text_id)\n        embedding = index.index_struct.embedding_dict[text_id]\n        assert (node.text, embedding, node.ref_doc_id) in actual_node_tups\n\n\n@patch_common\n@patch.object(\n    OpenAIEmbedding, \"get_text_embedding\", side_effect=mock_get_text_embedding\n)\n@patch.object(\n    OpenAIEmbedding, \"get_query_embedding\", side_effect=mock_get_query_embedding\n)\ndef test_simple_query(\n    _mock_query_embed: Any,\n    _mock_text_embed: Any,\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    documents: List[Document],\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test embedding query.\"\"\"\n    index_kwargs, query_kwargs = struct_kwargs\n    index = GPTSimpleVectorIndex(documents, **index_kwargs)\n\n    # test embedding query\n    query_str = \"What is?\"\n    response = index.query(query_str, **query_kwargs)\n    assert str(response) == (\"What is?:This is another test.\")\n\n    # test with keyword filter (required)\n    query_kwargs_copy = query_kwargs.copy()\n    query_kwargs_copy[\"similarity_top_k\"] = 5\n    response = index.query(query_str, **query_kwargs_copy, required_keywords=[\"Hello\"])\n    assert str(response) == (\"What is?:Hello world.\")\n\n    # test with keyword filter (exclude)\n    # insert into index\n    index.insert(Document(text=\"This is bar test.\"))\n    query_kwargs_copy = query_kwargs.copy()\n    query_kwargs_copy[\"similarity_top_k\"] = 2\n    response = index.query(query_str, **query_kwargs_copy, exclude_keywords=[\"another\"])\n    assert str(response) == (\"What is?:This is bar test.\")\n\n\n@patch_common\n@patch.object(\n    OpenAIEmbedding, \"_get_text_embedding\", side_effect=mock_get_text_embedding\n)\n@patch.object(\n    OpenAIEmbedding, \"_get_query_embedding\", side_effect=mock_get_query_embedding\n)\ndef test_query_and_count_tokens(\n    _mock_query_embed: Any,\n    _mock_text_embed: Any,\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test embedding query.\"\"\"\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    document = Document(doc_text)\n    index_kwargs, query_kwargs = struct_kwargs\n    index = GPTSimpleVectorIndex([document], **index_kwargs)\n    assert index.embed_model.total_tokens_used == 20\n\n    # test embedding query\n    query_str = \"What is?\"\n    index.query(query_str, **query_kwargs)\n    assert index.embed_model.last_token_usage == 3\n\n\n@patch_common\n@patch.object(\n    OpenAIEmbedding, \"_get_text_embedding\", side_effect=mock_get_text_embedding\n)\n@patch.object(\n    OpenAIEmbedding, \"_get_query_embedding\", side_effect=mock_get_query_embedding\n)\ndef test_query_and_similarity_scores(\n    _mock_query_embed: Any,\n    _mock_text_embed: Any,\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test that sources nodes have similarity scores.\"\"\"\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    document = Document(doc_text)\n    index_kwargs, query_kwargs = struct_kwargs\n    index = GPTSimpleVectorIndex([document], **index_kwargs)\n\n    # test embedding query\n    query_str = \"What is?\"\n    response = index.query(query_str, **query_kwargs)\n    assert len(response.source_nodes) > 0\n    assert response.source_nodes[0].similarity is not None\n\n\n@patch_common\n@patch.object(\n    OpenAIEmbedding, \"_get_text_embedding\", side_effect=mock_get_text_embedding\n)\n@patch.object(\n    OpenAIEmbedding, \"_get_query_embedding\", side_effect=mock_get_query_embedding\n)\ndef test_query_and_similarity_scores_with_cutoff(\n    _mock_query_embed: Any,\n    _mock_text_embed: Any,\n    _mock_init: Any,\n    _mock_predict: Any,\n    _mock_total_tokens_used: Any,\n    _mock_split_text_overlap: Any,\n    _mock_split_text: Any,\n    struct_kwargs: Dict,\n) -> None:\n    \"\"\"Test that sources nodes have similarity scores.\"\"\"\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    document = Document(doc_text)\n    index_kwargs, query_kwargs = struct_kwargs\n    index = GPTSimpleVectorIndex([document], **index_kwargs)\n\n    # test embedding query - no nodes\n    query_str = \"What is?\"\n    response = index.query(query_str, similarity_cutoff=1.1, **query_kwargs)\n    assert len(response.source_nodes) == 0\n\n    # test embedding query - 1 node\n    query_str = \"What is?\"\n    response = index.query(query_str, similarity_cutoff=0.9, **query_kwargs)\n    assert len(response.source_nodes) == 1\n", "doc_id": "a79e316284dd227757c065acf5e0401fcc19e75d", "embedding": null, "extra_info": {"file_path": "tests/indices/vector_store/test_base.py", "file_name": "test_base.py"}, "__type__": "Document"}, "ddb2540512e86c257e1680bbd058b930257b3b9e": {"text": "\"\"\"Test text splitter.\"\"\"\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\n\n\ndef test_split_token() -> None:\n    \"\"\"Test split normal token.\"\"\"\n    # tiktoken will say length is ~5k\n    token = \"foo bar\"\n    text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)\n    chunks = text_splitter.split_text(token)\n    assert chunks == [\"foo\", \"bar\"]\n\n    token = \"foo bar hello world\"\n    text_splitter = TokenTextSplitter(chunk_size=2, chunk_overlap=1)\n    chunks = text_splitter.split_text(token)\n    assert chunks == [\"foo bar\", \"bar hello\", \"hello world\"]\n\n\ndef test_truncate_token() -> None:\n    \"\"\"Test truncate normal token.\"\"\"\n    # tiktoken will say length is ~5k\n    token = \"foo bar\"\n    text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)\n    chunks = text_splitter.truncate_text(token)\n    assert chunks == \"foo\"\n\n\ndef test_split_long_token() -> None:\n    \"\"\"Test split a really long token.\"\"\"\n    # tiktoken will say length is ~5k\n    token = \"a\" * 100\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    text_splitter.split_text(token)\n\n    token = (\"a\" * 49) + \"\\n\" + (\"a\" * 50)\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    chunks = text_splitter.split_text(token)\n    assert len(chunks[0]) == 49\n    assert len(chunks[1]) == 50\n\n\ndef test_split_with_extra_info_str() -> None:\n    \"\"\"Test split while taking into account chunk size used by extra info str.\"\"\"\n    text = \" \".join([\"foo\"] * 20)\n    extra_info_str = \"test_extra_info_str\"\n\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    chunks = text_splitter.split_text(text)\n    assert len(chunks) == 1\n\n    text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)\n    chunks = text_splitter.split_text(text, extra_info_str=extra_info_str)\n    assert len(chunks) == 2\n", "doc_id": "ddb2540512e86c257e1680bbd058b930257b3b9e", "embedding": null, "extra_info": {"file_path": "tests/langchain_helpers/test_text_splitter.py", "file_name": "test_text_splitter.py"}, "__type__": "Document"}, "ce69c63dfad837fbe436965bac2049338e5fd9e5": {"text": "\"\"\"Shared decorator.\"\"\"\nimport functools\nfrom typing import Any, Callable\nfrom unittest.mock import patch\n\nfrom gpt_index.langchain_helpers.chain_wrapper import LLMPredictor\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom tests.mock_utils.mock_predict import mock_llmpredictor_predict\nfrom tests.mock_utils.mock_text_splitter import (\n    mock_token_splitter_newline,\n    mock_token_splitter_newline_with_overlaps,\n)\n\n\ndef patch_common(f: Callable) -> Callable:\n    \"\"\"Create patch decorator with common mocks.\"\"\"\n\n    @patch.object(\n        TokenTextSplitter, \"split_text\", side_effect=mock_token_splitter_newline\n    )\n    @patch.object(\n        TokenTextSplitter,\n        \"split_text_with_overlaps\",\n        side_effect=mock_token_splitter_newline_with_overlaps,\n    )\n    @patch.object(LLMPredictor, \"total_tokens_used\", return_value=0)\n    @patch.object(LLMPredictor, \"predict\", side_effect=mock_llmpredictor_predict)\n    @patch.object(LLMPredictor, \"__init__\", return_value=None)\n    @functools.wraps(f)\n    def functor(*args: Any, **kwargs: Any) -> Any:\n        return f(*args, **kwargs)\n\n    return functor\n", "doc_id": "ce69c63dfad837fbe436965bac2049338e5fd9e5", "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_decorator.py", "file_name": "mock_decorator.py"}, "__type__": "Document"}, "af0672c6bf667916d04434c0959d199c30af3bb8": {"text": "\"\"\"Mock predict.\"\"\"\n\nfrom typing import Any, Dict, Tuple\n\nfrom gpt_index.prompts.base import Prompt\nfrom gpt_index.prompts.prompt_type import PromptType\nfrom gpt_index.token_counter.utils import mock_extract_keywords_response\n\n\ndef _mock_summary_predict(prompt_args: Dict) -> str:\n    \"\"\"Mock summary predict.\"\"\"\n    return prompt_args[\"context_str\"]\n\n\ndef _mock_insert_predict() -> str:\n    \"\"\"Mock insert predict.\n\n    Used in GPT tree index during insertion\n    to select the next node.\n\n    \"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_query_select() -> str:\n    \"\"\"Mock query predict.\n\n    Used in GPT tree index during query traversal\n    to select the next node.\n\n    \"\"\"\n    return \"ANSWER: 1\"\n\n\ndef _mock_answer(prompt_args: Dict) -> str:\n    \"\"\"Mock answer.\"\"\"\n    return prompt_args[\"query_str\"] + \":\" + prompt_args[\"context_str\"]\n\n\ndef _mock_refine(prompt_args: Dict) -> str:\n    \"\"\"Mock refine.\"\"\"\n    return prompt_args[\"existing_answer\"]\n\n\ndef _mock_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"text\"])\n\n\ndef _mock_query_keyword_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock query keyword extract.\"\"\"\n    return mock_extract_keywords_response(prompt_args[\"question\"])\n\n\ndef _mock_schema_extract(prompt_args: Dict) -> str:\n    \"\"\"Mock schema extract.\"\"\"\n    return prompt_args[\"text\"]\n\n\ndef _mock_text_to_sql(prompt_args: Dict) -> str:\n    \"\"\"Mock text to sql.\"\"\"\n    # assume it's a select query\n    tokens = prompt_args[\"query_str\"].split(\":\")\n    table_name = tokens[0]\n    subtokens = tokens[1].split(\",\")\n    return \"SELECT \" + \", \".join(subtokens) + f\" FROM {table_name}\"\n\n\ndef mock_llmpredictor_predict(prompt: Prompt, **prompt_args: Any) -> Tuple[str, str]:\n    \"\"\"Mock predict method of LLMPredictor.\n\n    Depending on the prompt, return response.\n\n    \"\"\"\n    formatted_prompt = prompt.format(**prompt_args)\n    full_prompt_args = prompt.get_full_format_args(prompt_args)\n    if prompt.prompt_type == PromptType.SUMMARY:\n        response = _mock_summary_predict(full_prompt_args)\n    elif prompt.prompt_type == PromptType.TREE_INSERT:\n        response = _mock_insert_predict()\n    elif prompt.prompt_type == PromptType.TREE_SELECT:\n        response = _mock_query_select()\n    elif prompt.prompt_type == PromptType.REFINE:\n        response = _mock_refine(full_prompt_args)\n    elif prompt.prompt_type == PromptType.QUESTION_ANSWER:\n        response = _mock_answer(full_prompt_args)\n    elif prompt.prompt_type == PromptType.KEYWORD_EXTRACT:\n        response = _mock_keyword_extract(full_prompt_args)\n    elif prompt.prompt_type == PromptType.QUERY_KEYWORD_EXTRACT:\n        response = _mock_query_keyword_extract(full_prompt_args)\n    elif prompt.prompt_type == PromptType.SCHEMA_EXTRACT:\n        response = _mock_schema_extract(full_prompt_args)\n    elif prompt.prompt_type == PromptType.TEXT_TO_SQL:\n        response = _mock_text_to_sql(full_prompt_args)\n    else:\n        raise ValueError(\"Invalid prompt to use with mocks.\")\n\n    return response, formatted_prompt\n\n\ndef mock_llmchain_predict(**full_prompt_args: Any) -> str:\n    \"\"\"Mock LLMChain predict with a generic response.\"\"\"\n    return \"generic response from LLMChain.predict()\"\n", "doc_id": "af0672c6bf667916d04434c0959d199c30af3bb8", "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_predict.py", "file_name": "mock_predict.py"}, "__type__": "Document"}, "f058467d8a4c77454a032b47a6ca7c62fb7aced6": {"text": "\"\"\"Mock prompt utils.\"\"\"\n\nfrom gpt_index.prompts.prompts import (\n    KeywordExtractPrompt,\n    QueryKeywordExtractPrompt,\n    QuestionAnswerPrompt,\n    RefinePrompt,\n    SchemaExtractPrompt,\n    SummaryPrompt,\n    TableContextPrompt,\n    TextToSQLPrompt,\n    TreeInsertPrompt,\n    TreeSelectPrompt,\n)\n\nMOCK_SUMMARY_PROMPT_TMPL = \"{context_str}\\n\"\nMOCK_SUMMARY_PROMPT = SummaryPrompt(MOCK_SUMMARY_PROMPT_TMPL)\n\nMOCK_INSERT_PROMPT_TMPL = \"{num_chunks}\\n{context_list}{new_chunk_text}\\n\"\nMOCK_INSERT_PROMPT = TreeInsertPrompt(MOCK_INSERT_PROMPT_TMPL)\n\n# # single choice\nMOCK_QUERY_PROMPT_TMPL = \"{num_chunks}\\n\" \"{context_list}\\n\" \"{query_str}'\\n\"\nMOCK_QUERY_PROMPT = TreeSelectPrompt(MOCK_QUERY_PROMPT_TMPL)\n\n\nMOCK_REFINE_PROMPT_TMPL = \"{query_str}\\n\" \"{existing_answer}\\n\" \"{context_msg}\\n\"\nMOCK_REFINE_PROMPT = RefinePrompt(MOCK_REFINE_PROMPT_TMPL)\n\n\nMOCK_TEXT_QA_PROMPT_TMPL = \"{context_str}\\n\" \"{query_str}\\n\"\nMOCK_TEXT_QA_PROMPT = QuestionAnswerPrompt(MOCK_TEXT_QA_PROMPT_TMPL)\n\n\nMOCK_KEYWORD_EXTRACT_PROMPT_TMPL = \"{max_keywords}\\n{text}\\n\"\nMOCK_KEYWORD_EXTRACT_PROMPT = KeywordExtractPrompt(MOCK_KEYWORD_EXTRACT_PROMPT_TMPL)\n\n# TODO: consolidate with keyword extract\nMOCK_QUERY_KEYWORD_EXTRACT_PROMPT_TMPL = \"{max_keywords}\\n{question}\\n\"\nMOCK_QUERY_KEYWORD_EXTRACT_PROMPT = QueryKeywordExtractPrompt(\n    MOCK_QUERY_KEYWORD_EXTRACT_PROMPT_TMPL\n)\n\n\nMOCK_SCHEMA_EXTRACT_PROMPT_TMPL = \"{text}\\n{schema}\"\nMOCK_SCHEMA_EXTRACT_PROMPT = SchemaExtractPrompt(MOCK_SCHEMA_EXTRACT_PROMPT_TMPL)\n\nMOCK_TEXT_TO_SQL_PROMPT_TMPL = \"{schema}\\n{query_str}\"\nMOCK_TEXT_TO_SQL_PROMPT = TextToSQLPrompt(MOCK_TEXT_TO_SQL_PROMPT_TMPL)\n\n\nMOCK_TABLE_CONTEXT_PROMPT_TMPL = \"{schema}\\n{context_str}\\n{query_str}\"\nMOCK_TABLE_CONTEXT_PROMPT = TableContextPrompt(MOCK_TABLE_CONTEXT_PROMPT_TMPL)\n", "doc_id": "f058467d8a4c77454a032b47a6ca7c62fb7aced6", "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_prompts.py", "file_name": "mock_prompts.py"}, "__type__": "Document"}, "5a3061faae831fc4d5601f0ac4e30c811c1efe28": {"text": "\"\"\"Mock text splitter.\"\"\"\n\nfrom typing import List, Optional\n\nfrom gpt_index.langchain_helpers.text_splitter import TextSplit\n\n\ndef mock_token_splitter_newline(\n    text: str, extra_info_str: Optional[str] = None\n) -> List[str]:\n    \"\"\"Mock token splitter by newline.\"\"\"\n    if text == \"\":\n        return []\n    return text.split(\"\\n\")\n\n\ndef mock_token_splitter_newline_with_overlaps(\n    text: str, extra_info_str: Optional[str]\n) -> List[TextSplit]:\n    \"\"\"Mock token splitter by newline.\"\"\"\n    if text == \"\":\n        return []\n    strings = text.split(\"\\n\")\n    return [TextSplit(string, 0) for string in strings]\n", "doc_id": "5a3061faae831fc4d5601f0ac4e30c811c1efe28", "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_text_splitter.py", "file_name": "mock_text_splitter.py"}, "__type__": "Document"}, "894c95fe14837faf82dcd09828b0859c16101e81": {"text": "\"\"\"Mock utils.\"\"\"\n\nfrom typing import List, Optional, Set\n\nfrom gpt_index.indices.keyword_table.utils import simple_extract_keywords\n\n\ndef mock_tokenizer(text: str) -> List[str]:\n    \"\"\"Mock tokenizer.\"\"\"\n    tokens = text.split(\" \")\n    result = []\n    for token in tokens:\n        if token.strip() == \"\":\n            continue\n        result.append(token.strip())\n    return result\n\n\ndef mock_extract_keywords(\n    text_chunk: str, max_keywords: Optional[int] = None, filter_stopwords: bool = True\n) -> Set[str]:\n    \"\"\"Extract keywords (mock).\n\n    Same as simple_extract_keywords but without filtering stopwords.\n\n    \"\"\"\n    return simple_extract_keywords(\n        text_chunk, max_keywords=max_keywords, filter_stopwords=False\n    )\n", "doc_id": "894c95fe14837faf82dcd09828b0859c16101e81", "embedding": null, "extra_info": {"file_path": "tests/mock_utils/mock_utils.py", "file_name": "mock_utils.py"}, "__type__": "Document"}, "e4b5eca71db3211c5ea203a9274fc6029bb553be": {"text": "\"\"\"Test prompts.\"\"\"\n\nfrom typing import List\n\nimport pytest\nfrom langchain import PromptTemplate\n\nfrom gpt_index.prompts.base import Prompt\n\n\nclass TestPrompt(Prompt):\n    \"\"\"Test prompt class.\"\"\"\n\n    input_variables: List[str] = [\"text\", \"foo\"]\n\n\ndef test_prompt_validate() -> None:\n    \"\"\"Test prompt validate.\"\"\"\n    # assert passes\n    prompt_txt = \"hello {text} {foo}\"\n    TestPrompt(prompt_txt)\n\n    # assert fails (missing required values)\n    with pytest.raises(ValueError):\n        prompt_txt = \"hello {tmp}\"\n        TestPrompt(prompt_txt)\n\n    # assert fails (extraneous values)\n    with pytest.raises(ValueError):\n        prompt_txt = \"hello {text} {foo} {text2}\"\n        TestPrompt(prompt_txt)\n\n\ndef test_partial_format() -> None:\n    \"\"\"Test partial format.\"\"\"\n    prompt_txt = \"hello {text} {foo}\"\n    prompt = TestPrompt(prompt_txt)\n\n    prompt_fmt = prompt.partial_format(foo=\"bar\")\n\n    assert isinstance(prompt_fmt, TestPrompt)\n    assert prompt_fmt.format(text=\"world\") == \"hello world bar\"\n\n\ndef test_from_prompt() -> None:\n    \"\"\"Test new prompt from a partially formatted prompt.\"\"\"\n\n    class TestPromptTextOnly(Prompt):\n        \"\"\"Test prompt class.\"\"\"\n\n        input_variables: List[str] = [\"text\"]\n\n    prompt_txt = \"hello {text} {foo}\"\n    prompt = TestPrompt(prompt_txt)\n    prompt_fmt = prompt.partial_format(foo=\"bar\")\n\n    prompt_new = TestPromptTextOnly.from_prompt(prompt_fmt)\n    assert isinstance(prompt_new, TestPromptTextOnly)\n\n    assert prompt_new.format(text=\"world2\") == \"hello world2 bar\"\n\n\ndef test_from_langchain_prompt() -> None:\n    \"\"\"Test from langchain prompt.\"\"\"\n    prompt_txt = \"hello {text} {foo}\"\n    prompt = PromptTemplate(input_variables=[\"text\", \"foo\"], template=prompt_txt)\n    prompt_new = TestPrompt.from_langchain_prompt(prompt)\n\n    assert isinstance(prompt_new, TestPrompt)\n    assert prompt_new.prompt == prompt\n    assert prompt_new.format(text=\"world2\", foo=\"bar\") == \"hello world2 bar\"\n\n    # test errors if langchain prompt input var doesn't match\n    with pytest.raises(ValueError):\n        prompt_txt = \"hello {text} {foo} {tmp}\"\n        prompt = PromptTemplate(\n            input_variables=[\"text\", \"foo\", \"tmp\"], template=prompt_txt\n        )\n        TestPrompt.from_langchain_prompt(prompt)\n\n    # test errors if we specify both template and langchain prompt\n    with pytest.raises(ValueError):\n        prompt_txt = \"hello {text} {foo}\"\n        prompt = PromptTemplate(input_variables=[\"text\", \"foo\"], template=prompt_txt)\n        TestPrompt(template=prompt_txt, langchain_prompt=prompt)\n", "doc_id": "e4b5eca71db3211c5ea203a9274fc6029bb553be", "embedding": null, "extra_info": {"file_path": "tests/prompts/test_base.py", "file_name": "test_base.py"}, "__type__": "Document"}, "808ff010c8376b1ee787cab26b9817eab331054f": {"text": "\"\"\"Test file reader.\"\"\"\n\nfrom tempfile import TemporaryDirectory\nfrom typing import Any, Dict\n\nfrom gpt_index.readers.file.base import SimpleDirectoryReader\n\n\ndef test_recursive() -> None:\n    \"\"\"Test simple directory reader in recursive mode.\"\"\"\n    # test recursive\n    with TemporaryDirectory() as tmp_dir:\n        with open(f\"{tmp_dir}/test1.txt\", \"w\") as f:\n            f.write(\"test1\")\n        with TemporaryDirectory(dir=tmp_dir) as tmp_sub_dir:\n            with open(f\"{tmp_sub_dir}/test2.txt\", \"w\") as f:\n                f.write(\"test2\")\n            with TemporaryDirectory(dir=tmp_sub_dir) as tmp_sub_sub_dir:\n                with open(f\"{tmp_sub_sub_dir}/test3.txt\", \"w\") as f:\n                    f.write(\"test3\")\n                with open(f\"{tmp_sub_sub_dir}/test4.txt\", \"w\") as f:\n                    f.write(\"test4\")\n\n                    reader = SimpleDirectoryReader(tmp_dir, recursive=True)\n                    input_file_names = [f.name for f in reader.input_files]\n                    assert len(reader.input_files) == 4\n                    assert set(input_file_names) == {\n                        \"test1.txt\",\n                        \"test2.txt\",\n                        \"test3.txt\",\n                        \"test4.txt\",\n                    }\n\n    # test that recursive=False works\n    with TemporaryDirectory() as tmp_dir:\n        with open(f\"{tmp_dir}/test1.txt\", \"w\") as f:\n            f.write(\"test1\")\n        with TemporaryDirectory(dir=tmp_dir) as tmp_sub_dir:\n            with open(f\"{tmp_sub_dir}/test2.txt\", \"w\") as f:\n                f.write(\"test2\")\n            with TemporaryDirectory(dir=tmp_sub_dir) as tmp_sub_sub_dir:\n                with open(f\"{tmp_sub_sub_dir}/test3.txt\", \"w\") as f:\n                    f.write(\"test3\")\n                with open(f\"{tmp_sub_sub_dir}/test4.txt\", \"w\") as f:\n                    f.write(\"test4\")\n\n                    reader = SimpleDirectoryReader(tmp_dir, recursive=False)\n                    input_file_names = [f.name for f in reader.input_files]\n                    print(reader.input_files)\n                    assert len(reader.input_files) == 1\n                    assert set(input_file_names) == {\n                        \"test1.txt\",\n                    }\n\n    # test recursive with .md files\n    with TemporaryDirectory() as tmp_dir:\n        with open(f\"{tmp_dir}/test1.md\", \"w\") as f:\n            f.write(\"test1\")\n        with TemporaryDirectory(dir=tmp_dir) as tmp_sub_dir:\n            with open(f\"{tmp_sub_dir}/test2.txt\", \"w\") as f:\n                f.write(\"test2\")\n            with TemporaryDirectory(dir=tmp_sub_dir) as tmp_sub_sub_dir:\n                with open(f\"{tmp_sub_sub_dir}/test3.md\", \"w\") as f:\n                    f.write(\"test3\")\n                with open(f\"{tmp_sub_sub_dir}/test4.txt\", \"w\") as f:\n                    f.write(\"test4\")\n\n                    reader = SimpleDirectoryReader(\n                        tmp_dir, recursive=True, required_exts=[\".md\"]\n                    )\n                    input_file_names = [f.name for f in reader.input_files]\n                    assert len(reader.input_files) == 2\n                    assert set(input_file_names) == {\n                        \"test1.md\",\n                        \"test3.md\",\n                    }\n\n\ndef test_nonrecursive() -> None:\n    \"\"\"Test simple non-recursive directory reader.\"\"\"\n    # test nonrecursive\n    with TemporaryDirectory() as tmp_dir:\n        with open(f\"{tmp_dir}/test1.txt\", \"w\") as f:\n            f.write(\"test1\")\n        with open(f\"{tmp_dir}/test2.txt\", \"w\") as f:\n            f.write(\"test2\")\n        with open(f\"{tmp_dir}/test3.txt\", \"w\") as f:\n            f.write(\"test3\")\n        with open(f\"{tmp_dir}/test4.txt\", \"w\") as f:\n            f.write(\"test4\")\n        with open(f\"{tmp_dir}/.test5.txt\", \"w\") as f:\n            f.write(\"test5\")\n\n        # test exclude hidden\n        reader = SimpleDirectoryReader(tmp_dir, recursive=False)\n        input_file_names = [f.name for f in reader.input_files]\n        assert len(reader.input_files) == 4\n        assert input_file_names == [\"test1.txt\", \"test2.txt\", \"test3.txt\", \"test4.txt\"]\n\n        # test include hidden\n        reader = SimpleDirectoryReader(tmp_dir, recursive=False, exclude_hidden=False)\n        input_file_names = [f.name for f in reader.input_files]\n        assert len(reader.input_files) == 5\n        assert input_file_names == [\n            \".test5.txt\",\n            \"test1.txt\",\n            \"test2.txt\",\n            \"test3.txt\",\n            \"test4.txt\",\n        ]\n\n\ndef test_required_exts() -> None:\n    \"\"\"Test extension filter.\"\"\"\n    # test nonrecursive\n    with TemporaryDirectory() as tmp_dir:\n        with open(f\"{tmp_dir}/test1.txt\", \"w\") as f:\n            f.write(\"test1\")\n        with open(f\"{tmp_dir}/test2.md\", \"w\") as f:\n            f.write(\"test2\")\n        with open(f\"{tmp_dir}/test3.tmp\", \"w\") as f:\n            f.write(\"test3\")\n        with open(f\"{tmp_dir}/test4.json\", \"w\") as f:\n            f.write(\"test4\")\n        with open(f\"{tmp_dir}/test5.json\", \"w\") as f:\n            f.write(\"test5\")\n\n        # test exclude hidden\n        reader = SimpleDirectoryReader(tmp_dir, required_exts=[\".json\"])\n        input_file_names = [f.name for f in reader.input_files]\n        assert len(reader.input_files) == 2\n        assert input_file_names == [\"test4.json\", \"test5.json\"]\n\n\ndef test_num_files_limit() -> None:\n    \"\"\"Test num files limit.\"\"\"\n    # test num_files_limit (with recursion)\n    with TemporaryDirectory() as tmp_dir:\n        with open(f\"{tmp_dir}/test1.txt\", \"w\") as f:\n            f.write(\"test1\")\n        with TemporaryDirectory(dir=tmp_dir) as tmp_sub_dir:\n            with open(f\"{tmp_sub_dir}/test2.txt\", \"w\") as f:\n                f.write(\"test2\")\n            with open(f\"{tmp_sub_dir}/test3.txt\", \"w\") as f:\n                f.write(\"test3\")\n            with TemporaryDirectory(dir=tmp_sub_dir) as tmp_sub_sub_dir:\n                with open(f\"{tmp_sub_sub_dir}/test4.txt\", \"w\") as f:\n                    f.write(\"test4\")\n\n                    reader = SimpleDirectoryReader(\n                        tmp_dir, recursive=True, num_files_limit=2\n                    )\n                    input_file_names = [f.name for f in reader.input_files]\n                    assert len(reader.input_files) == 2\n                    assert set(input_file_names) == {\n                        \"test1.txt\",\n                        \"test2.txt\",\n                    }\n\n                    reader = SimpleDirectoryReader(\n                        tmp_dir, recursive=True, num_files_limit=3\n                    )\n                    input_file_names = [f.name for f in reader.input_files]\n                    assert len(reader.input_files) == 3\n                    assert set(input_file_names) == {\n                        \"test1.txt\",\n                        \"test2.txt\",\n                        \"test3.txt\",\n                    }\n\n                    reader = SimpleDirectoryReader(\n                        tmp_dir, recursive=True, num_files_limit=4\n                    )\n                    input_file_names = [f.name for f in reader.input_files]\n                    assert len(reader.input_files) == 4\n                    assert set(input_file_names) == {\n                        \"test1.txt\",\n                        \"test2.txt\",\n                        \"test3.txt\",\n                        \"test4.txt\",\n                    }\n\n\ndef test_file_metadata() -> None:\n    \"\"\"Test if file metadata is added to Document.\"\"\"\n    # test file_metadata\n    with TemporaryDirectory() as tmp_dir:\n        with open(f\"{tmp_dir}/test1.txt\", \"w\") as f:\n            f.write(\"test1\")\n        with open(f\"{tmp_dir}/test2.txt\", \"w\") as f:\n            f.write(\"test2\")\n        with open(f\"{tmp_dir}/test3.txt\", \"w\") as f:\n            f.write(\"test3\")\n\n        test_author = \"Bruce Wayne\"\n\n        def filename_to_metadata(filename: str) -> Dict[str, Any]:\n            return {\"filename\": filename, \"author\": test_author}\n\n        reader = SimpleDirectoryReader(tmp_dir, file_metadata=filename_to_metadata)\n\n        documents = reader.load_data()\n\n        for d in documents:\n            assert d.extra_info is not None and d.extra_info[\"author\"] == test_author\n\n        # There should be no metadata if we choose to concatenate files\n        documents = reader.load_data(concatenate=True)\n\n        for d in documents:\n            assert d.extra_info is None\n", "doc_id": "808ff010c8376b1ee787cab26b9817eab331054f", "embedding": null, "extra_info": {"file_path": "tests/readers/test_file.py", "file_name": "test_file.py"}, "__type__": "Document"}, "b74ba1a83056318d78436b8155c774bc3a42d4b6": {"text": "\"\"\"Test String Iterable Reader.\"\"\"\n\nfrom gpt_index.readers.string_iterable import StringIterableReader\n\n\ndef test_load() -> None:\n    \"\"\"Test loading data into StringIterableReader.\"\"\"\n    reader = StringIterableReader()\n    documents = reader.load_data(texts=[\"I went to the store\", \"I bought an apple\"])\n    assert len(documents) == 2\n", "doc_id": "b74ba1a83056318d78436b8155c774bc3a42d4b6", "embedding": null, "extra_info": {"file_path": "tests/readers/test_string_iterable.py", "file_name": "test_string_iterable.py"}, "__type__": "Document"}, "5542097578c1b4bdfd1108235f4f7c42f54f7a1c": {"text": "\"\"\"Test docstore.\"\"\"\n\nfrom typing import Dict, Type\n\nfrom gpt_index.data_structs.data_structs import IndexStruct, Node\nfrom gpt_index.docstore import DocumentStore\nfrom gpt_index.readers.schema.base import Document\n\n\ndef test_docstore() -> None:\n    \"\"\"Test docstore.\"\"\"\n    doc = Document(\"hello world\", doc_id=\"d1\", extra_info={\"foo\": \"bar\"})\n    node = Node(\"my node\", doc_id=\"d2\", node_info={\"node\": \"info\"})\n\n    type_to_struct: Dict[str, Type[IndexStruct]] = {\"node\": Node}\n\n    # test get document\n    docstore = DocumentStore.from_documents([doc, node])\n    gd1 = docstore.get_document(\"d1\")\n    assert gd1 == doc\n    gd2 = docstore.get_document(\"d2\")\n    assert gd2 == node\n\n    # test serialize/deserialize\n    doc_dict = docstore.serialize_to_dict()\n    assert doc_dict[\"docs\"] == {\n        \"d1\": {\n            \"text\": \"hello world\",\n            \"doc_id\": \"d1\",\n            \"embedding\": None,\n            \"extra_info\": {\"foo\": \"bar\"},\n            \"__type__\": \"Document\",\n        },\n        \"d2\": {\n            \"text\": \"my node\",\n            \"doc_id\": \"d2\",\n            \"embedding\": None,\n            \"extra_info\": None,\n            \"node_info\": {\"node\": \"info\"},\n            \"index\": 0,\n            \"child_indices\": [],\n            \"ref_doc_id\": None,\n            \"__type__\": \"node\",\n        },\n    }\n\n    docstore_loaded = DocumentStore.load_from_dict(doc_dict, type_to_struct)\n    assert docstore_loaded == docstore\n", "doc_id": "5542097578c1b4bdfd1108235f4f7c42f54f7a1c", "embedding": null, "extra_info": {"file_path": "tests/test_docstore.py", "file_name": "test_docstore.py"}, "__type__": "Document"}, "e53db54622ade21e7df6c38eaab8124ceeac9497": {"text": "\"\"\"Test utils.\"\"\"\n\nfrom typing import Optional, Type, Union\n\nimport pytest\n\nfrom gpt_index.utils import (\n    ErrorToRetry,\n    globals_helper,\n    retry_on_exceptions_with_backoff,\n)\n\n\ndef test_tokenizer() -> None:\n    \"\"\"Make sure tokenizer works.\n\n    NOTE: we use a different tokenizer for python >= 3.9.\n\n    \"\"\"\n    text = \"hello world foo bar\"\n    tokenizer = globals_helper.tokenizer\n    assert len(tokenizer(text)) == 4\n\n\ncall_count = 0\n\n\ndef fn_with_exception(\n    exception_cls: Optional[Union[Type[Exception], Exception]]\n) -> bool:\n    \"\"\"Return true unless exception is specified.\"\"\"\n    global call_count\n    call_count += 1\n    if exception_cls:\n        raise exception_cls\n    return True\n\n\nclass ConditionalException(Exception):\n    \"\"\"Exception that contains retry attribute.\"\"\"\n\n    def __init__(self, should_retry: bool) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        self.should_retry = should_retry\n\n\ndef test_retry_on_exceptions_with_backoff() -> None:\n    \"\"\"Make sure retry function has accurate number of attempts.\"\"\"\n    global call_count\n    assert fn_with_exception(None)\n\n    call_count = 0\n    with pytest.raises(ValueError):\n        fn_with_exception(ValueError)\n    assert call_count == 1\n\n    call_count = 0\n    with pytest.raises(ValueError):\n        retry_on_exceptions_with_backoff(\n            lambda: fn_with_exception(ValueError),\n            [ErrorToRetry(ValueError)],\n            max_tries=3,\n            min_backoff_secs=0.0,\n        )\n    assert call_count == 3\n\n    # different exception will not get retried\n    call_count = 0\n    with pytest.raises(TypeError):\n        retry_on_exceptions_with_backoff(\n            lambda: fn_with_exception(TypeError),\n            [ErrorToRetry(ValueError)],\n            max_tries=3,\n        )\n    assert call_count == 1\n\n\ndef test_retry_on_conditional_exceptions() -> None:\n    \"\"\"Make sure retry function works on conditional exceptions.\"\"\"\n    global call_count\n    call_count = 0\n    with pytest.raises(ConditionalException):\n        retry_on_exceptions_with_backoff(\n            lambda: fn_with_exception(ConditionalException(True)),\n            [ErrorToRetry(ConditionalException, lambda e: e.should_retry)],\n            max_tries=3,\n            min_backoff_secs=0.0,\n        )\n    assert call_count == 3\n\n    call_count = 0\n    with pytest.raises(ConditionalException):\n        retry_on_exceptions_with_backoff(\n            lambda: fn_with_exception(ConditionalException(False)),\n            [ErrorToRetry(ConditionalException, lambda e: e.should_retry)],\n            max_tries=3,\n            min_backoff_secs=0.0,\n        )\n    assert call_count == 1\n", "doc_id": "e53db54622ade21e7df6c38eaab8124ceeac9497", "embedding": null, "extra_info": {"file_path": "tests/test_utils.py", "file_name": "test_utils.py"}, "__type__": "Document"}, "e0e254d6052ab57612b1bac55836ca45a71c201b": {"text": "\"\"\"Test token predictor.\"\"\"\n\nfrom typing import Any\nfrom unittest.mock import patch\n\nfrom gpt_index.indices.keyword_table.base import GPTKeywordTableIndex\nfrom gpt_index.indices.list.base import GPTListIndex\nfrom gpt_index.indices.tree.base import GPTTreeIndex\nfrom gpt_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom gpt_index.readers.schema.base import Document\nfrom gpt_index.token_counter.mock_chain_wrapper import MockLLMPredictor\nfrom tests.mock_utils.mock_text_splitter import mock_token_splitter_newline\n\n\n@patch.object(TokenTextSplitter, \"split_text\", side_effect=mock_token_splitter_newline)\ndef test_token_predictor(mock_split: Any) -> None:\n    \"\"\"Test token predictor.\"\"\"\n    # here, just assert that token predictor runs (before checking behavior)\n    # TODO: mock token counting a bit more carefully\n    doc_text = (\n        \"Hello world.\\n\"\n        \"This is a test.\\n\"\n        \"This is another test.\\n\"\n        \"This is a test v2.\"\n    )\n    document = Document(doc_text)\n    llm_predictor = MockLLMPredictor(max_tokens=256)\n\n    # test tree index\n    index = GPTTreeIndex([document], llm_predictor=llm_predictor)\n    index.query(\"What is?\", llm_predictor=llm_predictor)\n\n    # test keyword table index\n    index_keyword = GPTKeywordTableIndex([document], llm_predictor=llm_predictor)\n    index_keyword.query(\"What is?\", llm_predictor=llm_predictor)\n\n    # test list index\n    index_list = GPTListIndex([document], llm_predictor=llm_predictor)\n    index_list.query(\"What is?\", llm_predictor=llm_predictor)\n", "doc_id": "e0e254d6052ab57612b1bac55836ca45a71c201b", "embedding": null, "extra_info": {"file_path": "tests/token_predictor/test_base.py", "file_name": "test_base.py"}, "__type__": "Document"}, "cce2181c-28ee-42a7-b5a2-b9bb94115ee5": {"text": null, "doc_id": "cce2181c-28ee-42a7-b5a2-b9bb94115ee5", "embedding": null, "extra_info": null, "class_prefix": "Gpt_Index_3460389699009219481", "__type__": "weaviate"}}}}